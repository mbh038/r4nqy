# Correlation

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

Part of this guide to correlation draws heavily on the very helpful chapter in Statistics, by David Freedman, Robert Pisani, Roger Purves and Ani Adhikari, 2nd ed., Norton.

For this chapter you will find it useful to have [this RStudio project folder](https://github.com/mbh038/correlation/archive/refs/heads/main.zip) if you wish to follow along and try out the exercises at the end.


```{r global-options, include=FALSE,echo=FALSE}
knitr::opts_chunk$set(fig.width=9,
                      fig.height=5,
                      fig.align='center',
                      warning=FALSE,
                      message=FALSE,
                      echo=FALSE)
```

```{r load packages, message=FALSE,warning=FALSE,include=FALSE}
library(tidyverse)
library(here)
library(readxl)
library(kableExtra)
library(cowplot)
library(gridExtra)


library(UsingR) # for Pearson's father-son data
data(PearsonLee) # for Pearson's mother-daughter data
data(iris)


#set.seed=232342
```

```{r,include=FALSE}
# we use the fabulous openintro here only for its colour scheme
library(openintro)
data(COL)
fill_colour1<-COL['blue','f3']
fill_colour2<-COL['red','f6']
fill_colour3<-COL['blue','full']
line_colour<-COL['blue','full']
func_colour<-"darkblue"
point_colour<-COL['blue','full']
error_colour<-COL['red','full']
```

```{r}
# bespoke themes to set axis styles. Minl
theme_no_x <- function () { 
    theme_cowplot() %+replace% 
  theme(
    axis.ticks.x=element_blank(),
    axis.text.x=element_blank(),
    axis.title.x=element_blank(),
    axis.line.x=element_blank()
        )
}

theme_no_y <- function () { 
    theme_cowplot() %+replace% 
  theme(
    axis.ticks.y=element_blank(),
    axis.text.y=element_blank(),
    axis.title.y=element_blank(),
    axis.line.y=element_blank()
        )
}

theme_no_axes <- function () { 
    theme_cowplot() %+replace% 
  theme(
    axis.ticks=element_blank(),
    axis.text=element_blank(),
    axis.title=element_blank(),
    axis.line=element_blank()
        )
}

theme_bare_axes <- function () { 
    theme_cowplot() %+replace% 
  theme(
    axis.ticks=element_blank(),
    axis.text=element_blank(),
    axis.title=element_blank(),
    # axis.line=element_blank()
        )
}
```


The notable statistician Karl Pearson (1857 - 1936) carried out a study to investigate the resemblance between children and their parents. As part of the study, Pearson measured the heights of 1078 parents and of their children at maturity. The heights of the children are plotted against the heights of the parents in the plots below, where we distinguish between father-son and mother-daughter pairs.

```{r,out.width="100%"}

# father.son |>
# ggplot(aes(x=fheight,y=sheight)) +
PearsonLee |>
  filter(gp %in% c("fs","md")) |>
  group_by(gp) |>
  mutate(ave_parent=mean(parent)) |>
  ungroup() |>
  ggplot(aes(x=parent,y=child)) +
  geom_abline(slope=1,linetype="dashed",colour="gray60") +
  geom_point(alpha=0.7,colour=COL['blue','full']) +
  xlim(58,80) +
  ylim(58,80) + 
  labs(x=" Parent height (inches) ",
       y= "Child height (inches)") +
  geom_vline(aes(xintercept=ave_parent-0.5),linetype="dotted") +
  geom_vline(aes(xintercept=ave_parent+0.5),linetype="dotted") +
  facet_wrap(~gp,
             labeller = labeller(gp = 
    c("fs" = "Father - Son",
      "md" = "Mother - Daughter"))) +
  theme_cowplot() +
  theme(strip.background=element_blank())
```

The taller a father, the taller his sons tend to be. It is the same with mothers and daughters.

There is a positive association between a father's height and the height of his sons.

But there is a lot variation - the association is weak.

If you know the height of a father, how much does that tell you about the height of his sons?

Consider fathers who are about about 67 inches tall, and look at the wide variation in the heights of their sons - - all the points between the two vertical dotted lines. The same is true for the daughters of mother who are about 63 inches tall.

If there is a strong association between two variables, then knowing one helps a lot in predicting the other. But when there is a weak association, information about one variable does not help much in guessing the other. When there is no association, it does not help at all.

## The correlation coefficient

Suppose we are looking at the relationship between two variables and have already plotted the scatter plot. The graph looks like a cloud of points.

How can we summarise it numerically?

The first thing we can do is to mark a point that shows the average of the *x*-values and the average of the *y*-values. This is the *point of averages*. It marks the centre of the cloud.

The next step is to measure the width of the cloud from side to side, in both the *x* and the *y* directions. This can be done using the standard deviations (SD) of the *x* and *y* values. Remember that if both *x* and *y* are normally distributed, then 95% of the data will lie within about 2 (1.96 if we want to be pernickety) standard deviations of the mean, in each direction.

```{r}
N <- 200# Number of random samples
#set.seed(123)
# Target parameters for univariate normal distributions
rho <- 1
mu1 <- 5; s1 <- 2
mu2 <- 5; s2 <- 1
```

```{r}
library(ggplot2)
# library(PlaneGeometry)
library(DescTools)


# define an ellipse
# ell <- Ellipse$new(center = means, rmajor = 2, rminor = 1, alpha = 45)
# 
# ell <- DrawEllipse(x = means[1], y = means[2], radius.x = 2, radius.y = 1, rot = 45,
#             nv = 100, border = par("fg"), col = par("bg"),
#             lty = par("lty"), lwd = par("lwd"), plot = FALSE)

big_faithful <- faithful |> filter(eruptions > 3)

mean_waiting <- mean(big_faithful$waiting)
mean_eruptions <- mean(big_faithful$eruptions)

means<-c(mean_waiting,mean_eruptions)

max_waiting <- max(big_faithful$waiting)
max_eruptions <- max(big_faithful$eruptions)

sd_waiting<-sd(big_faithful$waiting)
sd_eruptions <- sd(big_faithful$eruptions)

xmin<-mean_waiting-5*sd_waiting
xmax<-mean_waiting+5*sd_waiting

ymin<-mean_eruptions-5*sd_eruptions
ymax<-mean_eruptions+5*sd_eruptions

# ellipse as a path
# ellpath <- ell$path()
# the path is not closed; close it
# ellpath <- rbind(ellpath, ellpath[1,])

ellipse_plot<- big_faithful |>
  ggplot(aes(x = waiting, y = eruptions)) +
  geom_point(colour="white") +
  stat_ellipse(colour="red") +
  # geom_path(aes(x = x, y = y), as.data.frame(ellpath), color = "red") +
  geom_segment(aes(x=xmin,y=ymin,xend=xmax,yend=ymin),colour="gray40")+
  geom_segment(aes(x=xmin,y=ymin,xend=xmin,yend=ymax),colour="gray40")+
  xlim(xmin-2,xmax) +
  ylim(ymin,ymax) +
  theme_classic() +
  theme_no_axes()
# ellipse_plot

annotation_size<-3
```



```{r}
p1<-ellipse_plot +
  geom_segment(aes(x=means[1],y=ymin,xend=means[1],yend=means[2]),linetype='dashed',colour="gray60") +
  annotate("text",x=means[1],y=ymin,vjust=1.8,label="Mean of x",size=annotation_size)+
  annotate("text",x=xmin-1.7,y=means[2],hjust=0.5,label="Mean\nof y",size=annotation_size)+
  geom_segment(aes(x=xmin,y=means[2],xend=means[1],yend=means[2]),linetype='dashed',colour="gray60") +
  labs(title="The point of averages")
# p1
```

```{r}
p2<-ellipse_plot +
  annotate("text",x=means[1]-sd_waiting,y=ymin+0.3,label="2 SD x",size=annotation_size,vjust=-0.5)+
  annotate("text",x=means[1]+sd_waiting,y=ymin+0.3,label="2 SDx",size=annotation_size,vjust=-0.5)+
  geom_segment(aes(x=means[1]-2*sd_waiting,y=ymin,xend=means[1]-2*sd_waiting,yend=ymax),linetype='dashed',colour="gray60") +
  geom_segment(aes(x=means[1],y=ymin,xend=means[1],yend=ymax),linetype='dashed',colour="gray60") +
  geom_segment(aes(x=means[1]+2*sd_waiting,y=ymin,xend=means[1]+2*sd_waiting,yend=ymax),linetype='dashed',colour="gray60") +
  
  geom_segment(aes(x=means[1]-2*sd_waiting,y=ymin+0.2,xend=means[1],yend=ymin+0.2),colour="gray60",
                 arrow=arrow(length = unit(2, "mm"),ends = "both", type = "open")) +
  geom_segment(aes(x=means[1],y=ymin+0.2,xend=means[1]+2*sd_waiting,yend=ymin+0.2),colour="gray60",
                 arrow=arrow(length = unit(2, "mm"),ends = "both", type = "open")) +
  labs(title="The horizontal SD")
```

```{r}
p3<-ellipse_plot +
  annotate("text",x=xmin,y=means[2]-sd_eruptions,label="2 SDy",size=annotation_size,hjust=-0.4)+
  annotate("text",x=xmin,y=means[2]+sd_eruptions,label="2 SDy",size=annotation_size,hjust=-0.4)+
  geom_segment(aes(x=xmin,y=means[2]-2*sd_eruptions,xend=xmax,yend=means[2]-2*sd_eruptions),linetype='dashed',colour="gray60") +
  geom_segment(aes(x=xmin,y=means[2],xend=xmax,yend=means[2]),linetype='dashed',colour="gray60") +
  geom_segment(aes(x=xmin,y=means[2]+2*sd_eruptions,xend=xmax,yend=means[2]+2*sd_eruptions),linetype='dashed',colour="gray60") +
  
  geom_segment(aes(x=xmin+1.7,y=means[2]-2*sd_eruptions,xend=xmin+1.7,yend=means[2]),colour="gray60",
                 arrow=arrow(length = unit(2, "mm"),ends = "both", type = "open")) +
  geom_segment(aes(x=xmin+1.7,y=means[2],xend=xmin+1.7,yend=means[2]+2*sd_eruptions),colour="gray60",
                 arrow=arrow(length = unit(2, "mm"),ends = "both", type = "open")) +
  labs(title="The vertical SD")
# p3
```

```{r,fig.asp = 0.33}
grid.arrange(p1,p2,p3,nrow=1)
```

So far, our summary statistics are:

-   mean of the *x* values, SD of the *x* values.
-   mean of the *y* values, SD of the *y* values.

These statistics tell us where the centre of the cloud is and how far spread out it is both vertically and horizontally, but they do not tell the whole story.

Consider the following two sets of data plotted below. Both have the same centre and the same spread.

```{r,fig.asp=1}
N<-100
mu<-10
x<-rnorm(N,mean=mu,sd=10)
noise<-rnorm(N,mean=0,sd=0.7)
y<-x+noise

p1<-tibble(x=x,y=y) |>
  ggplot(aes(x=x,y=y)) +
  geom_point(alpha=0.7,size=2,colour=COL['blue','full']) +
  xlim(0,10) +
  ylim(0,10) +
  # geom_segment(x=1.5,y=8,xend=3,yend=6.5,arrow=arrow(length = unit(2, "mm"),ends = "last", type = "open")) +
  # geom_segment(x=8.5,y=2,xend=7,yend=3.5,arrow=arrow(length = unit(2, "mm"),ends = "last", type = "open")) +
  theme_cowplot() +
  theme(
    axis.ticks=element_blank(),
    axis.text=element_blank(),
    axis.title=element_blank(),
    #axis.line=element_blank()
        )

x<-rnorm(N,mean=mu,sd=10)
noise<-rnorm(N,mean=0,sd=2.7)
y<-x+noise
p2<-tibble(x=x,y=y) |>
  ggplot(aes(x=x,y=y)) +
  geom_point(alpha=0.7,size=2,colour=COL['blue','full']) +
  xlim(0,10) +
  ylim(0,10) +
  # geom_segment(x=1.5,y=8,xend=3,yend=6.5,arrow=arrow(length = unit(2, "mm"),ends = "last", type = "open")) +
  # geom_segment(x=8.5,y=2,xend=7,yend=3.5,arrow=arrow(length = unit(2, "mm"),ends = "last", type = "open")) +
  theme_cowplot() +
  theme(
    axis.ticks=element_blank(),
    axis.text=element_blank(),
    axis.title=element_blank(),
    #axis.line=element_blank()
        )
```

```{r, fig.asp=0.5,out.width="70%"}
grid.arrange(p1,p2,nrow=1)
```

However the points in the first cloud are tightly clustered around a line - there is a strong linear association between the two variables. In the second cloud, the clustering is much looser. The strength of the association is different in the two diagrams. To measure the association, one more summary statistic is needed - the *correlation coefficient*.

This coefficient is usually abbreviated as *r*, for no good reason.

------------------------------------------------------------------------

The correlation coefficient is a measure of linear association or clustering around a line. The relationship between two variables can be summarized by:

-   the average of the *x*-values, the SD of the *x*-values.
-   the average of the *y*-values, the SD of the *y*-values.
-   the correlation coefficient *r*

------------------------------------------------------------------------

## Different values of *r*.

Let us see how this looks graphically. In the Figure below we show six scatter plots for hypothetical data. In all six pictures the average is 3 and the standard deviation is 1 for both *x* and *y*. The correlation coefficient is printed in each case.

```{r}

ycalc <- function(r,x,a,b,sigma_y) {
   #N(a + b*x, (1-r2)*sY2)
  #if (missing(x)) x <- rnorm(length(y)) # Optional: supply a default if `x` is not given
  if(r!=0){
    y.out<-sign(r)*x+rnorm(length(x),sd=sigma_y)
  }else
  {
    y.out<-x+rnorm(length(x),sd=sigma_y)
  }
  #print(y.out)
  yfun<-function(x,r,a,b,sigma_y){rnorm(length(x),mean=(a+b*x),sd=sqrt(1-r^2)*sigma_y)}
  count=0
  while(abs(cor(y.out,x)-r)>0.005){
    count=count+1
    #sigma_y=sigma_y*0.99
    y.out<-sapply(x,yfun,r=r,a=a,b=b,sigma_y=sigma_y)
    #print(paste(r,cor(y.out,x)))
    
    if (abs(cor(y.out,x))-abs(r)>0){
      sigma_y=sigma_y+0.001
    }
    if (abs(cor(y.out,x))-abs(r)<0){
      sigma_y=sigma_y-0.001
    }
    if(count>10000){
      print(paste(r,count))
      break
    }
  }
  #y.out<-y.out+mean(x)-mean(y.out)
  y.out
}
```

```{r}
N<-50
mu=3
sigma<-1
rho<-c(0,0.4,0.6,0.8,0.9,0.95)
xbase<-seq(1,5,length.out=N)
y<-sapply(rho,ycalc,x=xbase,a=0,b=1,sigma_y=sigma)
y<-as.vector(y)
cor_data<-tibble(x=rep(xbase,length(rho)),
                 y=y,
                 r=rep(rho,1,each=N))

#cor_data<-neg_cor_data
actual_rhos<-cor_data |>
  group_by(r) |>
  summarise(actual_r=round(cor(x,y),2))
labels<-sapply(actual_rhos$actual_r,function(x){paste("Cor. coef.","=",x)})


cor_data$titles<-factor(cor_data$r,label=labels)

f_labels<-tibble(r=rho,label=labels)
ggplot(cor_data,aes(x=x,y=y)) +
  geom_point(colour=COL['blue','f1']) +
  scale_x_continuous(limits=c(0,6),breaks=seq(0,6))+
  scale_y_continuous(limits=c(0,7),breaks=seq(0,6))+
  geom_text(x = 3, y = 7, aes(label = label), data = f_labels) +
  facet_wrap(~r,label=label_value) +
  theme_cowplot() +
  theme(axis.title=element_blank(),
        strip.text=element_blank()) +
  theme(strip.background = element_blank())
```

The one top left shows a correlation of 0 and the cloud is completely formless. As *x* increases, *y* shows no tendency to increase or decrease. It just straggles around.

The next diagram has *r* = 0.4 and a linear pattern is just starting to emerge. The next has *r* = 0.6 with a stronger linear pattern, and so on. The closer *r* is to 1 the stronger is the linear association and the more tightly clustered are the points around a line.

A correlation of 1, which does not appear in the Figure is often referred to as a *perfect correlation*. It means that all the points lie exactly on a line so there is a perfect linear correlation between the two variables. Correlation coefficients are always between -1 and 1.

The correlation between the heights of identical twins is around 0.95. A scatter diagram for the heights of twins would thus look like the bottom right diagram in the Figure. We see that even with a coefficient this big there is a still a fair degree of scatter. The heights of identical twins are far from being equal all the time.

Real data in the life sciences never shows perfect correlation and rarely does it even show strong correlation. It is more common for it to look like Pearson's father-son data, with weak associations and *r* values in the range 0.3 to 0.7. This is even more true for data from the social sciences which concern human behaviour.

We can also have negative associations between variables. For example women with more education tend to have fewer children. Animals with higher body weight tend to have lower metabolic rates. As one variable increases, the other decreases. When there is negative association, the correlation coefficient has a negative sign.

Below we show six examples of negative correlation. As in the previous figure, all the data sets have a mean of 3 and a standard deviation of 1.

```{r}
N<-50


neg_cor_data<-tibble(r=rep(rho,1,each=N)) |>
  add_column(x=cor_data$x)

y_rev<-function(cor_data,rho){
  ys<-cor_data |>
  filter(r==rho) |>
  pull(y)
  rev(ys)
}

ys<-vector()
for (r in rho){
  newy<-y_rev(cor_data,r)
  ys<-cbind(ys,newy)
}
ys<-as.vector(ys)

neg_cor_data<-neg_cor_data |>
  add_column(y=ys)
```

```{r}
N<-50
mu=3
sigma<-1
rho<-c(-0.3,-0.5,-0.7,-0.9,-0.95,-0.99)
xbase<-seq(1,5,length.out=N)
y<-sapply(rho,ycalc,x=xbase,a=6,b=-1,sigma_y=sigma)
y<-as.vector(y)
neg_cor_data<-tibble(x=rep(xbase,length(rho)),
                 y=y,
                 r=rep(rho,1,each=N))
    
```

```{r}
actual_rhos<-neg_cor_data |>
  group_by(r) |>
  summarise(actual_r=round(cor(x,y),2)) |>
  arrange(-actual_r)
labels<-sapply(actual_rhos$actual_r,function(x){paste("Cor. coef.","=",x)})

# neg_cor_data<-neg_cor_data |>
#   arrange(desc(r))

neg_cor_data$titles<-factor(neg_cor_data$r,label=labels)



f_labels<-tibble(r=rho,label=labels) |>arrange(desc(r))
neg_cor_data |>
  mutate(r=as.factor(r)) |>
  mutate(r = fct_relevel(r, "-0.3", "-0.5", "-0.7","-0.9","-0.95","-0.99")) |>
  ggplot(aes(x=x,y=y)) +
  geom_point(colour=COL['blue','f1']) +
  scale_x_continuous(limits=c(0,6),breaks=seq(0,6))+
  scale_y_continuous(limits=c(0,7),breaks=seq(0,6))+
  geom_text(x = 3, y = 7, aes(label = label), data = f_labels) +
  facet_wrap(~r,label=label_value) +
  theme_cowplot() +
  theme(axis.title=element_blank(),
        strip.text=element_blank()) +
  theme(strip.background = element_blank())
```

------------------------------------------------------------------------

Correlations are always between -1 and 1, but can take any value in between. A positive correlation means that the cloud slopes up: as one variable increases, so does the other. A negative correlation means that the cloud slopes down. As one variable increases, the other decreases.

------------------------------------------------------------------------

## Using R to find the correlation coefficient.

First, let us try to find the correlation between two sets of data where we know what the correlation coefficient is, because we created the data ourselves. We will take the *x* and *y* data used above for which the correlation coefficient was fixed to be 0.8

```{r,echo=FALSE}
df<-cor_data |>
  filter (r==0.8)

x<-df$x
y<-df$y
```

```{r,echo=TRUE}
cor.test(x,y,method="pearson")
```

There are different ways to calculate the correlation coefficient. Which of them is appropriate depends mainly on the type of data, and if they are numeric, whether they are normally distributed. If they are, then we use the Pearson method. If they are not, for example because they are ordinal data, then we use the Spearman's Rank method and write *method="spearman"* instead. In this case we can relax the requirement that there is a linear association between the data sets, but there does still need to be a monotonic relationship. 

It is important to be able to interpret and report the output.

First, understand that R is carrying out a test, the null hypothesis of which is that there is no correlation between the values of *x* and the values of *y* among the populations from which the *x* and *y* data were drawn, so that the correlation coefficient between *x* and *y* within those populations is zero. It then reports a *p*-value: how likely it is that you would have got the data you got for this sample of data if that null hypothesis were true. As with most tests, to do this it uses the data to calculate a so-called *test-statistic*. How it does this need not concern us here. The details will differ from test to test, and the name given will differ. Here it is called *t*. It also reports the number of independent pieces of information used to calculate that statistic. This is called the *degrees of freedom*, here denoted *df*. This usually (but not necessarily) has a value that is 1,2 or 3 less than the number of data points.

Then it reports the *p*-value. A tiny (close to zero) value here means that it thinks it very unlikely that the samples would be as they are if the *x* and *y* variables were not correlated in the populations from which the samples were drawn. A high (by which we usually mean greater than 0.05) value means that there is a reasonable chance that the actual non-zero correlation coefficient could have been found between *x* and *y* in the samples, even though those values were not correlated in the wider populations from which the samples were drawn. In that case we would have found no evidence that the *x* and *y* data within the population were correlated. This doesn't mean that they aren't, just that we have insufficient evidence to reject the null hypothesis that they are not.

The *p*-value reported here is 4.3e-12. That is R's way of saying what in standard form would be written 4.3 x 10^-12^. This is a really tiny value. It is 0.0000000000043, which is a very inconvenient way to write such a small number. Hence R's way of doing it or the standard form way of doing it. In the context of a statistical test and when *p* is is that small we don't care about its exact value, we simply note that it is very, very small. We thus can confidently reject the null hypothesis and assert that the data provide evidence that *x* and *y* are correlated, in this case positively.

Further, it reports the actual correlation coefficient. Here it finds *r* = 0.797, which we happen to know to be correct because we created this data set ourselves, and a 95% confidence interval for the coefficient. The precise meaning of the confidence interval is subtle, but it is a kind of error bar for the correlation coefficient *r*. It means that if we drew sample after sample from the population and calculated the confidence interval for *r* for each sample, then 95% of the time that interval would capture the true value of *r*. Thus, you can reasonably think of the confidence interval as being the range of values within which the true population correlation coefficient plausibly lies, given the value that was found for the sample.

If the *p*-value is small enough that we reject the null-hypothesis, then this confidence interval should not encompass zero. Why? Because any value inside the confidence interval is a plausible value for the population correlation coefficient andif we are going to reject the null hypothesis, then zero should *not* be a plausible value for the population correlation coefficient, given the data. 

If the *p*-value is large enough that we do not reject the null hypothesis then this confidence interval *will* encompass zero. Why? Becuase if the confidence interval encompasses zero, then zero *is* a plausible value for the correlation coefficient and so we should not reject the null.

Here, the confidence interval is from 0.67 to 0.88. This does not encompass zero. In fact it is far from zero, so is consistent with our finding a really small *p*-value. On both groundss, we reject the null.

To report the result of this test we would say something like:

> We find evidence for a strong positive correlation between *x* and *y* (Pearson *r* =0.80, *t*=9.1, *df*=48, *p*\<0.001)

Note that when the *p*-value is much less than 0.05 as it is here we do not normally report its exact value, but simply write *p*\<0.01, or *p*\<0.001, and so on. The point is that these ways of reporting it tell the reader that *p* is *way* less than 0.05. This is all they need to know to see that we can confidently reject the null hypothesis.

## Correlations for real data {#sec-iris-correlation}

Let us look at the Iris data set that is built into R. It contains values for the Sepal Width, Sepal Length, Petal Width and Petal Length for samples of 50 plants from each of three species of Iris, *setosa*, *versicolor* and *virginica*. Here are the first few rows:

```{r}
head(iris)
```


We will look to see if the data allow us to reject the idea that sepal width and sepal length are not correlated within the wider populations of each of these species:

First, let's plot the data

```{r,echo=TRUE}
iris |>
  ggplot(aes(x=Sepal.Width,y=Sepal.Length,colour=Species)) +
  geom_point() +
  labs(x="Sepal Width (mm)",
       y="Sepal Length (mm)") +
  facet_wrap(~Species,nrow=1,scales="free") +
  theme_classic() +
  theme(legend.position="none") +
  theme(strip.background=element_blank())
```

### What we can tell from plotting the data

Having seen the plots, do you think it plausible that there is a linear relationship between sepal width and length? Only in this case can you use a Pearson correlation test (see below). If not linear, do you think there is at least a monotonic relationship between petal width and length (ie no peaks or troughs)? That, at least, would enable you to use a Spearman's Rank correlation test. If neither are true, then you can't use either test.

### Test for normality

It does look as though there is a plausibly linear relationship between sepal width and length, so we might be able to use the Pearson method for calculating correlation coefficient. This is the 'parametric' method that is more powerful than the 'non-parametric' alternative, the Spearman's rank method. 

In principle, however, this method requires that each group of the data be approximately normally distributed around its repective mean (that is what the word parametric is getting at), so we ought to test for this. We can do this either graphically or using a normality test such as the Shapiro wilk test. Let us do the latter here:

```{r}
iris |>
  group_by(Species) |>
  summarise(Sepal.Length_p.val = shapiro.test(Sepal.Length)$p.value,Sepal.Width_p.val = shapiro.test(Sepal.Width)$p.value)
```


All the p-values for this test are comfortably greater than 0.05 so we can reasonably presume that our data are drawn from normally distributed populations. This, plus the plausibly linear realtionships we have seen in the graphs means that can go ahead and use the Pearson method to calculate the correlation coefficient between sepal length and width for each species!

### Calculate the correlation coefficients

Looking at each graph, it appears that there is a positive correlation for each species, but that this is weaker for *versicolar* and *virginica* than it is for *setosa*. Knowing the sepal width for that species gives you a much better idea of the sepal length, and vice-versa, than is true for the other two species.

Let us find out:

```{r,echo=TRUE}
iris |>
  group_by(Species) |>
  summarise(r=cor.test(Sepal.Width,Sepal.Length)$estimate,
            lower.bound95=cor.test(Sepal.Width,Sepal.Length)$conf.int[1],
            upper.bound95=cor.test(Sepal.Width,Sepal.Length)$conf.int[2],
            "p value"=cor.test(Sepal.Width,Sepal.Length)$p.value) |>
  kbl(digits=3) |>
  kable_styling(full_width=0.7)
  
```

The table gives the estimated value for the Pearson correlation coefficient in each case, the lower and upper bound of the 95% confidence interval for that coefficient and the *p*-value.

Do these output provide evidence for a correlation between sepal length and sepal width in each case?

## The problem of missing confounding variables

Suppose in the above analysis we had not distinguished between the three species. If we had plotted the speal length and width data we would have seen this:

```{r,echo=TRUE}
iris |>
  ggplot(aes(x = Sepal.Width, y = Sepal.Length)) +
  geom_point() +
  labs(x="Sepal Width (mm)",
       y="Sepal Length (mm)") +
  theme_classic() +
  theme(legend.position = "none")
```

This looks like a weak *negative* correlation, as is confirmed by calculating the Spearman's (in this case) rank correlation coefficient. 

If we had in fact accounted for species and plotted the data sepatately for each species, or together but distinguishing each species by colour we would have seen the falsity of that conclusion:

```{r,echo=TRUE}
iris |>
  ggplot(aes(x = Sepal.Width, y = Sepal.Length, colour = Species)) +
  geom_point() +
  labs(x="Sepal Width (mm)",
       y="Sepal Length (mm)",
       colour = "Species") +
  theme_classic()
```

The message here is that failure to spot importnat 'missing' variables, in the case species, can lead to grossly misleading ideas as to whether two variables are correlated, and if so, how.




## The correlation coefficient measures the degree of *linear* association.

### Pearson correlation coefficient

Sometimes the Pearson correlation coefficient *r* is a poor measure of the degree of association within a data set. Outliers and non-linearity are two problem cases.

Consider first a data set where there is a very strong association between variables, but where the data sset contains an outlier, and then a data set where there is a strong but non-linear association between variables. Here we mean by 'strong' that knowing the value of one variable gives you a very good idea of the value of the other.

```{r}
x1<-c(1,2,3,4,6)
y1<-c(1,2,3,4,1)

p1<-ggplot(tibble(x=x1,y=y1),aes(x=x,y=y)) +
  geom_point(colour=COL["blue","f1"]) +
  xlim(0,6)+
  ylim(0,6)+
  labs(title="Outliers") +
  theme_cowplot() +
  theme(axis.title=element_blank())

x2<-sample(seq(0,35,0.1),35)
y2<- -(x2-18)^2 + 20*rnorm(35)



p2<-ggplot(tibble(x=x2,y=y2),aes(x=x,y=y)) +
  geom_point(colour=COL["blue","f1"]) +
  labs(title="Nonlinear association") +
  scale_x_continuous(limits=c(0,35),breaks=seq(0,35,5),labels=rep(" ",8)) +
  scale_y_continuous(limits=c(-350,0),breaks=seq(-350,0,50),labels=rep(" ",8)) +
  theme_cowplot() +
  theme(
    axis.ticks=element_blank(),
    axis.title=element_blank())
  

grid.arrange(p1,p2,nrow=1)
```

The outlier in the left-hand figure above brings the correlation coefficient down to `r round(cor.test(x1,y1)$estimate,3)`, which is close to zero. The correlation coefficient in the right-hand figure is similarly small at `r round(cor.test(x2,y2)$estimate,3)`, despite that there is a strong association between the *x* and *y* data. The reason is that the association is non-linear.

### Spearman's rank correlation coefficient *r*~Sp~

The Spearman's rank correlation coefficient is a valid measure of the association between two variables providing their relationship is *monotonic* - that is, continuously rising or flat, or continuously falling or flat. Linear relationships are monotonic, so the Spearman's rank correlation coefficient is always a useful (if not necessarily the most powerful - Pearson would trump it if it could be used) measure of association for such cases, but it will still be valid when the relationship is monotonic but non-linear, whereas the Pearson correlation coefficient would *not* be.\

Spearman's rank correlation coefficient *r*~Sp~ can also be be used for ordinal data, whereas Pearson's *r* coefficient cannot be. This makes it very useful in much of ecology, animal behaviour and environmental studies where ordinal scales are commonly used. 

## When is it appropriate to calculate a correlation coefficient?
So, to sum up, we note that the Pearson correlation coefficient is a measure of *linear* association, not of association in general. At least, this is true if you are calculating the Pearson correlation coefficient. If your data are not suitable for that and you decide to calculate the Spearman's Rank correlation coefficient, then the condition is relaxed somewhat: there might be but there no longer *needs* to be a linear relationship between the two variables, but there must be a *monotonic* one. That means that, as one variable increases, the other should either increase or remain constant, or decrease or remain constant - that is, there should be no peaks or troughs in the data.



## Association is not causation

A very important and often-repeated point to note is that correlation measures association. But association is not the same as causation. 

See [Spurious Correlations](https://www.tylervigen.com/spurious-correlations) for some amusing examples.

## Examples

### Cyclones

Cyclones are areas of low atmospheric pressure around which steep gradients in air pressure can cause strong winds to develop, which in turn may create large waves if the cylone is over the oceans. Depending on where they occur, these storms are variously also known as hurricanes and typhoons. Here we consider whether the peak wind speeds are associated with the depth of the low pressure at the eye of the storm, and whether the peak wave sizes are associated with how wide the storm is. If the answer to either of these questions is yes, then an outcome of practical importance - wind speed, wave height - can be predicted at least in part by a variable that can be measured easily - pressure, distance.

```{r, include = FALSE}
cyclones_filepath <- here("data","cyclones.xlsx")
```


```{r, include = FALSE}
pressure_wind_speed <- read_excel(cyclones_filepath,"pressure_wind_speed")
```


#### South West Indian Ocean intense tropical cyclones 1973 - 2024

Pressure gradients cause winds so it is reasoable to ask whether there a correlation between the peak wind speed and minimum pressure at the eye of cyclones. Here we look at data for for intense tropical cyclones in the south west Indian Ocean over the period 1973-2024. The data are taken (scraped using the R package `rvest`!) from:
https://en.wikipedia.org/wiki/List_of_South-West_Indian_Ocean_intense_tropical_cyclones . The original data sources are available on that site.



```{r, echo = FALSE}
pressure_wind_speed |>
  ggplot(aes(x = pressure, y = wind_speed)) +
  # geom_point(alpha=0.5) +
  geom_jitter(width = 2, height = 2, alpha = 0.5) +
  scale_y_continuous(limits = c(150,230)) +
  labs(x = "Pressure (hPa)",
       y = "Wind speed (kph)") +
  geom_text(aes(label=ifelse(pressure<910,str_c(name,"\n", year),'')),hjust=0,vjust=1, colour = "blue", size = 3) +
  annotate("text", x = 960, y = 220, label = deparse(bquote(italic(r)[sp]==-0.218~","~italic(p)==0.0289)), parse = TRUE)+
  # annotate("text",label=paste0("mu+","sigma"),x=960,y = 220,size = 4, colour = "darkred",parse=TRUE)+
  theme_cowplot()
```


In this figure we see that there is a weak but significant *negative* correlation between the peak recorded wind speed and the minimum recorded pressure at the centre of intense tropical cyclones that occurred in the south west Indian Ocean between 1973 and 2024. Neither wind speed nor pressure were normally distributed, so the Spearman's rank correlation coefficient *r*~Sp~ has been calculated, and not the Pearson *r* coefficient.

#### Significant wave height vs size of cyclone

Here we look at results displayed in Figure 3f of\
\
Oh, Y. et al. (2023) ‘Optimal tropical cyclone size parameter for determining storm-induced maximum significant wave height’, Frontiers in Marine Science, 10, p. 1134579. Available at: https://doi.org/10.3389/fmars.2023.1134579.

The authors seek to determine whether there is an association between the size of a cylone, measured by the 'R50' distance measured outward from the storm centre to where the wind speeds have subsided to 50 kph, and the maximum 'significant wave height' of the swell created by the storm. Significant wave height is a widely used measure that is the average height of the heighest 1/3  of the waves, these being the ones that impact most on practical matters like the fuel consumption of ships, the erosion of shores, and so on.

```{r, include = FALSE}
swh_vs_cyclone_size <- read_excel(cyclones_filepath,"cyclone_size_swh")
glimpse(swh_vs_cyclone_size)
```


```{r, include = FALSE}
shapiro.test(swh_vs_cyclone_size$R50)$p.value # <0.05
shapiro.test(swh_vs_cyclone_size$HS_max)$p.value # <0.05
cor.test(swh_vs_cyclone_size$R50,swh_vs_cyclone_size$HS_max,method="pearson") # not valid, not linear anyway
cor.test(swh_vs_cyclone_size$R50,swh_vs_cyclone_size$HS_max,method="spearman") # valid, monotonic
```
```{r}
swh_vs_cyclone_size |>
  ggplot(aes(x = R50, y = HS_max)) +
  geom_point(size = 3, alpha = 0.5) +
  labs(x = "Radius to 50 kph wind speed (km)",
       y = "Max. Significant wave height (m)") +
  scale_x_continuous(limits = c(40,260), breaks=seq(50,250,50)) +
  scale_y_continuous(limits = c(11,22), breaks=seq(12,22,2)) +
  annotate("text", x = 220, y = 14, label = deparse(bquote(italic(r)[sp]==0.948~","~italic(p)<0.001)), parse = TRUE) +
  theme_cowplot()
```

The plot shows that there is a monotonically rising relationship between the R50 radius of the cyclones included in the study and the maximum significant wave height of swell created by them. The relationship is not linear however, so the only appropriate measure of correlation coefficient is the Spearman's rank, which gives $r_{sp} = 0.948, p<0.001$, indicating a very strong positive association between the size of a cyclone and the height of the waves it creates.\

Why do you think this relationship flattens off for larger storm sizes?\

### Lichen abundance

::: csl-entry
Jovan, S. (2008). <i>Lichen Bioindication of Biodiversity, Air Quality, and Climate: Baseline Results From Monitoring in Washington, Oregon, and California</i>. <http://gis.nacse.org/lichenair/doc/Jovan2008.pdf>
:::

In this paper the authors investigate the utility of using lichen as bioindicators of air quality. is there an association between air quality and the abundance of this or that species of lichen? 

```{r}
# myimages<-here("figures","lichen_abundance.png")
# knitr::include_graphics(myimages)
```

```{r, include = FALSE}
lichen <- read_csv(here("data","jovan-2008-lichen.csv"))
glimpse(lichen)
shapiro.test(lichen$AQ)$p.value
shapiro.test(lichen$PNA)$p.value
cor.test(lichen$AQ,lichen$PNA,method="spearman")
```

```{r, echo = FALSE}
lichen |>
  ggplot(aes(x = AQ, y = PNA)) +
  geom_point(size = 3, alpha = 0.5) +
  scale_x_continuous(limits=c(-1.6,1.6), breaks=seq(-1.5,1.5,0.5)) +
  guides(
    x = guide_axis(minor.ticks = TRUE),
    y = guide_axis(minor.ticks = TRUE)
  ) +
  scale_y_continuous(limits=c(0,105), breaks=seq(0,110,20)) +
  labs(x = "Air Quality Scores",
       y = "Proportion of nitrophyte abundance\n (percent)") +
  annotate("text", x = 0.7, y = 90, label = deparse(bquote(italic(r)[sp]==-0.78~","~italic(p)<0.001)), parse = TRUE) +
  annotate("text", x = -1.5, y = 0, label = "High N/pH") +
  annotate("text", x = 1.5, y = 0, label = "Low N/pH") +
  theme_cowplot() +
  theme(axis.minor.ticks.length = rel(0.5))
```

  \
We note that the relationship between air quality score and proportion of nitrophyte abundance is plausibly linear.  \
\

There is a strong negative correlation (*r*~Sp~=-0.78, *p*<0.001) between air quality score and proportion of nitrophyte lichen. This suggests that this proportion can be used as a bioindicator of air quality.  \

\
A Spearman's rank correlation coefficient was calculated in this case rather than a Pearson correlation coefficient, despite the fact that both the *x* and *y* variables are plausibly drawn from normally distributed populations of values, according to a Shapiro-Wilk test. The problem is that both the air quality score and the proportion of nitrophyte abundance are ordinal variables - something we learn from reading the paper in which this figure appears. This means that analysis using parametric tests such as the Pearson method for determining linear correlation are not appropriate. A non-parametric method such as Spearman's rank method can be used instead.\

### Soil bacteria

::: csl-entry
Lauber, C. L., Hamady, M., Knight, R., & Fierer, N. (2009). Pyrosequencing-based assessment of soil pH as a predictor of soil bacterial community structure at the continental scale. <i>Applied and Environmental Microbiology</i>, <i>75</i>(15), 5111--5120. <https://doi.org/10.1128/AEM.00335-09>
:::

```{r}
myimages<-here("figures","soil_bacteria.png")
knitr::include_graphics(myimages)
```

In the Figure above, note that the Pearson *r*-values for **C** and **E** are close to zero, and the *p*-values are greater than 0.1, meaning that at this significance level there is no evidence from these data that there is *any* linear association between soil pH and the relative abundances of *Alphaproteobacteria* or *Beta/Gammaproteobacteria*. From the plots, it looks in **C** as if there no assocation at all, whereas in **E** it looks as though there might be, but if so then not a linear or even monotonic association, for which the correlation coefficient (Pearson *r* or Spearman *r*~Sp~) would be a poor measure.

### Birds

::: csl-entry
Pain, D.J., Mateo, R. and Green, R.E. (2019) ‘Effects of lead from ammunition on birds and other wildlife: A review and update’, <i>Ambio</i>, <i>48</i>(9), pp. 935–953. <https://doi.org/10.1007/s13280-019-01159-0>
:::

```{r}
myimages<-here("figures","bird_shot_ingestion.png")
knitr::include_graphics(myimages)
```
This figure is from a study on the impact on bird populations of ingestion of lead from spent lead ammunition arising from hunting using rifles and shot guns. For several species of wetland birds, the population trend (as measured by a proxy scale) is plotted against the prevalence of carcasses found to contain lead shot. 

There is a clear negative trend here that is plausibly linear, or at least monotonic. Shapiro-Wilk tests show that neither data set is plausibly drawn from a normally distributed population, so a Spearman's rank correlation coefficient is calculated. The result is $r_{\text{Sp}} = -0.697, p < 0.01$ so we can say that there is a evidence of a significant and strong negative correlation between lead shot ingestion of waterbird species and their population trends.
```{r, include = FALSE}

birds_lead <- read_csv(here("data","birds_lead_shot.csv"),skip=5)
glimpse(birds_lead)

shapiro.test(birds_lead$pc_lead) # p = 0.036
shapiro.test(birds_lead$pop_trend) # p = 0.030

#Have to use Spearman's rank
cor.test(birds_lead$pc_lead,birds_lead$pop_trend, method = "spearman")
```
### Heart rate vs life expectancy
```{r}
# myimages<-here("figures","heart_rate_vs_age.png")
# knitr::include_graphics(myimages)
```

```{r, include = FALSE}
hr_le  <- read_csv(here("data","heart_rate_vs_age.csv"))
glimpse(hr_le)
```
```{r, include = FALSE}
shapiro.test(hr_le$age)
```
```{r, include=FALSE}
shapiro.test(hr_le$lg10_hr)
```



```{r, include = FALSE}
cor.test(hr_le$age, hr_le$lg10_hr)
```


```{r, echo = FALSE}

hr_le  |>
  ggplot(aes(x = age, y=lg10_hr)) +
  geom_point(size = 3, colour="gray60") +
  geom_text(aes(label=Species), vjust = -0.9) +
  labs(x = "Life expectancy (years)",
       y = bquote("log"[10]~"Heart rate (bpm)")) +
    annotate("text", x = 60, y = 2.5, label = deparse(bquote(italic(r)==-0.858~","~italic(p)<0.001)), parse = TRUE) +
  theme_cowplot()
```
Here we see a strong negative linear correlation between the life expectancy of different species and the log of the mean heart rate in beats per minute. On this plot, humans are almost an outlier. In this case, use of a Shapirro Wilk test showed that both life expectancy and log of the heart rate were found to be consistent with being drawn from normally distributed populations, so the Pearson method was used to calculate the correlation coefficient.

## Exercise 1

```{r}
cor_plot <-function(plot_data){
plot_data |>
ggplot(aes(x=x,y=y)) +
  geom_point(alpha=0.5) +
  xlim(0,max(max(x,y))) +
  # ylim(0,NA) +
  # coord_fixed() +
  theme_cowplot() +
  theme_bare_axes()
}
```


```{r}
x<-seq(0,10,0.1)
d1 <- data.frame(x = x, y = 2 + 1 * x  + rnorm(sd=1,n=length(x)))
d2 <- data.frame(x = x, y = -1 * x  + rnorm(sd=1.3,n=length(x)))
d3 <- data.frame(x = x, y = (x-5)^2 + + rnorm(sd=1,n=length(x)))
d4 <- data.frame(x = x, y = -0.5*(x-5)^2 + + rnorm(sd=1,n=length(x)))
d5 <- data.frame(x = x, y = 0.5*(x-5)^3 + + rnorm(sd=5,n=length(x)))
d6 <- data.frame(x = x, y = -0.5*(x-5)^3 + + rnorm(sd=5,n=length(x)))
p1 <- cor_plot(d1)
p2 <- cor_plot(d2)
p3 <- cor_plot(d3)
p4 <- cor_plot(d4)
p5 <- cor_plot(d5)
p6 <- cor_plot(d6)

plot_grid(p1,p6,p4,p3,p2,p5,nrow=2,labels="AUTO",hjust=-4, label_colour= "darkred")
```
Plots A to F above show scatter plots of different data sets _Y_ against _X_. 

* Which of them show linear behaviour?
* Which of them show monotonic behaviour?
* For which of them might it be appropriate to calculate the following correlation coefficients between _X_ and _Y_?
  * Pearson *r*
  * Spearman rank *r*~sp~
  
  
## Exercise 2

Measurements were made on female Adelie penguins on a series of islands in the Antarctic. The bill lengths and depths of 73 individuals were recorded and are shown in the plot below.

```{r,include = FALSE}    
filepath<-here("data","difference_data.xlsx")
penguins<-read_excel(filepath,"penguins")
# glimpse(penguins)

adelieF<- penguins |>
  filter(species == "Adelie", sex=="female") |>
  dplyr::select(-species,-sex)


bill_lengths <- adelieF |> dplyr::select(bill_length_mm) |> pull()
bill_depths <- adelieF |> dplyr::select(bill_depth_mm) |> pull()
```




```{r, echo=FALSE}
adelieF |>
  ggplot(aes(x=bill_length_mm, y = bill_depth_mm)) +
  geom_point() +
  labs(x = "Bill length (mm)",
       y = "Bill depth (mm") +
  theme_cowplot()
```
From this information and  plot, decide

* Whether it is plausible that there is a linear relationship between the bill depths and lengths within the data set
* Whether the correlation coefficient within the data set is likely to be positive or negative
* Whether the correlation is 'strong' or 'weak' ie is the absolute value of the correlation coefficient likely to be close to 1 or close to zero
* Whether there might be evidence from this data that there is any correlation between bill depth and bill length in the population from which this data set was drawn.

#### Tests for normality

Shapiro-Wilk tests are carried out to check for normality of the two sets of data:

```{r, echo = TRUE}
shapiro.test(bill_depths)
```
```{r, echo = TRUE}
shapiro.test(bill_lengths)
```

On this basis, we see that we can use the Pearson  method to calculate the correlation coefficient between bill depth and bill length. What is telling us this?

#### Calculate correlation coefficient

For these data, we calculate the correlation coefficient using the Pearson method.

```{r, echo = TRUE}
# Pearson
cor.test(bill_depths,bill_lengths, method = "pearson")
```

Which part(s) of this output tell us that:

* There is a weak positive correlation between bill length and depth *within the sample*?
* There is no evidence that this correlation exists in the wider population from which this data set was drawn? 

How would you report this result?



## Exercise 3


```{r, include = FALSE}
library(janitor)
# Load penguin and iris data
penguins_filepath <- here("data", "penguins.csv")
penguins <- read_csv(penguins_filepath)
penguins <- penguins |> drop_na()

iris_filepath <- here("data", "iris.csv")
iris <- read_csv(iris_filepath)
glimpse(iris)
iris <- iris |> clean_names()
```

1. If you haven't done so already, click on [this link](https://github.com/mbh038/correlation/archive/refs/heads/main.zip) to download the practice project for correlation. You may then need to unzip it.

2. Open the project in RStudio using File/Open Project then navigating to the project and clicking on the `correlation.Rproj` file

3. Open the notebook `correlation.qmd` that you will find in the notebooks subfolder.

4. Run the initial code chunks to load packages and the penguin and iris data

5. Check in the Environment pane that you have created data frames called `penguins` and `iris`

6. Using the code in section @sec-iris-correlation as a guide, create a faceted plot of petal length against petal width for each species. 

7.  Again using @sec-iris-correlation as a guide, now calculate the Pearson correlation coefficient between petal length and petal width for each species, and display this, plus the lower and upper bounds of the confidence interval and the *p*-value for each species in a table.


-   Does it appear that the petal length and petal width are correlated for each species?\
-   Is the correlation positive or negative?\
-   For which species is the correlation strongest?\
-   Do the correlation coefficients make sense, given the plots?

8. Use the `filter()` function from the `dplyr` package in `tidyverse` to filter the penguin data down to Gentoo males. Save the data in data frame called `gentoo_m`. You can do this by including the following code chunk in your notebook:

```{{r}}
gentoo_m <- penguins |>
filter(sex == "male") |>
filter(species == "Gentoo")
```

9. Plot the bill depth vs bill length using code similar to this:


```{{r}}
gentoo_m |>
  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +
  geom_point() +
  labs(x = "Bill depth (mm)",
       y = "Bill length (mm)") +
  theme_classic()
```


10. Is it appropriate to use Pearson or Spearman's rank measures of correlation with this data?

11. Use the `shapiro.test()` function to determine if it is OK to use the Pearson measure of correlation.

12. Use the `cor.test()` function with the appropriate method argument (`method = "pearson"` or `method = "spearman"`), as appropriate, to determine if the bill length and bill depth variables are correlated.



