% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={From questions to data to answers using R},
  pdfauthor={Michael Hunt},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{From questions to data to answers using R}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{A guide for students at Newquay University Centre}
\author{Michael Hunt}
\date{}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Introduction}\label{introduction}
\addcontentsline{toc}{chapter}{Introduction}

\markboth{Introduction}{Introduction}

This is a compilation of the methods for data analysis that you are
likely to find useful in completing your studies at Newquay University
Centre. It is by no means exhaustive, but should address most of your
needs, most of the time.

Data analysis in the life sciences is only part of the wider process of
forming and then answering as best we can well-formed questions about
the way this or that aspect of the natural world works. If our question
is good, and our design is good, then it is more likely than not that
one of the standard analytical techniques described in this text will
our needs. If not, then maybe none of them will suit and we will have to
work harder to extract the answers we want from the data we have worked
hard to gather.

\section*{How to use this resource}\label{how-to-use-this-resource}
\addcontentsline{toc}{section}{How to use this resource}

\markright{How to use this resource}

You are not expected to read this text from beginning to end, although
you can if you want to. Each section explains\ldots..

\part{Preliminaries}

\chapter*{Test finder}\label{test-finder}
\addcontentsline{toc}{chapter}{Test finder}

\markboth{Test finder}{Test finder}

Use this flowchart to see which test is appropriate for your study
design and your data:

\includegraphics[width=24.04in,height=22.84in]{test_finder_files/figure-latex/mermaid-figure-1.png}

Now go to whichever chapter of this book covers the test you need.

\part{Working with R}

\chapter{A recommended analysis
workflow}\label{a-recommended-analysis-workflow}

This is intended as a rough outline of the sequence of steps one
commonly goes through when working on scripts:

\begin{itemize}
\tightlist
\item
  Before you start on the script, ask yourself: am I working in a
  Project?. If not, fix this!
\item
  What is my question? Am I clear about what I want the script to do?
\item
  Load packages
\item
  Load data
\item
  Inspect data
\item
  Clean up or tidy or manipulate the data in some way - whatever it
  takes to get it in the form needed for the analysis steps
\item
  Summarise the data
\item
  Plot the data
\item
  Do statistical analysis
\item
  Decide what all this has told you and report it in plain English.
\end{itemize}

Details will differ from script to script, but this sequence of steps is
very common.

\section{Are you working within your
Project?}\label{are-you-working-within-your-project}

Before we even think of the script, we need to make sure that we are
working within our Project. If we are not doing this, bad things will
happen. If you are, the Project name will be at the top right of the
RStudio window. If you are not, save the script you are working on, and
go to File/Open Project and open your Project. If you haven't even got a
`Project' or don't know what that means then just make sure that
everything you need for whatever you are working on is in one folder and
then turn that folder into a Project. (So a `Project' is just a regular
folder that has been given superpowers.) You do that by going to
File/New Project/Existing Directory. Then you navigate to your folder
and click on Create Project. RStudio will then restart and you will see
the name of your newly anointed Project folder at the top-right of the
RStudio window. You know that a folder is a `Project' because it will
have a .Rproj file inside it.

If all this sounds complicated, don't worry. It really isn't. Just get
someone to show you how to do it and you will be fine.

Now, to the script itself:

\section{Statement of the question(s) to be
investigated}\label{statement-of-the-questions-to-be-investigated}

Without thinking this through, you won't know what your script is
for\ldots{}

What is the analysis that will follow for? What question are you trying
to answer? What hypotheses are you trying to test?

Suppose we were trying to test the hypothesis that there is no
difference between the petal widths of the \emph{setosa},
\emph{versicolor} and \emph{virginica} species of iris. All we have to
go on are the petal widths of the plants we happened to measure. From
these measurements we want to make a statement about these three species
in general.

\section{Open a notebook}\label{open-a-notebook}

In RStudio, go to File/New File/ Quarto Document. Delete everything
below the yaml section at the top. This strangely named section is the
bit between the two lines with three dashes in. For the most part, we
will not need to worry about this section. We just should not delete it
entirely. What is useful to do is to amend the title to something
sensible, and to add \texttt{author:\ "your\ name"} and
\texttt{date:\ "the\ date\ in\ any\ old\ format"} lines. I also add a
couple of final lines that suppress warnings and messages that might
clutter up my printed output, so that your yaml will look something like
this:

\begin{verbatim}
---
title: "A typical workflow"
author: "Who wrote this?"
date: "Today's date"
output:
  html_document:
    df_print: paged
execute:
  message: false
  warning: false
---
\end{verbatim}

Delete everything beneath this yaml section. The big empty space that
then leaves you with is where you write your code. Remember that in
quarto documents, the code goes in `chunks' that are started and
finished with by lines with three backticks. Any other text goes between
the chunks and you can format this text using the simple rule of
Markdown, available in the RStudio Help menu at
\texttt{Help/Markdown\ Quick\ Reference}. Thus your script will end up
looking something like this:

\begin{verbatim}
---
title: "A typical workflow"
author: "Who wrote this?"
date: "Today's date"
output:
  html_document:
    df_print: paged
execute:
  message: false
  warning: false
---
\end{verbatim}

\section{First header}\label{first-header}

Any text we want to add. Note that a code chunk starts with \{r\} and
ends with `

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse) }\CommentTok{\# some actual R code}
\end{Highlighting}
\end{Shaded}

\section{Second header}\label{second-header}

Any text we want to add to explain what this next chunk does

\begin{verbatim}
```{r, include=FALSE}
library(tidyverse) # some actual R code
```
\end{verbatim}

\section{Load Packages}\label{load-packages}

You will nearly always want the first five packages, and often you will
appreciate the sixth, \texttt{janitor}. Others, such as \texttt{vegan}
will be useful from time to time, depending on what you are doing. If
any of these lines throw an error, it is most likely because you have
not yet installed that package. Do so in the console pane (not in this
script!) using the function
\texttt{install.packages("name\ of\ package")}. Then run this whole
chunk again.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{library}\NormalTok{(ggfortify)}
\FunctionTok{library}\NormalTok{(readxl)}
\FunctionTok{library}\NormalTok{(cowplot)}
\FunctionTok{library}\NormalTok{(janitor)}
\FunctionTok{library}\NormalTok{(vegan)}
\end{Highlighting}
\end{Shaded}

\section{Load data}\label{load-data}

There are several ways to do this, so details will differ depending on
what file type your data is saved in and where it is stored.

Here are some examples. In each case code here presumes that the data is
stored in a subfolder called `data' within the Project folder, and we
use the function \texttt{here()} from the \texttt{here} package. In my
experience this dramatically simplifies the business of finding your
data, wherever your script is. It makes it easier for you to share your
script with others and be confident that what worked for you will work
for them. It \emph{does} require that you are working within your
project.

\subsection{If from a csv file}\label{if-from-a-csv-file}

If you have your data in a \texttt{data} subfolder within your project,
this chunk will work. Just substitute the name of your data file

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filepath}\OtherTok{\textless{}{-}}\FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"iris.csv"}\NormalTok{)}
\NormalTok{iris}\OtherTok{\textless{}{-}}\FunctionTok{read\_csv}\NormalTok{(filepath)}
\FunctionTok{glimpse}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 150
Columns: 5
$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.~
$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.~
$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.~
$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.~
$ Species      <chr> "setosa", "setosa", "setosa", "setosa", "setosa", "setosa~
\end{verbatim}

\subsection{If from an Excel file}\label{if-from-an-excel-file}

You will need to use \texttt{read\_excel()} from the \texttt{readxl}
package, and you have to specify the name of the worksheet that holds
the data you want. You can, if you want, specify the exact range that is
occupied by the data. However I suggest you avoid doing this unless it
turns out that you need to do so. If your data is a nice, neat,
rectangular block of rows and columns, you should find that you don't
need to specify the range.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filepath}\OtherTok{\textless{}{-}}\FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"difference\_data.xlsx"}\NormalTok{)}
\NormalTok{iris}\OtherTok{\textless{}{-}}\FunctionTok{read\_excel}\NormalTok{(}\AttributeTok{path =}\NormalTok{ filepath,}
                 \AttributeTok{sheet =} \StringTok{"iris"}\NormalTok{, }\CommentTok{\# delete the comma if you choose not to specify the range in the line below}
                 \AttributeTok{range=} \StringTok{"A1:F151"} \CommentTok{\# optional {-} try leaving it out first. Only include if necessary.}
\NormalTok{                 ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{clean\_names}\NormalTok{()}
\FunctionTok{glimpse}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 150
Columns: 6
$ id           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17~
$ sepal_length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.~
$ sepal_width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.~
$ petal_length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.~
$ petal_width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.~
$ species      <chr> "setosa", "setosa", "setosa", "setosa", "setosa", "setosa~
\end{verbatim}

\subsection{If from a URL}\label{if-from-a-url}

You can load data into R directly from a URL if you are given one, and
that is in fact how you will mainly access data to be used in this
statistics text.

here, we load data from a file stored in a `'repo' on my github account:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris}\OtherTok{\textless{}{-}}\FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/mbh038/r4nqy/refs/heads/main/data/iris.csv"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{clean\_names}\NormalTok{()}
\FunctionTok{glimpse}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 150
Columns: 5
$ sepal_length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.~
$ sepal_width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.~
$ petal_length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.~
$ petal_width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.~
$ species      <chr> "setosa", "setosa", "setosa", "setosa", "setosa", "setosa~
\end{verbatim}

\section{Clean / Manipulate the data}\label{clean-manipulate-the-data}

Often we need to do some sort of data `wrangling' to get the data into
the form we want. For example we may wish to tidy it (this has a
particular meaning when applied to data sets), to remove rows with
missing values, to filter out rows from sites or time periods that we
don't want to include in our analysis, to create new columns and so on.

For example, lets create a new data frame for just the \emph{setosa}
species of iris:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{setosa }\OtherTok{\textless{}{-}}\NormalTok{ iris }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(species }\SpecialCharTok{==} \StringTok{"setosa"}\NormalTok{) }\CommentTok{\# filter picks out rows according to criteria being satisfied in some column}
\FunctionTok{glimpse}\NormalTok{(setosa)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 50
Columns: 5
$ sepal_length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.~
$ sepal_width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.~
$ petal_length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.~
$ petal_width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.~
$ species      <chr> "setosa", "setosa", "setosa", "setosa", "setosa", "setosa~
\end{verbatim}

or maybe we just want the columns that contain numeric data and not the
one containing the species identifiers, which is text:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris\_numeric }\OtherTok{\textless{}{-}}\NormalTok{ iris }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{species) }\CommentTok{\# select() retains or leaves out particular columns. Here, we leave out the species column.}
\FunctionTok{glimpse}\NormalTok{(iris\_numeric)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 150
Columns: 4
$ sepal_length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.~
$ sepal_width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.~
$ petal_length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.~
$ petal_width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.~
\end{verbatim}

\section{Summarise the data}\label{summarise-the-data}

How big is the difference between the mean of this group over here and
that group over there, and how big is that difference compared to the
precision with which we know those means? We nearly always want to do
this as a first way to get insight into whether we will or will not
reject our hypothesis. For example, let's find the mean petal widths of
the three species, the standard errors of those means and save the
results to a data frame called \texttt{petal\_summary}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{petal\_summary}\OtherTok{\textless{}{-}}\NormalTok{iris }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(species) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean.Pwidth =} \FunctionTok{mean}\NormalTok{(petal\_width),}
            \AttributeTok{se.Pwidth =} \FunctionTok{sd}\NormalTok{(petal\_width}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{n}\NormalTok{())))}
\NormalTok{petal\_summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 3
  species    mean.Pwidth se.Pwidth
  <chr>            <dbl>     <dbl>
1 setosa           0.246    0.0149
2 versicolor       1.33     0.0280
3 virginica        2.03     0.0388
\end{verbatim}

We can look at this table and already get an idea as to whether the
petal widths are the same or are different for the three species.

\section{Plot the data}\label{plot-the-data}

The next step is usually to plot the data in some way. We would
typically use the \texttt{ggplot2} package from \texttt{tidyverse} to do
this.

\subsection{Bar plot with error bars}\label{bar-plot-with-error-bars}

We could plot a bar plot with error bars, working from the summary data
frame that we created:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{petal\_summary }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ mean.Pwidth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{fill=}\StringTok{"\#a6bddb"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# this is the geom that gives us a bar plot when we have already done the calculations}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ mean.Pwidth }\SpecialCharTok{{-}}\NormalTok{ se.Pwidth, }\AttributeTok{ymax =}\NormalTok{ mean.Pwidth }\SpecialCharTok{+}\NormalTok{ se.Pwidth), }\AttributeTok{width =} \FloatTok{0.15}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x =} \StringTok{"species"}\NormalTok{,}
        \AttributeTok{y =} \StringTok{"Mean petal width (mm)"}\NormalTok{,}
        \AttributeTok{caption =} \StringTok{"Error bars are Â± one standard error of the mean"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# important to say what these error bars denote}
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{workflow_files/figure-pdf/unnamed-chunk-9-1.pdf}}

Note that we have given the bars a fill colour - we got this color from
\href{https://colorbrewer2.org/\#type=sequential&scheme=PuBu&n=3}{this
site} due to the cartographer Cynthia Brewer, who is behind the various
incarnations of the \texttt{Brewer} package in R, which is great for
getting colours that work well. We have used the same colour for each
species since the x-axis labels already tell us which bar relates to
which species. To use a different colour for each bar would imply there
is some extra information encoded by colour. Since there is not, it
serves no purpose to have different colours, and potentially confuses
the reader. Remember always that a plot is intended to convey a message.
Anything that detracts from that message should be avoided, however
pretty you think it is.

A couple of points could be made about this type of plot:

First, what about those error bars? Three types of error bar are in
common usage and there are arguments in favour and against the use of
each of them:

\begin{itemize}
\tightlist
\item
  the \emph{standard deviation} tells us about the spread of values in a
  sample, and is an estimate of the spread of values in a population;\\
\item
  the \emph{standard error of the mean}, as used here, is an estimate of
  the precision with which the sample means estimates the respective
  population means for each of the species.
\item
  the \emph{confidence interval}, typically a \emph{95\% confidence
  interval}, gives us the region within which we are (say) 95\%
  confident that the true species mean petal width might plausibly lie.
\end{itemize}

Which type of error bar is best to use depends on what story you want to
tell. Here, because we are interested in whether there is evidence of a
difference in the mean petal width of different species, we have gone
for the standard eror of the mean.

Regardless of which error bar you use and why, you should always tell
the reader which one you have gone for, as we have in the caption to the
figure.

A second point about this bar plot is that it doesn't tell us very much,
and indeed nothing that we didn't already know. It only conveys the mean
and standard error values for each species, which is information we
already have, arguably more compactly and in more easily readable form,
in the table we created. Further, it potentially obscures information
that might come from knowing the \emph{distribution} of the data.

Here are three other plot types that do show the distribution of petal
widths for each species and thus add extra information to what we
already know from the summary table

\subsection{Box plot}\label{box-plot}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{species, }\AttributeTok{y=}\NormalTok{petal\_width)) }\SpecialCharTok{+} \CommentTok{\# what we want to plot}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{fill=}\StringTok{"\#a6bddb"}\NormalTok{,}\AttributeTok{notch=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# what kind of plot we want}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width=}\FloatTok{0.1}\NormalTok{, }\AttributeTok{colour =} \StringTok{"\#f03b20"}\NormalTok{,}\AttributeTok{alpha=}\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{ (}\AttributeTok{x =} \StringTok{"species"}\NormalTok{,}
        \AttributeTok{y =} \StringTok{"Petal Width (mm)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{() }\CommentTok{\# choose a theme to give the plot a \textquotesingle{}look\textquotesingle{} that we like}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{workflow_files/figure-pdf/unnamed-chunk-10-1.pdf}}

Here, we have added the points themselves on top of the box plot. When
there are not too many data points, this can be useful. The `jitter'
adds some horizontal or vertical jitter, or both, so that the points do
not lie on top of each other. In this case we see that the variability
of petal widths is not the same for each species and that the data are
roughly symmetrically distributed around the median values in each case.
This information is useful in helping us determine which statistical
test might be appropriate for these data.

\subsection{Violin plot}\label{violin-plot}

A useful alternative to the box plot, especially when the data set is
large, is the violin plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ petal\_width)) }\SpecialCharTok{+} \CommentTok{\# what we want to plot}
  \FunctionTok{geom\_violin}\NormalTok{(}\AttributeTok{fill=}\StringTok{"\#a6bddb"}\NormalTok{,}\AttributeTok{notch=}\ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# what kind of plot we want}
  \CommentTok{\#geom\_jitter(width=0.1, colour = "\#f03b20",alpha=0.5) +}
  \FunctionTok{labs}\NormalTok{ (}\AttributeTok{x =} \StringTok{"species"}\NormalTok{,}
        \AttributeTok{y =} \StringTok{"Petal Width (mm)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{() }\CommentTok{\# choose a theme to give the plot a \textquotesingle{}look\textquotesingle{} that we like}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{workflow_files/figure-pdf/unnamed-chunk-11-1.pdf}}

The widths of the blobs (I am probably supposed to call them `violins'!)
show us the distribution of the data - where they are widest is where
the data are concentrated, while the height of the blobs shows us the
range of variation of the data. The positions of the blobs tells us the
mean petal widths of the different species and gives us an idea of the
differences between them.

\subsection{Ridge plot}\label{ridge-plot}

A bit like a violin plot. This needs the package \texttt{ggridges} to be
installed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggridges)}
\NormalTok{iris }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ petal\_width,}\AttributeTok{y =}\NormalTok{ species)) }\SpecialCharTok{+} \CommentTok{\# what we want to plot}
  \FunctionTok{geom\_density\_ridges}\NormalTok{(}\AttributeTok{fill=}\StringTok{"\#a6bddb"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# what kind of plot we want}
  \CommentTok{\#geom\_jitter(width=0.1, colour = "\#f03b20",alpha=0.5) +}
  \FunctionTok{labs}\NormalTok{ (}\AttributeTok{x =} \StringTok{"Petal Width (mm)"}\NormalTok{,}
        \AttributeTok{y =} \StringTok{"species"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{() }\CommentTok{\# choose a theme to give the plot a \textquotesingle{}look\textquotesingle{} that we like}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{workflow_files/figure-pdf/unnamed-chunk-12-1.pdf}}

Having seen the summary and one of these plots of the data, would you be
inclined to reject, or fail to reject, a null hypothesis that said that
there was no difference between the petal widths of the three species?

\section{Statistical analysis}\label{statistical-analysis}

Only now do we move on to the statistical analysis to try to answer our
intial question(s). But by now, after the summary and plot(s), we may
already have a pretty good idea what that answer will turn out to be.

The exact form of the analysis could take many forms. In a typical
ecology project you might carry out several types of analysis, each one
complementing the other. Here, an appropriate analysis might be to use
the linear model in the form of a one-way ANOVA, since we have one
factor (species) with three levels (\emph{setosa}, \emph{versicolor} and
\emph{virginica}) and an output variable that is numeric and likely to
be normally distributed. We can use the \texttt{lm()} function for this.

\subsection{Create the model object}\label{create-the-model-object}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pw.model }\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{ (petal\_width }\SpecialCharTok{\textasciitilde{}}\NormalTok{ species, }\AttributeTok{data =}\NormalTok{ iris)}
\end{Highlighting}
\end{Shaded}

\subsection{Check the validity of the
model}\label{check-the-validity-of-the-model}

We won't go into this here, but an important step is to check that the
data satisfy the often finicky requirements of whatever statistical test
we have decided to use. The \texttt{autoplot()} function form the
\texttt{ggfortify} package is great for doing this graphically.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autoplot}\NormalTok{( pw.model) }\SpecialCharTok{+} \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{workflow_files/figure-pdf/unnamed-chunk-14-1.pdf}}

Here we note in particular that although the spread of data within each
level is not roughly the same (top left figure)), the QQ plot is pretty
straight (top-right figure). This means that the data are approximately
normally distributed around their respective means. Taken together, this
means that these data satisfy reasonably well the requirements of a
linear model, so the output of that model should be reliable.

\subsection{The overall picture}\label{the-overall-picture}

Typically, statistical tests are testing the likelihood of the data
being as they are, or more `extreme' than they are, \emph{if} the null
hypothesis were true. Thus, the null hypothesis is central to
statistical testing.

The null hypothesis is typically that the `nothing going on', `no
difference' or ' no association' scenario is true. In this case, it
would be that there is no difference between the petal widths of the the
three species of iris being considered here.

Typically too, a test will in the end spit out a \emph{p}-value which is
the probability that we would have got the data we got, or more extreme
data, \emph{if} the null hypothesis were true. Being a probability, it
will always be a value between 0 and 1, where 0 means impossible, and 1
means certain. The closer the \emph{p}-value is to zero, the less likely
it is we would have got our data if the null hypothesis were true. At
some point, if the \emph{p}-value is small enough, we will decide that
the probability of getting the data we actually got if the null
hypothesis were true is so small that we reject the null hypothesis.
Typically, the threshold beyond which we do this is when \emph{p} =
0.05, but we could choose other thresholds. (Sounds arbitrary - yes, it
is, but the choice of 0.05 is a compromise value that makes the risk of
making each of two types of error - rejecting the null when we should
not, and failing to reject it when we should, both acceptably small.
This is a big topic which we won't explore further here.)

In the end, whatever other information we get from it, the outcome of a
statisical test is typically that we either reject the null hypothesis
or we fail to reject it. If we reject it then we are claiming to have
detected evidence for an `effect' and we go on to determine how big that
effect is and whether it is scientifically interesting. If we fail to
reject the null, that does not necessarily mean that there is no
`effect' (difference, trend, association etc). That might be the case,
but it might also just mean that we didn't find evidence for one from
our data.

It is all a bit like in a law court where the `null hypothesis' is that
the defendant is innocent, and at the end of the proceedings this null
is either rejected (Guilty!) because the evidence is such as to make it
untenable to hold onto the null hypothesis, or not rejected, because the
evidence is not strong enough to convict, in which case the defendant
walks free - but is not declared innocent. Formally, the court has
simply found insufficient evidence to convict. In the latter case, the
court would have failed to reject the null hypothesis. Crucially, it
would \emph{not} have declared that the defendant was innocent. In the
same way, in a scientific study, we either reject or fail to reject a
null hypothesis. We never \emph{ever} accept the null hypothesis as
true.

Actually, many researchers are unhappy wih this so-called `frequentist'
narrative and have sought to use an alternative `Bayesian' approach to
testing hypotheses. In this approach we can accept hypotheses and we can
bring in prior knowledge. This is an interesting topic, but a very big
one so we will not pursue it further here.

With all that behind us, we are in a better place to understand what the
output of the test is telling us.

For the 1-way ANOVA, as with other examples of the linear model, this
output comes in two stages:

\subsection{Overall picture}\label{overall-picture}

Is there evidence for a difference between at least two of the mean
values?

To see if there is evidence for this, an ANOVA test calculate the ratio
between the dfference \emph{betweeN} the groups compared to the
differences \emph{within} the groups. it calls this ratio \(F\). The
bigger \(F\) is, the more likely we are to reject the null hypothesis
that there is no difference between he groups.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(pw.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: petal_width
           Df Sum Sq Mean Sq F value    Pr(>F)    
species     2 80.413  40.207  960.01 < 2.2e-16 ***
Residuals 147  6.157   0.042                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The \emph{F} value is huge. The null hypothesis of this test, as with
many tests, is that there is no difference between the petal widths of
the three populations from which these samples have been drawn. In that
case, the \emph{F} value would be one. The \emph{p}-value is telling us
how likely it is that we would get an \emph{F} value as big or bigger
than the one we got for our samples if the null hypothesis were true.
Since the \emph{p}-value is effectively zero here, we reject the null
hypothesis: we have evidence from our data that there is a significant
varation of petal width between species.

The degrees of freedom \texttt{Df} tells us the number of independent
pieces of information that were used to calculate the result. Let's not
dwell on this here, but there are two that we have to report in this
case: the number of levels minus one ie 3-1 = 2, and the number of
individual data points in each level minus one, times the number of
levels ie (50-1) x 3 = 147.

\subsection{Effect size}\label{effect-size}

Now that we have established that at least two species of iris have
differing petal widths, we go onto investigate where the differences
lie, and how big they are. This is important: effect sizes matter. It is
one thing to establish that a difference is statistically significant
(and typically even the tiniest difference can show up as significant in
a study if the sample size is big enough), it is quite another to
establish whether the difference is big enough to be scientifically
interesting.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(pw.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = petal_width ~ species, data = iris)

Residuals:
   Min     1Q Median     3Q    Max 
-0.626 -0.126 -0.026  0.154  0.474 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        0.24600    0.02894    8.50 1.96e-14 ***
speciesversicolor  1.08000    0.04093   26.39  < 2e-16 ***
speciesvirginica   1.78000    0.04093   43.49  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.2047 on 147 degrees of freedom
Multiple R-squared:  0.9289,    Adjusted R-squared:  0.9279 
F-statistic:   960 on 2 and 147 DF,  p-value: < 2.2e-16
\end{verbatim}

The output here is typical of that from a 1-way ANOVA analysis in R.
Each line refers to one of the three levels of the factor being
investigated, which is petal width in this case. By default, those
levels are arranged alphabetically, so in this case the order is
\emph{setosa}, \emph{versicolor} then \emph{virginica}. The first row is
always labelled \texttt{(Intercept)}, so here that row is referring to
\emph{setosa}. This level is used as the `control' or reference level-
the one with which the others are compared. If we are happy to have
\emph{setosa} as that control then we can just carry on, but if we are
not, then we have to tell R which level we want to play that role. We'll
go through how to do that later on.

In the Estimate column the value 0.246 cm in the first row refers to the
actual mean petal width of the \emph{setosa} plants in the sample. If we
go back to the summary table we created earlier on, or look at one of
the plots we created, we see that that is the case.

For all other rows, the value in the Estimate column is not referring to
the absolute mean petal width but to the difference between the mean
petal width for that species and the mean petal width of the control
species. So we see that the mean petal width of the \emph{versicolor} in
our sample is 1.08 cm greater than that of \emph{setosa} and so is equal
to .246 + 1.08 = 1.326 cm, while that of the \emph{virginica} is 1.78 cm
greater and so is equal to 2.026 cm. Check from the table of mean values
we created and the plots that this is correct.

Here though, we are not interested in absolute values so much as we are
in differences, which is why that is what the summary table here gives
us. Look again at the differences between the mean petal widths for
\emph{versicolor} and \emph{virginica} and that for \emph{setosa} and
compare them with the standard erros of those differences, which are
given in the second column of the table. These standard errors are much
smaller than the differences, meaning that we can have confidence that
the differences are statistically significant.

This is borne out by the \emph{p}-value in the right hand column of the
table. The null hypothesis of this table is that there is no difference
in petal width between populations of the different species from which
these samples have been drawn.

Lastly, the adjusted \(R^2\) tells us the proportion of variation of
petal width that is accounted for by taking note of the species. Here,
the value is 0.93, which tells us hat little else besides species
determines the relative petal widths. Ther are no other variables that
we need to have taken into account.

\section{Report in plain English}\label{report-in-plain-english}

You would say something like

We find evidence that petal widths are not the same acros thhree species
of iris, with \emph{virginica} \textgreater{} \emph{versicolor}
\textgreater{} \emph{setosa}. (ANOVA, df = 2, p \textless{} 0.001)

\chapter{\texorpdfstring{Building plots using the package
\texttt{ggplot2}}{Building plots using the package ggplot2}}\label{building-plots-using-the-package-ggplot2}

In this exercise we are going to produce and improve a variety of useful
and widely used plots using the package \texttt{ggplot2} which is part
of the larger \texttt{tidyverse} package.

You will see that the code to do each plot is very similar, whatever the
type of plot, and that plots can be built up from very basic forms to
become really attractive, informative versions with very little
additional effort.

You need to read the examples in this worksheet and then fill in the
missing code or alter what is provided already in the empty code chunks
of the accompanying template script. Instructions for getting that are
given below.

As you complete each code chunk, try it out by pressing the green arrow
at the top right of the chunk. Sometimes you might want to try out an
individual line. You can do that by placing the cursor anywhere in the
line and pressing \texttt{Controll-Entr} (windows) or
\texttt{Command-Enter} (Mac)

Remember that the template is a markdown document, so you can add extra
text between the code chunks to explain to yourself what is going on.
You can format this test, if you wish, according to the very basic
markdown rules for doing this. See Help/Markdown Quick Reference. This
formatting is only useful if you `knit' the script, by pressing the knit
button at the top of the script pane. Try this! I suggest you knit to
html. This is how the worksheet you are working from was produced.

\section{Load packages}\label{load-packages-1}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("name of package") \# run this line once, if you need to, for any of the packages that need to be installed}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{library}\NormalTok{(palmerpenguins)}
\FunctionTok{library}\NormalTok{(devtools)}
\end{Highlighting}
\end{Shaded}

\section{Get the template script}\label{get-the-template-script}

This next chunk will download the template file that you need to fill in
as you work through this worksheet, and put it in the scripts subfolder
within your project folder. For it to work, you need to be `working in
your Project' - in which case the name of the project will appear in the
top right of the main RStudio window, and if you have a subfolder within
the project folder called `scripts'. If any of that is not true, it
needs to be sorted now!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_url }\OtherTok{\textless{}{-}} \StringTok{"https://raw.githubusercontent.com/mbh038/r{-}workshop/gh{-}pages/scripts/ggplot\_examples\_template.Rmd"}
\NormalTok{file\_dest }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"scripts"}\NormalTok{,}\StringTok{"my\_ggplot\_examples.rmd"}\NormalTok{)}
\FunctionTok{download.file}\NormalTok{(file\_url,file\_dest)}
\end{Highlighting}
\end{Shaded}

\section{Load the Palmer penguin
data}\label{load-the-palmer-penguin-data}

For this exercise we use the Palmer penguins data set which comes with
the package palmerpenguins

The \texttt{palmerpenguin} package contains two built-in data sets. One
is called \texttt{penguins}:

Here we load the data into this R session (you will now see it in the
Environment pane) and we inspect it using the function
\texttt{glimpse()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(penguins)}
\FunctionTok{glimpse}\NormalTok{(penguins)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 344
Columns: 8
$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~
$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse~
$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ~
$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ~
$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186~
$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ~
$ sex               <fct> male, female, female, NA, female, male, female, male~
$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007~
\end{verbatim}

How many rows are there and how many columns?

For more detailed meta-information on the data we just type the name of
the data set with a question mark before it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?penguins}
\end{Highlighting}
\end{Shaded}

\section{Summary stats on all the numeric
columns}\label{summary-stats-on-all-the-numeric-columns}

This is in general useful to get, at least for the columns that contain
numerical data, since it shows which columns contains NAs,which is
R-speak for missing data. They are how R represents what would be empty
cells in an Excel spreadsheet.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(penguins)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      species          island    bill_length_mm  bill_depth_mm  
 Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  
 Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  
 Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  
                                 Mean   :43.92   Mean   :17.15  
                                 3rd Qu.:48.50   3rd Qu.:18.70  
                                 Max.   :59.60   Max.   :21.50  
                                 NA's   :2       NA's   :2      
 flipper_length_mm  body_mass_g       sex           year     
 Min.   :172.0     Min.   :2700   female:165   Min.   :2007  
 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  
 Median :197.0     Median :4050   NA's  : 11   Median :2008  
 Mean   :200.9     Mean   :4202                Mean   :2008  
 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  
 Max.   :231.0     Max.   :6300                Max.   :2009  
 NA's   :2         NA's   :2                                 
\end{verbatim}

We see that there are some rows with NAs in for a few of the columns. We
need to be aware of this when doing calculations with the data, such as
taking means.

Here, we will remove those rows:

\section{Remove the rows with NAs}\label{remove-the-rows-with-nas}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\OtherTok{\textless{}{-}}\NormalTok{ penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{drop\_na}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\section{How many observations are there for each
species?}\label{how-many-observations-are-there-for-each-species}

Note the use of the pipe operator \texttt{\textbar{}\textgreater{}},
here and throughout. Think of it as meaning \texttt{and\ then}. It feeds
the output of one line into the function of the next line, where it is
used as that function's first argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(species)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 2
  species       n
  <fct>     <int>
1 Adelie      146
2 Chinstrap    68
3 Gentoo      119
\end{verbatim}

\section{Mean value for each numerical variable, for each
species}\label{mean-value-for-each-numerical-variable-for-each-species}

Here is an example of the use of the \texttt{group\_by()} then
\texttt{summarise()} combination, whereby data is first grouped, here by
species, then summary statistics (of your choice) are calculated for
each group.

In this example the data are grouped by species, then the mean value of
all the columns that contain numerical data are calculated, not just an
overall value for the whole column, but for each species

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(species) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric), mean, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{() }\CommentTok{\# good practice to include this at the end.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 6
  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year
  <fct>              <dbl>         <dbl>             <dbl>       <dbl> <dbl>
1 Adelie              38.8          18.3              190.       3706. 2008.
2 Chinstrap           48.8          18.4              196.       3733. 2008.
3 Gentoo              47.6          15.0              217.       5092. 2008.
\end{verbatim}

\subsection{Scatter plots}\label{scatter-plots}

Is flipper length correlated with body mass?

We could a do correlation test to find this out, but let us first plot
the data. We will show here how an elegant plot can be built up,
starting from a very basic one, so that you see what each line of code
for the finished version actually does. In the chunks below, run each
one in turn to see the effect of each successive change that you make.

First we feed the penguin data to the function \texttt{ggplot()}, and
use its \texttt{aes()} argument to tell it which variables are to be
`mapped' to which aesthetic (which means, roughly speaking, `visible')
features of the plot, such as the x-axis, the y-axis, point and line
colours, fill colours, symbol types and size etc:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm,}\AttributeTok{y=}\NormalTok{body\_mass\_g))}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/which variables-1.pdf}}

This produces the first layer of the eventual finished plot, an empty
plot, ready to display data. Before it can do this, \texttt{ggplot()}
needs to be told \emph{how} you want to do so - what type of plot do you
want? For that, we add a \texttt{geom.....()} line, to specify the type
of plot.

There are lots of geom types, but for a scatter plot we use
\texttt{geom\_point()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm,}\AttributeTok{y=}\NormalTok{body\_mass\_g)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/scatter plot-1.pdf}}

This gives us a recognisable scatter plot, but it is deficient in a
number of ways. For starters, we know that there are three species of
penguin. It would be better if each were plotted using symbols of a
different colour, shape or size. We can do this by adding in an extra
argument to the aesthetic in the first line. Here we include
\texttt{colour\ =\ species}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ flipper\_length\_mm,}\AttributeTok{y =}\NormalTok{ body\_mass\_g, }\AttributeTok{colour =}\NormalTok{ species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/scatter plot by species-1.pdf}}

Can you guess what you should have do if you wanted not the symbol
colour, but its shape or size to depend on species? Clue: change one
word!

Now we add labels, titles and so on, using the line \texttt{labs(...)}.
Note how we can actually write the arguments of this over several lines
on the page, for clarity.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm,}\AttributeTok{y=}\NormalTok{body\_mass\_g,}\AttributeTok{colour=}\NormalTok{species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Body mass (g)"}\NormalTok{,}
       \AttributeTok{colour=} \StringTok{"Species"}\NormalTok{, }\CommentTok{\# this changes the title of the legend.}
       \AttributeTok{title=}\StringTok{"Penguin size, Palmer Station LTER"}\NormalTok{,}
       \AttributeTok{subtitle=}\StringTok{"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Alternative place to put the information in the subtitle"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/add labels and titles-1.pdf}}

It can be useful to include some combination of titles, subtitles and
captions if the figure is to be used as part of a presentation or
poster, but if it is to go in a report, you would normally only include
a caption, and let the word-processing software do it, and if just for
exploratory analysis, not even that. I normally do include axis labels,
however.

Now we use a \texttt{theme} to alter the overall look of the figure.
There are several built-in themes you can choose from, and others from
packages that you can use. I usually use \texttt{theme\_cowplot()} from
the \texttt{cowplot} package. Try typing \texttt{?theme} at the command
prompt in the console window to see what is available. Here, we use the
built-in \texttt{theme\_bw()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm,}\AttributeTok{y=}\NormalTok{body\_mass\_g,}\AttributeTok{colour=}\NormalTok{species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Body mass (g)"}\NormalTok{,}
       \AttributeTok{colour=} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{title=}\StringTok{"Penguin size, Palmer Station LTER"}\NormalTok{,}
       \AttributeTok{subtitle=}\StringTok{"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/add theme-1.pdf}}

Now we reposition the legend. We don't have to, but we might not like
the default position of the legend. If not, we can move or even remove
it using another \texttt{theme()} line. The position argument of this
can be ``none'' if you want to remove it, top'', ``bottom'', ``left'',
``right'' or a numerical vector in relative coordinates, where c(0,0)
means bottom left within the plot, and c(1,1) means top-right. This is
what we use here. Play around with different values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm,}\AttributeTok{y=}\NormalTok{body\_mass\_g,}\AttributeTok{colour=}\NormalTok{species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Body mass (g)"}\NormalTok{,}
       \AttributeTok{colour=} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{title=}\StringTok{"Penguin size, Palmer Station LTER"}\NormalTok{,}
       \AttributeTok{subtitle=}\StringTok{"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{,}\FloatTok{0.8}\NormalTok{)) }\CommentTok{\# try "top", "left" etc}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/place the legend-1.pdf}}

Nicer colours. If you don't like the default colours offered by R, there
are several other palettes available, for example the \texttt{Brewer}
palettes, borrowed from the world of maps. See https://colorbrewer2.org
,and for a list of the available palettes, type
\textgreater{}\texttt{?scale\_colour\_brewer} into the console pane then
look at the help that appears in the Help pane (bottom right), and
scroll down to the palettes section. Note that we dont \emph{have} to
alter the colours. But doing so can make your plots not only look nicer,
but serve some other purpose, such as to be colour-blind friendly, or
have colours that are appropriate for the variables being plotted (eg
red points for red things, blue points for blue things). For an
assignment or dissertation report, it is a good idea to pick a palette
that you like and that works, and stick with it, so that all your plots
have the same general look. Here we choose the qualitative palette
\texttt{"Set2"} and use it by by adding the line
\texttt{scale\_colour\_brewer(palette="Set2")}. Try a few other
palettes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm,}\AttributeTok{y=}\NormalTok{body\_mass\_g,}\AttributeTok{colour=}\NormalTok{species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Body mass (g)"}\NormalTok{,}
       \AttributeTok{colour=} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{title=}\StringTok{"Penguin size, Palmer Station LTER"}\NormalTok{,}
       \AttributeTok{subtitle=}\StringTok{"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_brewer}\NormalTok{(}\AttributeTok{palette=}\StringTok{"Set2"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# try other palettes eg "Set3"}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{,}\FloatTok{0.8}\NormalTok{)) }\CommentTok{\# try "top", "left" etc}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/nicer colours-1.pdf}}

If we like, we can add best fit lines to each subset of the data, using
\texttt{geom\_smooth()}. To produce straight line fits,
\texttt{geom\_smooth()} needs to be told to use a linear model, using
the \texttt{method\ =\ "lm"} argument. By default, you will get lines
with a grey 95\% confidence band around them. This can be useful, but if
you don't want it, add the argument \texttt{se\ =\ FALSE}, as we have
done below. We have also altered the linewidth.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm,}\AttributeTok{y=}\NormalTok{body\_mass\_g,}\AttributeTok{colour=}\NormalTok{species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{linewidth=}\FloatTok{0.5}\NormalTok{,}\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# try leaving out the se argument}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Body mass (g)"}\NormalTok{,}
       \AttributeTok{colour=} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{title=}\StringTok{"Penguin size, Palmer Station LTER"}\NormalTok{,}
       \AttributeTok{subtitle=}\StringTok{"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins"}\NormalTok{) }\SpecialCharTok{+}
   \FunctionTok{scale\_colour\_brewer}\NormalTok{(}\AttributeTok{palette=}\StringTok{"Set2"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{,}\FloatTok{0.8}\NormalTok{)) }\CommentTok{\# also try legend.position = "top", "left" etc}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/add best fit lines-1.pdf}}

\subsection{Repeat for bill length and flipper
length}\label{repeat-for-bill-length-and-flipper-length}

Modify the code of the previous plot so that you now plot bill length vs
flipper length. Adjust any labels and titles as necessary. This time,
put the legend in the bottom right of the plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm,}\AttributeTok{y=}\NormalTok{bill\_length\_mm,}\AttributeTok{colour=}\NormalTok{species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{linewidth=}\FloatTok{0.5}\NormalTok{,}\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# try leaving out the se argument}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Bill length (mm)"}\NormalTok{,}
       \AttributeTok{colour=} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{title=}\StringTok{"Penguin size, Palmer Station LTER"}\NormalTok{,}
       \AttributeTok{subtitle=}\StringTok{"Flipper length and bill length for Adelie, Chinstrap, and Gentoo Penguins"}\NormalTok{) }\SpecialCharTok{+}
   \FunctionTok{scale\_colour\_brewer}\NormalTok{(}\AttributeTok{palette=}\StringTok{"Set2"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{,}\FloatTok{0.2}\NormalTok{)) }\CommentTok{\# play with the values to get it where you want it}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/bill length vs flipper length-1.pdf}}

Do you see how straightforwrd it is to adapt the code that produces one
plot to get the code you need for another, similar plot?

\subsection{Add yet more informtion to the
plot}\label{add-yet-more-informtion-to-the-plot}

Let us include the information of which island the penguins come from by
making the shape of the plotted points be dependent on that:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm,}\AttributeTok{y=}\NormalTok{bill\_length\_mm,}\AttributeTok{colour=}\NormalTok{species,}\AttributeTok{shape=}\NormalTok{island)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \CommentTok{\#geom\_smooth(method="lm", linewidth=0.5,se=FALSE) + \# try leaving out the se argument}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Bill length (mm)"}\NormalTok{,}
       \AttributeTok{colour=} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{shape=}\StringTok{"Island"}\NormalTok{,}
       \AttributeTok{title=}\StringTok{"Penguin size, Palmer Station LTER"}\NormalTok{,}
       \AttributeTok{subtitle=}\StringTok{"Flipper length and bill length for Adelie, Chinstrap, and Gentoo Penguins"}\NormalTok{) }\SpecialCharTok{+}
   \FunctionTok{scale\_colour\_brewer}\NormalTok{(}\AttributeTok{palette=}\StringTok{"Set2"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"right"}\NormalTok{) }\CommentTok{\# play with the position to get it where you want it. Try "top" etc.}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/islands and species-1.pdf}}

\subsection{Distribution of penguin flipper
lengths}\label{distribution-of-penguin-flipper-lengths}

The distribution of a data set is often a useful thing to know. Around
which value are the data grouped, how widely spread are they and are the
values symmetrically or asymmetrically distributed around the central
value? A number of plot types can show this for us. Here we illustrate
histograms, density plots, box plots, violin plots and ridge plots.

\subsubsection{Histogram}\label{histogram}

First, let's do a basic histogram. For this we use
\texttt{geom\_histogram()}. In the \texttt{ggplot} line, in the
\texttt{aes()} argument, we need only specify the variable that maps to
x, since the software will count how many observations lie within
specific narrow ranges of the variable, called bins. Those bin counts
will be the y variable of the histogram. To find the distribution of
flipper length, we use \texttt{flipper\_length\_mmm} as the x variable.
So we could try

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm)) }\SpecialCharTok{+}  \CommentTok{\# why is y not specified?}
  \FunctionTok{geom\_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/awful histogram-1.pdf}}

But this isn't useful. The histograms for the three species overlap each
other, so we need to give each one a different colour, and we need to
reduce the opacity of the bars so that the histograms behind are not
obscured by the ones in front, where they overlap. Further, we need to
stop \texttt{ggplot} from stacking the different histogram bars on top
of each other where those for different species are in the same bin.
Annoyingly, that is what it does by default, which makes seeing the
individual distributions clearly much more difficult.

Another thing with histograms, something that can make them a fiddle to
use, is that their usefulness in revealing a distrivution is affect by
how wide the bins are. By default, ggplot chooses the bin width such
that you get 30 bins altogether. This may not be optimal. Here, let's
try specifying the bin width to 4 mm (but see what happens when you try
other values, especially very large and very small values).

This we can achieve by:

\begin{itemize}
\tightlist
\item
  incuding \texttt{fill\ =\ species} in the aes() argument of ggplot.
\item
  sepcifying \texttt{position\ =\ identity} as an argument of
  \texttt{geom\_histogram()}, to stop the stacking.
\item
  specifying the opacity argument \texttt{alpha} to be a value less than
  1. Here we try alpha = 0.4` - but try other values in the range 0
  (transparent) - 1 (opaque), to reduce the opacity.
\item
  specifying binwidth = 4 - try other values
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm, }\AttributeTok{fill =}\NormalTok{ species)) }\SpecialCharTok{+}  \CommentTok{\# why is y not specified?}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{binwidth =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/better histogram-1.pdf}}

So, a lot going on, but still only three lines of code!

Now add good axis labels, an overall theme, and choose a colour scheme
you like, and the legend position, just as you have done before:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm,}\AttributeTok{fill=}\NormalTok{species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position=}\StringTok{"identity"}\NormalTok{,}\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{binwidth =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Count"}\NormalTok{,}
       \AttributeTok{fill=} \StringTok{"Species"}\NormalTok{, }\CommentTok{\# specifies the legend title. See what happens if you omit this line.}
       \AttributeTok{title=}\StringTok{"Penguin flipper lengths"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# we wouldn\textquotesingle{}t use this for a figure going in a report.}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette=}\StringTok{"Set2"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{,}\FloatTok{0.8}\NormalTok{)) }\CommentTok{\# play with the position to get it where you want it}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/hist for each species-1.pdf}}

In the scatter plot and the histogram, we have used colour to
distinguish the different species. We can do this because our data set
is tidy: there is just one column that species the species, and the same
for every other variable. That same feature of the data enables to use
another way to represent the different species:
\texttt{facet\_wrap(\textasciitilde{}species)}. This gives us three
separate plots, side by side or one above the other. See it used here:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm,}\AttributeTok{fill=}\NormalTok{species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position=}\StringTok{"identity"}\NormalTok{,}\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{binwidth =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Count"}\NormalTok{,}
       \AttributeTok{fill=} \StringTok{"Species"}\NormalTok{, }
       \AttributeTok{title=}\StringTok{"Penguin flipper lengths"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{species) }\SpecialCharTok{+} \CommentTok{\#try adding the argument ncol = 1.}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette=}\StringTok{"Set2"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# try other palettes, eg "Set1".}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"none"}\NormalTok{) }\CommentTok{\# we don\textquotesingle{}t need a legend!}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/hist with facet wrap different colours-1.pdf}}

Just a thought, but do the colours here serve any useful purpose? What
extra information do they convey? If you ever think that a feature of a
graph conveys no additional information, consider omitting it. Here is
the figue before without colours, but going for white brs with grey
outlines:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position=}\StringTok{"identity"}\NormalTok{,}\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{binwidth =} \DecValTok{4}\NormalTok{, }\AttributeTok{fill=}\StringTok{"white"}\NormalTok{,}\AttributeTok{colour=}\StringTok{"grey50"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Count"}\NormalTok{,}
       \AttributeTok{fill=} \StringTok{"Species"}\NormalTok{, }\CommentTok{\# specifies the legend title. See what happens if you omit this line.}
       \AttributeTok{title=}\StringTok{"Penguin flipper lengths"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# we wouldn\textquotesingle{}t use this for a figure going in a report.}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{species) }\SpecialCharTok{+} \CommentTok{\#try adding the argument ncol = 1.}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"none"}\NormalTok{) }\CommentTok{\# we don\textquotesingle{}t need a legend!}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/hist with facet wrap and no colour-1.pdf}}

Arguably, this is a better plot than the previous one because it
excludes the potentially confusing redundancy of using different colours
each species, when we already know which species is the subject of each
plot.

If you don't like white as the fill colour, try another one, for exampe
this one that I found on Cynthia Brewer's very useful map colour site:
https://colorbrewer2.org

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position=}\StringTok{"identity"}\NormalTok{,}\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{binwidth =} \DecValTok{4}\NormalTok{, }\AttributeTok{fill=}\StringTok{"\#a6bddb"}\NormalTok{,}\AttributeTok{colour=}\StringTok{"grey50"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Count"}\NormalTok{,}
       \AttributeTok{fill=} \StringTok{"Species"}\NormalTok{, }\CommentTok{\# specifies the legend title. See what happens if you omit this line.}
       \AttributeTok{title=}\StringTok{"Penguin flipper lengths"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# we wouldn\textquotesingle{}t use this for a figure going in a report.}
  \FunctionTok{facet\_grid}\NormalTok{(island}\SpecialCharTok{\textasciitilde{}}\NormalTok{species) }\SpecialCharTok{+} \CommentTok{\#try adding the argument ncol = 1.}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"none"}\NormalTok{) }\CommentTok{\# we don\textquotesingle{}t need a legend!}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/hist with facet wrap and one colour-1.pdf}}

Different fill colours \emph{would} be useful if the different penguin
species had distinctive dominant colours, but that isn't the case!

\subsubsection{Density plot}\label{density-plot}

An alternative to a histogram, the density plot, gives us a smoothed
version of the histogram. The vertical axis on these is not a count, but
a measure of the concentration of the data.

Here is one with overlapping density plots

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm,}\AttributeTok{fill=}\NormalTok{species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{alpha=}\FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Density"}\NormalTok{,}
       \AttributeTok{fill=} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{title=}\StringTok{"Penguin flipper lengths"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette=}\StringTok{"Set2"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"right"}\NormalTok{) }\CommentTok{\# play with the position to get it where you want it}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/density-1.pdf}}

We can also adapt this and do what was done for the histograms and do a
set of three, one for each species, using \texttt{facet\_wrap()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{flipper\_length\_mm)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{fill=}\StringTok{"\#a6bddb"}\NormalTok{,}\AttributeTok{colour=}\StringTok{"grey50"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Count"}\NormalTok{,}
       \AttributeTok{fill=} \StringTok{"Species"}\NormalTok{, }\CommentTok{\# specifies the legend title. See what happens if you omit this line.}
       \AttributeTok{title=}\StringTok{"Penguin flipper lengths"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# we wouldn\textquotesingle{}t use this for a figure going in a report.}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{species) }\SpecialCharTok{+} \CommentTok{\#try adding the argument ncol = 1.}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"none"}\NormalTok{) }\CommentTok{\# we don\textquotesingle{}t need a legend!}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/density with facet wrap-1.pdf}}

Which is more useful in this case: the overlapping plots on one chart,
or the separate charts done using \texttt{facet\_wrap()}? Whatever you
think here, the answer in other cases will sometimes be one, sometimes
the other. Now you have the tools to enable you to try both and make the
best choice.

\subsubsection{Box plots}\label{box-plots}

Box plots are a really useful way to summarize the distribution of
numerical response data such as \texttt{flipper\_length\_mm} across
different categorical variables, such as \texttt{species}. We use
\texttt{geom\_boxplot()} to produce them.

Let's do a basic box plot of flipper lengths for each penguin species:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{species,}\AttributeTok{y=}\NormalTok{flipper\_length\_mm,}\AttributeTok{fill=}\NormalTok{species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/box plot basic-1.pdf}}

Now let's use what we have done before to add code lines that

\begin{itemize}
\tightlist
\item
  include suitable axis labels and a title
\item
  give the same `theme' ie overall look as the previous graphs
\item
  fill the boxes with the same colour for each species.
\item
  remove the legend that you now have, because you don't need it (Why?).
  Use \texttt{theme(legend.position="none")} to do this.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{species,}\AttributeTok{y=}\NormalTok{flipper\_length\_mm,}\AttributeTok{fill=}\NormalTok{species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{title=}\StringTok{"Penguin flipper lengths"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette=}\StringTok{"Set2"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\CommentTok{\# no legend needed}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/box plot improved-1.pdf}}

\subsubsection{\texorpdfstring{Violin Plot using
\texttt{geom\_violin()}}{Violin Plot using geom\_violin()}}\label{violin-plot-using-geom_violin}

This is a variant on the box plot. Each `violin' is a sideways density
plot of the distribution of the data for each species, with its own
mirror image to make it look a bit like a violin. The code for these is
exactly as for box plots except we use \texttt{geom\_violin()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{species,}\AttributeTok{y=}\NormalTok{flipper\_length\_mm,}\AttributeTok{fill=}\NormalTok{species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_violin}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/violin basic-1.pdf}}

Now we write code to improve this, just as you did the box plot. The
final code is the same as for that apart from one line!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{species,}\AttributeTok{y=}\NormalTok{flipper\_length\_mm,}\AttributeTok{fill=}\NormalTok{species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_violin}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{title=}\StringTok{"Penguin flipper lengths"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette=}\StringTok{"Set2"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\CommentTok{\# no legend needed}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/violin improved-1.pdf}}

\subsubsection{Ridge plot}\label{ridge-plot-1}

This is a variant on the density plot, that is most useful when you have
lots of categorical variables. We have only three here, the three
penguin species, but let's try it anyway.

For this, we need the \texttt{ggridges} package. This is one of many
packages that extend the power of ggplot, and so work in much the same
way:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# library}
\CommentTok{\#install.packages("ggridges") \# use this once, if you have to, then comment it out.}
\FunctionTok{library}\NormalTok{(ggridges) }
 
\CommentTok{\# basic example}
\NormalTok{penguins }\SpecialCharTok{|\textgreater{}}
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ flipper\_length\_mm, }\AttributeTok{y =}\NormalTok{ species, }\AttributeTok{fill =}\NormalTok{ species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density\_ridges}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Flipper length (mm)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{title=}\StringTok{"Penguin flipper lengths"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_ridges}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/unnamed-chunk-9-1.pdf}}

Now try producing graphs like the ones above, but for body mass rather
than flipper length.

\subsection{Bar chart with error bar}\label{bar-chart-with-error-bar}

There are different ways to produce this commonly used way to summarise
data. For example we might use one to compare the mean flipper lengths
of the different penguin species. For a bar chart of these to be of any
use at all, it needs to include error bars that show standard deviations
of the samples, standard errors of the means, or confidence intervals
(Why?). Which you use depends on the story you are trying to tell.

First, we will add error bars that are Â± one standard deviation of the
samples.

I usually first create a summary of the data, in which we calculate the
means and appropriate error for each species for whichever variable I am
interested in, then feed this summary table to \texttt{ggplot} and use
\texttt{geom\_col()} to plot the bars, with \texttt{geom\_errorbar()} on
top of that to plot the error bars.

Let's do that first:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flipper\_summary }\OtherTok{\textless{}{-}}\NormalTok{ penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{drop\_na}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \CommentTok{\# these two lines produce a summary table}
  \FunctionTok{group\_by}\NormalTok{(species) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{fl.mean =} \FunctionTok{mean}\NormalTok{(flipper\_length\_mm), }\AttributeTok{fl.sd =} \FunctionTok{sd}\NormalTok{(flipper\_length\_mm))}
\NormalTok{flipper\_summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 3
  species   fl.mean fl.sd
  <fct>       <dbl> <dbl>
1 Adelie       190.  6.52
2 Chinstrap    196.  7.13
3 Gentoo       217.  6.59
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flipper\_summary }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ fl.mean)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ fl.mean}\SpecialCharTok{{-}}\NormalTok{fl.sd, }\AttributeTok{ymax =}\NormalTok{ fl.mean }\SpecialCharTok{+}\NormalTok{ fl.sd), }\AttributeTok{width =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Flipper length (mm)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/unnamed-chunk-11-1.pdf}}

\subsubsection{Standard deviation or standard
error?}\label{standard-deviation-or-standard-error}

The error bars in the plot above are Â± one standard deviation. A
standard deviation gives us an idea of the spread of values within a
sample or population. Remember that if a population is normally
distributed about its mean, there is a roughly 95\% probability that a
random chosen individual will lie within two standard deviations of the
mean.

So standard deviations of samples are useful. They are a useful way to
summarise the variability of the sample, they tell us about the likely
variability of the next sample, and they are our best estimate of the
variability of the population from which the sample was drawn.

Sometimes, though, we want to know more than that. We might want to know
how closely a sample mean is likely o be to the true mean of the
population from which the sample was drawn, and perhaps to get an idea
as to whether two or more populations are different, given the means of
samples drawn from those populations. For this, we need not the standard
deviation but the \emph{standard error}. These are the error bars that
are most commonly displayed on bar charts when you see them in papers.

To calculate standard deviation error bar lengths we use a formula
\(\text{SE} = \frac{\text{SD}}{\sqrt{n}}\) where \(n\) is the number of
observations, SD is the standard deviation of the sample and SE is the
standard error of the means of the sample. We can use the summary
functions \texttt{sd()} to calculate the standard deviation, and
\texttt{n()} to calculate \(n\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flipper\_summary2 }\OtherTok{\textless{}{-}}\NormalTok{ penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{drop\_na}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \CommentTok{\# these two lines produce a summary table}
  \FunctionTok{group\_by}\NormalTok{(species) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{fl.mean =} \FunctionTok{mean}\NormalTok{(flipper\_length\_mm), }\AttributeTok{fl.se =} \FunctionTok{sd}\NormalTok{(flipper\_length\_mm)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{n}\NormalTok{()))}
\NormalTok{flipper\_summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 3
  species   fl.mean fl.sd
  <fct>       <dbl> <dbl>
1 Adelie       190.  6.52
2 Chinstrap    196.  7.13
3 Gentoo       217.  6.59
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flipper\_summary2 }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ fl.mean)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin=}\NormalTok{fl.mean}\SpecialCharTok{{-}}\NormalTok{fl.se, }\AttributeTok{ymax =}\NormalTok{ fl.mean }\SpecialCharTok{+}\NormalTok{ fl.se),}\AttributeTok{width=}\FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Flipper length (mm)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/unnamed-chunk-13-1.pdf}}

These standard error bars are always smaller than the standard deviation
error bars (by a factor equal to the square root of the sample size).
Here they are so small as to be barely visible. They ar useful in bar
charts like this one since they give us a rough and ready way of
assessing whether the differences between the samples (the heights of
the bars) are likely to indicate real differences between the
populations. If the bar-height differences are much greater than the
size of the standard error bars, then they probably indicate significant
differences between the popultions. If not, then they probably don't.

Now let's alter this code so that each bar has a different fill colour,
and remove the legend that then appears, since it is unnecessary?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flipper\_summary }\SpecialCharTok{|\textgreater{}}
  \CommentTok{\# we add an argument to colour each bar according to species}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ fl.mean, }\AttributeTok{fill =}\NormalTok{ species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin=}\NormalTok{fl.mean}\SpecialCharTok{{-}}\NormalTok{fl.sd, }\AttributeTok{ymax =}\NormalTok{ fl.mean }\SpecialCharTok{+}\NormalTok{ fl.sd),}\AttributeTok{width=}\FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Flipper length (mm)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \CommentTok{\# include an arument to remove the legend}
  \FunctionTok{theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/unnamed-chunk-14-1.pdf}}

Now let us replace this colour scheme with nicer ones (not just nice,
but also colour-blind friendly, perhaps) offered by the Brewer palettes.

To do this we can add the line
\texttt{scale\_fill\_brewer(palette\ =\ "Set2")}. Note: we use
\texttt{scale\_colour\_brewer()} to alter the colours of points, like we
did above, or the outline colour of bars, and use
\texttt{scale\_fill\_brewer()} to alter the fill colour of bars. This is
what we want to do here.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flipper\_summary }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ fl.mean, }\AttributeTok{fill =}\NormalTok{ species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin=}\NormalTok{fl.mean}\SpecialCharTok{{-}}\NormalTok{fl.sd, }\AttributeTok{ymax =}\NormalTok{ fl.mean }\SpecialCharTok{+}\NormalTok{ fl.sd), }\AttributeTok{width=}\FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Flipper length (mm)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette=}\StringTok{"Set2"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/unnamed-chunk-15-1.pdf}}

If you don't like the colours of the palette ``Set2'' you can try
another one. To find out what palettes are available, remember, you can
type \texttt{?scale\_fill\_brewer()} into the console pane then look at
the help that appears in the Help pane (bottom right), and scroll down
to the Palettes section.

If you agree that having different fill colours for the bars is actually
confusing and brings no information to the plot that we do not already
know, you can modify the previous plot in the manner that you did for
the separate histograms:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flipper\_summary }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ fl.mean)) }\SpecialCharTok{+} 
  \CommentTok{\# add arguments here that give fill colour "\#a6bddb" and outline colour "grey50".}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{fill =} \StringTok{"\#a6bddb"}\NormalTok{, }\AttributeTok{colour =} \StringTok{"grey50"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin=}\NormalTok{fl.mean}\SpecialCharTok{{-}}\NormalTok{fl.sd, }\AttributeTok{ymax =}\NormalTok{ fl.mean }\SpecialCharTok{+}\NormalTok{ fl.sd), }\AttributeTok{width=}\FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Flipper length (mm)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ggplot_examples_files/figure-pdf/unnamed-chunk-16-1.pdf}}

\chapter{Titles, labels and
annotations}\label{titles-labels-and-annotations}

Often in plots one needs to use mathematical expressions,
suffixes/superscripts, greeek letters or other unusual formatting.

Here we show one way of doing this, using \texttt{bquote()}. Over the
years I have found this to be the simplest way.

\section{\texorpdfstring{Use
\texttt{bquote()}}{Use bquote()}}\label{use-bquote}

The four rules

\begin{itemize}
\tightlist
\item
  Strings -- Require quotes wrapped w/ tilde separator (e.g., ``my
  text'' \textasciitilde).
\item
  Math Expressions -- Unquoted \& follow \texttt{?plotmath}
\item
  Numbers -- Unquoted when part of math notation
\item
  Variables -- Use .() (pass in string or numeric)
\end{itemize}

Examples of all of these are shown in the labels, title and annotations
included in the plot below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{,}\AttributeTok{y=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)}
\NormalTok{cor }\OtherTok{\textless{}{-}} \FloatTok{0.456}
\NormalTok{df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x,}\AttributeTok{y=}\NormalTok{y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x =} \FunctionTok{bquote}\NormalTok{(}\StringTok{"An axis label with suffix and superscript:"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ x[i]}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), }
        \AttributeTok{y =} \FunctionTok{bquote}\NormalTok{(}\StringTok{"An axis label with greek letters:"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ alpha }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Beta }\SpecialCharTok{\textasciitilde{}}\NormalTok{ mu }\SpecialCharTok{\textasciitilde{}}\NormalTok{ m),}
        \AttributeTok{title =} \FunctionTok{bquote}\NormalTok{(}\StringTok{"Hello"}\SpecialCharTok{\textasciitilde{}}\NormalTok{ r[xy] }\SpecialCharTok{==}\NormalTok{ .(cor) }\SpecialCharTok{\textasciitilde{}} \StringTok{"and"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ CO[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \StringTok{"and"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ B}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{\textasciitilde{}} \StringTok{"and"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ m}\SpecialCharTok{\^{}}\NormalTok{\{}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{\})) }\SpecialCharTok{+}
  \FunctionTok{annotate}\NormalTok{(}\AttributeTok{geom=}\StringTok{"text"}\NormalTok{, }\AttributeTok{x =} \DecValTok{5}\NormalTok{, }\AttributeTok{y =} \DecValTok{7}\NormalTok{, }\AttributeTok{label =}  \FunctionTok{deparse}\NormalTok{(}\FunctionTok{bquote}\NormalTok{(}\StringTok{"Hello"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ r[xy] }\SpecialCharTok{==} \FloatTok{0.678} \SpecialCharTok{\textasciitilde{}} \StringTok{"and"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ B}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)), }\AttributeTok{parse =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{annotate}\NormalTok{(}\AttributeTok{geom=}\StringTok{"text"}\NormalTok{, }\AttributeTok{x =} \DecValTok{7}\NormalTok{, }\AttributeTok{y =} \DecValTok{9}\NormalTok{, }\AttributeTok{label =}  \FunctionTok{bquote}\NormalTok{(}\StringTok{"A big annotation"}\NormalTok{), }\AttributeTok{size =} \DecValTok{12}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{annotate}\NormalTok{(}\AttributeTok{geom=}\StringTok{"text"}\NormalTok{, }\AttributeTok{x =} \DecValTok{7}\NormalTok{, }\AttributeTok{y =} \DecValTok{2}\NormalTok{, }\AttributeTok{label =}  \FunctionTok{bquote}\NormalTok{(}\StringTok{"A red annotation"}\NormalTok{), }\AttributeTok{colour =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{titles_labels_and_annotations_files/figure-pdf/unnamed-chunk-2-1.pdf}}

\section{?plotmath}\label{plotmath}

\texttt{plotmath} expressions can be used for mathematical annotation in
text within plots in R when writing titles, subtitles, axis labels,
legends and annotations. It works in both base R graphics and ggplot2.

In the console pane, type \texttt{?plotmath} in the console pane to see
the full list of options.

Some key rules are:

\begin{itemize}
\tightlist
\item
  subscripts: O{[}2{]} gives O\textsubscript{2}
\item
  superscripts: m\^{}2 gives m\textsuperscript{2}, m\^{}\{-1\} gives
  m\textsuperscript{-1}
\item
  Lower case Greek: alpha, beta etc gives \(\alpha, \beta\) etc, so mu
\item
  Upper case Greek: Delta, Gamma etc gives \(\Delta, \Gamma\) etc
\end{itemize}

\part{Tests for difference}

\chapter{Tests for difference: one factor, two
levels}\label{tests-for-difference-one-factor-two-levels}

In this document we consider tests for difference where there are two
levels being compared.

\section{Test finder flow chart}\label{test-finder-flow-chart}

First, use this flowchart to see if a two-sample t-test or its
non-parametric counterpart the Mann-Whitney-U test is appropriate for
your question, your study design and your data:

\includegraphics[width=24.17in,height=23.59in]{tests_for_difference_two_levels_files/figure-latex/mermaid-figure-1.png}

If this chart suggests you need something other than the two sample
t-test or Mann-Whitney test, you need to go to the descriptions of that
test.

\section{Two sample t-test: the parametric
case}\label{two-sample-t-test-the-parametric-case}

In this exercise we find out how to run a two-sample \emph{t}-test, to
determine whether there is evidence to reject the hypothesis that two
samples are drawn from the same population.

\subsection{\texorpdfstring{When to use the two-sample
\emph{t}-test}{When to use the two-sample t-test}}\label{when-to-use-the-two-sample-t-test}

\begin{itemize}
\item
  It can be used when we have two independent samples of
  \textbf{numerical} (not ordinal) response data, and our question is
  whether the data provide evidence that the samples are drawn from
  different populations. Even when this criterion is met and the data
  are numerical and independent, the normality criterion described below
  still needs to be met. That apart, two common examples where we have
  two sets of data but we should not use the two sample \emph{t}-test
  are:

  \begin{itemize}
  \item
    If the data in your two samples are not independent because you have
    measured the same individual replicate before and after some event
    or treatment, then you should probably be using a \textbf{paired
    \emph{t}-test} instead. In this you don't have two samples, each
    comprising a separate and independent set of replicates. Instead,
    you have multiple pairs of values, one pair per replicate. The
    replicates are still assumed to be independent of each other
  \item
    If your response data are the answers to a Likert scale such as
    might be used in a survey then they are \emph{ordinal} in nature and
    not numerical, and you should probably be using the non-parametric
    equivalent of the two sample \emph{t}-test, which is variously known
    as the \textbf{Wilcoxon Rank Sum test} or as the \textbf{Mann
    Whitney U test}, or its paired sample version, if appropriate.
  \end{itemize}
\item
  It can be used when the data set is small. But not so small that there
  are no replicates. You \textbf{do} need replicates.
\item
  It can still be used when the data set is large.
\item
  It assumes that the data are drawn from a normally distributed
  population. There are various ways to test if this is plausibly the
  case, and you should try at least one of them, but with small samples,
  just where the \emph{t}-test is most useful, it can be difficult to
  tell. In the end we can also appeal to reason: is there good reason to
  suppose that the data would or would not be normally distributed?
\item
  When comparing the means of two samples, both samples should in
  principle have the same variance, which is a measure of the spread of
  the data, so in principle you need to check that this is at least
  \emph{approximately} the case, or have reason to suppose that it
  should be. \emph{However}, in an actual \emph{t}-test done using R,
  the Welch variant of the \emph{t}-test is carried out by default. This
  works even when the variances of the two sets are different, so in
  practice it is possible to ignore this equal variance requirement.
\item
  We only use it when we are comparing two samples, one for each of the
  two levels of a single factor. When we have samples for more than two
  levels and we use the \emph{t}-test to look for a difference between
  any two of them, it becomes increasingly likely, the more pairs of
  samples we compare, that we will decide that we have found a
  difference because we got a \emph{p}-value that was less than some
  pre-determined threshold (which could be anything, but is most often
  chosen to be 0.05) even if in reality there is none. This is the
  problem of high false positive rates arising from multiple pairwise
  testing and is where ANOVA comes in. \emph{t}-tests are only used to
  detect evidence for a difference between two groups, not more. ANOVAs
  (or their non-parametric equivalent) are used when we are looking for
  differences between more than two groups.
\end{itemize}

\subsection{Motivation and example}\label{motivation-and-example}

In our example we will consider the impact of pesticide use on the
masses of shells of garden snails (\emph{Cornu aspersum}), as measured
in gardens around a city, ten from randomly selected gardens that have
used a range of pesticides for at least two years and ten that are from
randomly selected gardens that have not ever used pesticides. We leave
aside here the issue of how those gardens were identified and how
randomisation was ensured.

\subsection{Questions and hypotheses}\label{questions-and-hypotheses}

Our \textbf{question} is:

\emph{Is there evidence for a difference between snail shell masses in
the gardens where pesticides were used compared to those where they were
not used?}

From which a suitable \textbf{null hypothesis} is:

\emph{There is no difference between shell masses in the gardens,
whether or not pesticides were used.}

and a suitable \textbf{alternate, two-sided hypothesis} is:

\emph{There is a difference between shell masses.}

\subsection{The data}\label{the-data}

Suppose we had our data arranged in a spreadsheet in three columns, one
giving the garden ID, G1 to G20, one telling us whether pesticides were
used in the garden, yes or no, and one telling us the masses in grams of
the snail shells from each garden. Afficioados of R will see that this
is `tidy' data. Each variable (ID, pesticide use, shell mass) occurs in
only one column, rather than being spread across several. It turns out
that this way of storing your data makes it much easier to analyse.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# there should be a \textquotesingle{}garden\_snails.csv\textquotesingle{} file in your data folder}

\NormalTok{filepath}\OtherTok{\textless{}{-}}\FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"garden\_snails.csv"}\NormalTok{)}
\NormalTok{snails}\OtherTok{\textless{}{-}}\FunctionTok{read\_csv}\NormalTok{(filepath)}

\CommentTok{\# if not, you should be able to get it from Mike\textquotesingle{}s github repo}

\CommentTok{\# file\_url \textless{}{-} "https://raw.githubusercontent.com/mbh038/r{-}workshop/refs/heads/gh{-}pages/data/garden\_snails.csv"}
\CommentTok{\# snails\textless{}{-}read\_csv(file\_url)}
\FunctionTok{head}\NormalTok{(snails,}\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 20 x 3
   garden.ID pesticide shell_mass_g
   <chr>     <chr>            <dbl>
 1 S1        Yes               1.37
 2 S2        Yes               1.15
 3 S3        Yes               0.73
 4 S4        Yes               0.65
 5 S5        Yes               1.03
 6 S6        Yes               1.8 
 7 S7        Yes               1.21
 8 S8        Yes               1.41
 9 S9        Yes               1.27
10 S10       Yes               1.08
11 S11       No                2   
12 S12       No                3.99
13 S13       No                1.99
14 S14       No                1.75
15 S15       No                2.81
16 S16       No                2.15
17 S17       No                1.87
18 S18       No                3.46
19 S19       No                2.8 
20 S20       No                2.89
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Is this a tidy data set?
\item
  Is the data in the \texttt{pesticide} column categorical?
\item
  If so, how many levels does it have and what are they?
\end{itemize}

\section{The Process}\label{the-process}

\subsection{Step One: Summarise the
data}\label{step-one-summarise-the-data}

With numerical data spread across more than one level of a categorical
variable, we often want summary information such as mean values and
standard errors of the mean for each level.

Here we will calculate the number of replicates, the mean and the
standard error of the mean for both levels of \texttt{pesticide} ie Yes
and No:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{snail.summary}\OtherTok{\textless{}{-}}\NormalTok{ snails }\SpecialCharTok{|\textgreater{}}
\FunctionTok{group\_by}\NormalTok{(pesticide) }\SpecialCharTok{|\textgreater{}}
\FunctionTok{summarise}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{(),}
          \AttributeTok{mean.mass =} \FunctionTok{mean}\NormalTok{(shell\_mass\_g),}
          \AttributeTok{se.mass =} \FunctionTok{sd}\NormalTok{(shell\_mass\_g)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{n}\NormalTok{()))}
\NormalTok{snail.summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
  pesticide     n mean.mass se.mass
  <chr>     <int>     <dbl>   <dbl>
1 No           10      2.57   0.236
2 Yes          10      1.17   0.105
\end{verbatim}

From these data, does it look as though there is evidence for a
difference between shell masses in the two types of garden? Clearly, the
snails in the ten gardens that did not use pesticide had a higher mean
shell mass than the ten from gardens that did use pesticide. But is this
a fluke? How precisely do we think these sample means reflect the truth
about the impact of the use of pesticides? That is what the standard
error column tells us. You can think of the standard error as being an
estimate of how far our sample means, drawn from just ten gardens of
each type are likely to differ from the true shell masses for all
gardens that did use pesticides and all gardens that did not.

Bottom line: the difference between the sample means is about ten times
the size of the standard errors of each. It really does look as though
snails shells in gardens where pesticides are not used are indeed
heavier than in gardens where pesticides are used.

\subsection{Step Two: Plot the data}\label{step-two-plot-the-data}

Remember, before we do any statistical analysis, it is almost always a
good idea to plot the data in some way. We can often get a very good
idea as to the answer to our research question just from the plots we
do.

In Figure~\ref{fig-histograms}, we will use \texttt{ggplot()} in R to
plot a histogram of ozone levels, one for each side of the city. We will
stack the histograms one above the other, all the better to help us spot
any differences between east and west.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{snails }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{shell\_mass\_g)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth=}\FloatTok{0.2}\NormalTok{,}\AttributeTok{fill=}\StringTok{"darkred"}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{pesticide,}\AttributeTok{ncol=}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{tests_for_difference_two_levels_files/figure-pdf/fig-histograms-1.pdf}}

}

\caption{\label{fig-histograms}Stacked histograms}

\end{figure}%

Instead of histograms, we could have drawn box plots, as in:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{snails }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{pesticide,}\AttributeTok{y=}\NormalTok{shell\_mass\_g))}\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Pestice use?"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Shell mass (g)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{tests_for_difference_two_levels_files/figure-pdf/fig-boxplot-1.pdf}}

}

\caption{\label{fig-boxplot}Side-byside box and whisker plots of the
distribution of values in each snail sample. The lower and upper edges
of each box show the 25th and 75th percentiles of each sample, and the
thick black line between them shows the median value ie the 50th
percentile}

\end{figure}%

or as a dot plot of the means with standard errors of the mean included
as error bars, as in Figure~\ref{fig-means-se}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# for this chart we will use the summary table that we created above.}

\NormalTok{snail.summary }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{pesticide,}\AttributeTok{y=}\NormalTok{mean.mass))}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size=}\DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin=}\NormalTok{mean.mass}\SpecialCharTok{{-}}\NormalTok{se.mass,}\AttributeTok{ymax=}\NormalTok{mean.mass}\SpecialCharTok{+}\NormalTok{se.mass),}\AttributeTok{width=}\FloatTok{0.1}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{4}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# try leaving this line out. What happens? Which is better?}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Pesticide use?"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Shell mass (g)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{tests_for_difference_two_levels_files/figure-pdf/fig-means-se-1.pdf}}

}

\caption{\label{fig-means-se}The data points show mean values, the error
bars show plus or minus one standard error of the mean}

\end{figure}%

\begin{itemize}
\item
  Do the data look as though they are inconsistent with the null
  hypothesis ?
\item
  In addition, do the data look as though each group is drawn from a
  normally distributed population? One of the types of graphs gives you
  no indication of that while the other two do. Which is the odd one
  out? Even when looking at the other two figures, when there are so few
  data it's kind of hard to tell, no?
\end{itemize}

Let's now do some stats.

\subsection{Step Three: Check the validity of the data - are the data
normally
distributed?}\label{step-three-check-the-validity-of-the-data---are-the-data-normally-distributed}

We can go about establishing this in three ways: using an analytical
test of normality, using a graphical method and by thinking about what
kind of data we have. Let's consider these in turn.

\subsubsection{Normality test - analytical
method}\label{normality-test---analytical-method}

There are several analytical tests one can run on a set of data to
determine if it is plausible that it has been drawn from a normally
distributed population. One is the Shapiro-Wilk test.

For more information on the Shapiro-Wilk test in R, type
\texttt{?shapiro.test} into the console window. For kicks, try it out on
the examples that appear in the help window (which is the bottom right
pane, Help tab). One example is testing a sample of data that explicitly
\emph{is} drawn from a normal distribution, the other tests a sample of
data that definitely \emph{is not}. What \emph{p}-value do you get in
each case? How closely do the histograms of each sample resemble a
normal distribution?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#first we create a data frame containing the two example data sets}
\NormalTok{example1}\OtherTok{\textless{}{-}}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{500}\NormalTok{, }\AttributeTok{mean =} \DecValTok{5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{3}\NormalTok{) }\CommentTok{\# first example from the help pane}
\NormalTok{example2}\OtherTok{\textless{}{-}}\FunctionTok{runif}\NormalTok{(}\DecValTok{500}\NormalTok{, }\AttributeTok{min =} \DecValTok{2}\NormalTok{, }\AttributeTok{max =} \DecValTok{4}\NormalTok{) }\CommentTok{\# second example from the help pane}

\NormalTok{df}\OtherTok{\textless{}{-}}\FunctionTok{tibble}\NormalTok{(}\AttributeTok{data=}\FunctionTok{c}\NormalTok{(example1,example2), }\AttributeTok{distribution=}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"example 1: normal"}\NormalTok{,}\DecValTok{500}\NormalTok{),}\FunctionTok{rep}\NormalTok{(}\StringTok{"example 2: not at all normal"}\NormalTok{,}\DecValTok{500}\NormalTok{)))}

\CommentTok{\# then we plot a histogram of each data set}
\FunctionTok{ggplot}\NormalTok{(df,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{data)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins=}\DecValTok{20}\NormalTok{,}\AttributeTok{fill=}\StringTok{"cornflowerblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{distribution) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{tests_for_difference_two_levels_files/figure-pdf/unnamed-chunk-5-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# and finally we run a Shapiro{-}Wilk normality test on each data set}
\FunctionTok{shapiro.test}\NormalTok{(example1) }\CommentTok{\# 100 samples drawn from a normally distributed population}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  example1
W = 0.99817, p-value = 0.8794
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(example2) }\CommentTok{\# 100 samples drawn from a uniformly (ie NOT normally) distributed population}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  example2
W = 0.95323, p-value = 1.744e-11
\end{verbatim}

For the examples above, we see that Shapiro-Wilk test gave a high
\emph{p}-value for the data that we knew \emph{were} drawn from a normal
distribution, and a very low \emph{p}-value for the data that we knew
were not.

The Shapiro-Wilk test tests your data against the null hypothesis that
it is drawn from a normally distributed population. It gives a
\emph{p}-value which, as always, is the probably of you having data as
far from normality, or further, as yours are if the null hypothesis were
true. If the \emph{p}-value is less than 0.05 then we reject the null
hypothesis and cannot suppose our data is drawn froma normally
distributed population. In that case we would have to ditch the
\emph{t}-test for a difference, and choose another difference test in
its place that could cope with data that was not normally distributed.
For a two-sample \emph{t}-test such as we are hoping to use here, the
so-called non-parametric alternative that we could use instead is the
Wilcoxon Rank Sum test, often called the Mann-Whitney U test.

Why don't we do that in the first place, I hear you ask? Why bother with
this finicky \emph{t}-test that requires that we go through the faff of
testing the data for normality before we can use it? The answer is that
it is more powerful than other, so-called non-parametric tests that
\emph{can} cope with non-normal data. It is more likely than they are to
spot a difference if there really is a difference. So if we can use it,
that is what we would rather do.

So, onwards, let's do the Shapiro-Wilk test on our data

We want to test each garden group for normality, so we group the data by
location as before and and then summarise, this time asking for the
\emph{p}-value returned by the Shapiro-Wilk test of normality.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{snails }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(pesticide) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\StringTok{\textquotesingle{}Shapiro{-}Wilk p{-}value\textquotesingle{}}\OtherTok{=}\FunctionTok{shapiro.test}\NormalTok{(shell\_mass\_g)}\SpecialCharTok{$}\NormalTok{p.value)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 2
  pesticide `Shapiro-Wilk p-value`
  <chr>                      <dbl>
1 No                         0.223
2 Yes                        0.854
\end{verbatim}

For both groups the \emph{p}-value is more than 0.05, so at the 5\%
significance level we cannot reject the null hypothesis that the data
are normally distributed, so we can go on and use the \emph{t}-test.
Yay!

\subsubsection{Graphical methods - the quantile-quantile or QQ
plot.}\label{graphical-methods---the-quantile-quantile-or-qq-plot.}

Confession: I don't normally bother with numerical tests for normality
such as Shapiro-Wilk. I usually use a graphical method instead.

We have already seen two ways of plotting the data that might help
suggest whether it is plausible that the data are drawn from normally
distributed populations. Histograms and box plots both indicate how data
is distributed, and for normally distributed data both would be
symmetrical. Well, they would be, more or less, if the data set was
large enough but for small data sets it can be quite hard to tell from
either type of plot whether the data are drawn from a normally
distributed population.

A better type of plot for making this judgement call is the
quantile-quantile or `qq' plot which basically compares the distribution
of your data to that of a normal distribution. If your data are
approximately normally distributed then a qq plot will give a
straight(-ish) line. Even with small data sets, this is usually easy to
spot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{snails }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample=}\NormalTok{shell\_mass\_g)) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{(}\AttributeTok{colour=}\StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{pesticide) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{tests_for_difference_two_levels_files/figure-pdf/unnamed-chunk-7-1.pdf}}

Nothing outrageously non-linear there, so that also suggests we can
safely use the \emph{t}-test.

For an overview of how normally distributed and non-normally distributed
data looks when plotted in histograms, box plots and quantile-quantile
plots, see \href{https://rpubs.com/mbh038/725314}{this review}

\subsubsection{The `thinking about the data' normality
test}\label{the-thinking-about-the-data-normality-test}

As you might have guessed, this isn't a test as such, but a suggestion
that you think about what kind of data you have: is it likely to be
normally distributed within its subgroups or not? If the data are
numerical values of some physical quantity that is the result of many
independent processes, and if the data are not bounded on either side
(say by 0 and 100 as for exam scores) then it is quite likely that that
they are. If they are count data, or ordinal data, then it is quite
likely that they are not.

This way of thinking may be all you can do when data sets are very small
and any of the more robust tests for normality presented here leave you
not much the wiser.

\subsection{\texorpdfstring{Do the actual two-sample
\emph{t}-test}{Do the actual two-sample t-test}}\label{do-the-actual-two-sample-t-test}

So, it looks as though it is plausible that the data are drawn from
normal distributions. That means we can go on to use a parametric test
such as a two sample \emph{t}-test and have confidence in its output.

If we were doing this in R we could use the \texttt{t.test()} function
for this (other functions are available!). This needs to be given a
formula and a data set as arguments. Look up \texttt{t.test()} in R's
help documentation, and see if you can get the \emph{t}-test to tell you
whether there is a significant difference between ozone levels in the
east and in the west of the city.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(shell\_mass\_g}\SpecialCharTok{\textasciitilde{}}\NormalTok{pesticide,}\AttributeTok{data=}\NormalTok{snails)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Welch Two Sample t-test

data:  shell_mass_g by pesticide
t = 5.4172, df = 12.442, p-value = 0.0001372
alternative hypothesis: true difference in means between group No and group Yes is not equal to 0
95 percent confidence interval:
 0.8397234 1.9622766
sample estimates:
 mean in group No mean in group Yes 
            2.571             1.170 
\end{verbatim}

\subsection{\texorpdfstring{Interpret the output of the
\emph{t}-test.}{Interpret the output of the t-test.}}\label{interpret-the-output-of-the-t-test.}

Study the output of the \emph{t}-test. Here are some questions to ask
yourself.

\begin{itemize}
\tightlist
\item
  What kind of test was carried out?

  \begin{itemize}
  \tightlist
  \item
    \emph{A Welch two sample t-test}
  \end{itemize}
\item
  What data was used for the test?

  \begin{itemize}
  \tightlist
  \item
    \emph{The snail shell mass in g (the output variable) and pesticide
    use (the explanatory variable)}
  \end{itemize}
\item
  What is the test statistic of the data?

  \begin{itemize}
  \tightlist
  \item
    \emph{This is t = 5.4172.}
  \end{itemize}
\item
  How many degrees of freedom were there? This number is the number of
  independent pices of information that were used to calculate the final
  result. It is usually one, two, or three or so less than the number of
  data points. Don't overthink it at this stage, especially not the fact
  that here it is not an integer.

  \begin{itemize}
  \tightlist
  \item
    \emph{df = 12.442}
  \end{itemize}
\item
  What is the p-value?

  \begin{itemize}
  \tightlist
  \item
    \emph{p = 0.0001372. You would most likely report this as p
    \textless{} 0.001}
  \end{itemize}
\item
  What does the p value mean?

  \begin{itemize}
  \tightlist
  \item
    \emph{It is the likelihood of seeing a difference between sample
    means as large or larger than the one we found if in fact pesticides
    made no difference to snail shell mass.}
  \end{itemize}
\item
  What is the confidence interval for the difference between shell
  masses in gardens that use pesticides and in gardens that do not? Does
  it encompass zero? Remember that the confidence interval gives the
  range of values within which the true difference between mean shell
  masses might reasonably lie, given the data. If that range includes
  zero then the test is telling is that zero is a plausible value for
  the difference, and hence that we cannot reject the null hypothesis.

  \begin{itemize}
  \tightlist
  \item
    \emph{The 95\% confidence interval has lower bound 0.8397 and upper
    bound 1.962.}
  \end{itemize}
\item
  Is there sufficient evidence to reject the null hypothesis?

  \begin{itemize}
  \tightlist
  \item
    \emph{Yes. We see this in two ways. First the p value is much less
    than 0.05 and second, the 95\% confidence interval does not
    encompass zero. In a way, the confidence interval is giving us more
    information than the p-value, since not only can we deduce whether
    there is evidence for a significant difference, we can also see how
    big that difference is and how precisely we know it.}
  \end{itemize}
\item
  What does the word `Welch' tell you - Google it or look it up in the
  help for \texttt{t.test()}.

  \begin{itemize}
  \tightlist
  \item
    \emph{It tells us that a variant of the t-test is being used in
    which it does not matter if the two. samples have difference
    variances (spreads).}
  \end{itemize}
\end{itemize}

\section{\texorpdfstring{Other examples where a two sample \emph{t}-test
might be
used.}{Other examples where a two sample t-test might be used.}}\label{other-examples-where-a-two-sample-t-test-might-be-used.}

Remember that \emph{t}-tests in general are used when you have
independent samples with multiple replicates drawn from populations
corresponding to \textbf{two} levels of some factor (eg north coast,
south coast; this beach, that beach; polluted place, clean place etc)
and you have measured something numerical, like a length or a mass,
temperature or concentration. You still have to do the tests for
normality described above, but these are the basic criteria.

\subsection{\texorpdfstring{Can you think of examples of where you might
use a two-sample
\emph{t}-test?}{Can you think of examples of where you might use a two-sample t-test?}}\label{can-you-think-of-examples-of-where-you-might-use-a-two-sample-t-test}

Here are a few suggestions:

\begin{itemize}
\item
  Is there a difference between the flight initiation distance of
  redstarts when confronted by dogs compared to when they are confronted
  by drones?
\item
  Is the nitrate concentration of water in a river below a beaver dam
  different from the nitrate content above that dam?
\end{itemize}

Can you think of another example?

\section{\texorpdfstring{What if I can't use a two sample
\emph{t}-test?}{What if I can't use a two sample t-test?}}\label{what-if-i-cant-use-a-two-sample-t-test}

\begin{itemize}
\item
  Assuming you have two independent samples, this might be because one
  or both sets failed the normality criterion, or your data are ordinal.
  In that case the likely alternative is the \textbf{non-parametric}
  equivalent of the \emph{t}-test, variously known as the Wilcoxon Rank
  Sum test or the Mann Whitney U test.
\item
  If your data are in fact sets of \textbf{paired values}, for example
  because you measured some attribute of the same individuals before and
  after some treatment, or at two points in time, then you need to use
  the paired \emph{t}-test.
\item
  If you only have \textbf{one sample} of replicates and want to compare
  its mean value to a threshold, then you use a one sample t-test. You
  might do this, for example, if you had collected sediment samples from
  an estuary, measured the concentration in those samples of some
  pollutant such as pathogens from sewage, or phosphates from farm
  runoff, and then wanted to see if the water was compliant with water
  quality thresholds as dictated by, say, the Water Framework Directive.
\end{itemize}

\section{The non-parametric case}\label{the-non-parametric-case}

A common scenario is that we have two sets of measurements, and we want
to see if there is evidence that they are drawn from different
populations. For some data types we can use a \emph{t}-test to do this,
but for others we cannot.

A \emph{t}-test requires in particular that the two sets of data are
normally distributed around their respective means. With \emph{ordinal}
data this makes no sense. The mean is undefined as a concept for such
data.

To see this , reflect that for a collection \(X\) of numerical data,
say, 5, 3, 3, 4, and 5 we would calculate the mean as:

\[
\bar{X} = \frac{5+3+3+4+5}{5} = \frac{20}{5}=4
\]

But trying doing the same to five responses of a Likert scale survey.
Say the responses you had to five Likert items (individual questions)
were ``strongly disagree'', ``strongly agree'', ``mildly disagree'',
``strongly disagree'' and ``don't care either way''. If you tried to
calculate a `mean' response you would be attempting to add up all these
responses and to divide the `sum' by five, like this:

\[\text{mean response}=\frac{\text{stongly disagree}+\text{strongly agree}+\text{mildly disagree}+\text{stongly disagree}+\text{don't care either way}}{5} = ?
\] This sum makes no sense, I hope you will agree. It makes no sense,
not because we are using words to describe our responses, but because,
as these are ordinal data, we do not know the size of the gaps between
the different points on the scale. Is the difference in agreement
between the lowest two, ``strongly disagree'' and ``midly disagree'' the
same as the gap between the highest two, ``mildly agree'' and ``strongly
agree''? We don't know, mainly because `agreement' is not something that
can be measured easily using something like a weighing machine. And if
we don't know, then we shouldn't really be adding these responses up or
dividing them by anything.

Nevertheless, ordinal data are very common, since they are typically
what is generated by survey data, where for example repondents may
answer a series of questions (`items'), each with typically five
possible responses, but maybe more or fewer, these responses being
ordinal in the sense that there is a definite order to them. They might
encompass responses like those above, say, or something similar like
``very unhappy'' to ``very happy''. They are also common in clinical and
veterinary practice where ordinal pain scores are widely used - patients
being asked (if they are human) or assessed as to their level of pain on
a scale of 1-10, for example. Note that even if the pain value is
recorded as a number it is actually a label, that could just as well
have been recorded as one of a series of letters, A, B, C etc or emojis,
or any symbol you like. You can't take the average of a set of faces!

Thus, formally, we need another kind of test for a difference. Broadly,
we need to use some form of \emph{non-parametric} test where we do not
assume that the data has any form of distribution, and where, often, we
do not use the actual values of the measurements in our dataset but
instead use only their \emph{ranks}. The smallest value would be given
rank 1, the next rank 2 and so on.

There are many non-parametric tests out there. Here we will look at only
one - the \textbf{Wilcoxon Rank Sum Test}, often referred to as a
\textbf{Mann-Whitney U} test for a difference. We can use this for the
scenario we have painted above, where we have two sets of data and we
wish to know if these provide evidence that the populations from which
the samples have been drawn are in fact different.

\subsection{Example}\label{example}

This example uses actual data gathered by a student at Newquay
University Centre.

The student wished to assess peoples' sense of wellbeing using two
different sets of questions designed to assess this. The scales chosen
were the Warwick--Edinburgh Mental Well-being Scale (WEMWBS) and the New
Ecological Paradigm (NEP) Scale. The student wished in particular to
determine whether this sense of well-being was affected by whether a
person often and actively frequented the coast and made it and the sea a
substantive part of their life in one way or another. ie to find out
whether there was evidence to support the notion that it could be good
for your mental wellbeing to be by the sea and to make it part of your
life.

Each scale used consists of 15 questions or `Likert items', each of
which is answered on a 5 point ordinal scale, where a score of 1
indicates lowest wellbeing and a score of 5 indicates highest wellbeing.
Thus each respondent could score anything from 15 to 75.

The student got responses from 374 people, 86 of whom were not
``marine'' users, while the other 288 say that they \emph{were} marine
users. The total scores from each respondent were recorded for each type
of survey and stored in the file \texttt{wellness.csv} which you should
find on the module Moodle site / Teams page. Please put this file in the
\texttt{data} folder of your R project.

\subsection{Script}\label{script}

Code chunks for a script to carry out the analysis of this data are
provided below. To use them you should create a new Quarto document
using File/New File/Quarto document, from which you delete all the
exemplar material below the yaml section at the top. The first few
chunks of this script carry out the same old-same old that we see in
script after script: load packages, load data, summarise data , plot
data. Copy and paste any chunks you want to use into your own script
then adapt them as necessary.

You can run your script by running each chunk in sequence, which you do
by clicking the green arrow in the top-right corner of each chunk.

Try also to `Render' the script by clicking on the Render button at the
top of the script pane.

\subsubsection{Load packages}\label{load-packages-2}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(here)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Load data}\label{load-data-1}

Our data set is in a \texttt{.csv} file which we have placed in the data
folder within our project folder.

Note that this data set has been stored in `tidy' form: each variable
appears in only column, and each observation appears in only one row.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filepath}\OtherTok{\textless{}{-}}\FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"wellness.csv"}\NormalTok{)}
\NormalTok{wellness}\OtherTok{\textless{}{-}}\FunctionTok{read\_csv}\NormalTok{(filepath)}
\FunctionTok{glimpse}\NormalTok{(wellness)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 748
Columns: 4
$ id          <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,~
$ scale       <chr> "WEMWBS", "WEMWBS", "WEMWBS", "WEMWBS", "WEMWBS", "WEMWBS"~
$ marine      <chr> "Yes", "No", "No", "No", "Yes", "Yes", "Yes", "No", "Yes",~
$ total_score <dbl> 48, 51, 37, 39, 38, 40, 54, 39, 54, 39, 51, 50, 49, 51, 54~
\end{verbatim}

\subsubsection{Summarise the data}\label{summarise-the-data-1}

We'll calculate the median score (50th percentile) and the 25th and 75th
percentile scores. For ordinal data, these summary statistics are well
defined, whereas means and standard deviations are not.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wellness }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(scale,marine) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{median.score=}\FunctionTok{median}\NormalTok{(total\_score),}\AttributeTok{iqr\_25=}\FunctionTok{quantile}\NormalTok{(total\_score,}\FloatTok{0.25}\NormalTok{),}\AttributeTok{iqr\_75=}\FunctionTok{quantile}\NormalTok{(total\_score,}\FloatTok{0.75}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 5
# Groups:   scale [2]
  scale  marine median.score iqr_25 iqr_75
  <chr>  <chr>         <dbl>  <dbl>  <dbl>
1 WEMWBS No             42.5   36       49
2 WEMWBS Yes            47     41       51
3 nep    No             51     48.2     53
4 nep    Yes            50     48       53
\end{verbatim}

\subsubsection{Plot the data}\label{plot-the-data-1}

Box plots are particularly suitable for ordinal data since they show the
25th and 75th percentiles of the data (the bottom and top of the box)
plus the 50th percentile aka the median, which is the thick line across
each box. All of these percentiles are well defined quantities for
ordinal data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wellness }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ scale,}\AttributeTok{y =}\NormalTok{ total\_score,}\AttributeTok{fill =}\NormalTok{ marine)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Likert Scale"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Wellbeing Score"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{tests_for_difference_two_levels_files/figure-pdf/unnamed-chunk-12-1.pdf}}

Looking at the plot, what do you think each scale suggests about whether
proximity to the sea makes a difference to wellbeing?

\subsubsection{Wilcoxon-Mann-Whitney U
test}\label{wilcoxon-mann-whitney-u-test}

First let's pull out the scores as measured by the WEMWBS scale and do a
test for a difference between the scores of marine users and those of
non-marine users. We can use the \texttt{filter()} function to do this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WEMWBS}\OtherTok{\textless{}{-}}\NormalTok{wellness }\SpecialCharTok{|\textgreater{}} \FunctionTok{filter}\NormalTok{(scale}\SpecialCharTok{==}\StringTok{"WEMWBS"}\NormalTok{) }\CommentTok{\# save the WEMWBS data into a data frame called WEMWBS}
\FunctionTok{glimpse}\NormalTok{(WEMWBS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 374
Columns: 4
$ id          <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,~
$ scale       <chr> "WEMWBS", "WEMWBS", "WEMWBS", "WEMWBS", "WEMWBS", "WEMWBS"~
$ marine      <chr> "Yes", "No", "No", "No", "Yes", "Yes", "Yes", "No", "Yes",~
$ total_score <dbl> 48, 51, 37, 39, 38, 40, 54, 39, 54, 39, 51, 50, 49, 51, 54~
\end{verbatim}

Now let's do the actual Wilcoxon-Mann-Whitney U test:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wilcox.test}\NormalTok{(total\_score}\SpecialCharTok{\textasciitilde{}}\NormalTok{marine,}\AttributeTok{data=}\NormalTok{WEMWBS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Wilcoxon rank sum test with continuity correction

data:  total_score by marine
W = 9077.5, p-value = 0.0001687
alternative hypothesis: true location shift is not equal to 0
\end{verbatim}

The null hypothesis of this test is that there is no evidence that the
data are drawn from different populations. In this case, the
\emph{p}-value is very small, so we can confidently reject that null
hypothesis and assert that there is evidence, according to the WEMWBS
scale that marine use makes a difference to peoples' sense of wellbeing.

Does it make it worse or better? - we can see from the summary table and
from the box plot that higher scores are associated with those people
who were exposed to a marine environment.

We might report this results as follows, first using a plain English
statement of the main finding, and then reporting the type of test use,
the value of the test statistic that it calculated and the p value. In
this case, because the p value is so small, we would not report its
exact value, but simply give an indication of how small it is:

\emph{We find evidence, according to the WEMWBS scale, that the
wellbeing score is 4.5 or about 10\% higher for people exposed to a
marine environment (Mann-Whitney U, W = 9077.5, p \textless{} 0.001).}

\subsection{Exercise}\label{exercise}

Adapt the code of the last chunk so that you can do the same test but
for data as recorded by the nep scale

\subsection{When should I use this Wilcoxon-Mann-Whitney U
test?}\label{when-should-i-use-this-wilcoxon-mann-whitney-u-test}

The test we have used here is an example of a \emph{non-parametric}
test. This means that it does not assume that the data follow a known
mathematical distribution and, further, that it can be used with ordinal
data.

We used the Mann-Whitney U test in particular because we were testing
for a difference, and because the factor of interest - marine exposure -
had just two levels - Yes or No.~This test is only suitable when there
are just two levels, so you can think of it as as a non-parametric
alternative to a t-test.

In another setting where we still had just one factor (eg zone of a
rocky shore) but there were more than two levels (eg low, mid and high
zones of the shore) and we decided that we wanted to do a non-parametric
test for a difference, then we would probably use the
\textbf{Kruskal-Wallis} test, which you can think of as the
non-parametric alternative to a one-way ANOVA.

In this example we used the Mann-Whitney U test because the data were
ordinal and thus not suitable to use with a parametric test (but see
below!). Where we can, we usually try to use a parametric test as they
are more powerful than their non-parametric equivalents, meaning, if
there is a trend or a difference in the data, they are better able to
detect it. However those parametric tests (t-test, ANOVA, pearson
correlation, PCA, GLM to name but a few) typically require not only that
the data are numerical but also a host of other things, including that
they follow a particular distribution, usually (but not always) the
normal distribution, and this is often not the case with real biological
data. Often, especially with count data, there are lots of zeros, or the
data distribution is heavily skewed, usually to the right. In these
cases, providing the data are independent of each other, we can usually
still use a non-parametric test such as we have here. it might not be
the most powerful test we can use (GLMs are typically way better if you
can use them), but it will work.

\subsection{Hang on!}\label{hang-on}

The eagle eyed among you may have spotted a massive flaw in the line of
argument presented above. We said that ordinal data can't be added up,
can't be used to calculate averages and so on. Thus we can't run
parametric tests on them and have to look for alternatives, namely,
non-parametric tests.

And yet, these non-parametric tests are usually run on the output of
Likert \emph{scales} such as we have considered here, where for each
person we have a number of Likert \emph{Items} (ie individual questions)
that together constitute the \emph{scale}, that each generate a score
1-5, then we \emph{add up the scores} to get a total score. But that
means we are adding up ordinal data!!!

It turns out that you actually get much the same results with Likert
scale data if you analyse them using supposedly inappropriate parametric
tests such as a 2-sample \emph{t}-test as you do if you use a
non-parametric test such as the one we considered here, the Mann-Whitney
test.

A study by De Winter and Dodou (2010) shows this convincingly.

de Winter, J. F. C., \& Dodou, D. (2010). Five-Point Likert Items: t
test versus Mann-Whitney-Wilcoxon (Addendum added October 2012).
Practical Assessment, Research, and Evaluation,15, 1--16.
https://doi.org/10.7275/bj1p-ts64

For an enlightening discussion of this paper, see
\href{https://statisticsbyjim.com/hypothesis-testing/analyze-likert-scale-data/}{this
blog by Jim Frost}

\section{Paired data}\label{paired-data}

Often one has a sample of replicated data where each element has a
counterpart in another matched sample - paired data. A common scenario
for this is when there are data for the same individual at two different
points in time, for example before and after some event such as the
application of a treatment.

In order to determine whether there is a difference between the two
sets, one should take the paired aspect into account and not simply
match the whole before-set against the whole after-set without doing
this. That would be to throw away the information whereby there is
likely to be a greater degree of correlation between the responses of an
individual before and after the event than there is between any randomly
chosen pairs of individuals before and after the event.

\subsection{Which test: paired t-test or Wilcoxon signed rank
test?}\label{which-test-paired-t-test-or-wilcoxon-signed-rank-test}

There is a choice between at least two tests: the parametric paired
t-test and the non-parametric Wilcoxon signed rank test. Ideally one
would use the t-test since it is more powerful than the Wilcoxon test.
This means several things, but in particular it means that, all else
being equal, it can detect a small difference with higher probability
than the Wilcoxon test can.

\subsection{The paired t-test}\label{the-paired-t-test}

Where the data are numerical (ie not ordinal) and where the before and
after data are both normally distributed around their respective mean
values one would use the \emph{paired t-test} in this scenario. One can
test for normality using either a test such as the Shapiro-Wilk test, or
graphically using either a histogram, a box plot, or (best), a
quantile-quantile plot.

\subsection{The Wilcoxon Signed Rank
test}\label{the-wilcoxon-signed-rank-test}

The t-test, an example of a so-called parametric test, is actually
pretty robust against departures from normality, but where one doubts
its validity due to extreme non-normality or for other reasons such as
the ordinal nature of the data, the Wilcoxon signed rank test is a
useful non-parametric alternative. It is called non-parametric because
it does not make any assumption about the distribution of the data
values. It only uses their ranks, where the smallest value gets rank 1,
the next smallest gets rank 2, and so on.

So, you typically use this test when you would like to use the paired
t-test, but you cannot because one or both of the data sets is way off
being normally distributed or is ordinal.

\subsubsection{Null Hypotheses}\label{null-hypotheses}

In both the t-test and the Wilcoxon signed rank tests, the null
hypothesis is the usual `nothing going on', `there is no difference'
scenario, but there is a subtle difference between them that reflects
the different information that they use. In the Wilcoxon signed rank
test the null is that the difference between the \emph{medians} of pairs
of observations is zero. This is different from the null hypothesis of
the paired t--test, which is that the difference between the
\emph{means} of pairs is zero.

\subsubsection{Test output}\label{test-output}

Both tests will give a p value. This is the probability that the mean
(t-test) or median (Wilcoxon signed rank) paired differences between the
corresponding before and after sample elements would be equal to or
greater than it actually is for the data if the null hypothesis were
true. If the p value is less than some pre-decided `significance level',
usually taken to be 0.05, then we reject the null hypothesis. If it is
not, then we fail to reject the null hypothesis.

\subsection{Example}\label{example-1}

We will use as an example a data set from Laureysens et al.~(2004) that
has measurements of metal content in the wood of 13 poplar clones
growing in a polluted area, once from each clone in August and once
again from each of them in November. The idea was to investigate the
extent to which poplars could absorb metals from the soil and thus be
useful in cleaning that up. Under a null hypothesis, there would be no
change in the metal concentrations in the plant tissue of each clone
between August and November. Under an alternate hypothesis, there would
be.

Laureysens, I. et al.~(2004) `Clonal variation in heavy metal
accumulation and biomass production in a poplar coppice culture: I.
Seasonal variation in leaf, wood and bark concentrations', Environmental
Pollution, 131(3), pp.~485--494. Available at:
https://doi.org/10.1016/j.envpol.2004.02.009.

Concentrations of aluminum (in micrograms of Al per gram of wood) are
shown below.

\textbf{Load packages}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{ (tidyverse)}
\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{library}\NormalTok{(cowplot) }\CommentTok{\# to make the plots look nice}
\end{Highlighting}
\end{Shaded}

\textbf{Load data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filepath }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"poplars{-}paired\_np.csv"}\NormalTok{)}
\NormalTok{poplars }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(filepath,}\AttributeTok{show\_col\_types =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{head}\NormalTok{(poplars,}\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 13 x 4
      ID Clone          August November
   <dbl> <chr>           <dbl>    <dbl>
 1     1 Balsam_Spire      8.1     11.2
 2     2 Beaupre          10       16.3
 3     3 Hazendans        16.5     15.3
 4     4 Hoogvorst        13.6     15.6
 5     5 Raspalje          9.5     10.5
 6     6 Unal              8.3     15.5
 7     7 Columbia_River   18.3     12.7
 8     8 Fritzi_Pauley    13.3     11.1
 9     9 Trichobel         7.9     19.9
10    10 Gaver             8.1     20.4
11    11 Gibecq            8.9     14.2
12    12 Primo            12.6     12.7
13    13 Wolterson        13.4     36.8
\end{verbatim}

\textbf{Plot the data}

Before we do any test on some data to find evidence for a difference or
a trend, it is a good idea to plot the data. This will reveal whatever
patterns there are in the data and how likely they are to reveal a truth
about the population from which they have been drawn.

\textbf{Tidy the data}

In this case there is work to do before we can plot the data. The
problem is that the data is `untidy'. The two levels of the factor
\texttt{month} are spread across two columns, August and November. For
plotting purposes it will be useful to `tidy' the data so that there is
only one column containing both levels of \texttt{month} and another
containing the aluminium concentrations. The function
\texttt{pivot\_longer()} can do this for us:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poplars\_tidy }\OtherTok{\textless{}{-}}\NormalTok{ poplars }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_longer}\NormalTok{ (August}\SpecialCharTok{:}\NormalTok{November,}\AttributeTok{names\_to=}\StringTok{"month"}\NormalTok{,}\AttributeTok{values\_to=}\StringTok{"Al\_conc"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(poplars\_tidy,}\DecValTok{8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 8 x 4
     ID Clone        month    Al_conc
  <dbl> <chr>        <chr>      <dbl>
1     1 Balsam_Spire August       8.1
2     1 Balsam_Spire November    11.2
3     2 Beaupre      August      10  
4     2 Beaupre      November    16.3
5     3 Hazendans    August      16.5
6     3 Hazendans    November    15.3
7     4 Hoogvorst    August      13.6
8     4 Hoogvorst    November    15.6
\end{verbatim}

Now we can plot the data as a box plot, with one box for August and one
for November ie one for each level of the factor \texttt{month}. Had we
not first tidied the data, we could not have done this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poplars\_tidy }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ month, }\AttributeTok{y =}\NormalTok{ Al\_conc, }\AttributeTok{fill =}\NormalTok{ month, }\AttributeTok{colour =}\NormalTok{ month)) }\SpecialCharTok{+} 
  \CommentTok{\# alpa (= opacity) \textless{} 1 in case any points are on top of each other}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{outlier.size=}\DecValTok{0}\NormalTok{,}\AttributeTok{alpha=}\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \CommentTok{\# group = ID makes the lines join elements of each pair}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{group=}\NormalTok{ID),}\AttributeTok{colour =} \StringTok{"grey60"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Month"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Al conc.(mu g Al / g wood)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{tests_for_difference_two_levels_files/figure-pdf/unnamed-chunk-18-1.pdf}}

Does it look as though the difference between the medians could
plausibly be zero for the population from which these samples were
drawn? Or, put another way, if it was zero, how big a fluke would this
sample be? That is what the p value actually tells us.

\subsection{Two sample paired t-test}\label{two-sample-paired-t-test}

\textbf{Check for normality of differences}

Before we use the t-test, we need to check that it is OK to do so. This
means checking whether the paired differences are plausibly drawn from a
normal distribution centred on zero.

The null hypothesis of the Shapiro-Wilk test is that the data set given
to it \emph{is} plausibly drawn from a normally distributed population.
So let us give our sample of paired differences:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(poplars}\SpecialCharTok{$}\NormalTok{August}\SpecialCharTok{{-}}\NormalTok{poplars}\SpecialCharTok{$}\NormalTok{November)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  poplars$August - poplars$November
W = 0.92667, p-value = 0.3081
\end{verbatim}

The p value is very high. Thus we do not reject the null hypothesis and
we can reasonably assume that the differences between the August and
November aluminium concentrations in the sample could plausibly have
been drawn from a normally distributed population, despite the outlier
value in the November sample. Thus we can reasonably test for difference
using a paired t-test.

\textbf{The actual t-test}

We can do this in R using the function \texttt{t.test()}, where we give
to the function both the August and the November data, knowing that each
August value has a counterpart November value, and we set the argument
\texttt{paired} to \texttt{TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(poplars}\SpecialCharTok{$}\NormalTok{August, poplars}\SpecialCharTok{$}\NormalTok{November, }\AttributeTok{paired =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Paired t-test

data:  poplars$August and poplars$November
t = -2.3089, df = 12, p-value = 0.03956
alternative hypothesis: true mean difference is not equal to 0
95 percent confidence interval:
 -9.5239348 -0.2760652
sample estimates:
mean difference 
           -4.9 
\end{verbatim}

All parts of the output have meaning and are useful, but here we will
focus on just two:

\begin{itemize}
\tightlist
\item
  the p value is equal to 0.040. Hence, if we have chosen the usual
  significance value of 0.05, we can take this to mean that there is
  evidence of a significant difference between the August and November
  values.
\item
  the lower and upper bounds of the 95\% confidence interval are (-9.52,
  -0.28). YOu can think of this interval as the range of values within
  which the difference can plausibly lie, at the 95\% confidence level.
  The key thing is that this range does not encompass zero. This means
  that we can be confident at the 95\% level that there is a non-zero
  change on going from August to November, and, in particular, that the
  August value is lower than the November value.
\end{itemize}

\subsection{The non-parametric alternative: The Wilcoxon signed rank
test}\label{the-non-parametric-alternative-the-wilcoxon-signed-rank-test}

To be safe, because of that outlier, let us test for difference using
the Wilcoxon signed rank test. In R this is done using the function
\texttt{wilcox.test()}, with the argument \texttt{paired} set to
\texttt{TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wilcox.test}\NormalTok{(poplars}\SpecialCharTok{$}\NormalTok{August, poplars}\SpecialCharTok{$}\NormalTok{November, }\AttributeTok{paired =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Wilcoxon signed rank exact test

data:  poplars$August and poplars$November
V = 16, p-value = 0.03979
alternative hypothesis: true location shift is not equal to 0
\end{verbatim}

We see that the conclusion (in this case) is the same.

\subsection{Relation to one-sample paired
test}\label{relation-to-one-sample-paired-test}

The two-sample paired tests as we have done above are the same as doing
a one-sample test to see if the differences between the August and
November paired values is different from zero. This is true whether we
do a t-test or a Wilcoxon signed rank test.

In either case, the first argument is the vector of differences, and the
second \texttt{mu} is the threshold value against which we want to
compare those differences, in this case zero.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(poplars}\SpecialCharTok{$}\NormalTok{August }\SpecialCharTok{{-}}\NormalTok{ poplars}\SpecialCharTok{$}\NormalTok{November, }\AttributeTok{mu =} \DecValTok{0}\NormalTok{, }\AttributeTok{data =}\NormalTok{ poplars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    One Sample t-test

data:  poplars$August - poplars$November
t = -2.3089, df = 12, p-value = 0.03956
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 -9.5239348 -0.2760652
sample estimates:
mean of x 
     -4.9 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wilcox.test}\NormalTok{(poplars}\SpecialCharTok{$}\NormalTok{August }\SpecialCharTok{{-}}\NormalTok{ poplars}\SpecialCharTok{$}\NormalTok{November, }\AttributeTok{mu =} \DecValTok{0}\NormalTok{, }\AttributeTok{data =}\NormalTok{ poplars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Wilcoxon signed rank exact test

data:  poplars$August - poplars$November
V = 16, p-value = 0.03979
alternative hypothesis: true location is not equal to 0
\end{verbatim}

Note that the output from both these one-sample tests, where the one
sample is the vector of differences and the threshold with which it is
compared is zero, is exactly the same as the output of the two-sample
tests where the two samples were the vectors between which we were
interested in detecting a difference, ie the August and November values.
This is not surprising since the two cases are just two ways of doing
exactly the same thing, which is to ask if there is evidence from the
sample for a difference in the population between the August and
November concentrations of aluminium.

\chapter{Analysis of Variance aka
ANOVA}\label{analysis-of-variance-aka-anova}

Material used from Chapter One of Grafen and Hails: Modern Statistics
for the Life Sciences

\section{What is ANOVA?}\label{what-is-anova}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fertilizer }\OtherTok{\textless{}{-}}\NormalTok{ fertilizer }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{FERTIL=}\FunctionTok{as.factor}\NormalTok{(FERTIL))}
\end{Highlighting}
\end{Shaded}

\subsection{The basic principles of
ANOVA}\label{the-basic-principles-of-anova}

In a simple case we consider the comparison of three means. This is done
by the analysis of variance (ANOVA). In this case we will go through an
example in detail and work out all the mechanics, but once we have done
that and seen how the output is derived from the input we will not need
to do it again. We will use R to do the heavy lifting. We will just need
to know when it is appropriate to use ANOVA, how to get R to do it and
how to interpret the output that R produces.

\subsection{The Scenario}\label{the-scenario}

Suppose we have three fertilizers and wish to compare their efficacy.
This has been done in a field experiment where each fertilizer is
applied to 10 plots and the 30 plots are later harvested, with the crop
yields being calculated. We end up with three groups of 10 figures and
we wish to know if there are any differences between these groups.

When we plot the data we see that the fertilizers do differ in the
amount of yield produced but that there is also a lot of variation
between the plots that were given the same fertilizer.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g1}\OtherTok{\textless{}{-}}\FunctionTok{ggplot}\NormalTok{(fertilizer,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{FERTIL,}\AttributeTok{y=}\NormalTok{YIELD, }\AttributeTok{fill=}\NormalTok{ FERTIL,}\AttributeTok{alpha=}\FloatTok{0.1}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{colour=}\NormalTok{FERTIL))}\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{\textquotesingle{}Fertilizer\textquotesingle{}}\NormalTok{, }\AttributeTok{y=}\StringTok{\textquotesingle{}Yield\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g2}\OtherTok{\textless{}{-}}\FunctionTok{ggplot}\NormalTok{(fertilizer,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{plot,}\AttributeTok{y=}\NormalTok{YIELD,}\AttributeTok{colour=}\NormalTok{FERTIL))}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{\textquotesingle{}Plot number\textquotesingle{}}\NormalTok{,}\AttributeTok{y=}\StringTok{\textquotesingle{}Yield per plot (tonnes)\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{grid.arrange}\NormalTok{(g2,g1,}\AttributeTok{nrow=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ANOVA_how_it_works_files/figure-pdf/unnamed-chunk-1-1.pdf}}

\subsection{What does an ANOVA do?}\label{what-does-an-anova-do}

An ANOVA (ANalysis Of VAriance) analysis attempts to determine whether
the differences between the effect of the fertilizers is significant by
investigating the variability in the data. We investigate how the
variability \emph{between} groups compares to the variability
\emph{within} groups.

\subsection{Grand Mean}\label{grand-mean}

First we calculate the `grand mean', the mean of the yields across all
30 plots:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grand\_mean}\OtherTok{=}\FunctionTok{mean}\NormalTok{(fertilizer}\SpecialCharTok{$}\NormalTok{YIELD)}
\NormalTok{grand\_mean}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.643667
\end{verbatim}

\subsubsection{Deviations from the grand
mean}\label{deviations-from-the-grand-mean}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SST.plot}\OtherTok{\textless{}{-}}\NormalTok{g2}\SpecialCharTok{+}\FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept=}\NormalTok{grand\_mean,}\AttributeTok{linetype=}\StringTok{\textquotesingle{}dashed\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ plot, }\AttributeTok{y =}\NormalTok{ YIELD, }\AttributeTok{xend =}\NormalTok{ plot, }\AttributeTok{yend =}\NormalTok{ grand\_mean),}\AttributeTok{linetype=}\StringTok{\textquotesingle{}dotted\textquotesingle{}}\NormalTok{)}
\NormalTok{SST.plot}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ANOVA_how_it_works_files/figure-pdf/SST plot-1.pdf}}

\subsubsection{Mean value of yield for each
fertilizer}\label{mean-value-of-yield-for-each-fertilizer}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f\_means}\OtherTok{\textless{}{-}}\NormalTok{fertilizer }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(FERTIL) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{fmean=}\FunctionTok{mean}\NormalTok{(YIELD))}
\NormalTok{f\_means}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 2
  FERTIL fmean
  <fct>  <dbl>
1 1       5.44
2 2       4.00
3 3       4.49
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fertilizer}\OtherTok{\textless{}{-}}\FunctionTok{mutate}\NormalTok{(fertilizer,}\AttributeTok{fmean=}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(f\_means}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{],}\DecValTok{10}\NormalTok{),}\FunctionTok{rep}\NormalTok{(f\_means}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{2}\NormalTok{],}\DecValTok{10}\NormalTok{),}\FunctionTok{rep}\NormalTok{(f\_means}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{3}\NormalTok{],}\DecValTok{10}\NormalTok{)))}

\NormalTok{f1}\OtherTok{\textless{}{-}}\FunctionTok{filter}\NormalTok{(fertilizer,FERTIL}\SpecialCharTok{==}\DecValTok{1}\NormalTok{)}
\NormalTok{f2}\OtherTok{\textless{}{-}}\FunctionTok{filter}\NormalTok{(fertilizer,FERTIL}\SpecialCharTok{==}\DecValTok{2}\NormalTok{)}
\NormalTok{f3}\OtherTok{\textless{}{-}}\FunctionTok{filter}\NormalTok{(fertilizer,FERTIL}\SpecialCharTok{==}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g3}\OtherTok{\textless{}{-}}\FunctionTok{ggplot}\NormalTok{()}\SpecialCharTok{+}
  
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data=}\NormalTok{f1,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{plot,}\AttributeTok{y=}\NormalTok{YIELD))}\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{min}\NormalTok{(f1}\SpecialCharTok{$}\NormalTok{plot),}\AttributeTok{y=}\NormalTok{f1}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{],}\AttributeTok{xend=}\FunctionTok{max}\NormalTok{(f1}\SpecialCharTok{$}\NormalTok{plot),}\AttributeTok{yend=}\NormalTok{f1}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{]))}\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ f1}\SpecialCharTok{$}\NormalTok{plot, }\AttributeTok{y =}\NormalTok{ f1}\SpecialCharTok{$}\NormalTok{YIELD, }\AttributeTok{xend =}\NormalTok{ f1}\SpecialCharTok{$}\NormalTok{plot, }\AttributeTok{yend =}\NormalTok{ f1}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{]),}\AttributeTok{linetype=}\StringTok{\textquotesingle{}dotted\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data=}\NormalTok{f2,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{plot,}\AttributeTok{y=}\NormalTok{YIELD))}\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{min}\NormalTok{(f2}\SpecialCharTok{$}\NormalTok{plot),}\AttributeTok{y=}\NormalTok{f2}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{],}\AttributeTok{xend=}\FunctionTok{max}\NormalTok{(f2}\SpecialCharTok{$}\NormalTok{plot),}\AttributeTok{yend=}\NormalTok{f2}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{]))}\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ f2}\SpecialCharTok{$}\NormalTok{plot, }\AttributeTok{y =}\NormalTok{ f2}\SpecialCharTok{$}\NormalTok{YIELD, }\AttributeTok{xend =}\NormalTok{ f2}\SpecialCharTok{$}\NormalTok{plot, }\AttributeTok{yend =}\NormalTok{ f2}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{]),}\AttributeTok{linetype=}\StringTok{\textquotesingle{}dotted\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data=}\NormalTok{f3,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{plot,}\AttributeTok{y=}\NormalTok{YIELD))}\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{min}\NormalTok{(f3}\SpecialCharTok{$}\NormalTok{plot),}\AttributeTok{y=}\NormalTok{f3}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{],}\AttributeTok{xend=}\FunctionTok{max}\NormalTok{(f3}\SpecialCharTok{$}\NormalTok{plot),}\AttributeTok{yend=}\NormalTok{f3}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{]))}\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ f3}\SpecialCharTok{$}\NormalTok{plot, }\AttributeTok{y =}\NormalTok{ f3}\SpecialCharTok{$}\NormalTok{YIELD, }\AttributeTok{xend =}\NormalTok{ f3}\SpecialCharTok{$}\NormalTok{plot, }\AttributeTok{yend =}\NormalTok{ f3}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{]),}\AttributeTok{linetype=}\StringTok{\textquotesingle{}dotted\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{\textquotesingle{}Plot number\textquotesingle{}}\NormalTok{,}\AttributeTok{y=}\StringTok{\textquotesingle{}Yield per plot (tonnes)\textquotesingle{}}\NormalTok{,}\AttributeTok{title=}\StringTok{"SSE: Error sum of squares"}\NormalTok{)}\SpecialCharTok{+}
  
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\subsection{Measures of variability}\label{measures-of-variability}

\subsubsection{SST - Total sum of
squares}\label{sst---total-sum-of-squares}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SST}\OtherTok{=}\FunctionTok{sum}\NormalTok{((fertilizer}\SpecialCharTok{$}\NormalTok{YIELD}\SpecialCharTok{{-}}\NormalTok{grand\_mean)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{SST}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 36.4449
\end{verbatim}

SST is the \textbf{total sum of squares.} It is the sum of squares of
the deviations of the data around the grand mean. This is a measure of
the total variability of the data set.

\subsubsection{SSE - Error sum of
squares}\label{sse---error-sum-of-squares}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SSE}\OtherTok{\textless{}{-}}\NormalTok{fertilizer }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(FERTIL) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{fmean=}\FunctionTok{mean}\NormalTok{(YIELD)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{se=}\NormalTok{(YIELD}\SpecialCharTok{{-}}\NormalTok{fmean)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{sse=}\FunctionTok{sum}\NormalTok{(se),}\AttributeTok{.groups =} \StringTok{\textquotesingle{}drop\textquotesingle{}}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{SSE=}\FunctionTok{sum}\NormalTok{(sse),}\AttributeTok{.groups =} \StringTok{\textquotesingle{}drop\textquotesingle{}}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(SSE)}
\end{Highlighting}
\end{Shaded}

SSE is the \textbf{error sum of squares}. It is the sum of the squares
of the deviations of the data around the three separate group means.
This is a measure of the variation between plots that have been given
the same fertilizer.

\subsubsection{SSF - Fertilizer sum of
squares}\label{ssf---fertilizer-sum-of-squares}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SSF}\OtherTok{\textless{}{-}}\NormalTok{fertilizer }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(FERTIL) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{fmean=}\FunctionTok{mean}\NormalTok{(YIELD)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{se=}\NormalTok{(fmean}\SpecialCharTok{{-}}\NormalTok{grand\_mean)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{sse=}\FunctionTok{sum}\NormalTok{(se),}\AttributeTok{.groups =} \StringTok{\textquotesingle{}drop\textquotesingle{}}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{SSF=}\FunctionTok{sum}\NormalTok{(sse),}\AttributeTok{.groups =} \StringTok{\textquotesingle{}drop\textquotesingle{}}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(SSF)}
\end{Highlighting}
\end{Shaded}

SSF is the \textbf{fertilizer sum of squares}. This is the sum of the
squares of the deviations of the group means from the grand mean. This
is a measure of the variation between plots given different fertilizers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g4}\OtherTok{\textless{}{-}}\FunctionTok{ggplot}\NormalTok{()}\SpecialCharTok{+}
  
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept=}\NormalTok{grand\_mean,}\AttributeTok{linetype=}\StringTok{\textquotesingle{}dashed\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{min}\NormalTok{(f1}\SpecialCharTok{$}\NormalTok{plot),}\AttributeTok{y=}\NormalTok{f1}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{],}\AttributeTok{xend=}\FunctionTok{max}\NormalTok{(f1}\SpecialCharTok{$}\NormalTok{plot),}\AttributeTok{yend=}\NormalTok{f1}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{]))}\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{mean}\NormalTok{(f1}\SpecialCharTok{$}\NormalTok{plot), }\AttributeTok{y =}\NormalTok{ grand\_mean, }\AttributeTok{xend =} \FunctionTok{mean}\NormalTok{(f1}\SpecialCharTok{$}\NormalTok{plot), }\AttributeTok{yend =}\NormalTok{ f1}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{]),}\AttributeTok{linetype=}\StringTok{\textquotesingle{}dotted\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{min}\NormalTok{(f2}\SpecialCharTok{$}\NormalTok{plot),}\AttributeTok{y=}\NormalTok{f2}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{],}\AttributeTok{xend=}\FunctionTok{max}\NormalTok{(f2}\SpecialCharTok{$}\NormalTok{plot),}\AttributeTok{yend=}\NormalTok{f2}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{]))}\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{mean}\NormalTok{(f2}\SpecialCharTok{$}\NormalTok{plot), }\AttributeTok{y =}\NormalTok{ grand\_mean, }\AttributeTok{xend =} \FunctionTok{mean}\NormalTok{(f2}\SpecialCharTok{$}\NormalTok{plot), }\AttributeTok{yend =}\NormalTok{ f2}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{]),}\AttributeTok{linetype=}\StringTok{\textquotesingle{}dotted\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{min}\NormalTok{(f3}\SpecialCharTok{$}\NormalTok{plot),}\AttributeTok{y=}\NormalTok{f3}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{],}\AttributeTok{xend=}\FunctionTok{max}\NormalTok{(f3}\SpecialCharTok{$}\NormalTok{plot),}\AttributeTok{yend=}\NormalTok{f3}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{]))}\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{mean}\NormalTok{(f3}\SpecialCharTok{$}\NormalTok{plot), }\AttributeTok{y =}\NormalTok{ grand\_mean, }\AttributeTok{xend =} \FunctionTok{mean}\NormalTok{(f3}\SpecialCharTok{$}\NormalTok{plot), }\AttributeTok{yend =}\NormalTok{ f3}\SpecialCharTok{$}\NormalTok{fmean[}\DecValTok{1}\NormalTok{]),}\AttributeTok{linetype=}\StringTok{\textquotesingle{}dotted\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}
  
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\FloatTok{2.5}\NormalTok{,}\FloatTok{7.5}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{\textquotesingle{}Plot number\textquotesingle{}}\NormalTok{,}\AttributeTok{y=}\StringTok{\textquotesingle{}Yield per plot (tonnes)\textquotesingle{}}\NormalTok{,}\AttributeTok{title=}\StringTok{"SSF:Fertilizer sum of squares"}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{grid.arrange}\NormalTok{(g3,g4,}\AttributeTok{nrow=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ANOVA_how_it_works_files/figure-pdf/unnamed-chunk-4-1.pdf}}

When the three group means are fitted, there is an obvious reduction in
variability around the three means compared to that around the grand
mean, but it is not obvious if the fertilizers have had an effect on
yield.

At what point do we decide if the amount of variation explained by
fitting the means is significant? By this, we mean, ``When is the
variability between the group means greater than we would expect by
chance alone?

First, we note that SSF and SSE partition between them the total
variability in the data:

\subsection{SST = SSF + SSE}\label{sst-ssf-sse}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SST}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 36.4449
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SSF}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 10.82275
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 25.62215
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SSF}\SpecialCharTok{+}\NormalTok{SSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 36.4449
\end{verbatim}

So the total variability has been divided into two components. That due
to differences between plots given different treatments and that due to
differences between plots given the same treatment. Variability must be
due to one or other of these components. Separating the total SS into
its component SS is known as partitioning the sums of squares.

A comparison of SSF and SSE is going to indicate whether fitting the
three fertilizer means accounts for a significant amount of variability.

However, to make a proper comparison, we really need to compare the
variability per degree of freedom ie the variance.

\subsection{Partitioning the degrees of
freedom}\label{partitioning-the-degrees-of-freedom}

Every sum of squares (SS) has been calculated using a number of
independent pieces of information. In each, case, we call this number
the number of degrees of freedom for the SS.

For SST this number is one less than the number of data points \emph{n}.
This is because when we calculate the deviations of each data point
around a grand mean there are only \emph{n}-1 of them that are
independent, since by definition the sum of these deviations is zero,
and so when \emph{n}-1 of them have been calculated, the final one is
pre-determined.

Similarly, when we calculate SSF, which measures the deviation of the
group means from the grand mean, we have \emph{k}-1 degrees of freedom,
(where in the present example \emph{k}, the number of treatments, is
equal to three) since the deviations must sum to zero, so when
\emph{k}-1 of them have been calculated, the last one is pre-determined.

Finally, SSE, which measure deviation around the group means will have
\emph{n}-\emph{k} degrees of freedom, since the sum of each of the
deviations around one of the group means must sum to zero, and so when
all but one of them have been calculated, the final one is
pre-determined. There are \(k\) group means, so the total degrees of
freedom for SSE is \emph{n}-\emph{k}.

The degrees of freedom are additive: \[
df(\text{SST}) = df(\text{SSE}) + df(\text{SSF})
\] Check:

\[
\begin{align*}
df(\text{SST}) &= n-1\\
df(\text{SSE}) &= k-1\\
df(\text{SSF}) &= n-k\\
\therefore df(\text{SSE}) + df(\text{SSF}) &= k-1 + n-k\\
&=n-1\\
&=df(\text{SST}) 
\end{align*}
\]

\subsection{Mean Squares}\label{mean-squares}

Now we can calculate the \emph{variances} which are a measure of the
amount of variability per degree of freedom.

In this context, we call them \emph{mean squares}. To find each one we
divided each of the sums of squares (SS) by their corresponding degrees
of freedom.

Fertiliser Mean Square (FMS) = SSF / \emph{k} - 1. This is the variation
per \emph{df} between plots given different fertilisers.

Error Mean Square (EMS) = SSE / \emph{n} - \emph{k}. This is the
variation per \emph{df} between plots given the same fertiliser.

Total Mean Square (TMS) = SST / \emph{n} - 1. This is the total variance
per \emph{df} of the dataset.

Unlike the SS, the MS are not additive. That is, FMS + EMS \(\neq\) TMS.

\subsection{\texorpdfstring{\emph{F}-ratios}{F-ratios}}\label{f-ratios}

If none of the fertilizers influenced yield, we would expect as much
variation between the plots treated with the same fertilizer as between
the plots treated with different fertilizers.

We can express this in terms of the mean squares: the mean square for
fertilizer would be the same as the mean square for error:

\[
\frac{\text{FMS}}{\text{EMS}}=1
\] We call this ratio the \emph{F}-ratio. It is the end result of ANOVA.
\emph{F}-ratios can never be negative since they are the ratio of two
mean square values, both of which must be non-negative, but there is no
limit to how large they can be.

Even if the fertilizers were identical, the \emph{F}-ratio is unlikely
to be exactly 1 - it could by chance take a whole range of values. The
\textbf{F-distribution} represents the range and likelihood of all
possible \emph{F} ratios under the null hypothesis. ie when the
fertilizers were identical.

The shape of the F distribution depends on the degrees of freedom of FMS
and EMS, and we normally specify it by giving the values of each. Below
we show F distributions for 2 and 27 degrees of freedom (ie 3 plots, so
\emph{k} = 3, so the degrees of freedom of FMS = \emph{k}-1 =2, and 10
plants per plot, so \emph{n} = 3 x 10 =30, and hence the degrees of
freedom of EMS = \emph{n}-\emph{k} = 30 - 3 = 27), and for 10 and 27
degrees of freedom.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfs}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"df = 2, 27"}\NormalTok{,}\DecValTok{601}\NormalTok{),}\FunctionTok{rep}\NormalTok{(}\StringTok{"df = 10, 27"}\NormalTok{,}\DecValTok{601}\NormalTok{))}
\NormalTok{xs}\OtherTok{\textless{}{-}}\FunctionTok{rep}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{6}\NormalTok{,}\FloatTok{0.01}\NormalTok{),}\DecValTok{2}\NormalTok{)}
\NormalTok{ys}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\FunctionTok{df}\NormalTok{(xs[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{601}\NormalTok{],}\DecValTok{2}\NormalTok{,}\DecValTok{27}\NormalTok{),}\FunctionTok{df}\NormalTok{(xs[}\DecValTok{602}\SpecialCharTok{:}\DecValTok{1202}\NormalTok{],}\DecValTok{10}\NormalTok{,}\DecValTok{27}\NormalTok{))}
\NormalTok{fdata}\OtherTok{\textless{}{-}}\FunctionTok{tibble}\NormalTok{(}\AttributeTok{x=}\NormalTok{xs,}\AttributeTok{y=}\NormalTok{ys,}\AttributeTok{dfs=}\NormalTok{dfs)}

\NormalTok{fdata }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{dfs =} \FunctionTok{fct\_relevel}\NormalTok{(dfs,}\StringTok{"df = 2, 27"}\NormalTok{,}\StringTok{"df = 10, 27"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{xs,}\AttributeTok{y=}\NormalTok{ys)) }\SpecialCharTok{+}
  \FunctionTok{xlim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"F{-}ratio"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Probability density"}\NormalTok{,}
       \AttributeTok{caption=}\StringTok{"The F{-}distributions for (left) 2 and 27 degrees of freedom and (right) 10 and 27 degrees of freedom"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{colour=}\StringTok{"darkblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{dfs) }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ANOVA_how_it_works_files/figure-pdf/unnamed-chunk-5-1.pdf}}

Note that, whatever the degrees of freedom, \emph{F}-distributions are
examples of so-called probability density functions. The area beneath
them between any two values of \emph{F}-ratio is equal to the
probability of getting an \emph{F}-ratio in that range. Hence the total
area under the curves is equal to 1, since the \emph{F}-ratio must take
some value between zero and infinity, and the area under the tail to the
right of any given \emph{F}-ratio is the probability of getting an
\emph{F}-ratio bigger than that value.

Hence, the probability under the null hypothesis of getting an
\emph{F}-ratio as large or larger than the value we actually got is the
area to the right of this \emph{F}-ratio under the appropriate F
distribution. We often call this probability the \emph{p}-value. p for
probability. p-values are the the probability of getting data as extreme
(same \emph{F}-ratio,) or more extreme (bigger \emph{F}-ratio) as the
data you got you got if the null hypothesis were true.

If the fertilizers were very different, then the FMS would be much
greater than the EMS and the \emph{F}-ratio would be greater than one.
However it can be quite large even when there are no treatment
differences. So how do we decide when the size of the \emph{F}-ratio is
due to treatment rather than to chance?

Traditionally, we decide that it sufficiently larger than one to be due
to treatment differences if it would be this large or larger under the
null hypothesis only 5\% or less of the time. If we had inside knowledge
that the null hypothesis was in fact true then we would still get an
\emph{F}-ratio that large or larger 5\% of the time.

Our \emph{p}-value ie the probability that the \emph{F}-ratio would have
been as large as it is or larger, under the null hypothesis, represents
the strength of evidence against the null hypothesis. The smaller it is,
the stronger the evidence, and only when it is less than 0.05 do we
regard the evidence as strong enough to reject the null.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fratio}\OtherTok{\textless{}{-}}\ControlFlowTok{function}\NormalTok{(p,k,st\_dev)\{}
\NormalTok{  n}\OtherTok{\textless{}{-}}\NormalTok{p}\SpecialCharTok{*}\NormalTok{k }\CommentTok{\# k plots, p replicates per plot}
\NormalTok{  plots}\OtherTok{\textless{}{-}}\FunctionTok{tibble}\NormalTok{(}\AttributeTok{plot=}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"a"}\NormalTok{,p),}\FunctionTok{rep}\NormalTok{(}\StringTok{"b"}\NormalTok{,p),}\FunctionTok{rep}\NormalTok{(}\StringTok{"c"}\NormalTok{,p)),}\AttributeTok{response=}\FunctionTok{rnorm}\NormalTok{(n,}\AttributeTok{mean=}\DecValTok{0}\NormalTok{,}\AttributeTok{sd=}\NormalTok{st\_dev))}
\NormalTok{  lm.mod}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(response}\SpecialCharTok{\textasciitilde{}}\NormalTok{plot,}\AttributeTok{data=}\NormalTok{plots)}
  \FunctionTok{tidy}\NormalTok{(}\FunctionTok{anova}\NormalTok{(lm.mod))}\SpecialCharTok{$}\NormalTok{statistic[}\DecValTok{1}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\section{ANOVA example 1}\label{anova-example-1}

We will carry out the ANOVA analysis of the fertilizer data discussed on
the previous tab.

Our question is whether yield depends on fertilizer.

What is our null hypothesis?

Start a new notebook with these two code chunks to begin with:

\begin{verbatim}
```{r global-options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, warning=FALSE, message=FALSE,echo=FALSE)
```

```{r load packages, message=FALSE,warning=FALSE,echo=FALSE}
library(tidyverse)
library(here)
library(cowplot)
library(gridExtra)
library(ggfortify)
```
\end{verbatim}

Load the \texttt{fertilizer.csv} data into an object call `fertilizer

Is it tidy data? If not, tidy it.

Convert the FERTIL column to a factor, using this code:

\begin{verbatim}
```{r make_factor}
fertilizer <- fertilizer |>
  mutate(FERTIL=as.factor(FERTIL))
```
\end{verbatim}

Make a box plot of yield vs fertilizer, like the one on the previous
tab.

Now use the \texttt{lm()} function to create the anova model (There are
several ways to do this in R - this is just one)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fertil.model}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(YIELD}\SpecialCharTok{\textasciitilde{}}\NormalTok{FERTIL,}\AttributeTok{data=}\NormalTok{fertilizer)}
\end{Highlighting}
\end{Shaded}

Now inspect the model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(fertil.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: YIELD
          Df Sum Sq Mean Sq F value   Pr(>F)   
FERTIL     2 10.823  5.4114  5.7024 0.008594 **
Residuals 27 25.622  0.9490                    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

So we find that we can reject the null hypothesis. There is thus
evidence that fertilizer does affect yield (ANOVA, df = 2,27, F=5.7,
p\textless{} 0.01)

What this test has not done so far is show us where the differences lie.
An ANOVA is a holistic test that tells you whether or not there is
evidence for a difference between at least one pair of groups being
compared. To identify which gruopd, if any, are differeny, we need to do
so-called post-hoc tests.

\section{ANOVA example 2}\label{anova-example-2}

An experiment was performed to compare four melon varieties. It was
designed so that each variety was grown in six plots, but two plots
growing variety 3 were accidentally destroyed.

We wish to find out if there is evidence for a difference in yield
between the varieties.

\subsection{Null hypothesis}\label{null-hypothesis}

What is the null hypothesis of this study?

The data are in the \texttt{melons.csv} dataset.

Write code chunks to

\subsection{Load data and inspect the
data}\label{load-data-and-inspect-the-data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filepath}\OtherTok{\textless{}{-}}\FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"melons.csv"}\NormalTok{)}
\NormalTok{melons}\OtherTok{\textless{}{-}}\FunctionTok{read\_csv}\NormalTok{(filepath)}
\FunctionTok{glimpse}\NormalTok{(melons)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 22
Columns: 2
$ YIELDM  <dbl> 25.12, 17.25, 26.42, 16.08, 22.15, 15.92, 40.25, 35.25, 31.98,~
$ VARIETY <dbl> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4,~
\end{verbatim}

\subsection{Prepare the data}\label{prepare-the-data}

Ensure that the VARIETY column is a factor

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{melons }\OtherTok{\textless{}{-}}\NormalTok{ melons }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{VARIETY=}\FunctionTok{as.factor}\NormalTok{(VARIETY))}
\end{Highlighting}
\end{Shaded}

\subsection{Plot the data}\label{plot-the-data-2}

Create a scatter plot of the yield.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{melons }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{VARIETY,}\AttributeTok{y=}\NormalTok{YIELDM)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{45}\NormalTok{),}\AttributeTok{breaks=}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{45}\NormalTok{,}\DecValTok{5}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Melon variety"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Yield"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ANOVA_how_it_works_files/figure-pdf/unnamed-chunk-12-1.pdf}}

\subsection{Summarise the data}\label{summarise-the-data-2}

Create a summary table that shows the mean yield for each variety.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{melons }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(VARIETY) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{N=}\FunctionTok{n}\NormalTok{(),}
    \AttributeTok{Mean=}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(YIELDM),}\DecValTok{2}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 3
  VARIETY     N  Mean
  <fct>   <int> <dbl>
1 1           6  20.5
2 2           6  37.4
3 3           4  20.5
4 4           6  29.9
\end{verbatim}

\subsection{1-way ANOVA}\label{way-anova}

\subsubsection{Create the model}\label{create-the-model}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{melons.model}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(YIELDM}\SpecialCharTok{\textasciitilde{}}\NormalTok{VARIETY,}\AttributeTok{data=}\NormalTok{melons)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Check the validity of the
model}\label{check-the-validity-of-the-model-1}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autoplot}\NormalTok{(melons.model,}\AttributeTok{smooth.colour=}\ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ANOVA_how_it_works_files/figure-pdf/unnamed-chunk-15-1.pdf}}

DO the data look as though they meet the criteria for an ANOVA? - the
variance of the residuals is roughly constant across all groups and the
qq-plot is fairly straight. We could confirm with a normality test if we
like. For example, we could use a Shapiro-Wilk test.

What is the null hypothesis of this test?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(melons.model}\SpecialCharTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  melons.model$residuals
W = 0.94567, p-value = 0.2586
\end{verbatim}

What do we conclude from this test?

\subsubsection{Inspect the model}\label{inspect-the-model}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(melons.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: YIELDM
          Df  Sum Sq Mean Sq F value    Pr(>F)    
VARIETY    3 1115.28  371.76  23.798 1.735e-06 ***
Residuals 18  281.19   15.62                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

What conclusions would you draw from the output of this model and the
table of mean yields for each variety?

\subsection{Confidence intervals of the
means}\label{confidence-intervals-of-the-means}

Let us find the 95\% confidence intervals for each mean

These we calculate as

\[
\text{Mean}\pm t_{\text{crit}}\text{SE}_{\text{mean}}
\] For a 95\% confidence interval and 18 degrees of freedom,
\(t_{\text{crit}}\) is 2.1, so we find that the intervals are:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_crit}\OtherTok{\textless{}{-}}\FunctionTok{qt}\NormalTok{(}\FloatTok{0.975}\NormalTok{,}\DecValTok{18}\NormalTok{)}
\NormalTok{melons }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(VARIETY) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{Mean=}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(YIELDM),}\DecValTok{2}\NormalTok{),}
    \AttributeTok{LB=}\FunctionTok{round}\NormalTok{(Mean}\SpecialCharTok{{-}}\NormalTok{t\_crit}\SpecialCharTok{*}\FunctionTok{sqrt}\NormalTok{(}\FloatTok{15.6}\NormalTok{)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{n}\NormalTok{()),}\DecValTok{2}\NormalTok{),}
    \AttributeTok{UB=}\FunctionTok{round}\NormalTok{(Mean}\SpecialCharTok{+}\NormalTok{t\_crit}\SpecialCharTok{*}\FunctionTok{sqrt}\NormalTok{(}\FloatTok{15.6}\NormalTok{)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{n}\NormalTok{()),}\DecValTok{2}\NormalTok{)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 4
  VARIETY  Mean    LB    UB
  <fct>   <dbl> <dbl> <dbl>
1 1        20.5  17.1  23.9
2 2        37.4  34.0  40.8
3 3        20.5  16.3  24.6
4 4        29.9  26.5  33.3
\end{verbatim}

\section{ANOVA example 2 solution}\label{anova-example-2-solution}

An experiment was performed to compare four melon varieties. It was
designed so that each variety was grown in six plots, but two plots
growing variety 3 were accidentally destroyed.

We wish to find out if there is evidence for a difference in yield
between the varieties.

\subsection{Null hypothesis}\label{null-hypothesis-1}

What is the null hypothesis of this study?

The data are in the \texttt{melons.csv} dataset.

Write code chunks to

\subsection{Load data and inspect the
data}\label{load-data-and-inspect-the-data-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filepath}\OtherTok{\textless{}{-}}\FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"melons.csv"}\NormalTok{)}
\NormalTok{melons}\OtherTok{\textless{}{-}}\FunctionTok{read\_csv}\NormalTok{(filepath)}
\FunctionTok{glimpse}\NormalTok{(melons)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 22
Columns: 2
$ YIELDM  <dbl> 25.12, 17.25, 26.42, 16.08, 22.15, 15.92, 40.25, 35.25, 31.98,~
$ VARIETY <dbl> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4,~
\end{verbatim}

\subsection{Prepare the data}\label{prepare-the-data-1}

Ensure that the VARIETY column is a factor

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{melons }\OtherTok{\textless{}{-}}\NormalTok{ melons }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{VARIETY=}\FunctionTok{as.factor}\NormalTok{(VARIETY))}
\end{Highlighting}
\end{Shaded}

\subsection{Plot the data}\label{plot-the-data-3}

Create a scatter plot of the yield.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{melons }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{VARIETY,}\AttributeTok{y=}\NormalTok{YIELDM)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{45}\NormalTok{),}\AttributeTok{breaks=}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{45}\NormalTok{,}\DecValTok{5}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Melon variety"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Yield"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ANOVA_how_it_works_files/figure-pdf/unnamed-chunk-21-1.pdf}}

\subsection{Summarise the data}\label{summarise-the-data-3}

Create a summary table that shows the mean yield for each variety.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{melons }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(VARIETY) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{N=}\FunctionTok{n}\NormalTok{(),}
    \AttributeTok{Mean=}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(YIELDM),}\DecValTok{2}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 3
  VARIETY     N  Mean
  <fct>   <int> <dbl>
1 1           6  20.5
2 2           6  37.4
3 3           4  20.5
4 4           6  29.9
\end{verbatim}

\subsection{1-way ANOVA}\label{way-anova-1}

\subsubsection{Create the model}\label{create-the-model-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{melons.model}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(YIELDM}\SpecialCharTok{\textasciitilde{}}\NormalTok{VARIETY,}\AttributeTok{data=}\NormalTok{melons)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Check the validity of the
model}\label{check-the-validity-of-the-model-2}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autoplot}\NormalTok{(melons.model,}\AttributeTok{smooth.colour=}\ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ANOVA_how_it_works_files/figure-pdf/unnamed-chunk-24-1.pdf}}

The data look as though they meet the criteria for an ANOVA - the
variance of the residuals is roughly constant across all groups and the
qq-plot is fairly straight. We could confirm with a normality test if we
like. For example, we could use a Shapiro-Wilk test.

The null hypothesis of this test is that the residuals are drawn from a
population that is normally distributed

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(melons.model}\SpecialCharTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  melons.model$residuals
W = 0.94567, p-value = 0.2586
\end{verbatim}

Since \emph{p}\textgreater0.05 we conclude that there is no reason to
reject the null hypothesis that the residuals are normally disributed.

\subsubsection{Inspect the model}\label{inspect-the-model-1}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(melons.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: YIELDM
          Df  Sum Sq Mean Sq F value    Pr(>F)    
VARIETY    3 1115.28  371.76  23.798 1.735e-06 ***
Residuals 18  281.19   15.62                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(melons.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = YIELDM ~ VARIETY, data = melons)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.4233 -2.2781 -0.5933  2.6694  5.9300 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  20.4900     1.6136  12.699 2.02e-10 ***
VARIETY2     16.9133     2.2819   7.412 7.14e-07 ***
VARIETY3     -0.0275     2.5513  -0.011  0.99152    
VARIETY4      9.4067     2.2819   4.122  0.00064 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.952 on 18 degrees of freedom
Multiple R-squared:  0.7986,    Adjusted R-squared:  0.7651 
F-statistic:  23.8 on 3 and 18 DF,  p-value: 1.735e-06
\end{verbatim}

What conclusions would you draw from the output of this model and the
table of mean yields for each variety?

We see that the null hypothesis is rejected with a \emph{p}-value of
less than 0.001. We conclude that there are significant differences in
the mean yield of melons across the varieties. We estimate that variety
2 has the highest mean yield and varieties 1 and 3 have the lowest mean
yields.

The unexplained variance ie the error \emph{s} for each group is 15.6
with 18 degrees of freedom. So the standard error for each group is
\(\frac{s}{\sqrt{n}}\) where s=\(\sqrt{15.6}\) = 3.95 divided by the
number of elements in each group, giving us standard errors of 1.97 for
variety 3, and 1.61 for the other varieties.

\subsection{Confidence intervals of the
means}\label{confidence-intervals-of-the-means-1}

Let us find the 95\% confidence intervals for each mean

These we calculate as

\[
\text{Mean}\pm t_{\text{crit}}\text{SE}_{\text{mean}}
\] For a 95\% confidence interval and 18 degrees of freedom,
\(t_{\text{crit}}\) is 2.1, so we find that the intervals are:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_crit}\OtherTok{\textless{}{-}}\FunctionTok{qt}\NormalTok{(}\FloatTok{0.975}\NormalTok{,}\DecValTok{18}\NormalTok{)}
\NormalTok{melons }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(VARIETY) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{Mean=}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(YIELDM),}\DecValTok{2}\NormalTok{),}
    \AttributeTok{LB=}\FunctionTok{round}\NormalTok{(Mean}\SpecialCharTok{{-}}\NormalTok{t\_crit}\SpecialCharTok{*}\FunctionTok{sqrt}\NormalTok{(}\FloatTok{15.6}\NormalTok{)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{n}\NormalTok{()),}\DecValTok{2}\NormalTok{),}
    \AttributeTok{UB=}\FunctionTok{round}\NormalTok{(Mean}\SpecialCharTok{+}\NormalTok{t\_crit}\SpecialCharTok{*}\FunctionTok{sqrt}\NormalTok{(}\FloatTok{15.6}\NormalTok{)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{n}\NormalTok{()),}\DecValTok{2}\NormalTok{)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 4
  VARIETY  Mean    LB    UB
  <fct>   <dbl> <dbl> <dbl>
1 1        20.5  17.1  23.9
2 2        37.4  34.0  40.8
3 3        20.5  16.3  24.6
4 4        29.9  26.5  33.3
\end{verbatim}

\chapter{One-way ANOVA}\label{one-way-anova}

\section{Introduction}\label{introduction-1}

In this exercise we will carry out a method of analysis known as ANOVA -
this is what is commonly used when you have one or more categorical
variables, such as species, sex and so on, and a numerical response
variable such as body mass and you want to know if there is a difference
in the response variable between when the levels of the factors take
different values.

A one-way ANOVA is used where where we have one categorical variable
with three or more level (you could also use it where there are just two
levels, but we use a t-test for that). For example if you want to see if
a captive bird species prefers red, green or blue food pellets, then the
factor would be be food colour, and three levels of that would be the
three colours.

A two-way ANOVA is used where we have two categorical variables, each of
which has at least two levels. (Even if both have two levels, we still
call it a two-way ANOVA. There is no such thing as a two-way t-test!).
So, sticking with the captive birds, if were interested in whether they
had a preference for colour (red, blue, green) and/or shape (round,
square) of food pellets, then we could use a two way ANOVA to investigae
the data, where the two factors or `ways' would be food colour and food
shape, with food colour having three levels (red, blue and green) and
food shape having two levels (round and square).

ANOVAs involving three ways or more (the horror, the horror!) are rarely
used since their interpretation is in practice difficult due to the
multiplicity of possible so-called `interaction' effects that commonly
arise, whereby the impact on the output of the levels of one factor
depend on the values of the levels of one or more other factors. Best
avoided!

ANOVAs, whatever their flavour (one way, two way, repeated measures,
ANCOVA etc) are examples of parametric tests. That is, they can only be
used if the data at least approximately meet certain requirements such
as equal variance across the data sets, normality of residuals etc. At
the very least, the data should be numerical and not ordinal. Hence
whenever we think of using an ANOVA we need to check that these
requirements are at least approximately met. If they are not, then we
may choose to turn to non-parametric alternatives.

A non-parametric alternative to a one-way ANOVAs is commonly used,
especially for studies that involve ordinal data such as Likert-type
outputs from surveys. It is called a Kruskal-Wallis test.

\section{Example: penguin data from the palmerpenguins
package}\label{example-penguin-data-from-the-palmerpenguins-package}

In the following, we show how a one-way ANOVA might be carried out on a
set of data on penguins, where we have a numerical output such as body
mass, and categorical variables such as species (three levels: Adelie,
Gentoo and Chinstrap) and sex (two levels: female and male). If we were
interested in whether there was a difference in the output across the
levels of species, then a one-way ANOVA might well be suitable.

\subsection{Follow along yourself:}\label{follow-along-yourself}

To follow through this exercise yourself, you should have an RStudio
project folder that contains:

In the Project/scripts folder:

ANOVA\_one\_way\_template.Rmd (the script where you fill in the code
chunks)

\section{A first look at the data}\label{a-first-look-at-the-data}

\subsection{Load packages}\label{load-packages-3}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse) }\CommentTok{\# for data manipulation and plots, and more besides}
\FunctionTok{library}\NormalTok{(ggfortify) }\CommentTok{\# this is useful for diagnostics}
\FunctionTok{library}\NormalTok{(palmerpenguins) }\CommentTok{\# for the palmer penguin data}
\end{Highlighting}
\end{Shaded}

The \texttt{palmerpenguins} package comes with two in-built data sets on
penguins. The simplest of them is called \texttt{penguins} and is the
one we will use in this exercise:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(penguins)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 344
Columns: 8
$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~
$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse~
$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ~
$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ~
$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186~
$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ~
$ sex               <fct> male, female, female, NA, female, male, female, male~
$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007~
\end{verbatim}

\subsection{Remove observations with missing
values}\label{remove-observations-with-missing-values}

We can see from the first few values of the glimpse table that some rows
have missing values (NAs). We need to decide what to do with them. Here
we will simply remove them! Here is a way to remove any row that
contains missing values in one column or another:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins\_clean }\OtherTok{\textless{}{-}}\NormalTok{ penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{drop\_na}\NormalTok{()}
\FunctionTok{glimpse}\NormalTok{(penguins\_clean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 333
Columns: 8
$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~
$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse~
$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6~
$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2~
$ flipper_length_mm <int> 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18~
$ body_mass_g       <int> 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800~
$ sex               <fct> male, female, female, female, male, female, male, fe~
$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007~
\end{verbatim}

That has removed 11 rows of data, so we haven't lost too much
information.

\subsection{Summary - group by species and
sex}\label{summary---group-by-species-and-sex}

Here we use the famliar \texttt{group\_by()} and \texttt{summarise()}
construction to find the mean body mass for each combination of species
and sex. We also calculate the standard error of those means and the
number of individuals in each group.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins\_clean }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(species, sex) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{(), }\AttributeTok{mean\_bm =} \FunctionTok{mean}\NormalTok{(body\_mass\_g), }\AttributeTok{se\_bm =} \FunctionTok{sd}\NormalTok{(body\_mass\_g)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{n}\NormalTok{()) ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
  species   sex        n mean_bm se_bm
  <fct>     <fct>  <int>   <dbl> <dbl>
1 Adelie    female    73   3369.  31.5
2 Adelie    male      73   4043.  40.6
3 Chinstrap female    34   3527.  48.9
4 Chinstrap male      34   3939.  62.1
5 Gentoo    female    58   4680.  37.0
6 Gentoo    male      61   5485.  40.1
\end{verbatim}

Looking at this table, does it look as though females and males have
different weights? If so, which is heavier? Is this true for all
species? Do the different species weigh the same?

\subsection{Plot the data}\label{plot-the-data-4}

To get further insight into these questions, we can plot the data. Here
we will do a box plot

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins\_clean  }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{species, }\AttributeTok{y =}\NormalTok{ body\_mass\_g, }\AttributeTok{fill =}\NormalTok{ sex)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Body mass (g)"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Sex"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.8}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ANOVA_one_way_files/figure-pdf/unnamed-chunk-5-1.pdf}}

What do you think now about size differences between species and the two
sexes?

There is a lot going on here, so let's approach this more simply to
begin with and concentrate solely on the difference between the females
of the species.

\section{One-way ANOVA}\label{one-way-anova-1}

Let's ask the question: do the body weights differ between females of
the different species?

There is just one \textbf{factor} here, species, and it has more than
two \textbf{levels} - the three different species - and the reponse
variable is numeric, so it is highly likely that the appropriate test to
answer this question is a one-way ANOVA. `One way' because there is one
factor, and `ANOVA' (instead of t-test) because there are more than two
levels.

\subsection{Null hypothesis}\label{null-hypothesis-2}

Pretty much all of the commonly used statistics tests are asking the
question: what is the probability that you would have got this data, or
more extreme data, if the null hypothesis were true? Their job is to
calculate that probability, which is called a p-value. There is a lot
more besides, but what this means is that in carrying out any of these
tests we at least need to have a hypothesis in mind and its
corresponding null hypothesis. The null, remember, is typically the
`nothing going on', there is no effect, no difference scenario.

So in this case, a suitable null hypothesis would be that there is no
difference in body mass between the females of the different penguin
species.

To see if there is evidence from the data to reject this null, we will
follow a sequence of steps that will be common to many analyses:

\begin{itemize}
\tightlist
\item
  get the data
\item
  clean/prepare the data
\item
  summarise the data
\item
  plot the data
\item
  construct the model using whatever test is appropriate, in this case a
  one-way ANOVA
\item
  check whether the model is valid
\item
  inspect the model output
\item
  reject or fail to reject the null hypothesis
\item
  if we reject the null, carry out post-hoc tests
\item
  (maybe) simplify the model and redo the analysis
\end{itemize}

For the penguin data, getting it was easy as it came with the
\texttt{palmerpenguins} package.

To prepare the data, we start with the full data set and narrow it down
to just the females, using the \texttt{filter()} function, and again
make sure there are no lines with missing values, using
\texttt{drop\_na()}. We save this cleaned data set in an object called
\texttt{females}.

\subsection{Filter and clean the data}\label{filter-and-clean-the-data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{females }\OtherTok{\textless{}{-}}\NormalTok{ penguins }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(sex }\SpecialCharTok{==} \StringTok{"female"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{drop\_na}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\subsection{Summarise the data}\label{summarise-the-data-4}

Then let's summarise these values to find the number of individuals, the
mean body mass for each species, and the standard errors of those means:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{females }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(species) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{(), }\AttributeTok{mean.mass\_f =} \FunctionTok{mean}\NormalTok{(body\_mass\_g), }\AttributeTok{se.mass\_f =} \FunctionTok{sd}\NormalTok{(body\_mass\_g)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{n}\NormalTok{()))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 4
  species       n mean.mass_f se.mass_f
  <fct>     <int>       <dbl>     <dbl>
1 Adelie       73       3369.      31.5
2 Chinstrap    34       3527.      48.9
3 Gentoo       58       4680.      37.0
\end{verbatim}

We should inspect this summary table and see what we already think about
whether the null hypothesis is likely to be rejected, or not.

Now let's plot them, using a box plot (but choose your favourite plot
type):

\subsection{Plot the data}\label{plot-the-data-5}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{females  }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{species, }\AttributeTok{y =}\NormalTok{ body\_mass\_g)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{fill =} \StringTok{"\#9ebcda"}\NormalTok{) }\SpecialCharTok{+}  \CommentTok{\# pick your favourite colour from https://colorbrewer2.org/}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Species"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Body mass (g)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ANOVA_one_way_files/figure-pdf/unnamed-chunk-8-1.pdf}}

From the summary table and the plot, what do you think? Do the masses
differ between the species?

\subsection{Do the actual ANOVA}\label{do-the-actual-anova}

You probably have a good idea what the answer is, as to our question,
but now we will move on to the actual statistics test, in this case a
one-way ANOVA.

An ANOVA is one variant of a range of anlysis techniques known as
`linear models'. If you were to look under the hood, you would see that
mathematics behind it is exactly the same as that behind linear
regression, which we use when we have a continuous explanatory variable
and where we fit straight lines onto a scatter plot. Thus it is no
surprise that the ANOVA is carried out in R in exactly the same way as
linear regression would be:

First, we use the \texttt{lm()} function to construct a linear model of
the data:

\subsubsection{Construct the model}\label{construct-the-model}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{females.model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(body\_mass\_g }\SpecialCharTok{\textasciitilde{}}\NormalTok{ species, }\AttributeTok{data =}\NormalTok{ females)}
\end{Highlighting}
\end{Shaded}

Here the \texttt{lm()} function has done all the maths of the ANOVA, and
we have saved the results of that in an object called
\texttt{females.model}. Note the use of the formula
\texttt{body\_mass\_g\ \textasciitilde{}\ species} as the first argument
of the \texttt{lm()} function, where this means `body mass as a function
of species'.

\subsubsection{Is the model valid?}\label{is-the-model-valid}

All linear models are only valid if the data meet a number of criteria.
Chief among these for an ANOVA is that the spread of the data should be
roughly the same in each subset, and that the data within each subset
should be normally distributed around their respective mean values. Only
if these conditions are at least approximately met can we just go on and
trust the output of the model. If they are not, we need to transform the
data in some way until they are, or use a different test. A commonly
used non-parametric alternative to the one-way ANOVA is the
Kruskal-Wallis test.

There are various ways we can find out whether these conditions are met.
A useful one is to do it graphically, and a useful way to do that is to
use the \texttt{autoplot()} function from the \texttt{ggfortify}
package. Let's do it:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autoplot}\NormalTok{(females.model) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ANOVA_one_way_files/figure-pdf/unnamed-chunk-10-1.pdf}}

All four graphs presented here tell us something about the validity or
not of our model. Here we will just focus on the upper two:

\begin{itemize}
\item
  top-left: this shows the spread of the residual masses (diference
  between an individual's mass and the mean mass for its species) for
  each species. We see that the spread of these values is aout the same
  for all three species. Check!
\item
  top-right: this is a \texttt{quantile-quantile} plot, often referred
  to as a qq-plot. This compares the distribution of the residuals for
  each species with a normal distribution. If the residuals are normally
  distributed, we will get a straight line. If not, we won't. To get an
  idea of what qq-plots, histograms and box-plots look like for data
  that definitely are not normally distriuted, see
  \href{https://rpubs.com/mbh038/725314}{this useful summary}. In our
  case, there is a hint of a curve, but this qq-plot is really a pretty
  good approximation to linear for a real data set. No such data is ever
  perfectly normally distributed, so the best we are looking for, in
  practice is something approximating a straight line, often with some
  raggedness at either end. So, check again!
\end{itemize}

On both counts, we are good to go: we can reasonably trust the output of
the ANOVA.

So what is this output? We find this in three steps

\subsubsection{Inspect the model}\label{inspect-the-model-2}

\textbf{The overall picture}

First, we use the \texttt{anova()} function

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(females.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: body_mass_g
           Df   Sum Sq  Mean Sq F value    Pr(>F)    
species     2 60350016 30175008  393.25 < 2.2e-16 ***
Residuals 162 12430757    76733                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

This gives us an overview of all the data and asks the question: how
likely is it that you would have got your data if species made no
difference to body mass. There are three things to note:

\begin{itemize}
\item
  the test statistic, here called an F-value. This is a number
  calculated from the data. Roughly speaking, it is the ratio of the
  spread of values (aka variance) between the subgroups to that within
  the subgroups If the validity criteria for the test have been met by
  the data, then this has a known distribution. The bigger the F-value,
  the more likely it is that the null will be rejected.
\item
  the \texttt{degrees\ of\ freedom}, here denoted as Df and listed in
  the first column. These are the number of independent pieces of
  information in the data, which here means, how many species and how
  many penguins.
\item
  the p-value, which is the probability of getting an F value as big as
  or bigger than the one actually found, if the null hypothesis were
  true. This is is the number listed at the right as Pr(\textgreater F).
\end{itemize}

The F value here is huge and the p-value is tiny, so tiny that it is
essentially zero. Thus we can confidently reject the null hypothesis and
assert that there is evidence from the data that body mass of females
differs between at least one pair of species. Which two, or between all
of them, and by how much we don't yet know. This first step just tells
us whether there is some difference somewhere. If there were no evidence
of any difference we would stop the analysis right here.

But there is a difference in this case, so we continue.

\textbf{The detailed picture}

We use the \texttt{summary()} function for this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(females.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = body_mass_g ~ species, data = females)

Residuals:
    Min      1Q  Median      3Q     Max 
-827.21 -193.84   20.26  181.16  622.79 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       3368.84      32.42 103.908  < 2e-16 ***
speciesChinstrap   158.37      57.52   2.754  0.00657 ** 
speciesGentoo     1310.91      48.72  26.904  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 277 on 162 degrees of freedom
Multiple R-squared:  0.8292,    Adjusted R-squared:  0.8271 
F-statistic: 393.2 on 2 and 162 DF,  p-value: < 2.2e-16
\end{verbatim}

There is a lot in ths output, so let's just consider the coefficient
table, to begin with. Focus first on the top left value, in the Estimate
column. This tells us the mean body mass of the reference or `Intercept'
species. In this case that is `Adelie', purely because `Adelie' comes
alphabetically before the other two species names, `Chinstrap' and
`Gentoo'. By default, R will always order levels of a factor
alpabetically. This is often a nuisance, and with many data sets we have
to tell R to reorder the levels the way we want them, but here the order
is OK.

So, the mean mass of female Adelie penguins in our sample is 3368 g.
Cross check that with your initial summary table and the box plot. What
about the other two species? Here's the thing: for all rows except the
first in the Estimate column we are not given the absolute value but the
difference between their respective mean values and the reference mean
in the first, `Intercept' row.

Thus, we are being told that Chinstrap females in the sample have a mean
body mass that is 158.37 g heavier than that of Adelie females, so that
their mean body mass is 3368.84 + 158.37 = 3527.27g. Again, cross check
that with your summary table and the box plot. Is it right?

What about Gentoo females? Were they heavier than Adelie penguins, and
if so, by how much? What was their mean body mass.

Why doesn't \texttt{summary()} just tell us the actual body masses
instead for all three species instead of doing it in this round about
way? The reason is that ANOVA is concerned with detecting evidence of
\emph{difference}. This is why we are being told what the differences
are between each of the levels and one reference level, which here is
Adelie.

Are those differences signifcant? We use the right hand p-value column
for that. Look in the rows for Chinstrap and Gentoo penguins. In both
cases the p values are much less than 0.05. This is telling us that in
both cases there is evidence that females of these species are
significantly heavier than those of the Adelie species.

Note that we have only been told, so far, about the magnitude and
significance of differences between all the levels and the reference
level. We are not told the significance of any difference between any
other pair of levels. So in particular, the ANOVA does not tell us
whether there is a significant difference between the masses of
Chinstrap and Gentoo females (although we may have a good idea what the
answer is, from our initial summary table and plot).

To find the answer to that, we do post-hoc tests:

\subsection{Post hoc tests.}\label{post-hoc-tests.}

A final step of most ANOVA analyses is to perform so-called post-hoc
(`after the fact') tests which make pairwise comparisons between all
possible pairs of levels, tell us what the differences are between those
pairs and whether these differences are significant. Whatever method is
used for this, it needs to take account of the danger of making Type-one
errors that arises when multiple pair-wise tests are done.

A commonly used function for doing this is Tukey's Honest Signficant
Difference: \texttt{TukeyHSD()}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{TukeyHSD}\NormalTok{(}\FunctionTok{aov}\NormalTok{(body\_mass\_g }\SpecialCharTok{\textasciitilde{}}\NormalTok{ species, }\AttributeTok{data =}\NormalTok{ females))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = body_mass_g ~ species, data = females)

$species
                      diff        lwr       upr     p adj
Chinstrap-Adelie  158.3703   22.32078  294.4197 0.0179471
Gentoo-Adelie    1310.9058 1195.64908 1426.1624 0.0000000
Gentoo-Chinstrap 1152.5355 1011.00620 1294.0648 0.0000000
\end{verbatim}

In each row of the output we see the difference between the mean masses
of the females of two species, where a positive value tells us that the
first named species has the heavier mass. So, we see that Gentoo females
in the sample were on average 1310.9 g heavier than Adelie females.

Compare these differences with your initial summary table and your box
plot. Do they agree? They should!

The right-hand column `p adj' tells us whether these difference are
significant. If the p values are less than 0.05 then they are, at the
5\% significance level. In this case they all are. The p values are so
tiny for the differences between Gentoo and the other two species that
that they are reported as zero.

\subsection{Reporting the Result.}\label{reporting-the-result.}

We try to use plain English to report our results, while still telling
the reader what test was used and the key outputs of the test. Try to
report the name of the test, the test statistic, the degrees of freedom,
and the p-value. if. the p-value is really small then it is common to
report it as p\textless0.01, or p\textless0.001. No one cares if it is a
billionth or a squillionth. It just matters that is t is \emph{really}
small, if that is the case. If it is only just below 0.05, then I would
report it in full, so we might write p = 0.018. If p \textgreater{} 0.05
then conventiallly it is not reported, except to say p \textgreater{}
0.05.

In this case, we might say something like:

\emph{We find evidence that there is a difference between the body
masses of females of the penguin species Adelie, Chinstrap and Gentoo
(ANOVA, df = 2, 162, F = 393, p \textless{} 0.001). In particular Gentoo
are more than 1 kg heavier than the other two (p\textless{} 0.001) while
the difference between Chinstrap and Adelie is smaller, at 158 g, but
still significant (p = 0.018).}

\chapter{Two-way ANOVA with model
simplification}\label{two-way-anova-with-model-simplification}

This exercise sheet is heavily indebted to Michael Crawley's
\emph{Statistics: An introduction using R}, 2nd Ed, Wiley. Published in
2015 this emphasises statistics over R (in fact, much of the R he
presents is written prior to the advent of the \texttt{tidyverse}
dialect which we use here, and so may seem terse if that is what you are
used to). It is very useful and is at a higher level than Beckerman,
Childs and Petchey's \emph{Getting Started in R: An introduction for
biologists}, 2nd Ed. OUP published in 2017. Their book also includes a
simpler version of the example explored here.

\section{Factorial experiments and model
simplification}\label{factorial-experiments-and-model-simplification}

The best model is the one that adequately explains the data with fewest
parameters. This means with the smallest possible number of degrees of
freedom.

If we have a very large number of parameters, a model can fit any data
set but be of limited use in generalising beyond it (we will have
overfitted the data). If we have too few we will not explain much of the
variance of the data. A balance must be struck. Hence we want the
\emph{minimal} \emph{adequate} model.

As Einstein almost said, a model should be as simple as possible, but no
simpler. (Not to be outdone, the British statistician George Box also
had a pithy saying about models: ``All models are wrong, but some are
useful''.)

A \textbf{factorial} experiment has two or more factors, each with two
or more levels, plus replication for each combination of factor levels.
This means that we can investigate whether statistical interactions
occur in which the effect of one factor depends on the value of another
factor.

We take an example from a farm-trial of animal diets. There are two
factors: \texttt{diet} and \texttt{supplement}. \texttt{diet} is a
factor with three levels: \texttt{barley}, \texttt{oats} and
\texttt{wheat}, where \texttt{barley} is the diet that has always been
used and the other two are potential alternatives. The purpose of the
trial is to see if their use makes a difference to growth outcomes.
\texttt{supplement} is a factor with four levels: \texttt{control},
\texttt{agrimore}, \texttt{supergain} and \texttt{supersupp}, where
\texttt{control} could mean the absence of any supplement or the
supplement used up to now, whose effects we are hoping to improve upon
through use of one of the others included in the trial. The response
variable \texttt{gain} is weight gain after 6 weeks. There were 48
individual cows in total with 4 for each combination of \texttt{diet}
and \texttt{supplement}. Having the same number of replicates for each
combination of levels means that this is a \emph{balanced} design.

\includegraphics[width=4.03in,height=\textheight,keepaspectratio]{figures/factorial_design.png}

\section{Files needed}\label{files-needed}

To follow through this exercise, you should have an RStudio project
folder that contains:

\begin{itemize}
\item
  In the Project/scripts folder:

  \begin{itemize}
  \tightlist
  \item
    \texttt{ANOVA\_two\_way\_template.Rmd} (the script where you fill in
    the code chunks)
  \end{itemize}
\item
  In the Project/data folder

  \begin{itemize}
  \tightlist
  \item
    \texttt{growth.csv}
  \end{itemize}
\end{itemize}

You can download a complete project folder with these files in from
\href{https://github.com/mbh038/ANOVA-two-way-with-model-simplification/archive/refs/heads/main.zip}{here}

In the following, we present the code you need to analyse this data
together with explanatory text. Read the text closely so that you
understand what each chunk of code is intended to do. In the
accompanying template file, fill in the code as you go in the empty
chunks, using this worksheet as a guide. As you complete each line of
code, run it using Ctrl-Enter, or Cmd-Enter on a Mac. Alternatively,
wait until you have completed the code for a chunk then run the whole
chunk in one go by pressing the little green arrow at the top right of
the chunk. Whichever way you choose, you are encouraged to view the code
presented here as \emph{one} way to do the analysis. Feel free to hack
away at it and change things, to try different approaches and see what
happens. That way you will learn. You may also wish to add your own text
between the chunks.

\section{Open your Project}\label{open-your-project}

You should be working within a folder that you have designated as what
RStudio calls a `Project'. If you are, the name of your Project will
appear at the top right of the RStudio window. Inside your Project
folder you should have a \texttt{scripts} folder for scripts like the
one you are working from, and a \texttt{data} folder for all the data
files. You will also see, at the top level of the Project, the
\texttt{.RProj} file. You can see all this in the Files pane,
bottom-right.

\section{Load packages}\label{load-packages-4}

I normally load all the packages in the chunk below into every script.
The most important is the \texttt{tidyverse} package which is a goody
bag containing several other packages. Loading this saves you from
having to load each of those individually. The most often used among
these is \texttt{readr} for reading and writing data from/to files,
\texttt{dplyr} for data manipulation, and \texttt{ggplot2} for plotting.
Others will be used from time to time, and we don't really need to be
aware of that when it happens or to worry about it, so long as
\texttt{tidyverse} has been loaded.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)  }\CommentTok{\# for data manipulation and plotting, and much else besides}
\FunctionTok{library}\NormalTok{(here) }\CommentTok{\# for finding our data easily}
\FunctionTok{library}\NormalTok{(cowplot) }\CommentTok{\# gives a nice theme for plots}
\FunctionTok{library}\NormalTok{(ggfortify) }\CommentTok{\# for diagnostic plots}
\end{Highlighting}
\end{Shaded}

\section{Read in the data}\label{read-in-the-data}

The \texttt{growth.csv} data file needs to be in the \texttt{data}
folder within the Project folder.

In this chunk we read the \texttt{growth.csv} data into an R object to
which we give the name \texttt{weights}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filepath }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"growth.csv"}\NormalTok{)}
\NormalTok{weights }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(filepath) }\CommentTok{\# this function is from the readr package, part of tidyverse}
\FunctionTok{glimpse}\NormalTok{(weights)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 48
Columns: 3
$ supplement <chr> "supergain", "supergain", "supergain", "supergain", "contro~
$ diet       <chr> "wheat", "wheat", "wheat", "wheat", "wheat", "wheat", "whea~
$ gain       <dbl> 17.37125, 16.81489, 18.08184, 15.78175, 17.70656, 18.22717,~
\end{verbatim}

You see from the output of the \texttt{glimpse()} function that
\texttt{weights} has three columns and 48 rows. Two columns are of data
type \texttt{\textless{}chr\textgreater{}} which is R-speak for text,
and the other is data type \texttt{\textless{}dbl\textgreater{}} which
is R-speak for numerical data with a decimal point.

\section{Clean the data}\label{clean-the-data}

Having read in the data some cleaning/wrangling/tidying or processing of
the data is often required before we can go further with the analysis.

\textbf{Warning!} This step can be the most time-consuming of the whole
analysis, particularly if you are using a large data set obtained from a
third party.

Here there is not much to do, but we would like to make R recognise the
categorical variables as factors, and order the levels.

\subsection{\texorpdfstring{Force R to recognise \texttt{supplement} and
\texttt{diet} as factors, and to reorder their
levels.}{Force R to recognise supplement and diet as factors, and to reorder their levels.}}\label{force-r-to-recognise-supplement-and-diet-as-factors-and-to-reorder-their-levels.}

At the moment, the contents within the variables \texttt{supplement} and
\texttt{diet} are not being recognised as levels of factors. R is just
thinking of them as text (or \texttt{\textless{}chr\textgreater{}} in
R-speak), as we can see from the output of the \texttt{glimpse()}
function in the chunk above. Let us fix that, as it will be useful for
them to be recognized for what they are so that we can order the levels
in a way that makes sense for our context, our plots and our analysis.

Sometimes levels of a factor have a natural order, such as \emph{Low},
\emph{Mid} and \emph{High} as the levels of the factor \emph{Tidal Zone}
and sometimes they do not, for example \emph{Apples}, \emph{Oranges} and
\emph{Pears} as levels of the factor \emph{Fruit}. Here, in the case of
both our factors, we only wish to impose order among the levels in so
far as we would like what we regard as the control or reference level to
be first. By default, R puts the levels of a factor in alphabetical
order. This is the order in which the boxes of a box plot would be
displayed, reading left to right. In an ANOVA setting it means that
differences of outcome (in this case, weight gain of the cows) are later
calculated for each combination of levels with respect to the outcome
for the combination of levels that are alphabetically first, in this
case \texttt{barley} for \texttt{diet} and \texttt{agrimore} for
\texttt{supplement}. In both the box-plot and the ANOVA output case this
default ordering is not necessarily what we want. Normally, we want what
we regard as the control levels to be the reference level and in this
case that means \texttt{barley} for \texttt{diet} and \texttt{control}
for \texttt{supplement}.

To ensure that a variable is regarded as a factor, and then to get its
levels in the order we would like, we use the \texttt{factor()}
function.

In the following chunk, \texttt{factor()} is used to designate both the
\texttt{supplement} and \texttt{diet} columns of the data set as
factors, and the level order of each is specified, with \texttt{control}
coming first for \texttt{supplement} and \texttt{barley} coming first
for \texttt{diet}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This line of code designates the supplement and diet columns of weights as factors, orders the levels of these factors as required and saves the result under the original name.}
\NormalTok{weights }\OtherTok{\textless{}{-}}\NormalTok{ weights }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{supplement =} \FunctionTok{factor}\NormalTok{(supplement, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"control"}\NormalTok{,}\StringTok{"agrimore"}\NormalTok{, }\StringTok{"supergain"}\NormalTok{, }\StringTok{"supersupp"}\NormalTok{))) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{diet =} \FunctionTok{factor}\NormalTok{(diet, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"barley"}\NormalTok{, }\StringTok{"oats"}\NormalTok{, }\StringTok{"wheat"}\NormalTok{)))}

\CommentTok{\# check that this worked}
\FunctionTok{glimpse}\NormalTok{(weights)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 48
Columns: 3
$ supplement <fct> supergain, supergain, supergain, supergain, control, contro~
$ diet       <fct> wheat, wheat, wheat, wheat, wheat, wheat, wheat, wheat, whe~
$ gain       <dbl> 17.37125, 16.81489, 18.08184, 15.78175, 17.70656, 18.22717,~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# check the level order of each factor {-} does the \textquotesingle{}reference\textquotesingle{} level come first?}
\FunctionTok{levels}\NormalTok{(weights}\SpecialCharTok{$}\NormalTok{supplement)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "control"   "agrimore"  "supergain" "supersupp"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{levels}\NormalTok{(weights}\SpecialCharTok{$}\NormalTok{diet)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "barley" "oats"   "wheat" 
\end{verbatim}

Do you see how the variable types of the \texttt{supplement} and
\texttt{diet} columns have been changed to
\texttt{\textless{}fct\textgreater{}}? It worked!

To get \texttt{control} to be the reference level of \texttt{supplement}
we needed to force the issue in this way. If we hadn't then
\texttt{agrimore} would have been regarded as such, since it is
alphabetically the first among the levels of \texttt{supplement}. We
didn't need to do this for \texttt{diet}, since the stipulated ordering
of the levels is just the alphabetical order and so we would have had
that by default anyway. Sometimes, though, it doesn't hurt to throw in a
little redundancy for the sake of clarity.

So, now we have \texttt{control} as the reference level for
\texttt{supplement} and \texttt{barley} as the reference level for
\texttt{diet}. Now we can see more easily in our analysis what
difference is made to weight gain when we change diet or supplement or
both from a `business as usual' combination of a \texttt{barley}diet and
the \texttt{control} supplement.

\section{Summarise the data}\label{summarise-the-data-5}

Our question is a difference question: is there evidence from the data
that using this or that diet in combination with this or that supplement
makes a difference to growth? For an answer to this we will end up doing
a 2-way ANOVA including the possibility of an interaction, then, as we
will see, a simpler ANOVA that ignores the possibility of the
interaction. All well and good, but before we go to those lengths, we do
something more basic: we calculate the mean and standard error of the
mean for each of the twelve combinations of diet and supplement.

There isn't a function in base R with which we can calculate standard
error of the mean directly, but we can do so knowing the standard
deviation of the sample \(\text{SD}\) (using \texttt{sd()}) and the
sample size \(n\) (using \texttt{n()}) using this formula:

\[ \text{SE}=\frac{SD}{\sqrt{n}}\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# we use the group\_by() and summarise() functions from dplyr (the package within tidyverse for data manipulation)}
\NormalTok{growth\_summary }\OtherTok{\textless{}{-}}\NormalTok{ weights }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(diet, supplement) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_gain =} \FunctionTok{mean}\NormalTok{(gain), }\AttributeTok{se\_gain =} \FunctionTok{sd}\NormalTok{(gain)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{n}\NormalTok{())) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{()}

\CommentTok{\# if we type the name of an object, it gets printed out for us}
\NormalTok{growth\_summary }\SpecialCharTok{|\textgreater{}} \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{l|l|r|r}
\hline
diet & supplement & mean\_gain & se\_gain\\
\hline
barley & control & 23.29665 & 0.7032491\\
\hline
barley & agrimore & 26.34848 & 0.9187479\\
\hline
barley & supergain & 22.46612 & 0.7710644\\
\hline
barley & supersupp & 25.57530 & 1.0599015\\
\hline
oats & control & 20.49366 & 0.5056319\\
\hline
oats & agrimore & 23.29838 & 0.6131592\\
\hline
oats & supergain & 19.66300 & 0.3489388\\
\hline
oats & supersupp & 21.86023 & 0.4132292\\
\hline
wheat & control & 17.40552 & 0.4604420\\
\hline
wheat & agrimore & 19.63907 & 0.7099260\\
\hline
wheat & supergain & 17.01243 & 0.4852821\\
\hline
wheat & supersupp & 19.66834 & 0.4746443\\
\hline
\end{tabular}
\end{table}

Note the ordering of the diet and supplement levels in their respective
columns: just what we have imposed!

\section{Plot the data}\label{plot-the-data-6}

The next step, as so often before we launch into actual statistics, is
to plot the data in a way that sheds light on the question we have.
Here, we can use the use the means and standard errors of the mean that
we have just calculated to produce a useful kind of line plot that in
this context is often referred to as an interaction plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{growth\_summary }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ supplement,}\AttributeTok{y =}\NormalTok{ mean\_gain, }\AttributeTok{colour =}\NormalTok{ diet, }\AttributeTok{group =}\NormalTok{ diet)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ mean\_gain }\SpecialCharTok{{-}}\NormalTok{ se\_gain, }\AttributeTok{ymax =}\NormalTok{ mean\_gain }\SpecialCharTok{+}\NormalTok{ se\_gain), }\AttributeTok{width =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Supplement"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Mean weight gain"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ANOVA_two_way_with_model_simplification_files/figure-pdf/interaction plot one-1.pdf}}

Note that on this plot the error bars are standard errors of the mean.
Any caption to a figure that contains error bars should explain what
those error bars mean. In particular, it should say whether they are
standard deviations of the sample, standard errors of the mean or
confidence intervals. These are all different from each other. A good
explanation of the difference is given by (Cumming, Fidler, and Vaux
2007)

This interaction plot is useful in that we see that both diet and
supplement have an effect on growth and that the effect of one is
altered little by the value of the other, the result of which is that
the lines are more or less parallel. This suggests that we have
\emph{main effects} of both diet and supplement, but little or no
\emph{interaction} between them.

\subsection{Questions}\label{questions}

What could the line plot look like if:

\begin{itemize}
\item
  There were no main effect of both diet and supplement, and no
  interaction
\item
  There were a main effect of diet, no main effect of supplement and no
  interaction?
\item
  There were no main effect of diet, a main effect of supplement and no
  interaction?
\item
  There were main effects of both and an interaction between them?
\end{itemize}

The plots tell you a great deal about what main effects and/or
interactions there may be.

\section{ANOVA}\label{anova}

Now for the actual statistical test. We will conduct a two-way ANOVA,
which will look to see if there is evidence that either diet or
supplement or both affect growth rate (the so-called main effects), and
if the effect of one depends on the nature of the other (the so-called
interaction).

The null hypothesis is that neither has any main effect and that there
is no interaction.

Now we can use either of the functions \texttt{aov()} or \texttt{lm()}
to carry out a factorial ANOVA (the choice affects only whether we get
an ANOVA table or a list of parameter estimates as the default output
from \texttt{summary()}.). Here, we will use \texttt{lm()}, partly
because we would also use it for one-way ANOVAs and linear regression,
and to do so here reminds of the common mathematical machinery that
underlies all these methods.

We estimate parameters for the main effects of each level of diet and
each level of supplement, plus terms for the interaction between diet
and supplement.

The interaction degrees of freedom are the product of those for diet and
supplement ie (3-1) x (4-1) = 6.

The model is:

\texttt{gain\ \textasciitilde{}\ diet\ +\ supplement\ +\ diet:supplement}

which can be written more simply using the asterisk notation as:

\texttt{gain\ \textasciitilde{}\ diet\ *\ supplement}

\subsection{Construct the model}\label{construct-the-model-1}

First we construct the model using \texttt{lm()} and store the outputs
of all the maths that `\texttt{lm()} does in an object called
\texttt{model0}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model0 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(gain }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diet }\SpecialCharTok{*}\NormalTok{ supplement, }\AttributeTok{data =}\NormalTok{ weights)}
\end{Highlighting}
\end{Shaded}

\subsection{Do we reject the null
hypothesis?}\label{do-we-reject-the-null-hypothesis}

To get an overall picture, we first use \texttt{anova()} to see if there
is evidence to reject the null

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(model0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: gain
                Df  Sum Sq Mean Sq F value    Pr(>F)    
diet             2 287.171 143.586 83.5201 2.999e-14 ***
supplement       3  91.881  30.627 17.8150 2.952e-07 ***
diet:supplement  6   3.406   0.568  0.3302    0.9166    
Residuals       36  61.890   1.719                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The ANOVA table shows that there a main effect of both \texttt{diet} and
\texttt{supplement} (\emph{p}\textless0.001 in both cases), but that
there is no hint of an interaction between \texttt{diet} and
\texttt{supplement} (\emph{p} = 0.917). Does that tally with what you
see in the interaction plot? Clearly therefore, the effects of
\texttt{diet} and \texttt{supplement} are merely additive (ie whichever
level of one you have it does not affect the impact on growth of
whichever level of the other you choose).

The ANOVA table does not show us effect sizes or allow us to work out
which if any of the levels of the two factors are significantly
different. For this, \texttt{summary()} is more useful:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = gain ~ diet * supplement, data = weights)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.48756 -1.00368 -0.07452  1.03496  2.68069 

Coefficients:
                                Estimate Std. Error t value Pr(>|t|)    
(Intercept)                   23.2966499  0.6555863  35.536  < 2e-16 ***
dietoats                      -2.8029851  0.9271390  -3.023  0.00459 ** 
dietwheat                     -5.8911317  0.9271390  -6.354 2.34e-07 ***
supplementagrimore             3.0518277  0.9271390   3.292  0.00224 ** 
supplementsupergain           -0.8305263  0.9271390  -0.896  0.37631    
supplementsupersupp            2.2786527  0.9271390   2.458  0.01893 *  
dietoats:supplementagrimore   -0.2471088  1.3111726  -0.188  0.85157    
dietwheat:supplementagrimore  -0.8182729  1.3111726  -0.624  0.53651    
dietoats:supplementsupergain  -0.0001351  1.3111726   0.000  0.99992    
dietwheat:supplementsupergain  0.4374395  1.3111726   0.334  0.74060    
dietoats:supplementsupersupp  -0.9120830  1.3111726  -0.696  0.49113    
dietwheat:supplementsupersupp -0.0158299  1.3111726  -0.012  0.99043    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.311 on 36 degrees of freedom
Multiple R-squared:  0.8607,    Adjusted R-squared:  0.8182 
F-statistic: 20.22 on 11 and 36 DF,  p-value: 3.295e-12
\end{verbatim}

This is a complex model as there are 12 estimated parameters: 6 main
effects and 6 interactions. Notice that although the `controls' for diet
and supplement (\texttt{barley} and \texttt{control}) do not appear to
be in the table, they are there really, in the first row.

The value 23.30 kg in the first row of the Estimate column on the left,
labelled `Intercept()' gives us the actual weight gain outcome for the
combination of the two control levels, \texttt{barley} as diet and
\texttt{control} as supplement. Check that this value tallies with what
is shown in summary tables above, and in the interaction plot.

The weight gain values for all the other combinations of the levels of
each factor are given as differences from this reference level.

So for example in row two, where diet is changed from barley to oats but
supplement is still control, the value in the table is -2.8. This means
that the weight gain when the diet is changed to oats but the supplement
left as the control is 2.80 kg less than the reference value, and so
must be 23.30-2.80 = 20.50 kg. This agrees with the value in the summary
table of mean values that was calculated above, and tallies with the
interaction plot.

In row seven we see that the effect of the interaction between the diet
\texttt{oats} and the supplement \texttt{agrimore} is - 0.247. This
means that on going from the reference levels of barley and control, for
which the gain is 23.30, the change in gain is not just the sum of the
two main effects (-2.80 for switch of diet to oats and +3.05 for switch
of supplement to agrimore, but is modified by their interaction, of size
- 0.247. Hence the mean gain for a diet of oats \emph{and} a supplement
of agrimore is the intercept value plus the sum of the two main effects,
plus the interaction term: 23.297 - 2.803 + 3.052 - 0.247 = 23.299)

See if you can tally the other effect values in the summary table with
the mean values given in table above and in the interaction plot for
other combinations of diet and supplement.

Here is a table to help you interpret the output of the
\texttt{summary()} function.

\begin{table}
\centering
\begin{tabular}[t]{l|l|l|r|r|l|l}
\hline
term & meaning & type\_of\_effect & estimate & absolute\_value & p\_value & significance\\
\hline
(Intercept) & barley + control & Main effect & 23.30 & 23.30 & <0.001 & ***\\
\hline
dietoats & oats + control & Main effect & -2.80 & 20.49 & 0.005 & **\\
\hline
dietwheat & wheat + control & Main effect & -5.89 & 17.41 & <0.001 & ***\\
\hline
supplementagrimore & barley + agrimore & Main effect & 3.05 & 26.35 & 0.002 & **\\
\hline
supplementsupergain & barley + supergain & Main effect & -0.83 & 22.47 & 0.376 & \\
\hline
supplementsupersupp & barley + supersupp & Main effect & 2.28 & 25.58 & 0.019 & *.\\
\hline
dietoats:supplementagrimore & oats + agrimore & Interaction & -0.25 & 23.05 & 0.852 & \\
\hline
dietwheat:supplementagrimore & wheat + agrimore & Interaction & -0.82 & 22.48 & 0.537 & \\
\hline
dietoats:supplementsupergain & oats + supergain & Interaction & 0.00 & 23.30 & 1.0 & \\
\hline
dietwheat:supplementsupergain & wheat + supergain & Interaction & 0.44 & 23.73 & 0.741 & \\
\hline
dietoats:supplementsupersupp & oats + supersupp & Interaction & -0.91 & 22.38 & 0.491 & \\
\hline
dietwheat:supplementsupersupp & wheat + supersupp & Interaction & -0.02 & 23.28 & 0.99 & \\
\hline
\end{tabular}
\end{table}

The output of the \texttt{summary()} function re-emphasises that none of
the interaction terms are significant. It also suggests that a minimum
adequate model will contain 5 parameters: an intercept, which just means
that there is non-zero growth when the diet and supplement are the
reference values, a difference from that growth due to changing the diet
to \texttt{oats}, a difference due to changing it to\texttt{wheat}, a
difference due to changing the supplement to \texttt{agrimore} while
keeping barley as the diet, and a difference due to changing the
supplement instead to \texttt{suppersupp}..

\section{Model Simplification}\label{model-simplification}

Given the results of the full interaction model, we begin model
simplification by leaving out the interaction terms, to leave us with an
additive model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(gain }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diet }\SpecialCharTok{+}\NormalTok{ supplement, }\AttributeTok{data =}\NormalTok{ weights)}
\FunctionTok{summary}\NormalTok{(model\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = gain ~ diet + supplement, data = weights)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.30792 -0.85929 -0.07713  0.92052  2.90615 

Coefficients:
                    Estimate Std. Error t value Pr(>|t|)    
(Intercept)          23.4263     0.4408  53.141  < 2e-16 ***
dietoats             -3.0928     0.4408  -7.016 1.38e-08 ***
dietwheat            -5.9903     0.4408 -13.589  < 2e-16 ***
supplementagrimore    2.6967     0.5090   5.298 4.03e-06 ***
supplementsupergain  -0.6848     0.5090  -1.345 0.185772    
supplementsupersupp   1.9693     0.5090   3.869 0.000375 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.247 on 42 degrees of freedom
Multiple R-squared:  0.8531,    Adjusted R-squared:  0.8356 
F-statistic: 48.76 on 5 and 42 DF,  p-value: < 2.2e-16
\end{verbatim}

\section{Check the validity of the additive
model}\label{check-the-validity-of-the-additive-model}

We ought to pause here for a moment and just check that we are OK to go
ahead and analyse our data using a general linear model (of which ANOVA
is an example, linear regression and t-tests being others). We will use
\texttt{autoplot()} from the \texttt{ggfortify} package, which gives us
the standard four diagnostic plots.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autoplot}\NormalTok{(model\_1) }\SpecialCharTok{+} \FunctionTok{theme\_cowplot}\NormalTok{() }\CommentTok{\# autoplot() is from the ggfortify package.}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ANOVA_two_way_with_model_simplification_files/figure-pdf/diagnostic plots-1.pdf}}

Well, that all looks fine. In particular, from the top-left figure we
see that the variance of the residuals is more or less constant and from
the top-right figure, the quantile-quantile plot, we get a pretty good
approximation of a straight line which tells us that the residuals are
more or less normally distributed. These are two key assumptions that
must be at least approximately satisfied by data if it is going to make
any sense to use a linear model to analyse it. We won't discuss here the
other two diagnostic plots, but they look fine too. So we are good to go
using ANOVA with this data.

Back to interpreting the output of the ANOVA:

It is clear that we need to retain all three levels of diet since the
effect values of each differ from each other by an amount that is
several times the standand errors, so that \emph{t}
\textgreater\textgreater{} 1. It is not clear that we need all the
levels of supplement, however. \texttt{supersupp} is not obviously
different from \texttt{agrimore} (difference = -0.727 with standard
error = 0.509), yet both are clearly different from \texttt{control}.
However \texttt{supergrain} is not obviously different from
\texttt{control} (difference = -0.68, error = 0.509). Hence we are
tempted to try a new model with just two levels of the factor supplement
which we might sensibly call ``best'', by which we mean
\texttt{agrimore} or \texttt{supersupp}, and ``worst'' by which we mean
\texttt{control} or \texttt{supergrain}. We'll name this new factor
\texttt{supp2}.

This code chunk amends the \texttt{weights} data frame by adding a new
column to it called \texttt{supp2} in which the values are either
\texttt{best} if the supplement is agrimore or supersupp, or
\texttt{worst} if the supplement is either of the other two

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weights }\OtherTok{\textless{}{-}}\NormalTok{ weights }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{supp2 =} \FunctionTok{ifelse}\NormalTok{(supplement }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"agrimore"}\NormalTok{, }\StringTok{"supersupp"}\NormalTok{), }\StringTok{"best"}\NormalTok{, }\StringTok{"worst"}\NormalTok{))}
\FunctionTok{glimpse}\NormalTok{(weights)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 48
Columns: 4
$ supplement <fct> supergain, supergain, supergain, supergain, control, contro~
$ diet       <fct> wheat, wheat, wheat, wheat, wheat, wheat, wheat, wheat, whe~
$ gain       <dbl> 17.37125, 16.81489, 18.08184, 15.78175, 17.70656, 18.22717,~
$ supp2      <chr> "worst", "worst", "worst", "worst", "worst", "worst", "wors~
\end{verbatim}

If we calculate the means and standard errors for weight gain under each
diet for each of the two new classifications of supplement, and then
plot them, we get this new interaction plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weights }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(diet, supp2) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_gain =} \FunctionTok{mean}\NormalTok{(gain), }\AttributeTok{se\_gain =} \FunctionTok{sd}\NormalTok{(gain)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{n}\NormalTok{())) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}

  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ supp2,}\AttributeTok{y =}\NormalTok{ mean\_gain,}\AttributeTok{colour =}\NormalTok{ diet, }\AttributeTok{group=}\NormalTok{diet)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size=}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ mean\_gain}\SpecialCharTok{{-}}\NormalTok{se\_gain, }\AttributeTok{ymax =}\NormalTok{ mean\_gain }\SpecialCharTok{+}\NormalTok{ se\_gain), }\AttributeTok{width=}\FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Supplement"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Mean weight gain"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ANOVA_two_way_with_model_simplification_files/figure-pdf/simplified additive model plot-1.pdf}}

From this we can see that diet clearly makes a difference to weight
gain, since the three lines are separated by a distance much larger than
the standard errors, and also that the best supplement clearly makes a
difference since there is a consistent drop on going from `best' to
`worst', again by an amount that is much larger than the error bars, and
there is clearly no interaction between diet and supplement, since the
lines are parallel within the wiggle-room allowed by the error bars,
which means that the effect of diet does not depend on supplement, and
the effect of supplement does not depend on diet.

Now we will make the simpler model, calling it model\_2 (for comparison
with the first additive model, model\_1)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# additive model whee the supplements have been condensed from four to two: best and worst}
\NormalTok{model\_2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(gain }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diet }\SpecialCharTok{+}\NormalTok{ supp2, }\AttributeTok{data =}\NormalTok{ weights)}
\end{Highlighting}
\end{Shaded}

and then compare the two additive models:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(model\_1, model\_2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: gain ~ diet + supplement
Model 2: gain ~ diet + supp2
  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     42 65.296                           
2     44 71.284 -2   -5.9876 1.9257 0.1584
\end{verbatim}

When we use \texttt{anova()} in this way it is testing the explanatory
power of the second model against that of the first ie how much of the
variance in the data does each explain. Its null hypothesis is that both
models explain just as much of the variance as the other.

The simpler model has saved two degrees of freedom and is not
significantly different in explanatory power than the more complex model
(\emph{p} = 0.158). Hence this is a better candidate as a minimal
adequate model. All the parameters are significantly different from zero
and from each other.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model\_2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = gain ~ diet + supp2, data = weights)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.6716 -0.9432 -0.1918  0.9293  3.2698 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  25.7593     0.3674  70.106  < 2e-16 ***
dietoats     -3.0928     0.4500  -6.873 1.76e-08 ***
dietwheat    -5.9903     0.4500 -13.311  < 2e-16 ***
supp2worst   -2.6754     0.3674  -7.281 4.43e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.273 on 44 degrees of freedom
Multiple R-squared:  0.8396,    Adjusted R-squared:  0.8286 
F-statistic: 76.76 on 3 and 44 DF,  p-value: < 2.2e-16
\end{verbatim}

In this table,

\begin{itemize}
\tightlist
\item
  line one \texttt{(Intercept)} tells us that the mean weight gain when
  on the barley diet and best supplement is 25.76 kg
\item
  line two \texttt{dietoats} tells us that there is a significant drop
  in weight gain of 3.1 kg when diet is changed to oats.
\item
  line three \texttt{dietwheat} tells us that there is a significant
  drop in weight gain of 5.99 kg when diet is changed to wheat.
\item
  line four \texttt{supp2worst} tells us that there is a significant
  drop in wight gain of 2.68 kg when supplement is changed to worst.
\end{itemize}

In all cases, p\textless{} 0.001, as indicated not only by the number in
the Pr(\textgreater\textbar t\textbar) column, but also by the `***' in
the right-most column of the table.

\section{Reporting the results}\label{reporting-the-results}

We have now reduced our initial 12 parameter model to a four parameter
model that is much more tractable and easier to communicate. Our advice
would be that for maximum weight gain a diet of \texttt{barley} with a
supplement of \texttt{agrimore} or \texttt{supersupp} would be best.

If we were reporting this as a statistical test, we might say something
like: A diet of barley with a supplement of agrimore or supersupp was to
offer significant improvements over alternatives. There was no evidence
of any interaction between diet and supplement. (ANOVA 2-way,
F\textsubscript{3,44} = 76.76, p \textless{} 0.001)

\part{Count data}

\chapter{Chi-squared analysis of count
data}\label{chi-squared-analysis-of-count-data}

A chi-square analysis is used when our data are in the form of raw
counts for two or more categorical groups eg pea plants with either
yellow peas or green peas, survival rate of mice if they took drug A or
took drug B, etc. Each independent observation must definitely belong to
either one group or the other, and there are no replicates. That is, for
each category we just have one count.

What we do is compare the counts we got to some \emph{expected} value
according either to chance or to some prior theory.

For example:

\begin{itemize}
\item
  If we were tossing a fair coin 1000 times we would \emph{expect} 500
  heads and 500 tails, ie heads and tails in the proportion 1:1. In
  reality, if the coin were fair, we would probably get roughly the same
  number of heads and tails, but probably not exactly 500 of each. How
  far from 50:50 would the proportion of heads and tails have to be
  before we would be justified in rejecting the idea that the coin is
  fair?
\item
  If we threw a fair dice a large number of times we would \emph{expect}
  each possible score, from 1 to 6, to occur the same number of times.
  ie each score would occur 1/6th of the time. In reality we would
  probably get each score \emph{roughly} 1/6 of the time, but not
  exactly 1/6. How far from the expected proportions could the numbers
  of each score get before we would be justified in thinking that the
  dice was not fair?
\item
  If the basic idea of Mendelian inheritance with independent assortment
  involving a single gene is correct, then we would expect that if we
  crossed a large number of pea plants that were heterozygous for yellow
  and green pea colour, with yellow being dominant, then the offspring
  would have yellow:green peas in the ratio 3:1 (plants with genotypes
  YY, Yy, yY would all have yellow seeds and only those with yy would
  have green seeds). In practice, if the inheritance \emph{were}
  according to the simplest type of Mendelian inheritance (ie involving
  just one gene, amongst other things) we would probably get
  yellow:green offspring in a ratio that was \emph{approximately} but
  not exactly 3:1. How far from 3:1 would the ratio need to be before we
  would justified in claiming that the outcome was inconsistent with the
  Mendelian prediction?
\end{itemize}

\includegraphics[width=5.29in,height=\textheight,keepaspectratio]{figures/mendelian_pea_plants.png}

\section{Chi-square goodness of fit
test}\label{chi-square-goodness-of-fit-test}

In a chi-square `goodness of fit' test, we are testing data where we
have a number of counts for each of two or more possible outcomes of
some procedure (heads/tails, dice scores etc). We have an idea of how
these counts should be distributed under some null hypothesis (the coin
is fair, the dice is fair, genetic inheritance works in this or that way
etc). The chi-square goodness of fit test tests how likely it is we
would have got the counts we actually got if that null hypothesis were
correct. We are testing how well our actual counts `fit' the expected
values.

In a typical software implementation of the test, such as in R, we give
it the counts we actually got for each possible outcome and also the
expected proportion for each outcome. The test then gives us a p-value,
a probability, for how likely it is that we would have got the counts we
actually got, or counts even further from the null hypothesis, if that
null hypothesis were correct. If this p-value is too small, and by that
we usually mean less than 0.05, then we reject the null hypothesis.

\subsection{Example}\label{example-2}

Suppose we have crossed pea plants that were all heterozygous for
yellow/green pea colour. In the F1 generation we get 176 offspring , of
which 130 were yellow and 46 were green.

The data here are raw counts, and an individual pea plant offspring
contributes either to the yellow count or to the green count, but not to
both.

Our expected counts of yellow and green are found by simply dividing the
total count of offspring, 176, in the ratio 3:1, giving us an expected
132 yellow pea plants and an expected 44 green pea plants in the
offspring F1 generation.

\subsection{Doing the chi-square test in
R}\label{doing-the-chi-square-test-in-r}

What we do in R is use the \texttt{chisq.test()} function to see how
likely it is we would have got counts of 130 and 46 if the null
hypothesis, with its expected counts in the ratio 3:1, were true.

We do it like this:

\texttt{chisq.test(c(130,46),p=c(0.75,0.25))}

There are two arguments. The first is the counts we got, which we enter
as a `vector' \texttt{c(z,y,....)}, so we write \texttt{c(130,46)}. The
second is a vector of the proportions we expect for the two counts,
where these proportions should add up to one. So for our expected 3:1
ratio we enter \texttt{c(0.75,0.25)}.

Let's do it: type the above function into the console window (bottom
left). You will get an output something like this:

\begin{verbatim}

    Chi-squared test for given probabilities

data:  c(130, 46)
X-squared = 0.12121, df = 1, p-value = 0.7277
\end{verbatim}

This output is typical of tests done in R. We get the `test statistic'
whose name varies depending on the test. Here it is called
\texttt{X-squared}, pronounced \texttt{chi-squared}. This is a number
that the test calculates, based on the data you have given it. For the
most part, we don't need to worry about how it does that. Then there is
the p-value, which is the probability of getting this test statistic if
the null hypothesis were true.

In this case, we see that the p-value is 0.73, which is large. We could
very plausibly have got yellow:green numbers of 130 and 46 if the null
hypothesis were true, so we cannot reject that null hypothesis. In other
words, our data are consistent at the 5\% significance level with the
predictions of simple Mendelian inheritance.

\subsection{Reporting the result in
English}\label{reporting-the-result-in-english}

In English, we might report this result as:

\emph{We found counts of 130 yellow plants and 46 green plants, which
are consistent at the 5\% significance level with the predictions of
Mendelian inheritance (chi-squared test, X-squared = 0.12, p=0.73).}

Note that we do not say we have proved Mendelian inheritance to be
correct. We haven't. We never prove things in science. We haven't said
anything about the truth of the null hypothesis. All we can say is
whether our data are or are not consistent with the null hypothesis. In
this case they are. We then report the test we used and the values of
the test statistic and p-value. Other tests might give you other details
to report too.

\subsection{Exercises}\label{exercises}

\textbf{Exercise 1}

Suppose you tossed a fair coin 100 times and got 45 heads and 55 tails.

\begin{itemize}
\tightlist
\item
  Under a null hypothesis that the coin is fair, what would the expected
  numbers of heads and tails be?
\end{itemize}

You use R to do a chi-square test of that null hypothesis. Here is the
code to do that and the output it would give:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{45}\NormalTok{,}\DecValTok{55}\NormalTok{),}\AttributeTok{p=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{,}\FloatTok{0.5}\NormalTok{)) }\CommentTok{\# we could leave out the second argument here}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Chi-squared test for given probabilities

data:  c(45, 55)
X-squared = 1, df = 1, p-value = 0.3173
\end{verbatim}

\begin{itemize}
\item
  What do you conclude?
\item
  How would you report the result?
\end{itemize}

\textbf{Exercise 2}

Suppose someone told you that the competence of scientists was linked to
their astrological zodiac sign. I won't name all of these, but there are
twelve of them: Pisces, Scorpio, Cancer etc. To test this hypothesis,
you spend a lot of time on Primo and identify 240 scientists, currently
active, that have each published at least five papers in high impact
journals in the last year. All of these people, you presume, are
successful scientists. You write to each of them and ask them their date
of birth. Amazingly(!) all of them respond. You then assign each of them
to a zodiac sign according to their birth date and get the following
counts for each sign:

In this code chunk we have typed out the counts and collected them as a
vector, using the function \texttt{c()}, we have saved this under the
name \texttt{stars}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stars}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{22}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{17}\NormalTok{,}\DecValTok{22}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{19}\NormalTok{,}\DecValTok{18}\NormalTok{,}\DecValTok{21}\NormalTok{,}\DecValTok{19}\NormalTok{,}\DecValTok{22}\NormalTok{,}\DecValTok{23}\NormalTok{,}\DecValTok{17}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  What would be a suitable null hypothesis in this investigation?
\item
  What proportion of the total count would we expect for each star sign
  if this null were true?
\item
  The data meet the criteria required for use of a chi-square goodness
  of fit test. How can we tell?
\item
  Use the \texttt{chisq.test()} function to implement this test.
\item
  On the basis of the output of the test, do you reject the null
  hypothesis?
\item
  Report the result of the test in plain English.
\end{itemize}

\subsection{Solutions}\label{solutions}

\textbf{Solution 1}

The expectation is that half the outcomes would be heads and half would
be tails.

The null hypothesis of this test is that heads and tails are equally
likely, ie that the coin is fair. Under this null hypothesis the
expected outcome is 50 heads and 50 tails. From the output of the R code
we see that the p-value, the probability of getting an outcome as far or
further from that, is 0.317. That is pretty high. Would you do anything
if you knew that the probability of a bad (or worse) outcome was 0.317?
In particular, this p-value is greater than 0.05, so we cannot reject
the null hypothesis that the coin is fair. That is, even with a fair
coin it is not at all unlikely that you would get head/tail numbers as
different from 50/50 as 45/55 if you tossed the coin 100 times. That
will happen about 1/3 of the time if you repeatedly do trials where you
toss the coin 100 times.

To report this result, you might say something like

\emph{From 100 coin tosses we got 45 heads and 55 tails. These counts
are consistent at the 5\% significance level with the coin being fair
(chi-squared test, X-squared = 1, p=0.317).}

\textbf{Solution 2}

\begin{itemize}
\tightlist
\item
  H0: There is no association between the astrological star sign of a
  researcher and their success in science (who knew?)
\item
  One twelfth for each sign ie a researcher is as likely to have one
  star sign as any other.
\item
  The are count data, there are at least five counts for every sign and
  the counts are independent - any individual researcher only
  contributes to one of the twelve counts.
\item
  Note that we do not need to include the second \texttt{p=...} argument
  in this case since the default presumption, that all proportions are
  equal, is true here.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(stars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Chi-squared test for given probabilities

data:  stars
X-squared = 2.3, df = 11, p-value = 0.9971
\end{verbatim}

\begin{itemize}
\tightlist
\item
  We see that the p-value is almost one so we emphatically do not reject
  the null hypothesis.
\item
  We find no evidence that star sign affects success in science
  (X-sq=2.29, df = 11, p=0.997)
\end{itemize}

Note the degrees of freedom that is reported: df = 11. The degrees of
freedom is the number of independent pieces of information. Here, given
that we know the total number of researchers, only eleven of the
individual counts are independent. Once they are known, the twelfth can
be calculated.

\section{Two-way Chi square analysis: test of
independence}\label{two-way-chi-square-analysis-test-of-independence}

\emph{Adapted from Chapter 5: Beckerman, Childs and Petchey: Getting
Started with R}

\begin{itemize}
\tightlist
\item
  For R code to implement this section, scroll down to the next section
  *
\end{itemize}

A common scenario where we have with count data is that there are two
explanatory factors each with two or more levels that enable us to
classify the data. This can happen when something is either true or not
true, and a test for for this truth gives either a positive or a
negative result. We might want to know if the test to determine truth
was any better than flipping a coin: is there some association between
what the truth is (eg I do or do not have a disease) and what the test
says about that (testing positive or negative for the disease). In this
example our data would be counts of people in each of four categories:
have the disease / test positive, have the disease / test negative, do
not have the disease / test positive and do not have the disease / test
negative.

\subsection{Example - red and black
ladybirds}\label{example---red-and-black-ladybirds}

We are going to analyse a scenario of this type to see if there is
evidence for an association between two factors. Suppose we have some
count data of ladybirds found in both an industrial and a rural
location. In each location, some of the ladybirds are red and some are
black. We would like to test for whether there is an association between
the ladybird colour and its location. If there isn't then we would
expect the proportion of black to red ladybirds to be roughly the same
in both habitats. If there is, then we would expect the proportions to
be different, meaning that knowing the habitat would tell us something
about the likelihood of a ladybird found there being black or red. That
is way of saying that the colour of the ladybirds would not be
independent of the location.

Behind this the research purpose might be to investigate whether
matching of morphological colour of the ladybirds to the prevalent
colour of the environment confers an evolutionary advantage. If it does
then we would expect there to be an association between morphological
colour and environment so that the proportion of black to red ladybirds
would be higher in a grimy industrial setting than in it would be in a
rural setting.

A \textbf{Chi-Square contingency analysis} can be used to investigate
this. This type of analysis is used when you have

\begin{itemize}
\tightlist
\item
  Count data - for example, how many red ladybirds in a rural setting,
  how many in an industrial setting, how many black ladydbirds in each
  of the settings?
\item
  Enough count data - typically at least 5 individuals for each
  combination of the levels in question, which would be rural/red,
  rural/black, industrial/red and industrial/black in this case.
\item
  Independent counts - each ladybird contributes to only one sub-total.
  For example, if it is red and found in a rural location, then it
  contributes to the count of red ladybirds found in a rural location,
  and not to any other sub-total, such as black ladybirds found in a
  rural location.
\end{itemize}

\subsubsection{Hypotheses}\label{hypotheses}

What do you think a suitable hypothesis should be for this
investigation, and what would the corresponding null hypothesis be?

\begin{itemize}
\tightlist
\item
  The null hypotheses could be: \textbf{H\textsubscript{0}}: There is no
  association between habitat and ladybird colour. This means that
  whatever the proportion is of black to red ladybirds, it is the same
  in both habitats.\\
\item
  The alternate hypothesis could be: \textbf{H1}: There is an
  association between habitat and ladybird colour.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load packages we need}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{library}\NormalTok{(cowplot) }\CommentTok{\# this makes your plots look nicer}
\FunctionTok{library}\NormalTok{(kableExtra)}
\FunctionTok{theme\_set}\NormalTok{(}\FunctionTok{theme\_cowplot}\NormalTok{()) }\CommentTok{\# this sets the cowplot theme to be the default theme for any plots we make..}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Import the data}
\NormalTok{filepath}\OtherTok{\textless{}{-}}\FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"ladybirds\_morph\_colour.csv"}\NormalTok{)}
\NormalTok{lady}\OtherTok{\textless{}{-}}\FunctionTok{read\_csv}\NormalTok{(filepath)}
\CommentTok{\#glimpse(lady)}
\end{Highlighting}
\end{Shaded}

\subsubsection{The data}\label{the-data-1}

The data consist of counts of the number of ladybirds of each colour
that were observed in 5 rural sites and 5 industrial sites. These data
are in tidy form, with one variable per column. There are 20 rows, with
each row containing the count of either red or black ladybirds found at
a given site.

\begin{verbatim}
# A tibble: 20 x 4
   Habitat    Site  morph_colour number
   <chr>      <chr> <chr>         <dbl>
 1 Rural      R1    black            10
 2 Rural      R2    black             3
 3 Rural      R3    black             4
 4 Rural      R4    black             7
 5 Rural      R5    black             6
 6 Rural      R1    red              15
 7 Rural      R2    red              18
 8 Rural      R3    red               9
 9 Rural      R4    red              12
10 Rural      R5    red              16
11 Industrial U1    black            32
12 Industrial U2    black            25
13 Industrial U3    black            25
14 Industrial U4    black            17
15 Industrial U5    black            16
16 Industrial U1    red              17
17 Industrial U2    red              23
18 Industrial U3    red              21
19 Industrial U4    red               9
20 Industrial U5    red              15
\end{verbatim}

The total counts for red and black ladybirds observed in industrial and
rural settings are shown below:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the totals of each colour in each habitat.}
\NormalTok{totals}\OtherTok{\textless{}{-}}\NormalTok{ lady }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(Habitat,morph\_colour) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{ (}\AttributeTok{total.number =} \FunctionTok{sum}\NormalTok{(number))}
\CommentTok{\# totals |\textgreater{}}
\CommentTok{\#   kbl() |\textgreater{}}
\CommentTok{\#   kable\_styling(full\_width=FALSE)}
\NormalTok{totals}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 3
# Groups:   Habitat [2]
  Habitat    morph_colour total.number
  <chr>      <chr>               <dbl>
1 Industrial black                 115
2 Industrial red                    85
3 Rural      black                  30
4 Rural      red                    70
\end{verbatim}

\subsubsection{Plot the data.}\label{plot-the-data.}

From these totals we can create a bar chart:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot the data, with sensible colours}
\NormalTok{totals }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Habitat,}\AttributeTok{y =}\NormalTok{ total.number,}\AttributeTok{fill=}\NormalTok{morph\_colour))}\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{position=}\StringTok{\textquotesingle{}dodge\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Habitat"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Count"}\NormalTok{,}
       \AttributeTok{fill=} \StringTok{"Colour"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\AttributeTok{black=}\StringTok{\textquotesingle{}\#312626\textquotesingle{}}\NormalTok{,}\AttributeTok{red=}\StringTok{\textquotesingle{}\#da1717\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+} \CommentTok{\# this line manually sets the fill colours for us}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chi-square_files/figure-pdf/unnamed-chunk-12-1.pdf}}

\subsubsection{Interpret the graph before we do any
`stats'}\label{interpret-the-graph-before-we-do-any-stats}

Look at the plot - does it look as though the proportion of black to red
ladybirds is the same in the two habitats? Do you expect to retain or to
reject the null hypothesis, which says that there is no association
between habitat and ladybird colour, and hence that the proportions are
the same?

A chi-square test of independence will enable us to determine how likely
it is that we would have got proportions of black to red as different as
or more different than they actually are if the null hypothesis were
true.

\subsubsection{The Chi-square test}\label{the-chi-square-test}

To do the chi square test, it helps to set out our count data as a 2 x 2
table of total counts:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lady.mat}\OtherTok{\textless{}{-}}\FunctionTok{xtabs}\NormalTok{(number}\SpecialCharTok{\textasciitilde{}}\NormalTok{Habitat }\SpecialCharTok{+}\NormalTok{ morph\_colour, }\AttributeTok{data=}\NormalTok{lady)}
\NormalTok{lady.mat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            morph_colour
Habitat      black red
  Industrial   115  85
  Rural         30  70
\end{verbatim}

This kind of table is sometimes called a \textbf{contingency table}.

When we give these numbers to some statistical software such as R and
ask it to carry out a `chi-square test' it will use the data to
calculate a `test statistic' \(X^2\) by comparing the actual counts of
the ladybirds in the table above with their expected counts under the
null hypothesis. The further the actual counts are from their expected
values, on the whole, the bigger this test statistic will be. For the
gory (they are not that gory!) details on how this is done, see section
4 below but do note that, while these are interesting, if you find that
kind of thing interesting, as I do, you do not need to be familiar with
them to be able to apply a chi-square test. What you do need to know is
when it is OK to use one, and when it is not, as is true for any
statistical test.

We'll turn to that issue now:

Providing a number of conditions are met by the data (principally, that
they are count data, that all the cell values are greater than or equal
to about five and that they are all independent ie any ladybird
contributes to the count of only one cell), this test statistic \(X^2\)
has a so-called `chi-squared' distribution. This is a known mathematical
distribution, which makes it possible to calculate the probability that
the statistic would be as big as it is, or bigger, if the null
hypothesis were true. We call this probability the \emph{p}-value.

This is generally how statistical tests work. They take your data and
use it in a carefully chosen way to calculate some number that in
general is called a \emph{test statistic} but which is referred to by
different names when calculated for particular tests. How it is
calculated depends on the test and these days we never have to manually
do the calculations ourselves. That's taken care of by software like R.
Providing the data meet certain criteria, the statistic will typically
have a known probability distribution. This means that the probability
that it will exceed a given value if the null hypothesis is true can be
calculated. This probability is the \emph{p}-value. If the
\emph{p}-value is very small, and by that we typically mean less than
0.05 or 0.01, then we can reject the null hypothesis.

When we run a chi-square test in R on the data in the table above it
gives us this as output:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(lady.mat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's Chi-squared test with Yates' continuity correction

data:  lady.mat
X-squared = 19.103, df = 1, p-value = 1.239e-05
\end{verbatim}

\subsubsection{Conclusion}\label{conclusion}

Study the output of the chi-square test. Note that you are given a
test-statistic (here called Chi-squared/X-squared) and a number of
degrees of freedom (\emph{df}) (in some tests you are given more than
one of these). This is the number of independent pieces of information
used to calculate the test statistic. Lastly, you are a given a
\emph{p}-value. This is the probability that you would have got a
chi-squared value as big or bigger than the one you got \emph{if} the
null hypothesis were true. Here the null hypothesis is that there is no
association between ladybird colour and location. Put another way, it
is, roughly speaking the probability of getting the data you actually
got if the null hypothesis were true.

Here, the p-value is much less than 0.05, so we can safely \emph{reject}
the null hypothesis. The ladybird colour does not appear to be
independent of the setting.

An appropriate way to report these data would be:

`Ladybird colour morphs are not equally common in the two habitats
(Chi-sq =19.3, df = 1, p\textless0.001)'

\subsubsection{Yates continuity
correction}\label{yates-continuity-correction}

This is mentioned is the output of the test. What does it mean? It
adjusts for the fact that our data are discrete and the chi-square
distribution that we are using to calculate the \emph{p}-values is
continuous. That's it.

\subsection{R script}\label{r-script}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_url }\OtherTok{\textless{}{-}} \StringTok{"https://raw.githubusercontent.com/mbh038/r4nqy/refs/heads/main/chi{-}square{-}test{-}of{-}independence{-}template.qmd"}
\FunctionTok{download.file}\NormalTok{(file\_url,}\StringTok{"chisq\_independence\_template.qmd"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Preliminaries}\label{preliminaries-1}

\begin{itemize}
\tightlist
\item
  Create an R notebook called \texttt{ladybirds} and save it in the
  scripts folder within your R project folder on your machine. You will
  find that it is saved as \texttt{ladybirds.Rmd}. Leave the
  \texttt{yaml} bit at the top, between the pairs of lines with three
  dashes (or maybe add your name and the date), then delete everything
  else.
\item
  Save the data file ``ladybirds\_morph\_colour.csv'' to the data folder
  in your R project folder, if is not already there.
\item
  Make sure that your Project folder is actually what RStudio recognises
  as a ``project''. You can navigate to and around it in the Files tab
  in the bottom right-hand pane of RStudio. If all is good you will see
  a .Rproj file at the top level of the project, and will apear at the
  top right of the RStudio window. If you don't see this, time now to
  turn your folder into a project!
\end{itemize}

Read the rest of this section and copy the code in the chunks provided
into chunks in your notebook. If you are lucky, your tutor will have
provided a template notebook for you! If you are not so lucky, remember
that each code chunk neeeds to be between two lines of three back ticks,
like this:

\begin{verbatim}
### A suitable heading
```{r}
# enter your code here

```
\end{verbatim}

so that your notebook will end up looking something like this:

\begin{verbatim}
---
title: " my R notebook"
date: "the date"
author: " your name"
---

### Load packages
```{r}
library(tidyverse)
librrary(here)
library(cowplot)
```

### Load data
```{r}
# enter your code here

```

### Summarise data
```{r}
# enter your code here

```

and so on...
\end{verbatim}

On some Windows machines, a shortcut to getting the back ticks around
each code chunk is to type \texttt{Ctrl-Alt-I}. Annoyingly, this does
not seem to work on all Windows machines. If that is the case for you,
you will have to type them in manually. On Macs, the shortcut is
\texttt{option-Cmd-I}.

Note that the code provided below is not the only way to do what we want
here. You are encouraged to play with it. For example, if you want to
see what a particular line does, you can `comment it out' by putting a
\# at the begining of the line, then running that line again. What
difference does it make?

\subsubsection{Load packages we need}\label{load-packages-we-need}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{library}\NormalTok{(cowplot) }\CommentTok{\# this makes your plots look nicer}
\end{Highlighting}
\end{Shaded}

\subsubsection{Import the data and inspect
it}\label{import-the-data-and-inspect-it}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filepath}\OtherTok{\textless{}{-}}\FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"ladybirds\_morph\_colour.csv"}\NormalTok{) }
\NormalTok{lady}\OtherTok{\textless{}{-}}\FunctionTok{read\_csv}\NormalTok{(filepath) }\CommentTok{\# read the data int an object (in this case, a data frame) called lady}
\FunctionTok{glimpse}\NormalTok{(lady) }\CommentTok{\# inspect it.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 20
Columns: 4
$ Habitat      <chr> "Rural", "Rural", "Rural", "Rural", "Rural", "Rural", "Ru~
$ Site         <chr> "R1", "R2", "R3", "R4", "R5", "R1", "R2", "R3", "R4", "R5~
$ morph_colour <chr> "black", "black", "black", "black", "black", "red", "red"~
$ number       <dbl> 10, 3, 4, 7, 6, 15, 18, 9, 12, 16, 32, 25, 25, 17, 16, 17~
\end{verbatim}

Are those sensible names for the variables? Is the data tidy?

\subsubsection{Calculate the totals of each colour in each
habitat.}\label{calculate-the-totals-of-each-colour-in-each-habitat.}

We will save these totals into a new data frame called \texttt{totals}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{totals}\OtherTok{\textless{}{-}}\NormalTok{ lady }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(Habitat,morph\_colour) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{ (}\AttributeTok{total.number =} \FunctionTok{sum}\NormalTok{(number))}
\NormalTok{totals}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 3
# Groups:   Habitat [2]
  Habitat    morph_colour total.number
  <chr>      <chr>               <dbl>
1 Industrial black                 115
2 Industrial red                    85
3 Rural      black                  30
4 Rural      red                    70
\end{verbatim}

Now that we have these totals we use them to plot a bar chart of the
data, using \texttt{geom\_col()} within\texttt{ggplot}:

\subsubsection{Plot the data}\label{plot-the-data-7}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{totals }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Habitat,}\AttributeTok{y =}\NormalTok{ total.number,}\AttributeTok{fill=}\NormalTok{morph\_colour))}\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{position=}\StringTok{\textquotesingle{}dodge\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Habitat"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Count"}\NormalTok{,}
       \AttributeTok{fill=} \StringTok{"Colour"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{() }\CommentTok{\# try leaving out this line (but if you do, leave out the final \textquotesingle{}+\textquotesingle{} in the line above). What happens?}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chi-square_files/figure-pdf/unnamed-chunk-19-1.pdf}}

\subsubsection{Fix the colours}\label{fix-the-colours}

The fill colours we got in the figure above are defaults from R, which
does not realise that a factor of interest for us is the actual colour
of the ladybirds. We would like the figure to reflect that, so let us
make the bars red and black for red and black ladybirds respectively.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot the data, with sensible colours}
\NormalTok{totals }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Habitat,}\AttributeTok{y =}\NormalTok{ total.number,}\AttributeTok{fill=}\NormalTok{morph\_colour))}\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{position=}\StringTok{\textquotesingle{}dodge\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Habitat"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Count"}\NormalTok{,}
       \AttributeTok{fill=} \StringTok{"Colour"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\AttributeTok{black=}\StringTok{\textquotesingle{}\#312626\textquotesingle{}}\NormalTok{,}\AttributeTok{red=}\StringTok{\textquotesingle{}\#da1717\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+} \CommentTok{\# this line manually sets the fill colours for us}
  \FunctionTok{theme\_cowplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"none"}\NormalTok{) }\CommentTok{\# this line removes the legend, since we no longer need it}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chi-square_files/figure-pdf/unnamed-chunk-20-1.pdf}}

\subsubsection{Interpret the graph before we do any
`stats'}\label{interpret-the-graph-before-we-do-any-stats-1}

Look at the plot - given these sample data, does it seem plausible that
the proportions among the populations of black ladybirds and red
ladybirdsare the same in both industrial and rural settings, or not? Do
you expect to retain or to reject the null hypothesis?

\subsubsection{Making the contingency table from the
data}\label{making-the-contingency-table-from-the-data}

\textbf{Preparation}

We will use the function \texttt{chisq.test()} to carry out the chi
square test. However, this requires a matrix of the total counts and our
data is in one column of a data frame, spread over 20 rows, one count
per colour per site. We need to convert this data frame into a 2x2
matrix of the total counts of each colour in each setting. We can use
the \texttt{xtabs()} function to do this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lady.mat}\OtherTok{\textless{}{-}}\FunctionTok{xtabs}\NormalTok{(number}\SpecialCharTok{\textasciitilde{}}\NormalTok{Habitat }\SpecialCharTok{+}\NormalTok{ morph\_colour, }\AttributeTok{data=}\NormalTok{lady)}
\NormalTok{lady.mat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            morph_colour
Habitat      black red
  Industrial   115  85
  Rural         30  70
\end{verbatim}

A matrix of this type is sometimes called a \textbf{contingency table}.

\subsubsection{The actual Chi-square
test}\label{the-actual-chi-square-test}

Let's do it\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(lady.mat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's Chi-squared test with Yates' continuity correction

data:  lady.mat
X-squared = 19.103, df = 1, p-value = 1.239e-05
\end{verbatim}

\subsubsection{Conclusion}\label{conclusion-1}

Study the output of the chi-square test. Note that you are given a
test-statistic (here called Chi-squared/X-squared), a number of degrees
of freedom (\emph{df}) (in some tests you are given more than one of
these). This is the number of independent pieces of information used to
calculate the test statistic. Lastly, you are a given a \emph{p}-value.
This is the probability that you would have got a chi-squared value as
big or bigger than the one you got \emph{if} the null hypothesis were
true, where here the null hypothesis is that there is no association
between ladybird colour and location. Put another way, the p-value is,
roughly speaking, the probability of getting the data you got or more
extreme data if the null hypothesis were true.

If the p-value is small (by which we usually mean less than 0.05) then
we rejectthe null hypothesis.

You are also told that this test was done with Yates' continuity
correction. All this means is that an adjustment has been made for the
fact that our data are discrete and the chi-square distribution that we
are using to calculate the \emph{p}-values is continuous. That's it.
Thank you, R, for doing this, but we don't need to worry about it.

\subsubsection{Reporting the result}\label{reporting-the-result}

Select which of the two following statements would be an appropriate way
to report these data, and fill in the missing values.

\subsubsection{Option 1}\label{option-1}

`Ladybird colour morphs are not equally common in the two habitats
(Chi-sq =19.3, df = 1, p\textless0.001)'

\subsubsection{Option 2}\label{option-2}

`We find insufficient evidence to reject the null hypothesis that
Ladybird colour morphs are equally distributed in the two habitats
(Chi-sq =, df = , p = )'

\subsection{Exercises}\label{exercises-1}

\subsubsection{Exercise One}\label{exercise-one}

A researcher investigates whether two species \textbf{A} and \textbf{B}
are associated with one another. If one is present at a site, does the
other tend to be present, and if one is absent, does the other tend to
be absent?. If the species were not associated with one another, then
the presence of one would say nothing about the likely presence or
absence of the other. Their occurrences would be independent.

The researcher goes to 100 sites and finds the following:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
What was found & Number \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
A present, B present & 33. \\
A present, B absent & 28. \\
B present, A absent & 12. \\
A absent, B absent & 27. \\
\end{longtable}

They enter these into a 2 x 2 contingency table in R, as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AB }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{33}\NormalTok{,}\DecValTok{28}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{27}\NormalTok{),}\AttributeTok{nrow =} \DecValTok{2}\NormalTok{)}
\NormalTok{AB }\CommentTok{\# R calls this a matrix {-} we will refer to it as a table.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1] [,2]
[1,]   33   12
[2,]   28   27
\end{verbatim}

Think about what each row and column in this table represents: as we
have constructed it, columns relate to A and rows relate to B. The left
hand column gives counts of sites where A was present, the right hand
column gives counts where it was absent. The top row gives gives counts
of sites where B was present, the bottom row gives counts of sites where
it was absent.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create this table yourself
\item
  Is it valid here to use a chi square test for an association between
  species A and species B?
\item
  Suppose the answer to 2. is `Yes', Use the table to determine whether
  there is evidence for an association between species A and species B.
\item
  What conclusion woud you reach if all four counts in the table were
  the same?
\item
  What conclusion would you reach if neither species was ever seen in
  the absence of the other - meaning that the off-diagonal elements of
  the table would be zero?
\end{enumerate}

\subsection{Solutions}\label{solutions-1}

\subsubsection{Exercise One}\label{exercise-one-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Yes, because the data are in the form of counts, each count is
  independent of the others and no count is less than 5.
\item
  We use the matrix AB as the argument for \texttt{chisq.test()}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(AB)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's Chi-squared test with Yates' continuity correction

data:  AB
X-squared = 4.3312, df = 1, p-value = 0.03742
\end{verbatim}

We find p \textless{} 0.05, so we reject the null hypothesis of no
association and can say that there is evidence, at the 5\% significance
level, that species A and B are associated.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Let us create a new matrix, AB\_uniform, with all values equal to 25:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AB\_uniform }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{,}\DecValTok{25}\NormalTok{,}\DecValTok{25}\NormalTok{,}\DecValTok{25}\NormalTok{),}\AttributeTok{nrow =} \DecValTok{2}\NormalTok{)}
\NormalTok{AB\_uniform}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1] [,2]
[1,]   25   25
[2,]   25   25
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(AB\_uniform)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's Chi-squared test

data:  AB_uniform
X-squared = 0, df = 1, p-value = 1
\end{verbatim}

Here , p = 1: there is no evidence from these data that species A and B
are associated.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Now we create a matrix wih both off-diagonal elements equal to zero
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AB\_diagonal }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{25}\NormalTok{),}\AttributeTok{nrow =} \DecValTok{2}\NormalTok{)}
\NormalTok{AB\_diagonal}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1] [,2]
[1,]   25    0
[2,]    0   25
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(AB\_diagonal)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's Chi-squared test with Yates' continuity correction

data:  AB_diagonal
X-squared = 46.08, df = 1, p-value = 1.135e-11
\end{verbatim}

In this case p\textless0.001. Hence the test tells us that in this case
there is evidence that species A and B are strongly associated.

\section{The Chi-Square Test explained
(optional)}\label{the-chi-square-test-explained-optional}

You can skip this section if you are not interested in how the
chi-square test works. If you are, read on.

Let's recall the number of sightings of each colour of ladybird in each
habitat:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lady.mat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            morph_colour
Habitat      black red
  Industrial   115  85
  Rural         30  70
\end{verbatim}

If there were no association between colour and habitat, then we would
expect the relative proportions of colour to be independent of habitat.
That is they should be the same in both rural and industrial habitats.
Looking at the table above we see that two thirds (200 out of 300) of
all sightings, regardless of colour, were in an industrial habitat.
Hence we would expect that two thirds of all 145 black sightings would
be in an industrial habitat. We thus arrive at an `expected' number for
sightings of black ladybirds in an industrial habitat to be
\((200/300) \times 145 = 96.7\). Similarly, we would expect one third
(100 out of 300) of all 155 red sightings to be in a rural habitat,
giving an `expected' number for this combination of levels to be
\((100/300)\times 155 = 51.7\). More generally, the expected number in
an any cell of the table, under the null hypothesis of no association
between the two factors is given by

\[\text{expected number}=\frac{\text{row total}\times\text{column total}}{\text{grand total}}\]
Using this method, these are the four expected numbers for each
combination of levels of the two factors we have:

\begin{verbatim}
      [,1]   [,2]
[1,] 96.67 103.33
[2,] 48.33  51.67
\end{verbatim}

To get a measurement of how far the actual table numbers are from their
expected values we can, for each cell, square the difference between the
expected and actual values, then divide the result by the expected
value, and finally sum the four results that we get. This has the effect
of giving equal weight to positive or negative deviations of the
observed values from the expected values, and scales each squared
deviation so that all four have equal weight in the final sum. The
result is the chi-squared test statistic:

\[
\begin{align*}
X^2&=\sum_{i=1}^4\frac{(O_i-E_i)^2}{E_i}\\
&=\frac{(115-96.67)^2}{96.67} + \frac{(85-103.33)^2}{103.33} + \frac{(30-48.33)^2}{48.33} + \frac{(70-51.67)^2}{51.67}\\
&=20.189
\end{align*}
\] \textbf{Yates continuity correction} (digression)

You may have noticed that the \(X^2 = 20.189\) value calculated above is
slightly larger than the value calculated by the \texttt{chisq.test()}
function, which found \(X^2 = 19.096\). This is because the function
uses Yates' continuity correction. This was suggested by Yates in 1934
to correct for the fact that the \(X^2\) statistic we calculate is
actually discrete (because we have categorical data) whereas the
chi-square distribution is continuous. This means that the value we
calculate tends to be too big so that our \emph{p}-values are too small.
The problem is most apparent where we have small numbers and one degree
of freedom, which is what you have in a \(2 \times 2\) contigency table
such s in the example above. Yates' fix is quite simple: just amend the
\(X^2\) statistic to the following:

\[
X^2=\sum_{i=1}^4\frac{(\lvert O_i-E_i \rvert - 0.5)^2}{E_i}
\]

where the vertical lines mean `take the absolute value of', so that
\textbar85-103.33\textbar{} = \textbar-18.33\textbar{} = 18.33

This gives us

\[
\begin{align*}
X^2&=\sum_{i=1}^4\frac{(\lvert O_i-E_i \rvert - 0.5)^2}{E_i}\\
&=\frac{(|115-96.67|-0.5)^2}{96.67} + \frac{(|85-103.33|-0.5)^2}{103.33} + \frac{(|30-48.33|-0.5)^2}{48.33} + \frac{(|70-51.67|-0.5)^2}{51.67}\\
&=19.096
\end{align*}
\] which is exactly what the \texttt{chisq.test()} function gives.

Under a null hypothesis of no association between habitat and colour all
the counts would be the `expected' values, and \(X^2\) would be zero.
The further away from these values the actual results are, the bigger
\(X^2\) will be and the more likely it is that we can reject the null.
For a sufficiently large value of \(X^2\) we \emph{will} reject the
null. To sum up, in general we will reject the null when \(X^2\) is
large, and fail to reject it when it is small.

But how large is large enough to reject the null?

To answer this we use the fact that the sampling distribution of \(X^2\)
is a chi-squared distribution with (in this case)
\((2-1) \times (2-1) = 1\) degrees of freedom. The sampling distribution
is the distribution you would get if you were to repeat the study
multiple times, each time calculating the \(X^2\) statistic from the
four counts of black/red, industrial/rural. Each time you would get a
slightly different value of \(X^2\). The spread of those hypothetical
values is what we call the sampling distribution. Providing none of the
cell values in the tables are too samll (see below) it turns out that
this distribution has a known mathematical form, known as a chi-square
distribution.

The p-value given in a chi-square test is the probability of getting a
chi-squared statistic \(X^2\) as big as or bigger than the one you
actually got. This is the area under the chi-squared distribution with
the appropriate number of degrees of freedom to the right of the
test-statistic value \(X^2\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xsquared}\OtherTok{\textless{}{-}}\FloatTok{19.1}
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{20}\NormalTok{)), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x)) }\SpecialCharTok{+} 
  \FunctionTok{stat\_function}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ dchisq, }\AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{df =} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{stat\_function}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ dchisq, }\AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{df =} \DecValTok{1}\NormalTok{), }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(xsquared, }\DecValTok{20}\NormalTok{),}
                  \AttributeTok{geom =} \StringTok{"area"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"\#84CA72"}\NormalTok{, }\AttributeTok{alpha =}\NormalTok{ .}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{""}\NormalTok{, }\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept=}\NormalTok{xsquared,}\AttributeTok{linewidth=}\FloatTok{0.2}\NormalTok{,}\AttributeTok{colour=}\StringTok{"gray80"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chi-square_files/figure-pdf/unnamed-chunk-31-1.pdf}}

\subsubsection{Why does the test statistic have this
distribution?}\label{why-does-the-test-statistic-have-this-distribution}

To answer this, let us recognise that in our observed contingency table,
where we have \emph{N} observations altogether, we can think of there
being a probability \(P_i\) that any individual observation will end up
in cell \emph{i}, with the actual observed frequency in that cell,
\(O_i\) being the product of this probability and the total number of
observations: \(O_i = P_i\times N\). If we repeated the study again and
again we would get slightly different numbers in each cell each time.
The distribution of the numbers in a particular cell would follow
similar rules as that of the number of heads we might get each time if
we tossed a coin \emph{N} times, then did the same again and again. This
distribution is called a \emph{binomial distribution}.

In other words, our observed frequencies have been obtained by sampling
from a binomial distribution, where \(O_i \sim \text{Binomial}(P_i,N)\).
Now, providing \emph{N} is large enough and providing too that the
probabilities \(P_i\) are not too close to 0 or 1, then the binomial
distribution resembles a normal distribution. Thus, providing
\(P_i\times N\), that is providing the observed frequencies
\(O_i=P_i\times N\) are large enough, then the \(O_i\) will be
approximately normally distributed.If this is the case, then so too is
\(\frac{(O_i-E_i)}{\sqrt{E_i}}\) since the expected values \(E_i\) are
fixed quantities and all this transformation does is turn our normal
distribution into a \emph{standard} normal distribution, with mean = 0
and standard deviation = 1. ie it shifts and squishes the distribution.

Hence in our expression for our test statistic \(X^2\) what we are doing
is adding up \emph{k} (= 4 in this case) squared standard normal
distributions. This is the definition of a chi-squared distribution with
four degrees of freedom. Thus we see that the sampling distribution of
our test statistic is a chi-squared distribution.

The one final slightly odd detail is that when we run a chi-square test
for a 2 x 2 contingency table we are told that there is one degree of
freedom. In general, for \(m \times  n\) table, there will be
\((m-1)\times(n-1)\) degrees of freedom. This is because the test
presumes that the row and column total are already known. If that is the
case then if one value of a 2x2 table is known, the other 3 values can
be found by deduction. Hence there only one value (it doesn't matter
which one) can be chosen freely.

\part{Trend data}

\chapter{Correlation}\label{correlation}

This guide to correlation draws heavily on the very helpful chapter in
Statistics, by David Freedman, Robert Pisani, Roger Purves and Ani
Adhikari, 2nd ed., Norton.

\section{Correlation}\label{correlation-1}

The notable statistician Karl Pearson (1857 - 1936) carried out a study
to investigate the resemblance between children and their parents. As
part of the study, Pearson measured the heights of 1078 parents and of
their children at maturity. The heights of the children are plotted
against the heights of the parents in the plots below, where we
distinguish between father-son and mother-daughter pairs.

\begin{center}
\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-4-1.pdf}
\end{center}

The taller a father, the taller his sons tend to be. It is the same with
mothers and daughters.

There is a positive association between a father's height and the height
of his sons.

But there is a lot variation - the association is weak.

If you know the height of a father, how much does that tell you about
the height of his sons?

Consider fathers who are about about 67 inches tall, and look at the
wide variation in the heights of their sons - - all the points between
the two vertical dotted lines. The same is true for the daughters of
mother who are about 63 inches tall.

If there is a strong association between two variables, then knowing one
helps a lot in predicting the other. But when there is a weak
association, information about one variable does not help much in
guessing the other. When there is no association, it does not help at
all.

\subsection{The correlation
coefficient}\label{the-correlation-coefficient}

Suppose we are looking at the relationship between two variables and
have already plotted the scatter plot. The graph looks like a cloud of
points.

How can we summarise it numerically?

The first thing we can do is to mark a point that shows the average of
the \emph{x}-values and the average of the \emph{y}-values. This is the
\emph{point of averages}. It marks the centre of the cloud.

The next step is to measure the width of the cloud from side to side, in
both the \emph{x} and the \emph{y} directions. This can be done using
the standard deviations (SD) of the \emph{x} and \emph{y} values.
Remember that if both \emph{x} and \emph{y} are normally distributed,
then 95\% of the data will lie within about 2 (1.96 if we want to be
pernickety) standard deviations of the mean, in each direction.

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-10-1.pdf}}
\end{center}

So far, our summary statistics are:

\begin{itemize}
\tightlist
\item
  mean of the \emph{x} values, SD of the \emph{x} values.
\item
  mean of the \emph{y} values, SD of the \emph{y} values.
\end{itemize}

These statistics tell us where the centre of the cloud is and how far
spread out it is both vertically and horizontally, but they do not tell
the whole story.

Consider the following two sets of data plotted below. Both have the
same centre and the same spread.

\begin{center}
\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-12-1.pdf}
\end{center}

However the points in the first cloud are tightly clustered around a
line - there is a strong linear association between the two variables.
In the second cloud, the clustering is much looser. The strength of the
association is different in the two diagrams. To measure the
association, one more summary statistic is needed - the
\emph{correlation coefficient}.

This coefficient is usually abbreviated as \emph{r}, for no good reason.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The correlation coefficient is a measure of linear association or
clustering around a line. The relationship between two variables can be
summarized by:

\begin{itemize}
\tightlist
\item
  the average of the \emph{x}-values, the SD of the \emph{x}-values.
\item
  the average of the \emph{y}-values, the SD of the \emph{y}-values.
\item
  the correlation coefficient \emph{r}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{\texorpdfstring{Different values of
\emph{r}.}{Different values of r.}}\label{different-values-of-r.}

Let us see how this looks graphically. In the Figure below we show six
scatter plots for hypothetical data. In all six pictures the average is
3 and the standard deviation is 1 for both \emph{x} and \emph{y}. The
correlation coefficient is printed in each case.

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-14-1.pdf}}
\end{center}

The one top left shows a correlation of 0 and the cloud is completely
formless. As \emph{x} increases, \emph{y} shows no tendency to increase
or decrease. It just straggles around.

The next diagram has \emph{r} = 0.4 and a linear pattern is just
starting to emerge. The next has \emph{r} = 0.6 with a stronger linear
pattern, and so on. The closer \emph{r} is to 1 the stronger is the
linear association and the more tightly clustered are the points around
a line.

A correlation of 1, which does not appear in the Figure is often
referred to as a \emph{perfect correlation}. It means that all the
points lie exactly on a line so there is a perfect linear correlation
between the two variables. Correlation coefficients are always between
-1 and 1.

The correlation between the heights of identical twins is around 0.95. A
scatter diagram for the heights of twins would thus look like the bottom
right diagram in the Figure. We see that even with a coefficient this
big there is a still a fair degree of scatter. The heights of identical
twins are far from being equal all the time.

Real data in the life sciences never shows perfect correlation and
rarely does it even show strong correlation. It is more common for it to
look like Pearson's father-son data, with weak associations and \emph{r}
values in the range 0.3 to 0.7. This is even more true for data from the
social sciences which concern human behaviour.

We can also have negative associations between variables. For example
women with more education tend to have fewer children. Animals with
higher body weight tend to have lower metabolic rates. As one variable
increases, the other decreases. When there is negative association, the
correlation coefficient has a negative sign.

Below we show six examples of negative correlation. As in the previous
figure, all the data sets have a mean of 3 and a standard deviation of
1.

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-17-1.pdf}}
\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Correlations are always between -1 and 1, but can take any value in
between. A positive correlation means that the cloud slopes up: as one
variable increases, so does the other. A negative correlation means that
the cloud slopes down. As one variable increases, the other decreases.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Using R to find the correlation
coefficient.}\label{using-r-to-find-the-correlation-coefficient.}

First, let us try to find the correlation between two sets of data where
we know what the correlation coefficient is, because we created the data
ourselves. We will take the \emph{x} and \emph{y} data used above for
which the correlation coefficient was fixed to be 0.8

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(x,y,}\AttributeTok{method=}\StringTok{"pearson"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  x and y
t = 9.1627, df = 48, p-value = 4.092e-12
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.6675004 0.8805030
sample estimates:
      cor 
0.7976475 
\end{verbatim}

There are different ways to calculate the correlation coefficient. Which
of them is appropriate depends mainly on the type of data, and if they
are numeric, whether they are normally distributed. If they are, then we
use the Pearson method. If they are not, for example because they are
ordinal data, then we use the Spearman's Rank method and write
\emph{method=``spearman''} instead. In this case we can relax the
requirement that there is a linear association between the data sets,
but there does still need to be a monotonic relationship.

It is important to be able to interpret and report the output.

First, understand that R is carrying out a test, the null hypothesis of
which is that there is no correlation between the values of \emph{x} and
the values of \emph{y} among the populations from which the \emph{x} and
\emph{y} data were drawn, so that the correlation coefficient between
\emph{x} and \emph{y} within those populations is zero. It then reports
a \emph{p}-value: how likely it is that you would have got the data you
got for this sample of data if that null hypothesis were true. As with
most tests, to do this it uses the data to calculate a so-called
\emph{test-statistic}. How it does this need not concern us here. The
details will differ from test to test, and the name given will differ.
Here it is called \emph{t}. It also reports the number of independent
pieces of information used to calculate that statistic. This is called
the \emph{degrees of freedom}, here denoted \emph{df}. This usually (but
not necessarily) has a value that is 1,2 or 3 less than the number of
data points.

Then it reports the \emph{p}-value. A tiny (close to zero) value here
means that it thinks it very unlikely that the samples would be as they
are if the \emph{x} and \emph{y} variables were not correlated in the
populations from which the samples were drawn. A high (by which we
usually mean greater than 0.05) value means that there is a reasonable
chance that the actual non-zero correlation coefficient could have been
found between \emph{x} and \emph{y} in the samples, even though those
values were not correlated in the wider populations from which the
samples were drawn. In that case we would have found no evidence that
the \emph{x} and \emph{y} data within the population were correlated.
This doesn't mean that they aren't, just that we have insufficient
evidence to reject the null hypothesis that they are not.

The \emph{p}-value reported here is 4.3e-12. That is R's way of saying
what in standard form would be written 4.3 x 10\textsuperscript{-12}.
This is a really tiny value. It is 0.0000000000043, which is a very
inconvenient way to write such a small number. Hence R's way of doing it
or the standard form way of doing it. In the context of a statistical
test and when \emph{p} is is that small we don't care about its exact
value, we simply note that it is very, very small. We thus can
confidently reject the null hypothesis and assert that the data provide
evidence that \emph{x} and \emph{y} are correlated, in this case
positively.

Further, it reports the actual correlation coefficient. Here it finds
\emph{r} = 0.797, which we happen to know to be correct because we
created this data set ourselves, and a 95\% confidence interval for the
coefficient. The precise meaning of the confidence interval is subtle,
but it is a kind of error bar for the correlation coefficient \emph{r}.
It means that if we drew sample after sample from the population and
calculated the confidence interval for \emph{r} for each sample, then
95\% of the time that interval would capture the true value of \emph{r}.
Thus, you can reasonably think of the confidence interval as being the
range of values within which the true population correlation coefficient
plausibly lies, given the value that was found for the sample.

If the \emph{p}-value is small enough that we reject the
null-hypothesis, then this confidence interval should not encompass
zero. Why? Because any value inside the confidence interval is a
plausible value for the population correlation coefficient andif we are
going to reject the null hypothesis, then zero should \emph{not} be a
plausible value for the population correlation coefficient, given the
data.

If the \emph{p}-value is large enough that we do not reject the null
hypothesis then this confidence interval \emph{will} encompass zero.
Why? Becuase if the confidence interval encompasses zero, then zero
\emph{is} a plausible value for the correlation coefficient and so we
should not reject the null.

Here, the confidence interval is from 0.67 to 0.88. This does not
encompass zero. In fact it is far from zero, so is consistent with our
finding a really small \emph{p}-value. On both groundss, we reject the
null.

To report the result of this test we would say something like:

\begin{quote}
We find evidence for a strong positive correlation between \emph{x} and
\emph{y} (Pearson \emph{r} =0.80, \emph{t}=9.1, \emph{df}=48,
\emph{p}\textless0.001)
\end{quote}

Note that when the \emph{p}-value is much less than 0.05 as it is here
we do not normally report its exact value, but simply write
\emph{p}\textless0.01, or \emph{p}\textless0.001, and so on. The point
is that these ways of reporting it tell the reader that \emph{p} is
\emph{way} less than 0.05. This is all they need to know to see that we
can confidently reject the null hypothesis.

\subsection{Correlations for real
data}\label{correlations-for-real-data}

Let us look at the Iris data set that is built into R. It contains
values for the Sepal Width, Sepal Length, Petal Width and Petal Length
for samples of 50 plants from each of three species of Iris,
\emph{setosa}, \emph{versicolor} and \emph{virginica}. Here are the
first few rows:

\begin{verbatim}
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}

We will look to see if the data allow us to reject the idea that sepal
width and sepal length are not correlated within the wider populations
of each of these species:

First, let's plot the data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sepal.Width,}\AttributeTok{y=}\NormalTok{Sepal.Length,}\AttributeTok{colour=}\NormalTok{Species)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Sepal Width (mm)"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Sepal Length (mm)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Species,}\AttributeTok{nrow=}\DecValTok{1}\NormalTok{,}\AttributeTok{scales=}\StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{strip.background=}\FunctionTok{element\_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-21-1.pdf}}
\end{center}

\subsubsection{What we can tell from plotting the
data}\label{what-we-can-tell-from-plotting-the-data}

Having seen the plots, do you think it plausible that there is a linear
relationship between sepal width and length? Only in this case can you
use a Pearson correlation test (see below). If not linear, do you think
there is at least a monotonic relationship between petal width and
length (ie no peaks or troughs)? That, at least, would enable you to use
a Spearman's Rank correlation test. If neither are true, then you can't
use either test.

\subsubsection{Test for normality}\label{test-for-normality}

It does look as though there is a plausibly linear relationship between
sepal width and length, so we might be able to use the Pearson method
for calculating correlation coefficient. This is the `parametric' method
that is more powerful than the `non-parametric' alternative, the
Spearman's rank method.

In principle, however, this method requires that each group of the data
be approximately normally distributed around its repective mean (that is
what the word parametric is getting at), so we ought to test for this.
We can do this either graphically or using a normality test such as the
Shapiro wilk test. Let us do the latter here:

\begin{verbatim}
# A tibble: 3 x 3
  Species    Sepal.Length_p.val Sepal.Width_p.val
  <fct>                   <dbl>             <dbl>
1 setosa                  0.460             0.272
2 versicolor              0.465             0.338
3 virginica               0.258             0.181
\end{verbatim}

All the p-values for this test are comfortably greater than 0.05 so we
can reasonably presume that our data are drawn from normally distributed
populations. This, plus the plausibly linear realtionships we have seen
in the graphs means that can go ahead and use the Pearson method to
calculate the correlation coefficient between sepal length and width for
each species!

\subsubsection{Calculate the correlation
coefficients}\label{calculate-the-correlation-coefficients}

Looking at each graph, it appears that there is a positive correlation
for each species, but that this is weaker for \emph{versicolar} and
\emph{virginica} than it is for \emph{setosa}. Knowing the sepal width
for that species gives you a much better idea of the sepal length, and
vice-versa, than is true for the other two species.

Let us find out:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(Species) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{r=}\FunctionTok{cor.test}\NormalTok{(Sepal.Width,Sepal.Length)}\SpecialCharTok{$}\NormalTok{estimate,}
            \AttributeTok{lower.bound95=}\FunctionTok{cor.test}\NormalTok{(Sepal.Width,Sepal.Length)}\SpecialCharTok{$}\NormalTok{conf.int[}\DecValTok{1}\NormalTok{],}
            \AttributeTok{upper.bound95=}\FunctionTok{cor.test}\NormalTok{(Sepal.Width,Sepal.Length)}\SpecialCharTok{$}\NormalTok{conf.int[}\DecValTok{2}\NormalTok{],}
            \StringTok{"p value"}\OtherTok{=}\FunctionTok{cor.test}\NormalTok{(Sepal.Width,Sepal.Length)}\SpecialCharTok{$}\NormalTok{p.value) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kbl}\NormalTok{(}\AttributeTok{digits=}\DecValTok{3}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kable\_styling}\NormalTok{(}\AttributeTok{full\_width=}\FloatTok{0.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X>{\raggedleft}X}
\hline
Species & r & lower.bound95 & upper.bound95 & p value\\
\hline
setosa & 0.743 & 0.585 & 0.846 & 0.000\\
\hline
versicolor & 0.526 & 0.290 & 0.702 & 0.000\\
\hline
virginica & 0.457 & 0.205 & 0.653 & 0.001\\
\hline
\end{tabu}

The table gives the estimated value for the Pearson correlation
coefficient in each case, the lower and upper bound of the 95\%
confidence interval for that coefficient and the \emph{p}-value.

Do these output provide evidence for a correlation between sepal length
and sepal width in each case?

\subsection{The problem of missing
variables}\label{the-problem-of-missing-variables}

Suppose in the above analysis we had not distinguished between the three
species. If we had plotted the speal length and width data we would have
seen this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sepal.Width,}\AttributeTok{y=}\NormalTok{Sepal.Length)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Sepal Width (mm)"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Sepal Length (mm)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{strip.background=}\FunctionTok{element\_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-24-1.pdf}}
\end{center}

This looks like a weak \emph{negative} correlation, as is confirmed by
calculating the Spearman's (in this case) rank correlation coefficient.

The message here is that failure to spot importnat `missing' variables,
in the case species, can lead to grossly misleading ideas as to whether
two variables are correlated, and if so, how.

\subsection{\texorpdfstring{The correlation coefficient measures the
degree of \emph{linear}
association.}{The correlation coefficient measures the degree of linear association.}}\label{the-correlation-coefficient-measures-the-degree-of-linear-association.}

\subsubsection{Pearson correlation
coefficient}\label{pearson-correlation-coefficient}

Sometimes the Pearson correlation coefficient \emph{r} is a poor measure
of the degree of association within a data set. Outliers and
non-linearity are two problem cases.

Consider first a data set where there is a very strong association
between variables, but where the data sset contains an outlier, and then
a data set where there is a strong but non-linear association between
variables. Here we mean by `strong' that knowing the value of one
variable gives you a very good idea of the value of the other.

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-25-1.pdf}}
\end{center}

The outlier in the left-hand figure above brings the correlation
coefficient down to 0.08, which is close to zero. The correlation
coefficient in the right-hand figure is similarly small at -0.05,
despite that there is a strong association between the \emph{x} and
\emph{y} data. The reason is that the association is non-linear.

\subsubsection{\texorpdfstring{Spearman's rank correlation coefficient
\emph{r}\textsubscript{Sp}}{Spearman's rank correlation coefficient rSp}}\label{spearmans-rank-correlation-coefficient-rsp}

The Spearman's rank correlation coefficient is a valid measure of the
association between two variables providing their relationship is
\emph{monotonic} - that is, continuously rising or flat, or continuously
falling or flat. Linear relationships are monotonic, so the Spearman's
rank correlation coefficient is always a useful (if not necessarily the
most powerful - Pearson would trump it if it could be used) measure of
association for such cases, but it will still be valid when the
relationship is monotonic but non-linear, whereas the Pearson
correlation coefficient would \emph{not} be.\\

Spearman's rank correlation coefficient \emph{r}\textsubscript{Sp} can
also be be used for ordinal data, whereas Pearson's \emph{r} coefficient
cannot be. This makes it very useful in much of ecology, animal
behaviour and environmental studies where ordinal scales are commonly
used.

\subsection{When is it appropriate to calculate a correlation
coefficient?}\label{when-is-it-appropriate-to-calculate-a-correlation-coefficient}

So, to sum up, we note that the Pearson correlation coefficient is a
measure of \emph{linear} association, not of association in general. At
least, this is true if you are calculating the Pearson correlation
coefficient. If your data are not suitable for that and you decide to
calculate the Spearman's Rank correlation coefficient, then the
condition is relaxed somewhat: there might be but there no longer
\emph{needs} to be a linear relationship between the two variables, but
there must be a \emph{monotonic} one. That means that, as one variable
increases, the other should either increase or remain constant, or
decrease or remain constant - that is, there should be no peaks or
troughs in the data.

\subsection{Association is not
causation}\label{association-is-not-causation}

A very important and often-repeated point to note is that correlation
measures association. But association is not the same as causation.

See \href{https://www.tylervigen.com/spurious-correlations}{Spurious
Correlations} for some amusing examples.

\section{Examples}\label{examples}

\subsection{Cyclones}\label{cyclones}

Cyclones are areas of low atmospheric pressure around which steep
gradients in air pressure can cause strong winds to develop, which in
turn may create large waves if the cylone is over the oceans. Depending
on where they occur, these storms are variously also known as hurricanes
and typhoons. Here we consider whether the peak wind speeds are
associated with the depth of the low pressure at the eye of the storm,
and whether the peak wave sizes are associated with how wide the storm
is. If the answer to either of these questions is yes, then an outcome
of practical importance - wind speed, wave height - can be predicted at
least in part by a variable that can be measured easily - pressure,
distance.

\subsubsection{South West Indian Ocean intense tropical cyclones 1973 -
2024}\label{south-west-indian-ocean-intense-tropical-cyclones-1973---2024}

Pressure gradients cause winds so it is reasoable to ask whether there a
correlation between the peak wind speed and minimum pressure at the eye
of cyclones. Here we look at data for for intense tropical cyclones in
the south west Indian Ocean over the period 1973-2024. The data are
taken (scraped using the R package \texttt{rvest}!) from:
https://en.wikipedia.org/wiki/List\_of\_South-West\_Indian\_Ocean\_intense\_tropical\_cyclones
. The original data sources are available on that site.

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-28-1.pdf}}
\end{center}

In this figure we see that there is a weak but significant
\emph{negative} correlation between the peak recorded wind speed and the
minimum recorded pressure at the centre of intense tropical cyclones
that occurred in the south west Indian Ocean between 1973 and 2024.
Neither wind speed nor pressure were normally distributed, so the
Spearman's rank correlation coefficient \emph{r}\textsubscript{Sp} has
been calculated, and not the Pearson \emph{r} coefficient.

\subsubsection{Significant wave height vs size of
cyclone}\label{significant-wave-height-vs-size-of-cyclone}

Here we look at results displayed in Figure 3f of\\
\strut \\
Oh, Y. et al.~(2023) `Optimal tropical cyclone size parameter for
determining storm-induced maximum significant wave height', Frontiers in
Marine Science, 10, p.~1134579. Available at:
https://doi.org/10.3389/fmars.2023.1134579.

The authors seek to determine whether there is an association between
the size of a cylone, measured by the `R50' distance measured outward
from the storm centre to where the wind speeds have subsided to 50 kph,
and the maximum `significant wave height' of the swell created by the
storm. Significant wave height is a widely used measure that is the
average height of the heighest 1/3 of the waves, these being the ones
that impact most on practical matters like the fuel consumption of
ships, the erosion of shores, and so on.

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-31-1.pdf}}
\end{center}

The plot shows that there is a monotonically rising relationship between
the R50 radius of the cyclones included in the study and the maximum
significant wave height of swell created by them. The relationship is
not linear however, so the only appropriate measure of correlation
coefficient is the Spearman's rank, which gives
\(r_{sp} = 0.948, p<0.001\), indicating a very strong positive
association between the size of a cyclone and the height of the waves it
creates.\\

Why do you think this relationship flattens off for larger storm
sizes?\\

\subsection{Lichen abundance}\label{lichen-abundance}

Jovan, S. (2008). Lichen Bioindication of Biodiversity, Air Quality, and
Climate: Baseline Results From Monitoring in Washington, Oregon, and
California. \url{http://gis.nacse.org/lichenair/doc/Jovan2008.pdf}

In this paper the authors investigate the utility of using lichen as
bioindicators of air quality. is there an association between air
quality and the abundance of this or that species of lichen?

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-34-1.pdf}}
\end{center}

\hfill\break
We note that the relationship between air quality score and proportion
of nitrophyte abundance is plausibly linear.\\
\strut \\

There is a strong negative correlation
(\emph{r}\textsubscript{Sp}=-0.78, \emph{p}\textless0.001) between air
quality score and proportion of nitrophyte lichen. This suggests that
this proportion can be used as a bioindicator of air quality.\\

\hfill\break
A Spearman's rank correlation coefficient was calculated in this case
rather than a Pearson correlation coefficient, despite the fact that
both the \emph{x} and \emph{y} variables are plausibly drawn from
normally distributed populations of values, according to a Shapiro-Wilk
test. The problem is that both the air quality score and the proportion
of nitrophyte abundance are ordinal variables - something we learn from
reading the paper in which this figure appears. This means that analysis
using parametric tests such as the Pearson method for determining linear
correlation are not appropriate. A non-parametric method such as
Spearman's rank method can be used instead.\\

\subsection{Soil bacteria}\label{soil-bacteria}

Lauber, C. L., Hamady, M., Knight, R., \& Fierer, N. (2009).
Pyrosequencing-based assessment of soil pH as a predictor of soil
bacterial community structure at the continental scale. Applied and
Environmental Microbiology, 75(15), 5111--5120.
\url{https://doi.org/10.1128/AEM.00335-09}

\begin{center}
\includegraphics[width=4.32in,height=\textheight,keepaspectratio]{figures/soil_bacteria.png}
\end{center}

In the Figure above, note that the Pearson \emph{r}-values for
\textbf{C} and \textbf{E} are close to zero, and the \emph{p}-values are
greater than 0.1, meaning that at this significance level there is no
evidence from these data that there is \emph{any} linear association
between soil pH and the relative abundances of
\emph{Alphaproteobacteria} or \emph{Beta/Gammaproteobacteria}. From the
plots, it looks in \textbf{C} as if there no assocation at all, whereas
in \textbf{E} it looks as though there might be, but if so then not a
linear or even monotonic association, for which the correlation
coefficient (Pearson \emph{r} or Spearman \emph{r}\textsubscript{Sp})
would be a poor measure.

\subsection{Birds}\label{birds}

Pain, D.J., Mateo, R. and Green, R.E. (2019) `Effects of lead from
ammunition on birds and other wildlife: A review and update', Ambio,
48(9), pp.~935--953. \url{https://doi.org/10.1007/s13280-019-01159-0}

\begin{center}
\includegraphics[width=7.15in,height=\textheight,keepaspectratio]{figures/bird_shot_ingestion.png}
\end{center}

This figure is from a study on the impact on bird populations of
ingestion of lead from spent lead ammunition arising from hunting using
rifles and shot guns. For several species of wetland birds, the
population trend (as measured by a proxy scale) is plotted against the
prevalence of carcasses found to contain lead shot.

There is a clear negative trend here that is plausibly linear, or at
least monotonic. Shapiro-Wilk tests show that neither data set is
plausibly drawn from a normally distributed population, so a Spearman's
rank correlation coefficient is calculated. The result is
\(r_{\text{Sp}} = -0.697, p < 0.01\) so we can say that there is a
evidence of a significant and strong negative correlation between lead
shot ingestion of waterbird species and their population trends.

\subsection{Heart rate vs life
expectancy}\label{heart-rate-vs-life-expectancy}

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-43-1.pdf}}
\end{center}

Here we see a strong negative linear correlation between the life
expectancy of different species and the log of the mean heart rate in
beats per minute. On this plot, humans are almost an outlier. In this
case, use of a Shapirro Wilk test showed that both life expectancy and
log of the heart rate were found to be consistent with being drawn from
normally distributed populations, so the Pearson method was used to
calculate the correlation coefficient.

\section{Exercise 1}\label{exercise-1}

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-45-1.pdf}}
\end{center}

Plots A to F above show scatter plots of different data sets \emph{Y}
against \emph{X}.

\begin{itemize}
\tightlist
\item
  Which of them show linear behaviour?
\item
  Which of them show monotonic behaviour?
\item
  For which of them might it be appropriate to calculate the following
  correlation coefficients between \emph{X} and \emph{Y}?

  \begin{itemize}
  \tightlist
  \item
    Pearson \emph{r}
  \item
    Spearman rank \emph{r}\textsubscript{sp}
  \end{itemize}
\end{itemize}

\section{Exercise 2}\label{exercise-2}

Measurements were made on female Adelie penguins on a series of islands
in the Antarctic. The bill lengths and depths of 73 individuals were
recorded and are shown in the plot below.

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{correlation_files/figure-pdf/unnamed-chunk-47-1.pdf}}
\end{center}

From this information and plot, decide

\begin{itemize}
\tightlist
\item
  Whether it is plausible that there is a linear relationship between
  the bill depths and lengths within the data set
\item
  Whether the correlation coefficient within the data set is likely to
  be positive or negative
\item
  Whether the correlation is `strong' or `weak' ie is the absolute value
  of the correlation coefficient likely to be close to 1 or close to
  zero
\item
  Whether there might be evidence from this data that there is any
  correlation between bill depth and bill length in the population from
  which this data set was drawn.
\end{itemize}

\subsubsection{Tests for normality}\label{tests-for-normality}

Shapiro-Wilk tests are carried out to check for normality of the two
sets of data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(bill\_depths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  bill_depths
W = 0.9831, p-value = 0.4364
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(bill\_lengths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  bill_lengths
W = 0.99117, p-value = 0.8952
\end{verbatim}

On this basis, we see that we can use the Pearson method to calculate
the correlation coefficient between bill depth and bill length. What is
telling us this?

\subsubsection{Calculate correlation
coefficient}\label{calculate-correlation-coefficient}

For these data, we calculate the correlation coefficient using the
Pearson method.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Pearson}
\FunctionTok{cor.test}\NormalTok{(bill\_depths,bill\_lengths, }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  bill_depths and bill_lengths
t = 1.3714, df = 71, p-value = 0.1746
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.07209557  0.37677877
sample estimates:
      cor 
0.1606361 
\end{verbatim}

Which part(s) of this output tell us that:

\begin{itemize}
\tightlist
\item
  There is a weak positive correlation between bill length and depth
  \emph{within the sample}?
\item
  There is no evidence that this correlation exists in the wider
  population from which this data set was drawn?
\end{itemize}

How would you report this result?

\section{Exercise 3}\label{exercise-3}

Open a new R notebook

In the usual way, include to start with code chunks to

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load the packages needed \texttt{tidyverse} and \texttt{here}.
\item
  Read the data set \texttt{iris.csv} (which should be in your data
  folder already) into an object called \texttt{iris}
\end{enumerate}

You can do this with this code chunk:

\begin{verbatim}
```{r}     
filepath<-here("data","iris.csv")
iris<-read_csv(filepath)
glimpse(iris)
```
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Create a faceted plot of sepal length against sepal width for each
  species.
\item
  Calculate the Pearson correlation coefficient between sepal length and
  sepal width for each species, and display this, plus the lower and
  upper bounds of the confidence interval and the \emph{p}-value for
  each species in a table.
\end{enumerate}

For (4) and (5) you can adapt code used on the main Correlation tab.

Now:

\begin{itemize}
\tightlist
\item
  Does it appear that the sepal length and sepal width are correlated
  for each species?\\
\item
  Is the correlation positive or negative?\\
\item
  For which species is the correlation strongest?\\
\item
  Do the correlation coefficients make sense, given the plots?
\end{itemize}

\chapter{Simple linear regression}\label{simple-linear-regression}

\section{Introduction}\label{introduction-2}

A class of analytical models that you will use often go under the name
\textbf{General Linear Models}. They include linear regression, multiple
regression, ANOVA, ANCOVA, Pearson correlation and t-tests.

Despite appearances, these models are all fundamentally linear models.
They share a common framework for estimation (least squares) and a
common set of criteria that the data must satisfy before they can be
used. These criteria centre around the idea of normally distributed
residuals. An important stage of any analysis that uses linear models is
that these assumptions are checked, as part of the \emph{Plot}
-\textgreater{} \emph{Model} -\textgreater{} \emph{Check Assumptions}
-\textgreater{} \emph{Interpret} -\textgreater{} \emph{Plot again}
workflow.

Here, we will go through an example of simple linear regression -
suitable for trend data where we wish to predict a continuously varying
response, given a value of a continuous explanatory variable. As we go
we show code snippets from an R script that does this job, and, at the
bottom, an example complete script that you could adapt to your own
needs.

\section{Simple Linear Regression - plant
growth}\label{simple-linear-regression---plant-growth}

As a first example, we ask the question: does plant growth rate depend
on soil moisture content?

We predict that more moisture will probably allow higher growth rates.
We note that this means there will be a clear relationship between the
variables, one that should be apparent if we plot the response
(dependent) variable - plant growth rate - against the explanatory
(independent) variable - soil moisture content. We note that both the
explanatory variable and the dependent variables are \emph{continuous} -
they do not have categories.

What we want to do in linear regression is be able to predict the value
of the dependent variable, knowing the value of the independent
variable. In practice, this means drawing a `best fit' straight line
through the data and determining the intercept and gradient of this
line.

\subsection{Load packages}\label{load-packages-5}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{library}\NormalTok{(ggfortify)}
\FunctionTok{library}\NormalTok{(cowplot)}
\CommentTok{\# un{-}comment and run the next line if you have not yet installed mbhR.}
\CommentTok{\# remotes::install\_github("mbh038/mbhR")}
\FunctionTok{library}\NormalTok{(mbhR)}
\end{Highlighting}
\end{Shaded}

\subsection{Get the data}\label{get-the-data}

We have a data set to explore our question: The \texttt{plants} data set
is available through the \texttt{mbhR} package which you have already
installed and loaded

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(plants)}
\FunctionTok{glimpse}\NormalTok{(plants)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 50
Columns: 2
$ soil.moisture.content <dbl> 0.4696876, 0.5413106, 1.6979915, 0.8255799, 0.85~
$ plant.growth.rate     <dbl> 21.31695, 27.03072, 38.98937, 30.19529, 37.06547~
\end{verbatim}

We see that the data set contains two continuous variables, as expected.

\subsection{Plot the data}\label{plot-the-data-8}

We can use the package \texttt{ggplot2}, which is part of
\texttt{tidyverse} to do this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plants }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{soil.moisture.content, }\AttributeTok{y=}\NormalTok{plant.growth.rate)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Soil moisture content"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Plant growth rate (mm/week)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{50}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{simple-linear-regression_files/figure-pdf/unnamed-chunk-3-1.pdf}}

From the plot, we note that:

\begin{itemize}
\tightlist
\item
  there is an upward trend that is plausibly linear within this range of
  soil misture content. The more moisture there is in the soil, the
  greater the growth rate of the plants appears to be.
\item
  the variance of the data, that is the range of vertical spread is
  approximately constant for the whole range of soil moisture content.
  This is one of the key criteria that data must satisfy if we are to
  analyse them using a linear model such as simple linear regression.
\item
  we can estimate the intercept and gradient of a best fit line just by
  looking at the plot. Roughly speaking, the moisture content varies
  from 0 to 2, while the growth rate rises from 20 to 50, a rise of
  about 30. Hence the gradient is about 30/2 = 15 mm/week, while the
  intercept is somewhere between 15 mm and 20 mm / week.
\end{itemize}

It is always good practice to examine the data before you go on to do
any statistical analysis. For all but the smallest data sets, that means
plotting them.

\subsection{Make a simple model using linear
regression}\label{make-a-simple-model-using-linear-regression}

We use the function \texttt{lm()} to do this, and we save the results in
an object to which we give the name \texttt{model\_pgr}. This function
needs a formula and some data as its arguments:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_pgr}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(plant.growth.rate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ soil.moisture.content, }\AttributeTok{data =}\NormalTok{ plants)}
\end{Highlighting}
\end{Shaded}

This reads: `Fit a linear model, where we hypothesize that plant growth
rate is a function of soil moisture content, using the variables from
the \texttt{plants} data frame.'

\subsection{Check assumptions}\label{check-assumptions}

Before we rush into interpreting the output of the model, we need to
check whether it was valid to use a linear model in the first place.
Whatever the test within which we are using a linear model, we should do
the necessary diagnostic checks at this stage.

You can do this using tests designed for the purpose, but I prefer to do
it graphically, using a function \texttt{autoplot()} from the package
\texttt{ggfortify}. You give this the model we have just created using
\texttt{lm()} and it produces four very useful graphs. I suggest that,
after once installing \texttt{ggfortify} you include the line
\texttt{library(ggfortify)} at the start of every script.

Here is how you use it:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autoplot}\NormalTok{(model\_pgr, }\AttributeTok{smooth.colour=}\ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{simple-linear-regression_files/figure-pdf/unnamed-chunk-5-1.pdf}}

The \texttt{theme\_cowplot()} part is not necessary, but it gives the
plots a nice look, so why not?

These plots are all based around the `residuals', which is the vertical
distance between observed values and fitted values ie between each point
and the best fit line through the points - the line which the linear
model is finding for us, by telling us its intercept and gradient.

Note that in simple linear regression, the best fit line is the one that
minimises that sum of the squared residuals.

So what do these plots mean?

\begin{itemize}
\tightlist
\item
  \emph{Top-left}: This tells you about the structure of the model. Was
  it a good idea to try to fit a straight line to the data? If not, for
  example because the data did follow a linear trend, then there will be
  humps or troughs in this plot.
\item
  \emph{Top-right}: This evaluates the assumption of normality of the
  residuals. The dots are the residuals and the dashed line is the
  expectation under the normality assumption. This is a \emph{much}
  better way to check normality than making a histogram of the
  residuals, especially with small samples.
\item
  \emph{Bottom-left}: This examines the assumption of equal variance of
  the residuals. A linear model assumes that the variance is constant
  over all predicted values of the response variables. There should be
  no pattern. Often, however, there is. With count data, for example,
  the variance typically increases with the mean.
\item
  \emph{Bottom-right}: This detects leverage - which means points that
  have undue influence on the gradient of the fitted line, and outliers.
  If you have outliers in your data, you need to decide what to do with
  them.
\end{itemize}

In the case of these data, we are good to go! There is no discernible
pattern in either of the left-hand plots, the qq-plot is about as
straight as you ever see with real data, and there are no points
exerting undue high influence.

\subsection{Interpretation of the
model}\label{interpretation-of-the-model}

Now that we have established that the data meet the criteria required
for the model to be valid, we can go ahead and inspect its output. We
will do this using two tools that we also use for every other general
linear model we implement (t-test, ANOVA etc). These are
\texttt{anova()} and \texttt{summary()}

Let us first use \texttt{anova()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(model\_pgr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: plant.growth.rate
                      Df  Sum Sq Mean Sq F value    Pr(>F)    
soil.moisture.content  1 2521.15 2521.15  156.08 < 2.2e-16 ***
Residuals             48  775.35   16.15                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The F value here is an example of a `test statistic', a number that a
test calculates from the data, from which it is possible to further
calulate how likely it is that you would have got the data you got if
the null hypothesis were true. This particular test statistic is the
ratio of the variation in the data that is explained by the explanatory
variable to the leftover variance. The bigger it is, the better the job
that the explanatory variable is doing at explaining the variation in
the dependent variable. The p value, which here is effectively zero, is
the chance you would have got an F value this big or bigger from the
data in the sample if in fact there were no relationship between plant
growth rate and soil moisture content. If the p value is small (and by
that we usually mean less than 0.05) then we can reject the null
hypothesis that there is no relationship between plant growth rate and
soil moisture content.

Hence, in this case, we emphatically reject the null: there is clear
evidence that plant growth rate is at least in part explained by soil
moisture content.

Now we use the \texttt{summary()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model\_pgr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = plant.growth.rate ~ soil.moisture.content, data = plants)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.9089 -3.0747  0.2261  2.6567  8.9406 

Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
(Intercept)             19.348      1.283   15.08   <2e-16 ***
soil.moisture.content   12.750      1.021   12.49   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.019 on 48 degrees of freedom
Multiple R-squared:  0.7648,    Adjusted R-squared:  0.7599 
F-statistic: 156.1 on 1 and 48 DF,  p-value: < 2.2e-16
\end{verbatim}

This gives us estimates of the intercept (19.348) and gradient (12.750)
of the best fit line through the data. The null hypothesis is that both
these values are zero, and the p-value is our clue as to whether we can
reject this null. Here, in both cases, we clearly can.

We also see the \emph{Adjusted R-squared} value of 0.7599. This is the
proportion of the variance in the dependent variable that is explained
by the explanatory variable. Thus it can vary between 0 and 1. A large
value like this indicates that soil moisture content is a good predictor
of plant growth rate.

\subsection{Back to the figure}\label{back-to-the-figure}

Typically, a final step in our analysis involves including the model we
have fitted into the original figure, if that is possible in a
straightforward way. In the case of simple linear regression, it is. It
means adding a straight line with the intercept and gradient displayed
by the \texttt{summary()} function. We do this by adding a line
\texttt{geom\_smooth(method\ =\ "lm")} to our plot code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plants }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{soil.moisture.content, }\AttributeTok{y=}\NormalTok{plant.growth.rate)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Soil moisture content"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Plant growth rate (mm/week)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{50}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{simple-linear-regression_files/figure-pdf/unnamed-chunk-8-1.pdf}}

This gives both a straight line and the `standard error' of that line -
meaning, roughly speaking, the wiggle room within which the `true' line
, for the population as opposed to this sample drawn from it, probably
lies.

\subsection{Report the result}\label{report-the-result}

We would likely want to include this plot in our report, along with a
statement like:

We find evidence for a linear increase in plant growth rate with soil
moisture content (p\textless0.001), with an additional 12.75 mm of
growth per unit increase in soil moisture content.

\subsection{Conclusion}\label{conclusion-2}

We have carried out a simple linear regression on continuous data. This
is an example of a general linear model. We first plotted the data, then
we used \texttt{lm()} to fit the model. Next we inspected the validity
of the model using \texttt{autoplot}. We then inspected the model itself
using first \texttt{anova()} then \texttt{summary()}. Finally we
included the output of the model on the plot, in this case by adding to
it a straight line with the intercept and gradient determined by the
regression model, and reported the result in plain English.

\section{Simple linear regression - ocean
pH}\label{simple-linear-regression---ocean-ph}

In this second example we provide. the script but leave you to interpret
the outcome of each step. You can use the first example to help you do
this.

Here we use data from Figure 5.20 of AR6, WG1 from the IPCC. It shows
ocean pH measurements from a location (137\(^{\circ}\)E, 5\(^{\circ}\)N)
in the western Pacific between 1980 and 2020.

Your task is to assess whether there is a significant linear trend in pH
with time and if so to determiine the change in pH per year or per
decade during the forty year period from 1980 to 2020.

\subsection{Load packages}\label{load-packages-6}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{library}\NormalTok{(ggfortify)}
\FunctionTok{library}\NormalTok{(cowplot)}
\end{Highlighting}
\end{Shaded}

\subsection{Load data}\label{load-data-2}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pH\_filepath }\OtherTok{\textless{}{-}} \FunctionTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"ipcc\_AR6\_WGI\_Figure\_5\_20{-}pH.csv"}\NormalTok{)}
\NormalTok{pH}\OtherTok{\textless{}{-}}\FunctionTok{read\_csv}\NormalTok{(pH\_filepath,}\AttributeTok{skip=}\DecValTok{6}\NormalTok{) }\CommentTok{\# we have to skip the first 6 lines becuase of meta{-}data {-} check it out!}
\FunctionTok{glimpse}\NormalTok{(pH)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 81
Columns: 2
$ year <dbl> 1984.121, 1985.171, 1986.066, 1987.060, 1987.488, 1988.058, 1989.~
$ pH   <dbl> 8.100000, 8.097917, 8.086458, 8.102604, 8.071875, 8.098437, 8.095~
\end{verbatim}

\subsection{Plot data}\label{plot-data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pH }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ pH)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Ocean pH"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\FloatTok{8.0}\NormalTok{,}\FloatTok{8.12}\NormalTok{), }\AttributeTok{breaks=}\FunctionTok{seq}\NormalTok{(}\FloatTok{8.0}\NormalTok{,}\FloatTok{8.2}\NormalTok{,}\FloatTok{0.02}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{simple-linear-regression_files/figure-pdf/unnamed-chunk-12-1.pdf}}

Is there a linear trend?

\subsection{Fit linear model}\label{fit-linear-model}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pH\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(pH }\SpecialCharTok{\textasciitilde{}}\NormalTok{ year, }\AttributeTok{data=}\NormalTok{ pH)}
\end{Highlighting}
\end{Shaded}

\subsection{Check model validity}\label{check-model-validity}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autoplot}\NormalTok{(pH\_model) }\SpecialCharTok{+} \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{simple-linear-regression_files/figure-pdf/unnamed-chunk-14-1.pdf}}

Is it reasonable to apply a linear model to these data? Remember that
each of these plots tell you something about whether this is the case.

\subsection{Inspect model}\label{inspect-model}

\subsubsection{ANOVA}\label{anova-1}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(pH\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: pH
          Df    Sum Sq   Mean Sq F value    Pr(>F)    
year       1 0.0169414 0.0169414  199.87 < 2.2e-16 ***
Residuals 79 0.0066962 0.0000848                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Are we OK to reject the null hypothesis that pH does not change over
this time period?

\subsection{Summary}\label{summary}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(pH\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = pH ~ year, data = pH)

Residuals:
       Min         1Q     Median         3Q        Max 
-0.0247092 -0.0049642  0.0002185  0.0060392  0.0168809 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 11.234405   0.224330   50.08   <2e-16 ***
year        -0.001583   0.000112  -14.14   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.009207 on 79 degrees of freedom
Multiple R-squared:  0.7167,    Adjusted R-squared:  0.7131 
F-statistic: 199.9 on 1 and 79 DF,  p-value: < 2.2e-16
\end{verbatim}

What is the change in ocean pH per decade over the last four decades? Is
this change statistically significant? Does the linear model account for
much of the variance in the data?

\subsection{Replot the data, model
included}\label{replot-the-data-model-included}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pH }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ pH)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{colour =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# add the line. Method ="lm" for a straight line.}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Ocean pH"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\FloatTok{8.0}\NormalTok{,}\FloatTok{8.12}\NormalTok{), }\AttributeTok{breaks=}\FunctionTok{seq}\NormalTok{(}\FloatTok{8.0}\NormalTok{,}\FloatTok{8.2}\NormalTok{,}\FloatTok{0.02}\NormalTok{)) }\SpecialCharTok{+} \CommentTok{\# fix limits and break points of y{-}axis}
  \FunctionTok{annotate}\NormalTok{(}\AttributeTok{geom=}\StringTok{"text"}\NormalTok{, }\AttributeTok{x =} \DecValTok{2015}\NormalTok{, }\AttributeTok{y =} \FloatTok{8.11}\NormalTok{, }\AttributeTok{label =} \StringTok{"137W, 5N"}\NormalTok{, }\AttributeTok{size =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# measurement site}
  \FunctionTok{theme\_cowplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{simple-linear-regression_files/figure-pdf/unnamed-chunk-17-1.pdf}}

How would you report this result?

\section{Simple linear regression: Body mass vs metabolic
rate}\label{simple-linear-regression-body-mass-vs-metabolic-rate}

Here we use data from

\section{Sample script}\label{sample-script}

A notebook to do linear regression might look like the following, here
written as a .Rmd notebook. For this to work you will need to work
within a project, with the data in a sub-folder of that called ``data''.
Here, the data is taken to be a .csv file called \texttt{mydata.csv},
with two columns of data, one called \texttt{x\_values} and the other
called \texttt{y\_values}. You need to change these to suit your own
data.

To use this, open a new notebook of your own
(\texttt{File/New\ File/R\ Notebook}), delete everything, paste in all
the code below, then adapt the code as needed. Remember to save the
notebook in your project scripts folder!

\begin{verbatim}
---
title: "Sensible title"
author: "your name"
date: "the date"
output: html_notebook
---

### load packages
```{r}
library(tidyverse)
library(here)
library(ggfortify)
library(cowplot)
```

### load data
```{r load_data}
filepath <- here("data", "mydata.csv")
mydata <- read_csv(filepath)
glimpse(mydata)
```

### plot the data
```{r}
mydata |>
  ggplot(aes(x=x_values, y=y_values)) +
  geom_point() +
  labs(x = "X variable",
       y = "Y variable") +
  theme_cowplot()
```

### fit the model
```{r}
mydata.model <- lm (y_values ~ x_values, data = mydata)
```

### diagnostics
```{r}
autoplot(mydata.model)
```

### investigate the model
```{r}
anova(mydata.model)
```

```{r}
summary(mydata.model)
```

### replot the data, now with the model included
```{r}
mydata |>
  ggplot(aes(x=x_values, y=y_values)) +
  geom_point() +
  geom_smooth(method="linear") +
  labs(x = "X variable",
       y = "Y variable") +
  theme_cowplot()
```
\end{verbatim}

\part{Additional help}

\chapter{Quantile-quantile plots}\label{quantile-quantile-plots}

Adapted from an exercise by Jon Yearsley (School of Biology and
Environmental Science, UCD)

\subsection{Introduction}\label{introduction-3}

Q-Q plots can play a useful role when trying to decide whether a dataset
is normally distributed, and if it is not, then how it differs from
normality.

We will investigate the types of quantile-quantile plots you get from
different types of distributions.

We will look at data distributed according to

\begin{itemize}
\tightlist
\item
  A normal distribution\\
\item
  A right-skewed distribution\\
\item
  A left-skewed distribution
\item
  An under-dispersed distribution
\item
  An over-dispersed distribution
\end{itemize}

\section{What is a Q-Q plot?}\label{what-is-a-q-q-plot}

Quantiles partition a dataset into equal subsets. For example, if we
wished to partition a standard normal (mean = 0, standard deviation = 1)
population into 4 equal subsets, the 3 quantiles (ie the three values of
x) that would do this are -0.675, 0 and 0.675. In this way, 25\% of the
population would have a value greater than 0.675, 25\% between 0 and
0.675, 25\% between -0.675 and 0 and the final 25\% would have a value
less than -0.675. When we draw the distribution, the areas under the
curve between nqeighbouring quantiles will be equal:

\pandocbounded{\includegraphics[keepaspectratio]{qq_plots_files/figure-pdf/unnamed-chunk-3-1.pdf}}

\section{Normally distributed data.}\label{normally-distributed-data.}

Below we show an example of 150 observations that are drawn from a
normal distribution. The normal distribution is symmetric, so has no
skew. Its mean is equal to its median.

On a Q-Q plot normally distributed data lie roughly on a straight line,
perhaps looking a bit ragged at each end. The box plot is symmetric with
few or no outliers.

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{qq_plots_files/figure-pdf/normal-1.pdf}}
\end{center}

\section{Right-skewed data.}\label{right-skewed-data.}

Right skewed distributions are non-symmetric and have a long tail
heading towards extreme values on the right-hand side of the
distribution. The mean is more positive than the median.

In the example we show an exponential distribution.

In the Q-Q plot, such distributions give a distinctive convex curvature.
The box-plot may show outliers out towards large values.

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{qq_plots_files/figure-pdf/right-skew-1.pdf}}
\end{center}

\section{Left-skewed data.}\label{left-skewed-data.}

Left skewed distributions are non-symmetric and have a long tail heading
towards extreme values on the left-hand side of the distribution. The
mean is more negative than the median. The box plot may show outliers
down towards small values.

In the example we show a negative exponential distribution.

In the Q-Q plot, such distributions give a distinctive concave
curvature.

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{qq_plots_files/figure-pdf/left-skew-1.pdf}}
\end{center}

\section{Under-dispersed data}\label{under-dispersed-data}

Under-dispersed data are data whose distribution is more concentrated
around a central value than is the case for normally distributed data.
There are fewer outliers and the tails of the distribution are lighter.
As an example here we show 150 points drawn from a uniform distribution.

Note the distinctive curvature of the Q-Q plot. The `box' of the boxplot
is bigger than for a normal distribution, since the interquartile range
covers a larger range of values.

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{qq_plots_files/figure-pdf/under dispersed-1.pdf}}
\end{center}

\section{Over-dispersed data}\label{over-dispersed-data}

Over-dispersed data are data whose distribution is more widely spread
around a central value than is the case for normally distributed data.
There are more outliers and the tails of the distribution are fatter. As
an example here we show 150 points drawn from a laplace distribution.

Note the distinctive curvature of the Q-Q plot - like the previous one
but curving the other way. The `box' of the boxplot is smaller than for
a normal distribution, since the interquartile range covers a smaller
range of values.

\begin{center}
\pandocbounded{\includegraphics[keepaspectratio]{qq_plots_files/figure-pdf/over-dispersed-1.pdf}}
\end{center}

\section{Caution}\label{caution}

With \emph{small} data sets, the scatter in the data can make it
difficult to tell from the histogram (especially) or even the Q-Q plot
whether the dataset is plausibly drawn from a normally distributed
population. In that case you might choose to combine use of the plots
with a normality test, such as Kolmogorov-Smirnov or Shapiro-Wilk. The
null hypothesis of these is that the data ARE drawn from normally
distributed populations, so the smaller the p-value when they are
applied to a dataset, the less likely it is that this is true.

With \emph{large} data sets,the Kolmogorov-Smirnov and Shapiro-Wilk
tests become very sensitive to even small deviations from normality and
might give a p-value that would lead you to suppose that a dataset was
not drawn from a normally distributed population. Since no data set is
ever truly normal, even when we consider the whole population, all we
really need to know is whether the data are close enough to normal that
the various tests (eg \emph{t}-test, ANOVA, correlation, least square
regression) that require it are going to work well enough. For these
large data sets, histograms and Q-Q plots can be very useful indicators
of approximate normality.

\section{Who cares about normality anyway? The central limit
theorem.}\label{who-cares-about-normality-anyway-the-central-limit-theorem.}

Lastly, for large enough data sets, we don't actually need the data to
be normally distributed for the tests that require normality to work!
This is because what they require is not that the dataset itself be
normal, but that the distribution of the means of many such data sets,
the so-called sampling distribution, be normal. A very important
mathematical result known as the \textbf{Central Limit Theorem}
guarantees that this will be the case \emph{whatever} the distribution
of each of the data sets, as long as these datasets are large enough!

How large is large enough? There's the rub! A common rule of thumb is
that if the dataset has size \emph{N}\textgreater30 or so, then it is
safe to use tests that require normality. Indeed, one does find that
sampling distributions for data drawn from uniform or mildly skewed
distributions such as the exponential distribution are roughly normal
when \emph{N} exceeds 30 or so, but for more skewed datasets, a larger
dataset can be needed - it depends on how far from normality the
distribution is. The further from normal it is the larger the dataset
needs to be before the Central Limit Theorem applies to a good
approximation. For a highly skewed dataset, for example one distributed
according to something like a log-normal distribution, it can require
\emph{N}\textgreater200 or so, or even more before it is OK to use
\emph{t}-tests and the like.

\chapter{Power Analysis}\label{power-analysis}

This topic is all about how to design our studies such that they are
likely to detect an effect, whether that is a correlation or a
difference, if there really is one there to be detected. If there were,
and we didn't that would be a shame (and a waste of time and money and
possibly an ethical hoohah).

In what follows we focus on a simple study that seeks to detect a
difference between two populations, but the ideas generalise to other
designs.

Suppose we have a magic soil supplement that we hope will enhance the
growth rate of tomato plants, make us rich and pay for a comfortable
retirement.

Before riches, however, we have to be sure that it has the desired
effect. For the supplement to be a money spinner let's suppose that we
need it to enhance the growth rate of plants by 10\% and so we must do a
study to see if this is in fact the case.

In a properly randomised design, we take \emph{N} plants and give them
`normal' plant food, and another \emph{N} plants and give them the
normal food plus our supplement. We keep all other conditions the same
for the two groups of plants.

After 30 days, The plants grown with the usual food grow with a mean
mass of 300g and a standard deviation of 30g. The other plants will need
to have a mean mass of 330g, and we will suppose that they too will have
a standard deviation of 30g.

\section{Simulation of one trial}\label{simulation-of-one-trial}

Suppose we chose \emph{N} = 100. Then we can simulate the masses of the
individual plants in the two samples, supposing there were a 10\% effect
of supplement. We do this by using the \texttt{rnorm()} function to draw
samples of 100 replicates each from a normally distributed population,
in one case with a mean of 300g and in the other with a mean of 330g,
both with standard deviations of 30g

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N}\OtherTok{\textless{}{-}}\DecValTok{100}
\NormalTok{delta}\OtherTok{\textless{}{-}}\FloatTok{0.05} \CommentTok{\# ie 10\% ie the mean of the treatment population is 10\% greater than that of the control population}

\CommentTok{\# set up the sample parameters: means and standard deviations}
\NormalTok{m1}\OtherTok{\textless{}{-}}\DecValTok{300} 
\NormalTok{sd1}\OtherTok{\textless{}{-}}\DecValTok{30}
\NormalTok{m2}\OtherTok{\textless{}{-}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{delta)}\SpecialCharTok{*}\NormalTok{m1}
\NormalTok{sd2}\OtherTok{\textless{}{-}}\NormalTok{sd1}

\NormalTok{untreated}\OtherTok{\textless{}{-}}\FunctionTok{rnorm}\NormalTok{(N,m1,sd1) }\CommentTok{\# the control sample}
\NormalTok{treated}\OtherTok{\textless{}{-}}\FunctionTok{rnorm}\NormalTok{(N,m2,sd2) }\CommentTok{\# the treated sample}

\CommentTok{\# put these samples  in a tidy tibble}
\NormalTok{trial\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{treatment =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"untreated"}\NormalTok{, N), }\FunctionTok{rep}\NormalTok{(}\StringTok{"treated"}\NormalTok{, N)),}
                     \AttributeTok{mass =} \FunctionTok{c}\NormalTok{(untreated, treated))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot these simulated data}
\NormalTok{trial\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{mass,}\AttributeTok{colour =}\NormalTok{ treatment, }\AttributeTok{fill=}\NormalTok{treatment)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{bins=}\DecValTok{20}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{+}
  \CommentTok{\# geom\_jitter(colour="gray50",width=0.2) +}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Plant mass (g)"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Frequency"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{power-analysis_files/figure-pdf/unnamed-chunk-3-1.pdf}}

Suppose the supplement really did work like this, and really did improve
growth rates by 10\%. How many plants, ie what value of \emph{N} would
we need in each group in order to have an 80\% chance of correctly
spotting this difference? This probability of detection of an effect
that is there is what we call the \textbf{power} of a study. If the
effect is smaller than 10\% (or whatever boost we deemed sufficient) and
we do not detect it we do not mind, since that means that our supplement
had not worked as we had hoped, but it would be a waste of our time and
money if there really were a sizeable effect but we did not spot it
because our sample size was too small. Equally, we do not want a sample
size that is larger than is necessary to achieve sufficient power. Doing
so would incur unnecessary time and money costs and possibly have
ethical implications.

In thinking about how we might analyse our data we formulate a null
hypothesis:

\emph{The supplement has no effect.}

In that case we would expect the difference between the masses of the
treated plants and untreated plants to be zero.

If in fact the supplement has an effect on growth, then the data should
force us to reject this null hypothesis. In order that there be at least
an 80\% chance that it does this, there has to be an at least 80\%
chance that the mean value of our treated plants lies outside the
rejection regime of the null hypothesis.

With one pair of samples of plants, we might or might not detect the
effect, depending on which individuals ended up being included in the
samples that we drew from their respective populations, since this would
determine whether the mean of the treated plants was or was not in the
rejection regime of the null. Thus, if we ran a t-test on our two
simulated samples, we might or might not get a p-value that is less than
our chosen significance level,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(mass}\SpecialCharTok{\textasciitilde{}}\NormalTok{treatment,}\AttributeTok{data=}\NormalTok{trial\_data)}\SpecialCharTok{$}\NormalTok{p.value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.01162367
\end{verbatim}

Wht we want to know is: what is the chance that we would get the
`correct' result, which in our case is that there \emph{is} a difference
and which would be indicated to us by the p-value being less than our
chosen significance level (for brevity's sake, let's just assume from
now on that we chose this to be 0.05), so that we (correctly) reject the
null hypothesis of there being no difference.

\section{Simulation of many trials}\label{simulation-of-many-trials}

A way to find out is to carry out this simulation many times and see in
what fraction of our trials we see a significant effect (that is, p
\textless{} 0.05), supposing that the supplement really does work. That
will give us an idea of the power of our study.

That is what we will now do:

We write a function that will return TRUE or FALSE depending on the
\emph{p}-value of a trial in which we specifiy the mean value of the
control group, the standard deviation of the control group (assumed to
be the same in the treatment group), the size of the effect
\texttt{delta}, where if delta is 0.1, say, then we mean that the effect
size is a 10\% increase in growth mass, \emph{N} is the sample size for
each group and \emph{alpha} is the significance level that we choose
(most likely, but not necessarily 0.05).

In the function we carry out a \emph{t}-test for each pair of samples to
determine whether we can reject the null hypothesis that the treated
plants are drawn from a population with the same mass as the untreated
plants.

We know that the null hypothesis is false because we have drawn our
samples from populations that \emph{do} differ in their mean values. We
want to see if our test correctly rejects the null so that we detect the
effect, which here is the mass difference between the means of the two
groups of plants.

If the \emph{p}-value is less than our chosen significance level then we
reject the null, if not, we do not.

Rejecting the null in this case means we have a `True Positive' and so
we make our function return the logical value TRUE in that case. Failing
to reject the null in this case is a mistake. We call this kind of
mistake a 'False Negative'' and when this happens me make out function
return the logical value FALSE.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effect\_detected}\OtherTok{\textless{}{-}}\ControlFlowTok{function}\NormalTok{(m1,sd1,delta,N,alpha)\{}
\NormalTok{  pop1}\OtherTok{\textless{}{-}}\FunctionTok{rnorm}\NormalTok{(N,m1,sd1)}
\NormalTok{  pop2}\OtherTok{\textless{}{-}}\FunctionTok{rnorm}\NormalTok{(N,m1}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{delta),sd1)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{t.test}\NormalTok{(pop1,pop2)}\SpecialCharTok{$}\NormalTok{p.value}\SpecialCharTok{\textless{}}\NormalTok{alpha) }\CommentTok{\#return TRUE if p \textless{}0.05, return FALSE if not.}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\section{Calculate the power of the study for a range of sample and
effect
sizes}\label{calculate-the-power-of-the-study-for-a-range-of-sample-and-effect-sizes}

Now we run this trial as many times as we want. Let us run it 10,000
times, for a range of sample sizes and effect sizes. In any one trial we
may or may not reject the null. We want to see, for a given set of
conditons, in what fraction of trials, in the long run, we do reject the
null. That will be an estimate of the power of our study: the likelihood
that we will detect an effect if the effect really exists, as it does
here.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trials}\OtherTok{\textless{}{-}}\DecValTok{10000}
\NormalTok{m1}\OtherTok{\textless{}{-}}\DecValTok{300}
\NormalTok{sd1}\OtherTok{\textless{}{-}}\DecValTok{30} \CommentTok{\# population standard deviation}
\NormalTok{deltas}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{,}\FloatTok{0.02}\NormalTok{,}\FloatTok{0.03}\NormalTok{,}\FloatTok{0.04}\NormalTok{,}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.1}\NormalTok{) }\CommentTok{\# effect size where, for example, 0.05 means that the effect is 5\% the size of the control mean}
\NormalTok{N}\OtherTok{\textless{}{-}}\DecValTok{100} \CommentTok{\# sample size}
\NormalTok{alpha}\OtherTok{\textless{}{-}}\FloatTok{0.05} \CommentTok{\# chosen significance level}

\NormalTok{Ns}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{1}\NormalTok{),}\FunctionTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{50}\NormalTok{,}\DecValTok{10}\NormalTok{),}\FunctionTok{seq}\NormalTok{(}\DecValTok{60}\NormalTok{,}\DecValTok{200}\NormalTok{,}\DecValTok{20}\NormalTok{),}\FunctionTok{seq}\NormalTok{(}\DecValTok{240}\NormalTok{,}\DecValTok{480}\NormalTok{,}\DecValTok{40}\NormalTok{))}

\NormalTok{power\_vals\_raw}\OtherTok{\textless{}{-}}\FunctionTok{matrix}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FunctionTok{length}\NormalTok{(Ns)}\SpecialCharTok{*}\FunctionTok{length}\NormalTok{(deltas)),}\AttributeTok{ncol=}\FunctionTok{length}\NormalTok{(deltas))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(Ns))\{}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(deltas))\{}
\NormalTok{    trial\_results }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{(trials,}\FunctionTok{effect\_detected}\NormalTok{(m1,sd1,deltas[j],Ns[i],alpha))}
\NormalTok{    power\_vals\_raw[i,j]}\OtherTok{\textless{}{-}}\FunctionTok{mean}\NormalTok{(trial\_results)}
\NormalTok{  \}}
\NormalTok{\}}

\NormalTok{power\_vals }\OtherTok{\textless{}{-}} \FunctionTok{as.tibble}\NormalTok{(power\_vals\_raw)}
\FunctionTok{names}\NormalTok{(power\_vals) }\OtherTok{\textless{}{-}}\NormalTok{ deltas}
\NormalTok{power\_vals }\OtherTok{\textless{}{-}}\NormalTok{ power\_vals }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{N=}\NormalTok{Ns) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{N,}\AttributeTok{names\_to =} \StringTok{"effect\_size"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"power"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Plotted, this looks like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{power\_vals }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ok=}\NormalTok{power}\SpecialCharTok{\textgreater{}}\FloatTok{0.8}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{N,}\AttributeTok{y=}\NormalTok{power,}\AttributeTok{colour=}\NormalTok{effect\_size)) }\SpecialCharTok{+}
  \CommentTok{\# geom\_point() +}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"loess"}\NormalTok{, }\AttributeTok{span=}\FloatTok{0.25}\NormalTok{, }\AttributeTok{linewidth=}\FloatTok{0.6}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \CommentTok{\# geom\_line() +}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks=}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FunctionTok{max}\NormalTok{(Ns),}\DecValTok{50}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{breaks=}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\FloatTok{0.2}\NormalTok{), }\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept=}\FloatTok{0.8}\NormalTok{,}\AttributeTok{linetype=}\StringTok{"dashed"}\NormalTok{,}\AttributeTok{colour=}\StringTok{"darkblue"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept=}\FloatTok{0.9}\NormalTok{,}\AttributeTok{linetype=}\StringTok{"dashed"}\NormalTok{,}\AttributeTok{colour=}\StringTok{"darkblue"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Sample size N"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Power"}\NormalTok{,}
       \AttributeTok{colour =}\StringTok{"Effect size"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{power-analysis_files/figure-pdf/unnamed-chunk-7-1.pdf}}

In this plot we show how the power varies with sample size N, for
different effect sizes (where 0.05, say, means a difference of 5\% in
populations means), with a population standard devation of 30 g on a
control mean of 300g, and a significance level of 0.05.

\textbf{What is a suitable sample size for a power of 80\%?}

We see that we need a sample size of about 65 to get a power of 80\%.

\textbf{What sample size would be needed for a power of 90\%, or 99\%?}

We need sample sizes of 85 for a power of 90\% and of 150 for a power of
99\%. There are progressively diminshing returns once you try to make
the power much greater than 80\% or 90\% or so. The sample sizes needed
become very large.

Now try varying the effect size, the population variation and the chosen
significance level and see how they affect the power.

We should find that, all else being equal:

\begin{itemize}
\tightlist
\item
  If the effect size is reduced, the power decreases. It is harder to
  tell apart two populations that do not differ by much.
\item
  If the population variation goes down the power increases. It is
  easier to tell apart two populations if there is little variation
  within each population.
\item
  If the signicance level is reduced, say to 0.01 from 0.05, the power
  will go down. We are reducing the type 1 error rate, but at the same
  increasing the type 2 error rate. It is les likely, even if there is
  an effect, that we will detect it.
\end{itemize}

\section{Factors that affect the power of a
study}\label{factors-that-affect-the-power-of-a-study}

\textbf{What factors about a study force you to have larger sample sizes
for a given power? }

If the effect size is small, the population variation is large or the
significance level required is small.

\textbf{How would you know what the variation of your population is? How
could you reduce this?}.

You might look at the literature. Very likely, someone has done a study
similar to yours. What variation did they see? Alternatively, you could
do a pilot study and with relatively little effort get an idea yourself
of the variation within your population of interest of the attrivute you
want to measure.

\textbf{What downside might there be to reducing the population
variation, supposing you could do it, in order to increase power?}.

You can sometimes reduce the variation within the population that you
are studying by restricting variation in causal factors that do not
interest you for that particular study, but which might also be
contributing to variation in the outcome variable that does interest
you. So while your study might focus on the impact of soil treatments on
growth rates of seedlings, for example, it may be that soil moisture
also affects growth rate. Hence if you ensure that all seedlings are
grown under the same moisture conditions, you will remove any variation
in growth rates due to that, and hence reduce the total varation. This
will increase the power of your study - where by that we mwan the
likelihood that you will detect any difference that your soil supplement
made to growth rate.

The downside to this approach is that you will restrict the
applicability of your study. In the example above, having applied the
supplement and observed some difference or not compared to a control
sample, you could only make inferences to populations of seedlings that
were grown under precisely the soil moisture conditions you chose for
your study. You could no longer make statements about the supplement
preferences of seedlings grown under any old soil moisture conditions.

An alternative approach, which preserves both power and range of
applicability is to change the design of the study. In this case,
instead of making it a one-factor study in which growth rate is measured
against supplement presence or absence, we could also measure the soil
moisture and include this in the analysis. If we had measured discrete
levels of soil moisture, we would now have a 2-way ANOVA, and if we had
measured it as a continuous variable we would have an ANCOVA.

\textbf{How would you know what the effect size was, and could you
increase it?}

The same applies here as for the variation. You could look at the
literature. Someone else has very likely done a similar study. What
effect size did they observe? Or, you could do a quick and dirty pilot
study. Unlike the population variation thing, however, it is easier to
detect an effect if the effect size is bigger. Is there anything you can
do to increase it? In an observational study in the wild, perhaps not
but in a manipulative study perhaps you can. In our example we might use
large doses of supplement rather than small ones, hoping to see a larger
effect as a result.

\textbf{What assumptions have gone into this power calculation?}

We assumed here that our samples were drawn from normally distributed
populations with equal variances. In our simulations we knew that was
the case because me made it so by design, but we could have given them
any distribution or variance we wanted. Simulations are a very powerful
tool.

\part{References}

\chapter*{References}\label{references-1}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-cumming2007}
Cumming, Geoff, Fiona Fidler, and David L. Vaux. 2007. {``Error Bars in
Experimental Biology.''} \emph{Journal of Cell Biology} 177 (1): 7--11.
\url{https://doi.org/10.1083/jcb.200611141}.

\end{CSLReferences}




\end{document}
