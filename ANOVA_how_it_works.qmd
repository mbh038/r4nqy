
# ANOVA principles


Material used from Chapter One of Grafen and Hails: Modern Statistics for the Life Sciences

```{r global-options, include=FALSE,echo=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, warning=FALSE, message=FALSE,echo=FALSE)
```

```{r load packages, message=FALSE,warning=FALSE,echo=FALSE}
library(tidyverse)
library(here)
library(cowplot)
library(gridExtra)
library(ggfortify)
```

## What is ANOVA?

```{r load data,echo=FALSE}
filepath<-here("data","fertilizer.csv")
fertilizer<-read_csv(filepath)
```

```{r look at the data,echo=FALSE}
# glimpse(fertilizer)
# head(fertilizer)
# summary(fertilizer)
```

```{r make_factor}
fertilizer <- fertilizer |>
  mutate(FERTIL=as.factor(FERTIL))
```

### The basic principles of ANOVA

In a simple case we consider the comparison of three means. This is done by the analysis of variance (ANOVA). In this case we will go through an example in detail and work out all the mechanics, but once we have done that and seen how the output is derived from the input we will not need to do it again. We will use R to do the heavy lifting. We will just need to know when it is appropriate to use ANOVA, how to get R to do it and how to interpret the output that R produces.

### The Scenario

Suppose we have three fertilizers and wish to compare their efficacy. This has been done in a field experiment where each fertilizer is applied to 10 plots and the 30 plots are later harvested, with the crop yields being calculated. We end up with three groups of 10 figures and we wish to know if there are any differences between these groups.

When we plot the data we see that the fertilizers do differ in the amount of yield produced but that there is also a lot of variation between the plots that were given the same fertilizer.

```{r eda boxplot}
g1<-ggplot(fertilizer,aes(x=FERTIL,y=YIELD, fill= FERTIL,alpha=0.1))+
  geom_boxplot()+
  geom_point(aes(colour=FERTIL))+
  scale_y_continuous(limits=c(0,10))+
  labs(x='Fertilizer', y='Yield')+
  theme_cowplot()+
  theme(legend.position = "none")
```

```{r basic plot}
g2<-ggplot(fertilizer,aes(x=plot,y=YIELD,colour=FERTIL))+
  geom_point()+
  scale_y_continuous(limits=c(0,10))+
  labs(x='Plot number',y='Yield per plot (tonnes)')+
  theme_cowplot()
```

```{r}
grid.arrange(g2,g1,nrow=1)
```

### What does an ANOVA do?

An ANOVA (ANalysis Of VAriance) analysis attempts to determine whether the differences between the effect of the fertilizers is significant by investigating the variability in the data. We investigate how the variability *between* groups compares to the variability *within* groups.

### Grand Mean

First we calculate the 'grand mean', the mean of the yields across all 30 plots:

```{r Grand mean}
grand_mean=mean(fertilizer$YIELD)
grand_mean
```

#### Deviations from the grand mean

```{r SST plot}
SST.plot<-g2+geom_hline(yintercept=grand_mean,linetype='dashed')+
  geom_segment(aes(x = plot, y = YIELD, xend = plot, yend = grand_mean),linetype='dotted')
SST.plot
```

#### Mean value of yield for each fertilizer

```{r fertilizer means}
f_means<-fertilizer |>
  group_by(FERTIL) |>
  summarise(fmean=mean(YIELD))
f_means
fertilizer<-mutate(fertilizer,fmean=c(rep(f_means$fmean[1],10),rep(f_means$fmean[2],10),rep(f_means$fmean[3],10)))

f1<-filter(fertilizer,FERTIL==1)
f2<-filter(fertilizer,FERTIL==2)
f3<-filter(fertilizer,FERTIL==3)
```

```{r S}

g3<-ggplot()+
  
  geom_point(data=f1,aes(x=plot,y=YIELD))+
  geom_segment(aes(x=min(f1$plot),y=f1$fmean[1],xend=max(f1$plot),yend=f1$fmean[1]))+
  geom_segment(aes(x = f1$plot, y = f1$YIELD, xend = f1$plot, yend = f1$fmean[1]),linetype='dotted')+
  
  geom_point(data=f2,aes(x=plot,y=YIELD))+
  geom_segment(aes(x=min(f2$plot),y=f2$fmean[1],xend=max(f2$plot),yend=f2$fmean[1]))+
  geom_segment(aes(x = f2$plot, y = f2$YIELD, xend = f2$plot, yend = f2$fmean[1]),linetype='dotted')+
  
  geom_point(data=f3,aes(x=plot,y=YIELD))+
  geom_segment(aes(x=min(f3$plot),y=f3$fmean[1],xend=max(f3$plot),yend=f3$fmean[1]))+
  geom_segment(aes(x = f3$plot, y = f3$YIELD, xend = f3$plot, yend = f3$fmean[1]),linetype='dotted')+
  
  scale_y_continuous(limits=c(0,10))+
  labs(x='Plot number',y='Yield per plot (tonnes)',title="SSE: Error sum of squares")+
  
  theme_cowplot()
```

### Measures of variability

#### SST - Total sum of squares

```{r SST total sum of squares 2}
SST=sum((fertilizer$YIELD-grand_mean)^2)
SST
```

SST is the **total sum of squares.** It is the sum of squares of the deviations of the data around the grand mean. This is a measure of the total variability of the data set.

#### SSE - Error sum of squares

```{r}
SSE<-fertilizer |>
  group_by(FERTIL) |>
  mutate(fmean=mean(YIELD)) |>
  mutate(se=(YIELD-fmean)^2) |>
  summarise(sse=sum(se),.groups = 'drop') |>
  summarise(SSE=sum(sse),.groups = 'drop') |>
  pull(SSE)
```

SSE is the **error sum of squares**. It is the sum of the squares of the deviations of the data around the three separate group means. This is a measure of the variation between plots that have been given the same fertilizer.

#### SSF - Fertilizer sum of squares

```{r SSF}

SSF<-fertilizer |>
  group_by(FERTIL) |>
  mutate(fmean=mean(YIELD)) |>
  mutate(se=(fmean-grand_mean)^2) |>
  summarise(sse=sum(se),.groups = 'drop') |>
  summarise(SSF=sum(sse),.groups = 'drop') |>
  pull(SSF)
```

SSF is the **fertilizer sum of squares**. This is the sum of the squares of the deviations of the group means from the grand mean. This is a measure of the variation between plots given different fertilizers.

```{r}
g4<-ggplot()+
  
  geom_hline(yintercept=grand_mean,linetype='dashed')+
  
  geom_segment(aes(x=min(f1$plot),y=f1$fmean[1],xend=max(f1$plot),yend=f1$fmean[1]))+
  geom_segment(aes(x = mean(f1$plot), y = grand_mean, xend = mean(f1$plot), yend = f1$fmean[1]),linetype='dotted')+
  
  geom_segment(aes(x=min(f2$plot),y=f2$fmean[1],xend=max(f2$plot),yend=f2$fmean[1]))+
  geom_segment(aes(x = mean(f2$plot), y = grand_mean, xend = mean(f2$plot), yend = f2$fmean[1]),linetype='dotted')+
  
  geom_segment(aes(x=min(f3$plot),y=f3$fmean[1],xend=max(f3$plot),yend=f3$fmean[1]))+
  geom_segment(aes(x = mean(f3$plot), y = grand_mean, xend = mean(f3$plot), yend = f3$fmean[1]),linetype='dotted')+
  
  scale_y_continuous(limits=c(2.5,7.5))+
  labs(x='Plot number',y='Yield per plot (tonnes)',title="SSF:Fertilizer sum of squares")+
  theme_cowplot()
```

```{r}
grid.arrange(g3,g4,nrow=1)
```

When the three group means are fitted, there is an obvious reduction in variability around the three means compared to that around the grand mean, but it is not obvious if the fertilizers have had an effect on yield.

At what point do we decide if the amount of variation explained by fitting the means is significant? By this, we mean, "When is the variability between the group means greater than we would expect by chance alone?

First, we note that SSF and SSE partition between them the total variability in the data:

### SST = SSF + SSE

```{r pythagoras}
SST
SSF
SSE
SSF+SSE
```

So the total variability has been divided into two components. That due to differences between plots given different treatments and that due to differences between plots given the same treatment. Variability must be due to one or other of these components. Separating the total SS into its component SS is known as partitioning the sums of squares.

A comparison of SSF and SSE is going to indicate whether fitting the three fertilizer means accounts for a significant amount of variability.

However, to make a proper comparison, we really need to compare the variability per degree of freedom ie the variance.

### Partitioning the degrees of freedom

Every sum of squares (SS) has been calculated using a number of independent pieces of information. In each, case, we call this number the number of degrees of freedom for the SS.

For SST this number is one less than the number of data points *n*. This is because when we calculate the deviations of each data point around a grand mean there are only *n-1* of them that are independent, since by definition the sum of these deviations is zero, and so when *n-1* of them have been calculated, the final one is pre-determined.

Similarly, when we calculate SSF, which measures the deviation of the group means from the grand mean, we have $k$-1 degrees of freedom, (where in the present example $k$, the number of treatments, is equal to three) since the deviations must sum to zero, so when $k$-1 of them have been calculated, the last one is pre-determined.

Finally, SSE, which measure deviation around the group means will have *n*-*k* degrees of freedom, since the sum of each of the deviations around one of the group means must sum to zero, and so when all but one of them have been calculated, the final one is pre-determined. There are $k$ group means, so the total degrees of freedom for SSE is *n*-*k*.

The degrees of freedom are additive: $$
df(\text{SST}) = df(\text{SSE}) + df(\text{SSF})
$$ Check:

```{=latex}
\begin{align*}
df(\text{SST}) &= n-1\\
df(\text{SSE}) &= k-1\\
df(\text{SSF}) &= n-k\\
\therefore df(\text{SSE}) + df(\text{SSF}) &= k-1 + n-k\\
&=n-1\\
&=df(\text{SST}) 

\end{align*}
```

### Mean Squares

Now we can calculate the *variances* which are a measure of the amount of variability per degree of freedom.

In this context, we call them *mean squares*. To find each one we divided each of the sums of squares (SS) by their corresponding degrees of freedom.

Fertiliser Mean Square (FMS) = SSF / *k* - 1. This is the variation per *df* between plots given different fertilisers.

Error Mean Square (EMS) = SSE / *n* - *k*. This is the variation per *df* between plots given the same fertiliser.

Total Mean Square (TMS) = SST / *n* - 1. This is the total variance per *df* of the dataset.

Unlike the SS, the MS are not additive. That is, FMS + EMS $\neq$ TMS.

### *F*-ratios

If none of the fertilizers influenced yield, we would expect as much variation between the plots treated with the same fertilizer as between the plots treated with different fertilizers.

We can express this in terms of the mean squares: the mean square for fertilizer would be the same as the mean square for error:

$$
\frac{\text{FMS}}{\text{EMS}}=1
$$ We call this ratio the *F*-ratio. It is the end result of ANOVA. *F*-ratios can never be negative since they are the ratio of two mean square values, both of which must be non-negative, but there is no limit to how large they can be.

Even if the fertilizers were identical, the *F*-ratio is unlikely to be exactly 1 - it could by chance take a whole range of values. The **F-distribution** represents the range and likelihood of all possible *F* ratios under the null hypothesis. ie when the fertilizers were identical.

The shape of the F distribution depends on the degrees of freedom of FMS and EMS, and we normally specify it by giving the values of each. Below we show F distributions for 2 and 27 degrees of freedom (ie 3 plots, so *k* = 3, so the degrees of freedom of FMS = *k*-1 =2, and 10 plants per plot, so *n* = 3 x 10 =30, and hence the degrees of freedom of EMS = *n*-*k* = 30 - 3 = 27), and for 10 and 27 degrees of freedom.

```{r}
dfs<-c(rep("df = 2, 27",601),rep("df = 10, 27",601))
xs<-rep(seq(0,6,0.01),2)
ys<-c(df(xs[1:601],2,27),df(xs[602:1202],10,27))
fdata<-tibble(x=xs,y=ys,dfs=dfs)

fdata |>
  mutate(dfs = fct_relevel(dfs,"df = 2, 27","df = 10, 27")) |>
  ggplot(aes(x=xs,y=ys)) +
  xlim(0,6) +
  ylim(0,1) +
  labs(x="F-ratio",
       y="Probability density",
       caption="The F-distributions for (left) 2 and 27 degrees of freedom and (right) 10 and 27 degrees of freedom") +
  geom_line(colour="darkblue") +
  facet_wrap(~dfs) +
  theme_cowplot()
```

Note that, whatever the degrees of freedom, *F*-distributions are examples of so-called probability density functions. The area beneath them between any two values of *F*-ratio is equal to the probability of getting an *F*-ratio in that range. Hence the total area under the curves is equal to 1, since the *F*-ratio must take some value between zero and infinity, and the area under the tail to the right of any given *F*-ratio is the probability of getting an *F*-ratio bigger than that value.

Hence, the probability under the null hypothesis of getting an *F*-ratio as large or larger than the value we actually got is the area to the right of this *F*-ratio under the appropriate F distribution. We often call this probability the *p*-value. p for probability. p-values are the the probability of getting data as extreme (same *F*-ratio,) or more extreme (bigger *F*-ratio) as the data you got you got if the null hypothesis were true.

If the fertilizers were very different, then the FMS would be much greater than the EMS and the *F*-ratio would be greater than one. However it can be quite large even when there are no treatment differences. So how do we decide when the size of the *F*-ratio is due to treatment rather than to chance?

Traditionally, we decide that it sufficiently larger than one to be due to treatment differences if it would be this large or larger under the null hypothesis only 5% or less of the time. If we had inside knowledge that the null hypothesis was in fact true then we would still get an *F*-ratio that large or larger 5% of the time.

Our *p*-value ie the probability that the *F*-ratio would have been as large as it is or larger, under the null hypothesis, represents the strength of evidence against the null hypothesis. The smaller it is, the stronger the evidence, and only when it is less than 0.05 do we regard the evidence as strong enough to reject the null.

```{r}

Fratio<-function(p,k,st_dev){
  n<-p*k # k plots, p replicates per plot
  plots<-tibble(plot=c(rep("a",p),rep("b",p),rep("c",p)),response=rnorm(n,mean=0,sd=st_dev))
  lm.mod<-lm(response~plot,data=plots)
  tidy(anova(lm.mod))$statistic[1]
}
```

```{r,echo=FALSE}

trials<-1000
replicates<-10 # pots per combination of factor levels
k<-5 # number of factor levels
st_dev<-1
binwidth<-0.2


#Fs<-tibble(Fval=replicate(trials,Fratio(replicates,k,st_dev)))

myfun<-function(x,replicates,k,N,binwidth){
  df2=replicates*k-k
  df1=k-1
  N*binwidth*df(x,df1,df2)
}

# Fs |> 
#   filter(Fval<8) |>
#   ggplot(aes(x=Fval)) +
#   geom_histogram(binwidth=binwidth,fill=fill_colour1) +
#   geom_function(fun = myfun, args = list(replicates,k,N=trials,binwidth=binwidth),colour=line_colour) +
#   labs(x="F",
#        y=) +
#   theme_cowplot()
```

## ANOVA example 1

We will carry out the ANOVA analysis of the fertilizer data discussed on the previous tab.

Our question is whether yield depends on fertilizer.

What is our null hypothesis?

Start a new notebook with these two code chunks to begin with:

````         
```{r global-options, include=FALSE}`r ''`
knitr::opts_chunk$set(fig.width=12, fig.height=8, warning=FALSE, message=FALSE,echo=FALSE)
```

```{r load packages, message=FALSE,warning=FALSE,echo=FALSE}`r ''`
library(tidyverse)
library(here)
library(cowplot)
library(gridExtra)
library(ggfortify)
```
````

Load the `fertilizer.csv` data into an object call \`fertilizer

Is it tidy data? If not, tidy it.

Convert the FERTIL column to a factor, using this code:

````         
```{r make_factor}`r ''`
fertilizer <- fertilizer |>
  mutate(FERTIL=as.factor(FERTIL))
```
````

Make a box plot of yield vs fertilizer, like the one on the previous tab.

Now use the `lm()` function to create the anova model (There are several ways to do this in R - this is just one)

```{r,echo=TRUE}
fertil.model<-lm(YIELD~FERTIL,data=fertilizer)
```

Now inspect the model:

```{r,echo=TRUE}
anova(fertil.model)
```

So we find that we can reject the null hypothesis. There is thus evidence that fertilizer does affect yield (ANOVA, df = 2,27, F=5.7, p\< 0.01)

What this test has not done so far is show us where the differences lie. An ANOVA is a holistic test that tells you whether or not there is evidence for a difference between at least one pair of groups being compared. To identify which gruopd, if any, are differeny, we need to do so-called post-hoc tests.

## ANOVA example 2

An experiment was performed to compare four melon varieties. It was designed so that each variety was grown in six plots, but two plots growing variety 3 were accidentally destroyed.

We wish to find out if there is evidence for a difference in yield between the varieties.

### Null hypothesis

What is the null hypothesis of this study?

The data are in the `melons.csv` dataset.

Write code chunks to

### Load data and inspect the data

```{r}
filepath<-here("data","melons.csv")
melons<-read_csv(filepath)
glimpse(melons)
```

### Prepare the data

Ensure that the VARIETY column is a factor

```{r}
melons <- melons |>
  mutate(VARIETY=as.factor(VARIETY))
```

### Plot the data

Create a scatter plot of the yield.

```{r}
melons |>
  ggplot(aes(x=VARIETY,y=YIELDM)) +
  geom_point() +
  scale_y_continuous(limits=c(0,45),breaks=seq(0,45,5))+
  labs(x = "Melon variety", y="Yield") +
  theme_cowplot()
```

### Summarise the data

Create a summary table that shows the mean yield for each variety.

```{r}
melons |>
  group_by(VARIETY) |>
  summarise(
    N=n(),
    Mean=round(mean(YIELDM),2)
  )
```

### 1-way ANOVA

#### Create the model

```{r,echo=TRUE}
melons.model<-lm(YIELDM~VARIETY,data=melons)
```

#### Check the validity of the model

```{r,echo=TRUE}
autoplot(melons.model,smooth.colour=NA) + theme_cowplot()
```

DO the data look as though they meet the criteria for an ANOVA? - the variance of the residuals is roughly constant across all groups and the qq-plot is fairly straight. We could confirm with a normality test if we like. For example, we could use a Shapiro-Wilk test.

What is the null hypothesis of this test?

```{r,echo=TRUE}
shapiro.test(melons.model$residuals)
```

What do we conclude from this test?

#### Inspect the model

```{r,echo=TRUE}
anova(melons.model)
```

What conclusions would you draw from the output of this model and the table of mean yields for each variety?

### Confidence intervals of the means

Let us find the 95% confidence intervals for each mean

These we calculate as

$$
\text{Mean}\pm t_{\text{crit}}\text{SE}_{\text{mean}}
$$ For a 95% confidence interval and 18 degrees of freedom, $t_{\text{crit}}$ is `r round(qt(0.975,18),2)`, so we find that the intervals are:

```{r}
t_crit<-qt(0.975,18)
melons |>
  group_by(VARIETY) |>
  summarise(
    Mean=round(mean(YIELDM),2),
    LB=round(Mean-t_crit*sqrt(15.6)/sqrt(n()),2),
    UB=round(Mean+t_crit*sqrt(15.6)/sqrt(n()),2)
    )
```

## ANOVA example 2 solution

An experiment was performed to compare four melon varieties. It was designed so that each variety was grown in six plots, but two plots growing variety 3 were accidentally destroyed.

We wish to find out if there is evidence for a difference in yield between the varieties.

### Null hypothesis

What is the null hypothesis of this study?

The data are in the `melons.csv` dataset.

Write code chunks to

### Load data and inspect the data

```{r,echo=TRUE}
filepath<-here("data","melons.csv")
melons<-read_csv(filepath)
glimpse(melons)
```

### Prepare the data

Ensure that the VARIETY column is a factor

```{r,echo=TRUE}
melons <- melons |>
  mutate(VARIETY=as.factor(VARIETY))
```

### Plot the data

Create a scatter plot of the yield.

```{r,echo=TRUE}
melons |>
  ggplot(aes(x=VARIETY,y=YIELDM)) +
  geom_point() +
  scale_y_continuous(limits=c(0,45),breaks=seq(0,45,5))+
  labs(x = "Melon variety", y="Yield") +
  theme_cowplot()
```

### Summarise the data

Create a summary table that shows the mean yield for each variety.

```{r,echo=TRUE}
melons |>
  group_by(VARIETY) |>
  summarise(
    N=n(),
    Mean=round(mean(YIELDM),2)
  )
```

### 1-way ANOVA

#### Create the model

```{r,echo=TRUE}
melons.model<-lm(YIELDM~VARIETY,data=melons)
```

#### Check the validity of the model

```{r,echo=TRUE}
autoplot(melons.model,smooth.colour=NA) + theme_cowplot()
```

The data look as though they meet the criteria for an ANOVA - the variance of the residuals is roughly constant across all groups and the qq-plot is fairly straight. We could confirm with a normality test if we like. For example, we could use a Shapiro-Wilk test.

The null hypothesis of this test is that the residuals are drawn from a population that is normally distributed

```{r,echo=TRUE}
shapiro.test(melons.model$residuals)
```

Since *p*\>0.05 we conclude that there is no reason to reject the null hypothesis that the residuals are normally disributed.

#### Inspect the model

```{r,echo=TRUE}
anova(melons.model)
```

```{r,echo=TRUE}
summary(melons.model)
```

What conclusions would you draw from the output of this model and the table of mean yields for each variety?

We see that the null hypothesis is rejected with a *p*-value of less than 0.001. We conclude that there are significant differences in the mean yield of melons across the varieties. We estimate that variety 2 has the highest mean yield and varieties 1 and 3 have the lowest mean yields.

The unexplained variance ie the error *s* for each group is 15.6 with 18 degrees of freedom. So the standard error for each group is $\frac{s}{\sqrt{n}}$ where s=$\sqrt{15.6}$ = `r round(sqrt(15.6),2)` divided by the number of elements in each group, giving us standard errors of 1.97 for variety 3, and 1.61 for the other varieties.

### Confidence intervals of the means

Let us find the 95% confidence intervals for each mean

These we calculate as

$$
\text{Mean}\pm t_{\text{crit}}\text{SE}_{\text{mean}}
$$ For a 95% confidence interval and 18 degrees of freedom, $t_{\text{crit}}$ is `r round(qt(0.975,18),2)`, so we find that the intervals are:

```{r}
t_crit<-qt(0.975,18)
melons |>
  group_by(VARIETY) |>
  summarise(
    Mean=round(mean(YIELDM),2),
    LB=round(Mean-t_crit*sqrt(15.6)/sqrt(n()),2),
    UB=round(Mean+t_crit*sqrt(15.6)/sqrt(n()),2)
    )
```
