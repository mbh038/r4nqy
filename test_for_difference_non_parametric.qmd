# Tests for difference - non parametric


A common scenario is that we have two sets of measurements, and we want to see if there is evidence that they are drawn from different populations. For some data types we can use a *t*-test to do this, but for others we cannot.

A *t*-test requires in particular that the two sets of data are normally distributed around their respective means. With *ordinal* data this makes no sense. The mean is undefined as a concept for such data.

To see this , reflect that for a collection $X$ of numerical data, say, 5, 3, 3, 4, and 5 we would calculate the mean as:

$$
\bar{X} = \frac{5+3+3+4+5}{5} = \frac{20}{5}=4
$$

But trying doing the same to five responses of a Likert scale survey. Say the responses you had to five Likert items (individual questions) were "strongly disagree", "strongly agree", "mildly disagree", "strongly disagree" and "don't care either way".  If you tried to calculate a 'mean' response you would be attempting to add up all these responses and to divide the 'sum' by five, like this:

$$\text{mean response}=\frac{\text{stongly disagree}+\text{strongly agree}+\text{mildly disagree}+\text{stongly disagree}+\text{don't care either way}}{5} = ?
$$
This sum makes no sense, I hope you will agree. It makes no sense, not because we are using words to describe our responses, but because, as these are ordinal data, we do not know the size of the gaps between the different points on the scale. Is the difference in agreement between the lowest two, "strongly disagree" and "midly disagree" the same as the gap between the highest two, "mildly agree" and "strongly agree"? We don't know, mainly because 'agreement' is not something that can be measured easily using something like a weighing machine. And if we don't know, then we shouldn't really be adding these responses up or dividing them by anything.

Nevertheless, ordinal data are very common, since they are typically what is generated by survey data, where for example repondents may answer a series of questions ('items'), each with typically five possible responses, but maybe more or fewer, these responses being ordinal in the sense that there is a definite order to them. They might encompass responses like those above, say, or something similar like "very unhappy" to "very happy". They are also common in clinical and veterinary practice where ordinal pain scores are widely used - patients being asked (if they are human) or assessed as to their level of pain on a scale of 1-10, for example. Note that even if the pain value is recorded as a number it is actually a label, that could just as well have been recorded as one of a series of letters, A, B, C etc or emojis, or any symbol you like. You can't take the average of a set of faces!

Thus, formally,  we need another kind of test for a difference. Broadly, we need to use some form of *non-parametric* test where we do not assume that the data has any form of distribution, and where, often, we do not use the actual values of the measurements in our dataset but instead use only their *ranks*. The smallest value would be given rank 1, the next rank 2 and so on. 

There are many non-parametric tests out there. Here we will look at only one - the **Wilcoxon Rank Sum Test**, often referred to as a **Mann-Whitney U** test for a difference. We can use this for the scenario we have painted above, where we have two sets of data and we wish to know if these provide evidence that the populations from which the samples have been drawn are in fact different.

## Example

This example uses actual data gathered by an Honours Project student.

The student wished to assess peoples' sense of wellbeing using two different sets of questions designed to assess this. The scales chosen were the Warwick–Edinburgh Mental Well-being Scale (WEMWBS) and the New Ecological Paradigm (NEP) Scale. The student wished in particular to determine whether this sense of well-being was affected by whether a person often and actively frequented the coast and made it and the sea a substantive part of their life in one way or another. ie to find out whether there was evidence to support the notion that it could be good for your mental wellbeing to be by the sea and to make it part of your life. 

Each scale used consists of 15 questions or 'Likert items', each of which is answered on a 5 point ordinal scale, where a score of 1 indicates lowest wellbeing and a score of 5 indicates highest wellbeing. Thus each respondent could score anything from 15 to 75.

The student got responses from 374 people, 86 of whom were not "marine" users, while the other 288 say that they *were* marine users. The total scores from each respondent were recorded for each type of survey and stored in the file `wellness.csv` which you should find on the module Moodle site / Teams page. Please put this file in the `data` folder of your R project.

## Script

The first few chunks of this script carry out the same old-same old that we see in script after script: load packages, load data, summarise data , plot data.

You can run this script by running each chunk in sequence, which you do by clicking the green arrow in the top-right corner of each chunk.

Try also to 'Knit' the script by clicking on the Knit button at the top of the script pane.

If you want to create your own script to use with your own data then you can copy and paste into it any code chunks that would be of use and adapt them as necessary.

### Load packages
```{r}
library(tidyverse)
library(here)
```

### Load data

Our data set is in a `.csv` file which we have placed in the data folder within our project folder.

Note that this data set has been stored in 'tidy' form: each variable appears in only column, and each observation appears in only one row.

```{r}
filepath<-here("data","wellness.csv")
wellness<-read_csv(filepath)
glimpse(wellness)
```

### Summarise the data

We'll calculate the median score (50th percentile) and the 25th and 75th percentile scores. For ordinal data, these summary statistics are well defined, whereas means and standard deviations are not.

```{r}
wellness |>
  group_by(scale,marine) |>
  summarise(median.score=median(total_score),iqr_25=quantile(total_score,0.25),iqr_75=quantile(total_score,0.75))
```

### Plot the data

Box plots are particularly suitable for ordinal data since they show the 25th and 75th percentiles of the data (the bottom and top of the box) plus the 50th percentile aka the median, which is the thick line across each box. All of these percentiles are well defined quantities for ordinal data.

```{r}
wellness |>
  ggplot(aes(x = scale,y = total_score,fill = marine)) +
  geom_boxplot() +
  labs(x = "Likert Scale",
       y = "Wellbeing Score") +
  theme_classic()
```

Looking at the plot, what do you think each scale suggests about whether proximity to the sea makes a difference to wellbeing?

### Wilcoxon-Mann-Whitney U test

First let's pull out the scores as measured by the WEMWBS scale and do a test for a difference between the scores of marine users and those of non-marine users. We can use the `filter()` function to do this.

```{r}
WEMWBS<-wellness |> filter(scale=="WEMWBS")
wilcox.test(total_score~marine,data=WEMWBS)
```

The null hypothesis of this test is that there is no evidence that the data are drawn from different populations. In this case, the *p*-value is very small, so we can confidently reject that null hypothesis and assert that there is evidence, according to the WEMWBS scale that marine use makes a difference to peoples' sense of wellbeing. 

Does it make it worse or better? - we can see from the summary table and from the box plot that higher scores are associated with those people who were exposed to a marine environment.

We might report this results as follows, first using a plain English statement of the main finding, and then reporting the type of test use, the value of the test statistci that it calculated and the p value. In this case, because the p value is so small, we would not report its exact value, but simle give an indication of how small it is:

We find evidence, according to the WEMWBS scale, that the wellbeing score is 4.5 or about 10% higher for people exposed to a marine environment (Mann-Whitney U, W = 9077.5, p < 0.001).


## Exercise

Adapt the code of the last chunk so that you can do the same test but for data as recorded by the nep scale

## When should I use this Wilcoxon-Mann-Whitney U test?

The test we have used here is an example of a *non-parametric* test. This means that it does not assume that the data follow a known mathematical distribution and, further, that it can be used with ordinal data.

We used the Mann-Whitney U test in particular because we were testing for a difference, and because the factor of interest - marine exposure - had just two levels - Yes or No. This test is only suitable when there are just two levels, so you can think of it as as a non-parametric alternative to a t-test. 

In another setting where we still had just one factor (eg zone of a rocky shore) but there were more than two levels (eg low, mid and high zones of the shore) and we decided that we wanted to do a non-parametric test for a difference, then we would probably use the Kruskal-Wallis test, which you can think of as the non-parametric alternative to a one-way ANOVA.

In this example we used the Mann-Whitney U test because the data were ordinal and thus not suitable to use with a parametric test (but see below!). Where we can, we usually try to use a parametric test as they are more powerful than their non-parametric equivalents, meaning, if there is a trend or a difference in the data, they are better able to detect it.  However those parametric tests (t-test, ANOVA, pearson correlation, PCA, GLM to name but a few) typically require not only that the data are numerical but also a host of other things, including that they follow a particular distribution, usually (but not always) the normal distribution, and this is often not the case with real biological data. Often, especially with count data, there are lots of zeros, or the data distribution is heavily skewed, usually to the right. In these cases, providing the data are independent of each other, we can usually still use a non-parametric test such as we have here. it might not be the most powerful test we can use (GLMs are typically way better if you can use them), but it will work.

## Hang on!

The eagle eyed among you may have spotted a massive flaw in the line of argument presented above. We said that ordinal data can't be added up, can't be used to calculate averages and so on. Thus we can't run parametric tests on them and have to look for alternatives, namely, non-parametric tests.

And yet, these non-parametric tests are usually run on the output of Likert *scales* such as we have considered here, where for each person we have a number of Likert *Items* (ie individual questions) that together constitute the *scale*, that each generate a score 1-5, then we *add up the scores* to get a total score. But that means we are adding up ordinal data!!!

It turns out that you actually get much the same results with Likert scale data if you analyse them using supposedly inappropriate parametric tests such as a 2-sample *t*-test as you do if you use a non-parametric test such as the one we considered here, the Mann-Whitney test.

A study by De Winter and Dodou (2010) shows this convincingly.

de Winter, J. F. C., &#38; Dodou, D. (2010). Five-Point Likert Items: t test versus Mann-Whitney-Wilcoxon (Addendum added October 2012). Practical Assessment, Research, and Evaluation,15, 1–16. https://doi.org/10.7275/bj1p-ts64


For an enlightening discussion of this paper, see [this blog by Jim Frost](https://statisticsbyjim.com/hypothesis-testing/analyze-likert-scale-data/)





