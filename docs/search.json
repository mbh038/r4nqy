[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "From questions to data to answers using R",
    "section": "",
    "text": "Introduction\nThis is a compilation of the methods for data analysis that you are likely to find useful in completing your studies at Newquay University Centre. It is by no means exhaustive, but should address most of your needs, most of the time.\nData analysis in the life sciences is only part of the wider process of forming and then answering as best we can well-formed questions about the way this or that aspect of the natural world works. If our question is good, and our design is good, then it is more likely than not that one of the standard analytical techniques described in this text will our needs. If not, then maybe none of them will suit and we will have to work harder to extract the answers we want from the data we have worked hard to gather.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-resource",
    "href": "index.html#how-to-use-this-resource",
    "title": "From questions to data to answers using R",
    "section": "How to use this resource",
    "text": "How to use this resource\nYou are not expected to read this text from beginning to end, although you can if you want to. Each section explains the topic at hand and also gives you a link to a skeleton R project that you can download, unzip, and use to follow along for yourself.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-to-make-a-book-like-this.",
    "href": "index.html#how-to-make-a-book-like-this.",
    "title": "From questions to data to answers using R",
    "section": "How to make a book like this.",
    "text": "How to make a book like this.\nFollow the very useful guide given by James Bartlett of Glasgow University.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "test_finder.html",
    "href": "test_finder.html",
    "title": "Test finder",
    "section": "",
    "text": "You can either use this text version of a test finder, taken from (Gilbert, McGregor, and Barnard 2017)\nor use this flowchart to see which test is appropriate for your study design and your data:\n\n\n\n\n\n%%| fig-width: 10\n%%| fig-align: \"centre\"\n%%| column: page-right\n\nflowchart TD\n  A{Difference or Trend Question?} --&gt; B[Trend]\n  B --&gt; B2{Are you testing for degree of associationor are you trying to make predictions?}\n  B2 --&gt; B33[Association:Test for correlation coefficient Pearson or  Spearman's Rank]\n  B2 --&gt; B44[Predictions: Simple linear regression]\n  A --&gt; C[Difference]\n  C --&gt; C22{Do you have replicates?}\n  C22 --&gt; C22Y[Yes]\n  C22 --&gt; C22N[No]\n  C22N --&gt; C23{Do you have count data?}\n  C23 --&gt; C23Y[Yes]\n  C23 --&gt; C23N[N0]\n  C23Y --&gt; C24(Chi square test: Goodness of fit or Test of independence)\n  C23N --&gt; C25[These data cannot be analysed]\n  C22Y --&gt; D{How many factors?}\n  D --&gt; F[Two or more]\n  F --&gt; F2{Independent samples?}\n  F2 --&gt; F2Y[Yes]\n  F2 --&gt; F2N[No]\n  F2Y --&gt; F22Y(n-way ANOVA)\n  F2N --&gt; F22N(n-way repeated measures ANOVA)\n  D --&gt; E[One]\n  E --&gt; G{How many levels?}\n  G --&gt; H[Two]\n  G --&gt; I[More than two]\n  I --&gt; J{Independent samples?}\n  J --&gt; K[No]\n  J --&gt; L[Yes]\n  K --&gt; S(Repeated measures one-way ANOVA or Friedman Test)\n  L --&gt; T(One way ANOVA or Kruskal Wallis one-way test)\n  H --&gt; M{Independent samples?}\n  M --&gt; N[No]\n  M --&gt; O[Yes]\n  N --&gt; P(paired t-test or Signed rank test)\n  O --&gt; Q(two sample t-test or Matt Whitney U test)\n  \n\n\n\n\n\n\nNow go to whichever chapter of this book covers the test you need.\n\n\n\n\nGilbert, Francis, Peter McGregor, and Chris Barnard. 2017. Asking Questions in Biology; a Guide to Hypothesis Testing, Experimental Design and Presentation in Practical Work and Research Projects. 5th ed. Benjamin Cummings.",
    "crumbs": [
      "Preliminaries",
      "Test finder"
    ]
  },
  {
    "objectID": "install_packages.html",
    "href": "install_packages.html",
    "title": "1  Install packages",
    "section": "",
    "text": "When you first download and install it, R is an already very powerful and versatile programming language aimed mainly at those who want to do statistical analysis of data. However a major strength of it and other open source software is that anyone can contribute to their development. In the case of R this has led to it being augmented by the creation of so-called packages, of which there are now thousands, from which you can pick and choose and add to your instance of R whenever you want. Each packag comprises additional functions and/or data sets Often these are aimed at particular specialisms that not everyone wants\nThis code chunk will install on your machine the list of packages that you see, unless you already have them installed. To run it you can either include the chunk in a notebook an then run that, or, most easily, you can copy the whole chunk, paste it into the console pane (the one at the bottom left of the screen, where the command proment &gt; is, and press Enter. That’ll do it.)\nAs time goes on this list may be updated (or you may update it!), so it will be worth coming back to this script and running it again, from time to time.\n\nlist.of.packages &lt;- c(\"tidyverse\",\n                      \"lubridate\",\n                      \"here\",\n                      \"readxl\",\n                      \"googlesheets4\",\n                      \"janitor\",\n                      \"cowplot\",\n                      \"ggfortify\",\n                      \"ggridges\",\n                      \"GGally\",\n                      \"sf\",\n                      \"terra\",\n                      \"vegan\",\n                      \"lme4\",\n                      \"Rcpp\",\n                      \"rgbif\",\n                      \"leaflet\",\n                      \"tmap\",\n                      \"broom\",\n                      \"remotes\",\n                      \"palmerpenguins\",\n                      \"rmarkdown\",\n                      \"knitr\",\n                      \"data.table\",\n                      \"lidR\",\n                      \"rstatix\",\n                      \"quarto\",\n                      \"kableExtra\",\n                      \"openintro\",\n                      \"RColorBrewer\")\n\nnew.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\nif(length(new.packages)) install.packages(new.packages)",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Install packages</span>"
    ]
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "3  A recommended analysis workflow",
    "section": "",
    "text": "3.1 Are you working within your Project?\nClick here to download a basic RStudio project folder already set up with data and notebook subfolders. Put the folder anywhere on your computer, just not inside another RStudio project folder.\nThis is intended as a rough outline of the sequence of steps one commonly goes through when working on notebooks:\nDetails will differ from notebook to notebook, but this sequence of steps is very common.\nBefore we even think of the notebook, we need to make sure that we are working within our Project. If we are not doing this, bad things will happen. If you are, the Project name will be at the top right of the RStudio window. If you are not, save the notebook you are working on, and go to File/Open Project and open your Project. If you haven’t even got a ‘Project’ or don’t know what that means then just make sure that everything you need for whatever you are working on is in one folder and then turn that folder into a Project. (So a ‘Project’ is just a regular folder that has been given superpowers.) You do that by going to File/New Project/Existing Directory. Then you navigate to your folder and click on Create Project. RStudio will then restart and you will see the name of your newly anointed Project folder at the top-right of the RStudio window. You know that a folder is a ‘Project’ because it will have a .Rproj file inside it.\nIf all this sounds complicated, don’t worry. It really isn’t. Just get someone to show you how to do it and you will be fine.\nNow, to the notebook itself:",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#statement-of-the-questions-to-be-investigated",
    "href": "workflow.html#statement-of-the-questions-to-be-investigated",
    "title": "3  A recommended analysis workflow",
    "section": "3.2 Statement of the question(s) to be investigated",
    "text": "3.2 Statement of the question(s) to be investigated\nWithout thinking this through, you won’t know what your notebook is for…\nWhat is the analysis that will follow for? What question are you trying to answer? What hypotheses are you trying to test?\nSuppose we were trying to test the hypothesis that there is no difference between the body masss of the setosa, versicolor and virginica species of penguin. All we have to go on are the body masss of the plants we happened to measure. From these measurements we want to make a statement about these three species in general.",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#open-a-notebook",
    "href": "workflow.html#open-a-notebook",
    "title": "3  A recommended analysis workflow",
    "section": "3.3 Open a notebook",
    "text": "3.3 Open a notebook\nIn RStudio, go to File/New File/ Quarto Document. Delete everything below the yaml section at the top. This strangely named section is the bit between the two lines with three dashes in. For the most part, we will not need to worry about this section. We just should not delete it entirely. What is useful to do is to amend the title to something sensible, and to add author: \"your name\" and date: \"the date in any old format\" lines. I also add a couple of final lines that suppress warnings and messages that might clutter up my printed output, so that your yaml will look something like this:\n---\ntitle: \"A typical workflow\"\nauthor: \"Who wrote this?\"\ndate: \"Today's date\"\noutput:\n  html_document:\n    df_print: paged\nexecute:\n  message: false\n  warning: false\n---\nDelete everything beneath this yaml section. The big empty space that then leaves you with is where you write your code. Remember that in quarto documents, the code goes in ‘chunks’ that are started and finished with by lines with three backticks. Any other text goes between the chunks and you can format this text using the simple rule of Markdown, available in the RStudio Help menu at Help/Markdown Quick Reference. Thus your notebook will end up looking something like this:\n---\ntitle: \"A typical workflow\"\nauthor: \"Who wrote this?\"\ndate: \"Today's date\"\noutput:\n  html_document:\n    df_print: paged\nexecute:\n  message: false\n  warning: false\n---\n\n## First header\n\nAny text we want to add. Note that a code chunk starts with delimiters, like this:\n\n```{r}\n\n```\n\n\nThose are back-ticks, not apostrophes!\n\n```{r}\nlibrary(tidyverse) # some actual R code\n```\n\n## Second header\n\nAny text we want to add to explain what this next chunk does\n\n```{r}\nlibrary(tidyverse) # some actual R code\n```",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#load-packages",
    "href": "workflow.html#load-packages",
    "title": "3  A recommended analysis workflow",
    "section": "3.4 Load Packages",
    "text": "3.4 Load Packages\nYou will nearly always want the first five packages, and often you will appreciate the sixth, janitor. Others, such as vegan will be useful from time to time, depending on what you are doing. If any of these lines throw an error, it is most likely because of a typo or because you have not yet installed that package. Do so in the console pane (not in this notebook!) using the function install.packages(\"name of package\"). Then run this whole chunk again.\n\n```{r}\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggfortify)\nlibrary(readxl)\nlibrary(cowplot)\nlibrary(janitor)\nlibrary(vegan)\n```",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#load-data",
    "href": "workflow.html#load-data",
    "title": "3  A recommended analysis workflow",
    "section": "3.5 Load data",
    "text": "3.5 Load data\nThere are several ways to do this, so details will differ depending on what file type your data is saved in and where it is stored.\nHere are some examples. In each case code here presumes that the data is stored in a subfolder called ‘data’ within the Project folder, and we use the function here() from the here package. In my experience this dramatically simplifies the business of finding your data, wherever your notebook is. It makes it easier for you to share your notebook with others and be confident that what worked for you will work for them. It does require that you are working within your project.\n\n3.5.1 If from a csv file\nIf you have your data in a data subfolder within your project, this chunk will work. Just substitute the name of your data file\n\n```{r}\nfilepath&lt;-here(\"data\",\"ozone.csv\")\nozone&lt;-read_csv(filepath)\nglimpse(ozone)\n```\n\nRows: 20\nColumns: 3\n$ garden.id       &lt;chr&gt; \"G1\", \"G2\", \"G3\", \"G4\", \"G5\", \"G6\", \"G7\", \"G8\", \"G9\", …\n$ garden.location &lt;chr&gt; \"West\", \"West\", \"West\", \"West\", \"West\", \"West\", \"West\"…\n$ ozone           &lt;dbl&gt; 61.7, 64.0, 72.4, 56.8, 52.4, 44.8, 70.4, 67.6, 68.8, …\n\n\n\n\n3.5.2 If from an Excel file\nYou will need to use read_excel() from the readxl package, and you have to specify the name of the worksheet that holds the data you want. You can, if you want, specify the exact range that is occupied by the data. However I suggest you avoid doing this unless it turns out that you need to do so. If your data is a nice, neat, rectangular block of rows and columns, you should find that you don’t need to specify the range.\n\n```{r}\nfilepath&lt;-here(\"data\",\"difference_data.xlsx\")\npenguins &lt;- read_excel(path = filepath, sheet = \"penguins\") |&gt;\n  clean_names()\nglimpse(penguins)\n```\n\nRows: 342\nColumns: 8\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2…\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, 193, 190, 181, 195, 193, 190, 186, 18…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3475, 4250…\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"femal…\n$ year              &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n3.5.3 If from a URL\nYou can load data into R directly from a URL if you are given one.\nHere, we load data from a file stored in a ‘repo’ on my github account:\n\n```{r}\n# penguin&lt;-read_csv(\"https://raw.githubusercontent.com/mbh038/r4nqy/refs/heads/main/data/penguin.csv\")\n#   clean_names()\n# glimpse(penguin)\n```",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#clean-manipulate-the-data",
    "href": "workflow.html#clean-manipulate-the-data",
    "title": "3  A recommended analysis workflow",
    "section": "3.6 Clean / Manipulate the data",
    "text": "3.6 Clean / Manipulate the data\nOften we need to do some sort of data ‘wrangling’ to get the data into the form we want. For example we may wish to tidy it (this has a particular meaning when applied to data sets), to remove rows with missing values, to filter out rows from sites or time periods that we don’t want to include in our analysis, to create new columns and so on.\nFor example, let’s create a new data frame for just the adelie penguins:\n\n```{r}\nadelie &lt;- penguins |&gt;\n  filter(species == \"Adelie\") # filter picks out rows according to criteria being satisfied in some column\nglimpse(adelie)\n```\n\nRows: 151\nColumns: 8\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2…\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, 193, 190, 181, 195, 193, 190, 186, 18…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3475, 4250…\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"femal…\n$ year              &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nor maybe we just want the columns that contain numeric data and not the one containing the species identifiers, islands, and sex which are text:\n\n```{r}\npenguin_numeric &lt;- penguins |&gt;\n  dplyr::select(bill_length_mm:body_mass_g) # select() retains or leaves out particular columns. Here, we leave out the species column.\nglimpse(penguin_numeric)\n```\n\nRows: 342\nColumns: 4\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2…\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, 193, 190, 181, 195, 193, 190, 186, 18…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3475, 4250…",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#summarise-the-data",
    "href": "workflow.html#summarise-the-data",
    "title": "3  A recommended analysis workflow",
    "section": "3.7 Summarise the data",
    "text": "3.7 Summarise the data\nHow big is the difference between the mean of this group over here and that group over there, and how big is that difference compared to the precision with which we know those means? We nearly always want to do this as a first way to get insight into whether we will or will not reject our hypothesis. For example, let’s find the mean body masss of the three species, the standard errors of those means and save the results to a data frame called petal_summary\n\n```{r}\npenguin_summary&lt;-penguins |&gt;\n  group_by(species) |&gt;\n  summarise(n = n(),\n            mean_body_mass = mean(body_mass_g),\n            se_body_mass = sd(body_mass_g/sqrt(n())))\npenguin_summary\n```\n\n# A tibble: 3 × 4\n  species       n mean_body_mass se_body_mass\n  &lt;chr&gt;     &lt;int&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n1 Adelie      151          3701.         37.3\n2 Chinstrap    68          3733.         46.6\n3 Gentoo      123          5076.         45.5\n\n\nWe can look at this table and already get an idea as to whether the body masses are the same or are different for the three species.",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#plot-the-data",
    "href": "workflow.html#plot-the-data",
    "title": "3  A recommended analysis workflow",
    "section": "3.8 Plot the data",
    "text": "3.8 Plot the data\nThe next step is usually to plot the data in some way. We would typically use the ggplot2 package from tidyverse to do this.\n\n3.8.1 Bar plot with error bars\nWe could plot a bar plot with error bars, working from the summary data frame that we created:\n\n```{r}\npenguin_summary |&gt;\n  ggplot(aes(x = species, y = mean_body_mass)) +\n  geom_col(fill=\"#a6bddb\") + # this is the geom that gives us a bar plot when we have already done the calculations\n  geom_errorbar(aes(ymin = mean_body_mass - se_body_mass, ymax = mean_body_mass + se_body_mass), width = 0.15) +\n  labs( x = \"species\",\n        y = \"Mean body mass (g)\",\n        caption = \"Error bars are ± one standard error of the mean\") + # important to say what these error bars denote\n  theme_cowplot()\n```\n\n\n\n\n\n\n\n\nNote that we have given the bars a fill colour - we got this color from this site due to the cartographer Cynthia Brewer, who is behind the various incarnations of the Brewer package in R, which is great for getting colours that work well. We have used the same colour for each species since the x-axis labels already tell us which bar relates to which species. To use a different colour for each bar would imply there is some extra information encoded by colour. Since there is not, it serves no purpose to have different colours, and potentially confuses the reader. Remember always that a plot is intended to convey a message. Anything that detracts from that message should be avoided, however pretty you think it is.\nA couple of points could be made about this type of plot:\nFirst, what about those error bars? Three types of error bar are in common usage and there are arguments in favour and against the use of each of them:\n\nthe standard deviation tells us about the spread of values in a sample, and is an estimate of the spread of values in a population;\n\nthe standard error of the mean, as used here, is an estimate of the precision with which the sample means estimates the respective population means for each of the species.\nthe confidence interval, typically a 95% confidence interval, gives us the region within which we are (say) 95% confident that the true species mean body mass might plausibly lie.\n\nWhich type of error bar is best to use depends on what story you want to tell. Here, because we are interested in whether there is evidence of a difference in the mean body mass of different species, we have gone for the standard eror of the mean.\nRegardless of which error bar you use and why, you should always tell the reader which one you have gone for, as we have in the caption to the figure.\nA second point about this bar plot is that it doesn’t tell us very much, and indeed nothing that we didn’t already know. It only conveys the mean and standard error values for each species, which is information we already have, arguably more compactly and in more easily readable form, in the table we created. Further, it potentially obscures information that might come from knowing the distribution of the data.\nHere are three other plot types that do show the distribution of body masss for each species and thus add extra information to what we already know from the summary table\n\n\n3.8.2 Box plot\n\n```{r}\npenguins |&gt;\n  ggplot(aes(x=species, y=body_mass_g)) + # what we want to plot\n  geom_boxplot(fill=\"#a6bddb\",notch=FALSE) + # what kind of plot we want\n  geom_jitter(width=0.1, colour = \"#f03b20\",alpha=0.5) +\n  labs (x = \"species\",\n        y = \"Body mass (g)\") +\n  theme_cowplot() # choose a theme to give the plot a 'look' that we like\n```\n\n\n\n\n\n\n\n\nHere, we have added the points themselves on top of the box plot. When there are not too many data points, this can be useful. The ‘jitter’ adds some horizontal or vertical jitter, or both, so that the points do not lie on top of each other. In this case we see that the variability of body masss is not the same for each species and that the data are roughly symmetrically distributed around the median values in each case. This information is useful in helping us determine which statistical test might be appropriate for these data.\n\n\n3.8.3 Violin plot\nA useful alternative to the box plot, especially when the data set is large, is the violin plot:\n\n```{r}\npenguins |&gt;\n  ggplot(aes(x = species, y = body_mass_g)) + # what we want to plot\n  geom_violin(fill=\"#a6bddb\",notch=TRUE) + # what kind of plot we want\n  #geom_jitter(width=0.1, colour = \"#f03b20\",alpha=0.5) +\n  labs (x = \"species\",\n        y = \"Body mass (g)\") +\n  theme_cowplot() # choose a theme to give the plot a 'look' that we like\n```\n\n\n\n\n\n\n\n\nThe widths of the blobs (I am probably supposed to call them ‘violins’!) show us the distribution of the data - where they are widest is where the data are concentrated, while the height of the blobs shows us the range of variation of the data. The positions of the blobs tells us the mean body masss of the different species and gives us an idea of the differences between them.\n\n\n3.8.4 Ridge plot\nA bit like a violin plot. This needs the package ggridges to be installed.\n\nlibrary(ggridges)\n#| echo: fenced\n\npenguins |&gt;\n  ggplot(aes(x = body_mass_g, y = species)) + # what we want to plot\n  geom_density_ridges(fill=\"#a6bddb\") + # what kind of plot we want\n  #geom_jitter(width=0.1, colour = \"#f03b20\",alpha=0.5) +\n  labs (x = \"Body mass (g)\",\n        y = \"Species\") +\n  theme_cowplot() # choose a theme to give the plot a 'look' that we like\n\n\n\n\n\n\n\n\nHaving seen the summary and one of these plots of the data, would you be inclined to reject, or fail to reject, a null hypothesis that said that there was no difference between the body masss of the three species?",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#statistical-analysis",
    "href": "workflow.html#statistical-analysis",
    "title": "3  A recommended analysis workflow",
    "section": "3.9 Statistical analysis",
    "text": "3.9 Statistical analysis\nOnly now do we move on to the statistical analysis to try to answer our intial question(s). But by now, after the summary and plot(s), we may already have a pretty good idea what that answer will turn out to be.\nThe exact form of the analysis could take many forms. In a typical ecology project you might carry out several types of analysis, each one complementing the other. Here, an appropriate analysis might be to use the linear model in the form of a one-way ANOVA, since we have one factor (species) with three levels (Adelie, Chinstrap and Gentoo) and an output variable that is numeric and likely to be normally distributed. We can use the lm() function for this.\n\n3.9.1 Create the model object\n\n```{r}\npenguin.model &lt;-lm (body_mass_g ~ species, data = penguins)\n```\n\n\n\n3.9.2 Check the validity of the model\nWe won’t go into this here, but an important step is to check that the data satisfy the often finicky requirements of whatever statistical test we have decided to use. The autoplot() function form the ggfortify package is great for doing this graphically.\n\n```{r}\nautoplot( penguin.model) + theme_cowplot()\n```\n\n\n\n\n\n\n\n\nHere we note in particular that although the spread of data within each level is not roughly the same (top left figure)), the QQ plot is pretty straight (top-right figure). This means that the data are approximately normally distributed around their respective means. Taken together, this means that these data satisfy reasonably well the requirements of a linear model, so the output of that model should be reliable.\n\n\n3.9.3 The overall picture\nTypically, statistical tests are testing the likelihood of the data being as they are, or more ‘extreme’ than they are, if the null hypothesis were true. Thus, the null hypothesis is central to statistical testing.\nThe null hypothesis is typically that the ‘nothing going on’, ‘no difference’ or ’ no association’ scenario is true. In this case, it would be that there is no difference between the body masss of the the three species of penguin being considered here.\nTypically too, a test will in the end spit out a p-value which is the probability that we would have got the data we got, or more extreme data, if the null hypothesis were true. Being a probability, it will always be a value between 0 and 1, where 0 means impossible, and 1 means certain. The closer the p-value is to zero, the less likely it is we would have got our data if the null hypothesis were true. At some point, if the p-value is small enough, we will decide that the probability of getting the data we actually got if the null hypothesis were true is so small that we reject the null hypothesis. Typically, the threshold beyond which we do this is when p = 0.05, but we could choose other thresholds. (Sounds arbitrary - yes, it is, but the choice of 0.05 is a compromise value that makes the risk of making each of two types of error - rejecting the null when we should not, and failing to reject it when we should, both acceptably small. This is a big topic which we won’t explore further here.)\nIn the end, whatever other information we get from it, the outcome of a statisical test is typically that we either reject the null hypothesis or we fail to reject it. If we reject it then we are claiming to have detected evidence for an ‘effect’ and we go on to determine how big that effect is and whether it is scientifically interesting. If we fail to reject the null, that does not necessarily mean that there is no ‘effect’ (difference, trend, association etc). That might be the case, but it might also just mean that we didn’t find evidence for one from our data.\nIt is all a bit like in a law court where the ‘null hypothesis’ is that the defendant is innocent, and at the end of the proceedings this null is either rejected (Guilty!) because the evidence is such as to make it untenable to hold onto the null hypothesis, or not rejected, because the evidence is not strong enough to convict, in which case the defendant walks free - but is not declared innocent. Formally, the court has simply found insufficient evidence to convict. In the latter case, the court would have failed to reject the null hypothesis. Crucially, it would not have declared that the defendant was innocent. In the same way, in a scientific study, we either reject or fail to reject a null hypothesis. We never ever accept the null hypothesis as true.\nActually, many researchers are unhappy wih this so-called ‘frequentist’ narrative and have sought to use an alternative ‘Bayesian’ approach to testing hypotheses. In this approach we can accept hypotheses and we can bring in prior knowledge. This is an interesting topic, but a very big one so we will not pursue it further here.\nWith all that behind us, we are in a better place to understand what the output of the test is telling us.\nFor the 1-way ANOVA, as with other examples of the linear model, this output comes in two stages:\n\n\n3.9.4 Overall picture\nIs there evidence for a difference between at least two of the mean values?\nTo see if there is evidence for this, an ANOVA test calculate the ratio between the dfference betweeN the groups compared to the differences within the groups. it calls this ratio \\(F\\). The bigger \\(F\\) is, the more likely we are to reject the null hypothesis that there is no difference between he groups.\n\n```{r}\nanova(penguin.model)\n```\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n           Df   Sum Sq  Mean Sq F value Pr(&gt;F)    \nspecies     2 1.47e+08 73432107     344 &lt;2e-16 ***\nResiduals 339 7.24e+07   213698                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe F value is huge. The null hypothesis of this test, as with many tests, is that there is no difference between the body masses of the three populations from which these samples have been drawn. In that case, the F value would be one. The p-value is telling us how likely it is that we would get an F value as big or bigger than the one we got for our samples if the null hypothesis were true. Since the p-value is effectively zero here, we reject the null hypothesis: we have evidence from our data that there is a significant varation of body mass between species.\nThe degrees of freedom Df tells us the number of independent pieces of information that were used to calculate the result. Let’s not dwell on this here, but there are two that we have to report in this case: the number of levels minus one ie 3-1 = 2, and the number of individual data points minus the number of levels ie 342-1 = 339.\n\n\n3.9.5 Effect size\nNow that we have established that at least two species of penguin have differing body masses, we go on to investigate where the differences lie, and how big they are. This is important: effect sizes matter. It is one thing to establish that a difference is statistically significant (and typically even the tiniest difference can show up as significant in a study if the sample size is big enough), it is quite another to establish whether the difference is big enough to be scientifically interesting.\n\n```{r}\nsummary(penguin.model)\n```\n\n\nCall:\nlm(formula = body_mass_g ~ species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1126.0  -333.1   -33.1   316.9  1224.0 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        3700.7       37.6   98.37   &lt;2e-16 ***\nspeciesChinstrap     32.4       67.5    0.48     0.63    \nspeciesGentoo      1375.4       56.1   24.50   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 462 on 339 degrees of freedom\nMultiple R-squared:  0.67,  Adjusted R-squared:  0.668 \nF-statistic:  344 on 2 and 339 DF,  p-value: &lt;2e-16\n\n\nThe output here is typical of that from a 1-way ANOVA analysis in R. Each line refers to one of the three levels of the factor being investigated, which in this case is body mass. By default, those levels are arranged alphabetically, so in this case the order is Adelie, Chinstrap then Gentoo. The first row is always labelled (Intercept), so here that row is referring to Adelie. This level is used as the ‘control’ or reference level- the one with which the others are compared. If we are happy to have Adelie as that control then we can just carry on, but if we are not, then we have to tell R which level we want to play that role. We’ll go through how to do that later on.\nIn the Estimate column the value 3700.66 g in the first row refers to the actual mean body mass of the Adelie penguins in the sample. If we go back to the summary table we created earlier on, or look at one of the plots we created, we see that that is the case.\nFor all other rows, the value in the Estimate column is not referring to the absolute mean body mass but to the difference between the mean body mass for that species and the mean body mass of the control species. So we see that the mean body mass of the Chinstrap in our sample is 32.43 g greater than that of Adelie and so is equal to 3700.66 + 32.43 = 3733.09 g, while that of the Gentoo is 1375.35 g greater and so is equal to 5076.01 g. Check from the table of mean values we created and the plots that this is correct.\nHere though, we are not interested in absolute values so much as we are in differences, which is why that is what the summary table here gives us. Look again at the differences between the mean body masss for Chinstrap and Gentoo and that for Adelie and compare them with the standard errors of those differences, which are given in the second column of the table. These standard errors are much smaller than the differences, meaning that we can have confidence that the differences are statistically significant.\nThis is borne out by the p-value in the right hand column of the table. The null hypothesis of this table is that there is no difference in body mass between populations of the different species from which these samples have been drawn.\nLastly, the adjusted \\(R^2\\) tells us the proportion of variation of body mass that is accounted for by taking note of the species. Here, the value is 0.668, which tells us that about two thirds of the variability in body mass is aconted for by species.",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#report-in-plain-english",
    "href": "workflow.html#report-in-plain-english",
    "title": "3  A recommended analysis workflow",
    "section": "3.10 Report in plain English",
    "text": "3.10 Report in plain English\nYou would say something like\nWe find evidence that body masss are not the same acros thhree species of penguin, with Gentoo penguins having a signifiantly greater mass than Adelie and Chinstrap. (ANOVA, df = 2, p &lt; 0.001)",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html",
    "href": "ggplot_examples.html",
    "title": "4  Building plots using the package ggplot2",
    "section": "",
    "text": "4.1 Load packages\nIn this exercise we are going to produce and improve a variety of useful and widely used plots using the package ggplot2 which is part of the larger tidyverse package.\nYou will see that the code to do each plot is very similar, whatever the type of plot, and that plots can be built up from very basic forms to become really attractive, informative versions with very little additional effort.\nYou need to read the examples in this worksheet and then fill in the missing code or alter what is provided already in the empty code chunks of the accompanying template script. Instructions for getting that are given below.\nAs you complete each code chunk, try it out by pressing the green arrow at the top right of the chunk. Sometimes you might want to try out an individual line. You can do that by placing the cursor anywhere in the line and pressing Controll-Entr (windows) or Command-Enter (Mac)\nRemember that the template is a markdown document, so you can add extra text between the code chunks to explain to yourself what is going on. You can format this test, if you wish, according to the very basic markdown rules for doing this. See Help/Markdown Quick Reference. This formatting is only useful if you ‘knit’ the script, by pressing the knit button at the top of the script pane. Try this! I suggest you knit to html. This is how the worksheet you are working from was produced.\n# install.packages(\"name of package\") # run this line once, if you need to, for any of the packages that need to be installed\nlibrary(tidyverse)\nlibrary(here)\nlibrary(palmerpenguins)\nlibrary(devtools)",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#get-the-template-script",
    "href": "ggplot_examples.html#get-the-template-script",
    "title": "4  Building plots using the package ggplot2",
    "section": "4.2 Get the template script",
    "text": "4.2 Get the template script\nThis next chunk will download the template file that you need to fill in as you work through this worksheet, and put it in the scripts subfolder within your project folder. For it to work, you need to be ‘working in your Project’ - in which case the name of the project will appear in the top right of the main RStudio window, and if you have a subfolder within the project folder called ‘scripts’. If any of that is not true, it needs to be sorted now!\n\nfile_url &lt;- \"https://raw.githubusercontent.com/mbh038/r-workshop/gh-pages/scripts/ggplot_examples_template.Rmd\"\nfile_dest &lt;- here(\"scripts\",\"my_ggplot_examples.rmd\")\ndownload.file(file_url,file_dest)",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#load-the-palmer-penguin-data",
    "href": "ggplot_examples.html#load-the-palmer-penguin-data",
    "title": "4  Building plots using the package ggplot2",
    "section": "4.3 Load the Palmer penguin data",
    "text": "4.3 Load the Palmer penguin data\nFor this exercise we use the Palmer penguins data set which comes with the package palmerpenguins\nThe palmerpenguin package contains two built-in data sets. One is called penguins:\nHere we load the data into this R session (you will now see it in the Environment pane) and we inspect it using the function glimpse().\n\ndata(penguins)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nHow many rows are there and how many columns?\nFor more detailed meta-information on the data we just type the name of the data set with a question mark before it:\n\n?penguins\n\nHelp on topic 'penguins' was found in the following packages:\n\n  Package               Library\n  palmerpenguins        /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library\n  datasets              /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library\n\n\nUsing the first match ...",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#summary-stats-on-all-the-numeric-columns",
    "href": "ggplot_examples.html#summary-stats-on-all-the-numeric-columns",
    "title": "4  Building plots using the package ggplot2",
    "section": "4.4 Summary stats on all the numeric columns",
    "text": "4.4 Summary stats on all the numeric columns\nThis is in general useful to get, at least for the columns that contain numerical data, since it shows which columns contains NAs,which is R-speak for missing data. They are how R represents what would be empty cells in an Excel spreadsheet.\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nWe see that there are some rows with NAs in for a few of the columns. We need to be aware of this when doing calculations with the data, such as taking means.\nHere, we will remove those rows:",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#remove-the-rows-with-nas",
    "href": "ggplot_examples.html#remove-the-rows-with-nas",
    "title": "4  Building plots using the package ggplot2",
    "section": "4.5 Remove the rows with NAs",
    "text": "4.5 Remove the rows with NAs\n\npenguins &lt;- penguins |&gt;\n  drop_na()",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#how-many-observations-are-there-for-each-species",
    "href": "ggplot_examples.html#how-many-observations-are-there-for-each-species",
    "title": "4  Building plots using the package ggplot2",
    "section": "4.6 How many observations are there for each species?",
    "text": "4.6 How many observations are there for each species?\nNote the use of the pipe operator |&gt;, here and throughout. Think of it as meaning and then. It feeds the output of one line into the function of the next line, where it is used as that function’s first argument.\n\npenguins |&gt;\n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      146\n2 Chinstrap    68\n3 Gentoo      119",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#mean-value-for-each-numerical-variable-for-each-species",
    "href": "ggplot_examples.html#mean-value-for-each-numerical-variable-for-each-species",
    "title": "4  Building plots using the package ggplot2",
    "section": "4.7 Mean value for each numerical variable, for each species",
    "text": "4.7 Mean value for each numerical variable, for each species\nHere is an example of the use of the group_by() then summarise() combination, whereby data is first grouped, here by species, then summary statistics (of your choice) are calculated for each group.\nIn this example the data are grouped by species, then the mean value of all the columns that contain numerical data are calculated, not just an overall value for the whole column, but for each species\n\npenguins |&gt;\n  group_by(species) |&gt;\n  summarize(across(where(is.numeric), mean, na.rm = TRUE)) |&gt;\n  ungroup() # good practice to include this at the end.\n\n# A tibble: 3 × 6\n  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n  &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie              38.8          18.3              190.       3706. 2008.\n2 Chinstrap           48.8          18.4              196.       3733. 2008.\n3 Gentoo              47.6          15.0              217.       5092. 2008.\n\n\n\n4.7.1 Scatter plots\nIs flipper length correlated with body mass?\nWe could a do correlation test to find this out, but let us first plot the data. We will show here how an elegant plot can be built up, starting from a very basic one, so that you see what each line of code for the finished version actually does. In the chunks below, run each one in turn to see the effect of each successive change that you make.\nFirst we feed the penguin data to the function ggplot(), and use its aes() argument to tell it which variables are to be ‘mapped’ to which aesthetic (which means, roughly speaking, ‘visible’) features of the plot, such as the x-axis, the y-axis, point and line colours, fill colours, symbol types and size etc:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g))\n\n\n\n\n\n\n\n\nThis produces the first layer of the eventual finished plot, an empty plot, ready to display data. Before it can do this, ggplot() needs to be told how you want to do so - what type of plot do you want? For that, we add a geom.....() line, to specify the type of plot.\nThere are lots of geom types, but for a scatter plot we use geom_point():\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis gives us a recognisable scatter plot, but it is deficient in a number of ways. For starters, we know that there are three species of penguin. It would be better if each were plotted using symbols of a different colour, shape or size. We can do this by adding in an extra argument to the aesthetic in the first line. Here we include colour = species.\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm,y = body_mass_g, colour = species)) +\n  geom_point()\n\n\n\n\n\n\n\n\nCan you guess what you should have do if you wanted not the symbol colour, but its shape or size to depend on species? Clue: change one word!\nNow we add labels, titles and so on, using the line labs(...). Note how we can actually write the arguments of this over several lines on the page, for clarity.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\", # this changes the title of the legend.\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\",\n       caption = \"Alternative place to put the information in the subtitle\")\n\n\n\n\n\n\n\n\nIt can be useful to include some combination of titles, subtitles and captions if the figure is to be used as part of a presentation or poster, but if it is to go in a report, you would normally only include a caption, and let the word-processing software do it, and if just for exploratory analysis, not even that. I normally do include axis labels, however.\nNow we use a theme to alter the overall look of the figure. There are several built-in themes you can choose from, and others from packages that you can use. I usually use theme_cowplot() from the cowplot package. Try typing ?theme at the command prompt in the console window to see what is available. Here, we use the built-in theme_bw():\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nNow we reposition the legend. We don’t have to, but we might not like the default position of the legend. If not, we can move or even remove it using another theme() line. The position argument of this can be “none” if you want to remove it, top”, “bottom”, “left”, “right” or a numerical vector in relative coordinates, where c(0,0) means bottom left within the plot, and c(1,1) means top-right. This is what we use here. Play around with different values.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\") +\n  theme_bw() +\n  theme(legend.position = c(0.2,0.8)) # try \"top\", \"left\" etc\n\n\n\n\n\n\n\n\nNicer colours. If you don’t like the default colours offered by R, there are several other palettes available, for example the Brewer palettes, borrowed from the world of maps. See https://colorbrewer2.org ,and for a list of the available palettes, type &gt;?scale_colour_brewer into the console pane then look at the help that appears in the Help pane (bottom right), and scroll down to the palettes section. Note that we dont have to alter the colours. But doing so can make your plots not only look nicer, but serve some other purpose, such as to be colour-blind friendly, or have colours that are appropriate for the variables being plotted (eg red points for red things, blue points for blue things). For an assignment or dissertation report, it is a good idea to pick a palette that you like and that works, and stick with it, so that all your plots have the same general look. Here we choose the qualitative palette \"Set2\" and use it by by adding the line scale_colour_brewer(palette=\"Set2\"). Try a few other palettes.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\") +\n  scale_colour_brewer(palette=\"Set2\") + # try other palettes eg \"Set3\"\n  theme_bw() +\n  theme(legend.position = c(0.2,0.8)) # try \"top\", \"left\" etc\n\n\n\n\n\n\n\n\nIf we like, we can add best fit lines to each subset of the data, using geom_smooth(). To produce straight line fits, geom_smooth() needs to be told to use a linear model, using the method = \"lm\" argument. By default, you will get lines with a grey 95% confidence band around them. This can be useful, but if you don’t want it, add the argument se = FALSE, as we have done below. We have also altered the linewidth.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", linewidth=0.5,se=FALSE) + # try leaving out the se argument\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\") +\n   scale_colour_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = c(0.2,0.8)) # also try legend.position = \"top\", \"left\" etc\n\n\n\n\n\n\n\n\n\n\n4.7.2 Repeat for bill length and flipper length\nModify the code of the previous plot so that you now plot bill length vs flipper length. Adjust any labels and titles as necessary. This time, put the legend in the bottom right of the plot.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=bill_length_mm,colour=species)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", linewidth=0.5,se=FALSE) + # try leaving out the se argument\n  labs(x = \"Flipper length (mm)\",\n       y = \"Bill length (mm)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and bill length for Adelie, Chinstrap, and Gentoo Penguins\") +\n   scale_colour_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = c(0.9,0.2)) # play with the values to get it where you want it\n\n\n\n\n\n\n\n\nDo you see how straightforwrd it is to adapt the code that produces one plot to get the code you need for another, similar plot?\n\n\n4.7.3 Add yet more informtion to the plot\nLet us include the information of which island the penguins come from by making the shape of the plotted points be dependent on that:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=bill_length_mm,colour=species,shape=island)) +\n  geom_point() +\n  #geom_smooth(method=\"lm\", linewidth=0.5,se=FALSE) + # try leaving out the se argument\n  labs(x = \"Flipper length (mm)\",\n       y = \"Bill length (mm)\",\n       colour= \"Species\",\n       shape=\"Island\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and bill length for Adelie, Chinstrap, and Gentoo Penguins\") +\n   scale_colour_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"right\") # play with the position to get it where you want it. Try \"top\" etc.\n\n\n\n\n\n\n\n\n\n\n4.7.4 Distribution of penguin flipper lengths\nThe distribution of a data set is often a useful thing to know. Around which value are the data grouped, how widely spread are they and are the values symmetrically or asymmetrically distributed around the central value? A number of plot types can show this for us. Here we illustrate histograms, density plots, box plots, violin plots and ridge plots.\n\n4.7.4.1 Histogram\nFirst, let’s do a basic histogram. For this we use geom_histogram(). In the ggplot line, in the aes() argument, we need only specify the variable that maps to x, since the software will count how many observations lie within specific narrow ranges of the variable, called bins. Those bin counts will be the y variable of the histogram. To find the distribution of flipper length, we use flipper_length_mmm as the x variable. So we could try\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm)) +  # why is y not specified?\n  geom_histogram()\n\n\n\n\n\n\n\n\nBut this isn’t useful. The histograms for the three species overlap each other, so we need to give each one a different colour, and we need to reduce the opacity of the bars so that the histograms behind are not obscured by the ones in front, where they overlap. Further, we need to stop ggplot from stacking the different histogram bars on top of each other where those for different species are in the same bin. Annoyingly, that is what it does by default, which makes seeing the individual distributions clearly much more difficult.\nAnother thing with histograms, something that can make them a fiddle to use, is that their usefulness in revealing a distrivution is affect by how wide the bins are. By default, ggplot chooses the bin width such that you get 30 bins altogether. This may not be optimal. Here, let’s try specifying the bin width to 4 mm (but see what happens when you try other values, especially very large and very small values).\nThis we can achieve by:\n\nincuding fill = species in the aes() argument of ggplot.\nsepcifying position = identity as an argument of geom_histogram(), to stop the stacking.\nspecifying the opacity argument alpha to be a value less than 1. Here we try alpha = 0.4` - but try other values in the range 0 (transparent) - 1 (opaque), to reduce the opacity.\nspecifying binwidth = 4 - try other values\n\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm, fill = species)) +  # why is y not specified?\n  geom_histogram(position = \"identity\", alpha = 0.4, binwidth = 4)\n\n\n\n\n\n\n\n\nSo, a lot going on, but still only three lines of code!\nNow add good axis labels, an overall theme, and choose a colour scheme you like, and the legend position, just as you have done before:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,fill=species)) +\n  geom_histogram(position=\"identity\",alpha = 0.4, binwidth = 4) +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", # specifies the legend title. See what happens if you omit this line.\n       title=\"Penguin flipper lengths\") + # we wouldn't use this for a figure going in a report.\n  scale_fill_brewer(palette=\"Set2\") + \n  theme_bw() +\n  theme(legend.position = c(0.9,0.8)) # play with the position to get it where you want it\n\n\n\n\n\n\n\n\nIn the scatter plot and the histogram, we have used colour to distinguish the different species. We can do this because our data set is tidy: there is just one column that species the species, and the same for every other variable. That same feature of the data enables to use another way to represent the different species: facet_wrap(~species). This gives us three separate plots, side by side or one above the other. See it used here:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,fill=species)) +\n  geom_histogram(position=\"identity\",alpha = 0.4, binwidth = 4) +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", \n       title=\"Penguin flipper lengths\") + \n  facet_wrap(~species) + #try adding the argument ncol = 1.\n  scale_fill_brewer(palette=\"Set2\") + # try other palettes, eg \"Set1\".\n  theme_bw() +\n  theme(legend.position=\"none\") # we don't need a legend!\n\n\n\n\n\n\n\n\nJust a thought, but do the colours here serve any useful purpose? What extra information do they convey? If you ever think that a feature of a graph conveys no additional information, consider omitting it. Here is the figue before without colours, but going for white brs with grey outlines:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm)) +\n  geom_histogram(position=\"identity\",alpha = 0.4, binwidth = 4, fill=\"white\",colour=\"grey50\") +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", # specifies the legend title. See what happens if you omit this line.\n       title=\"Penguin flipper lengths\") + # we wouldn't use this for a figure going in a report.\n  facet_wrap(~species) + #try adding the argument ncol = 1.\n  theme_bw() +\n  theme(legend.position=\"none\") # we don't need a legend!\n\n\n\n\n\n\n\n\nArguably, this is a better plot than the previous one because it excludes the potentially confusing redundancy of using different colours each species, when we already know which species is the subject of each plot.\nIf you don’t like white as the fill colour, try another one, for exampe this one that I found on Cynthia Brewer’s very useful map colour site: https://colorbrewer2.org\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm)) +\n  geom_histogram(position=\"identity\",alpha = 0.4, binwidth = 4, fill=\"#a6bddb\",colour=\"grey50\") +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", # specifies the legend title. See what happens if you omit this line.\n       title=\"Penguin flipper lengths\") + # we wouldn't use this for a figure going in a report.\n  facet_grid(island~species) + #try adding the argument ncol = 1.\n  theme_bw() +\n  theme(legend.position=\"none\") # we don't need a legend!\n\n\n\n\n\n\n\n\nDifferent fill colours would be useful if the different penguin species had distinctive dominant colours, but that isn’t the case!\n\n\n4.7.4.2 Density plot\nAn alternative to a histogram, the density plot, gives us a smoothed version of the histogram. The vertical axis on these is not a count, but a measure of the concentration of the data.\nHere is one with overlapping density plots\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,fill=species)) +\n  geom_density(alpha=0.2) +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Density\",\n       fill= \"Species\",\n       title=\"Penguin flipper lengths\") +\n  scale_fill_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"right\") # play with the position to get it where you want it\n\n\n\n\n\n\n\n\nWe can also adapt this and do what was done for the histograms and do a set of three, one for each species, using facet_wrap():\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm)) +\n  geom_density(alpha = 0.2, fill=\"#a6bddb\",colour=\"grey50\") +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", # specifies the legend title. See what happens if you omit this line.\n       title=\"Penguin flipper lengths\") + # we wouldn't use this for a figure going in a report.\n  facet_wrap(~species) + #try adding the argument ncol = 1.\n  theme_bw() +\n  theme(legend.position=\"none\") # we don't need a legend!\n\n\n\n\n\n\n\n\nWhich is more useful in this case: the overlapping plots on one chart, or the separate charts done using facet_wrap()? Whatever you think here, the answer in other cases will sometimes be one, sometimes the other. Now you have the tools to enable you to try both and make the best choice.\n\n\n4.7.4.3 Box plots\nBox plots are a really useful way to summarize the distribution of numerical response data such as flipper_length_mm across different categorical variables, such as species. We use geom_boxplot() to produce them.\nLet’s do a basic box plot of flipper lengths for each penguin species:\n\npenguins |&gt;\n  ggplot(aes(x=species,y=flipper_length_mm,fill=species)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nNow let’s use what we have done before to add code lines that\n\ninclude suitable axis labels and a title\ngive the same ‘theme’ ie overall look as the previous graphs\nfill the boxes with the same colour for each species.\nremove the legend that you now have, because you don’t need it (Why?). Use theme(legend.position=\"none\") to do this.\n\n\npenguins |&gt;\n  ggplot(aes(x=species,y=flipper_length_mm,fill=species)) +\n  geom_boxplot() +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\",\n       title=\"Penguin flipper lengths\") +\n  scale_fill_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"none\") # no legend needed\n\n\n\n\n\n\n\n\n\n\n4.7.4.4 Violin Plot using geom_violin()\nThis is a variant on the box plot. Each ‘violin’ is a sideways density plot of the distribution of the data for each species, with its own mirror image to make it look a bit like a violin. The code for these is exactly as for box plots except we use geom_violin().\n\npenguins |&gt;\n  ggplot(aes(x=species,y=flipper_length_mm,fill=species)) +\n  geom_violin() \n\n\n\n\n\n\n\n\nNow we write code to improve this, just as you did the box plot. The final code is the same as for that apart from one line!\n\npenguins |&gt;\n  ggplot(aes(x=species,y=flipper_length_mm,fill=species)) +\n  geom_violin() +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\",\n       title=\"Penguin flipper lengths\") +\n  scale_fill_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"none\") # no legend needed\n\n\n\n\n\n\n\n\n\n\n4.7.4.5 Ridge plot\nThis is a variant on the density plot, that is most useful when you have lots of categorical variables. We have only three here, the three penguin species, but let’s try it anyway.\nFor this, we need the ggridges package. This is one of many packages that extend the power of ggplot, and so work in much the same way:\n\n# library\n#install.packages(\"ggridges\") # use this once, if you have to, then comment it out.\nlibrary(ggridges) \n \n# basic example\npenguins |&gt;\nggplot(aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Species\",\n       title=\"Penguin flipper lengths\") +\n  theme_ridges() + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nNow try producing graphs like the ones above, but for body mass rather than flipper length.\n\n\n\n4.7.5 Bar chart with error bar\nThere are different ways to produce this commonly used way to summarise data. For example we might use one to compare the mean flipper lengths of the different penguin species. For a bar chart of these to be of any use at all, it needs to include error bars that show standard deviations of the samples, standard errors of the means, or confidence intervals (Why?). Which you use depends on the story you are trying to tell.\nFirst, we will add error bars that are ± one standard deviation of the samples.\nI usually first create a summary of the data, in which we calculate the means and appropriate error for each species for whichever variable I am interested in, then feed this summary table to ggplot and use geom_col() to plot the bars, with geom_errorbar() on top of that to plot the error bars.\nLet’s do that first:\n\nflipper_summary &lt;- penguins |&gt;\n  drop_na() |&gt;\n  # these two lines produce a summary table\n  group_by(species) |&gt;\n  summarise(fl.mean = mean(flipper_length_mm), fl.sd = sd(flipper_length_mm))\nflipper_summary\n\n# A tibble: 3 × 3\n  species   fl.mean fl.sd\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie       190.  6.52\n2 Chinstrap    196.  7.13\n3 Gentoo       217.  6.59\n\n\n\nflipper_summary |&gt;\n  ggplot(aes(x = species, y = fl.mean)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = fl.mean-fl.sd, ymax = fl.mean + fl.sd), width = 0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  theme_bw() \n\n\n\n\n\n\n\n\n\n4.7.5.1 Standard deviation or standard error?\nThe error bars in the plot above are ± one standard deviation. A standard deviation gives us an idea of the spread of values within a sample or population. Remember that if a population is normally distributed about its mean, there is a roughly 95% probability that a random chosen individual will lie within two standard deviations of the mean.\nSo standard deviations of samples are useful. They are a useful way to summarise the variability of the sample, they tell us about the likely variability of the next sample, and they are our best estimate of the variability of the population from which the sample was drawn.\nSometimes, though, we want to know more than that. We might want to know how closely a sample mean is likely o be to the true mean of the population from which the sample was drawn, and perhaps to get an idea as to whether two or more populations are different, given the means of samples drawn from those populations. For this, we need not the standard deviation but the standard error. These are the error bars that are most commonly displayed on bar charts when you see them in papers.\nTo calculate standard deviation error bar lengths we use a formula \\(\\text{SE} = \\frac{\\text{SD}}{\\sqrt{n}}\\) where \\(n\\) is the number of observations, SD is the standard deviation of the sample and SE is the standard error of the means of the sample. We can use the summary functions sd() to calculate the standard deviation, and n() to calculate \\(n\\).\n\nflipper_summary2 &lt;- penguins |&gt;\n  drop_na() |&gt;\n  # these two lines produce a summary table\n  group_by(species) |&gt;\n  summarise(fl.mean = mean(flipper_length_mm), fl.se = sd(flipper_length_mm)/sqrt(n()))\nflipper_summary\n\n# A tibble: 3 × 3\n  species   fl.mean fl.sd\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie       190.  6.52\n2 Chinstrap    196.  7.13\n3 Gentoo       217.  6.59\n\n\n\nflipper_summary2 |&gt;\n  ggplot(aes(x = species, y = fl.mean)) +\n  geom_col() +\n  geom_errorbar(aes(ymin=fl.mean-fl.se, ymax = fl.mean + fl.se),width=0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  theme_bw() \n\n\n\n\n\n\n\n\nThese standard error bars are always smaller than the standard deviation error bars (by a factor equal to the square root of the sample size). Here they are so small as to be barely visible. They ar useful in bar charts like this one since they give us a rough and ready way of assessing whether the differences between the samples (the heights of the bars) are likely to indicate real differences between the populations. If the bar-height differences are much greater than the size of the standard error bars, then they probably indicate significant differences between the popultions. If not, then they probably don’t.\nNow let’s alter this code so that each bar has a different fill colour, and remove the legend that then appears, since it is unnecessary?\n\nflipper_summary |&gt;\n  # we add an argument to colour each bar according to species\n  ggplot(aes(x = species, y = fl.mean, fill = species)) +\n  geom_col() +\n  geom_errorbar(aes(ymin=fl.mean-fl.sd, ymax = fl.mean + fl.sd),width=0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  theme_bw() +\n  # include an arument to remove the legend\n  theme()\n\n\n\n\n\n\n\n\nNow let us replace this colour scheme with nicer ones (not just nice, but also colour-blind friendly, perhaps) offered by the Brewer palettes.\nTo do this we can add the line scale_fill_brewer(palette = \"Set2\"). Note: we use scale_colour_brewer() to alter the colours of points, like we did above, or the outline colour of bars, and use scale_fill_brewer() to alter the fill colour of bars. This is what we want to do here.\n\nflipper_summary |&gt;\n  ggplot(aes(x = species, y = fl.mean, fill = species)) +\n  geom_col() +\n  geom_errorbar(aes(ymin=fl.mean-fl.sd, ymax = fl.mean + fl.sd), width=0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  scale_fill_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nIf you don’t like the colours of the palette “Set2” you can try another one. To find out what palettes are available, remember, you can type ?scale_fill_brewer() into the console pane then look at the help that appears in the Help pane (bottom right), and scroll down to the Palettes section.\nIf you agree that having different fill colours for the bars is actually confusing and brings no information to the plot that we do not already know, you can modify the previous plot in the manner that you did for the separate histograms:\n\nflipper_summary |&gt;\n  ggplot(aes(x = species, y = fl.mean)) + \n  # add arguments here that give fill colour \"#a6bddb\" and outline colour \"grey50\".\n  geom_col(fill = \"#a6bddb\", colour = \"grey50\") +\n  geom_errorbar(aes(ymin=fl.mean-fl.sd, ymax = fl.mean + fl.sd), width=0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "titles_labels_and_annotations.html",
    "href": "titles_labels_and_annotations.html",
    "title": "5  Titles, labels and annotations",
    "section": "",
    "text": "5.1 Use bquote()\nOften in plots one needs to use mathematical expressions, suffixes/superscripts, greeek letters or other unusual formatting.\nHere we show one way of doing this, using bquote(). Over the years I have found this to be the simplest way.\nThe four rules\nExamples of all of these are shown in the labels, title and annotations included in the plot below:\ndf &lt;- tibble(x=1:10,y=1:10)\ncor &lt;- 0.456\ndf |&gt;\n  ggplot(aes(x=x,y=y)) +\n  geom_point() +\n  labs( x = bquote(\"An axis label with suffix and superscript:\" ~ x[i]^2), \n        y = bquote(\"An axis label with greek letters:\" ~ alpha ~ Beta ~ mu ~ m),\n        title = bquote(\"Hello\"~ r[xy] == .(cor) ~ \"and\" ~ CO[2] ~ \"and\" ~ B^2 ~ \"and\" ~ m^{-1})) +\n  annotate(geom=\"text\", x = 5, y = 7, label =  deparse(bquote(\"Hello\" ~ r[xy] == 0.678 ~ \"and\" ~ B^2)), parse = TRUE) +\n  annotate(geom=\"text\", x = 7, y = 9, label =  bquote(\"A big annotation\"), size = 12) +\n  annotate(geom=\"text\", x = 7, y = 2, label =  bquote(\"A red annotation\"), colour = \"red\") +\n  theme_classic()",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Titles, labels and annotations</span>"
    ]
  },
  {
    "objectID": "titles_labels_and_annotations.html#use-bquote",
    "href": "titles_labels_and_annotations.html#use-bquote",
    "title": "5  Titles, labels and annotations",
    "section": "",
    "text": "Strings – Require quotes wrapped w/ tilde separator (e.g., “my text” ~).\nMath Expressions – Unquoted & follow ?plotmath\nNumbers – Unquoted when part of math notation\nVariables – Use .() (pass in string or numeric)",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Titles, labels and annotations</span>"
    ]
  },
  {
    "objectID": "titles_labels_and_annotations.html#plotmath",
    "href": "titles_labels_and_annotations.html#plotmath",
    "title": "5  Titles, labels and annotations",
    "section": "5.2 ?plotmath",
    "text": "5.2 ?plotmath\nplotmath expressions can be used for mathematical annotation in text within plots in R when writing titles, subtitles, axis labels, legends and annotations. It works in both base R graphics and ggplot2.\nIn the console pane, type ?plotmath in the console pane to see the full list of options.\nSome key rules are:\n\nsubscripts: O[2] gives O2\nsuperscripts: m^2 gives m2, m^{-1} gives m-1\nLower case Greek: alpha, beta etc gives \\(\\alpha, \\beta\\) etc, so mu\nUpper case Greek: Delta, Gamma etc gives \\(\\Delta, \\Gamma\\) etc",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Titles, labels and annotations</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html",
    "href": "tests_for_difference_two_levels.html",
    "title": "6  Tests for difference: one factor, two levels",
    "section": "",
    "text": "6.1 Two sample t-test: the parametric case\nIn this chapter we consider tests for difference where there are two levels being compared.\nIn this exercise we find out how to run a two-sample t-test, to determine whether there is evidence to reject the hypothesis that two samples are drawn from the same population.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#two-sample-t-test-the-parametric-case",
    "href": "tests_for_difference_two_levels.html#two-sample-t-test-the-parametric-case",
    "title": "6  Tests for difference: one factor, two levels",
    "section": "",
    "text": "6.1.1 When to use the two-sample t-test\n\nIt can be used when we have two independent samples of numerical (not ordinal) response data, and our question is whether the data provide evidence that the samples are drawn from different populations. Even when this criterion is met and the data are numerical and independent, the normality criterion described below still needs to be met. That apart, two common examples where we have two sets of data but we should not use the two sample t-test are:\n\nIf the data in your two samples are not independent because you have measured the same individual replicate before and after some event or treatment, then you should probably be using a paired t-test instead. In this you don’t have two samples, each comprising a separate and independent set of replicates. Instead, you have multiple pairs of values, one pair per replicate. The replicates are still assumed to be independent of each other\nIf your response data are the answers to a Likert scale such as might be used in a survey then they are ordinal in nature and not numerical, and you should probably be using the non-parametric equivalent of the two sample t-test, which is variously known as the Wilcoxon Rank Sum test or as the Mann Whitney U test, or its paired sample version, if appropriate.\n\nIt can be used when the data set is small. But not so small that there are no replicates. You do need replicates.\nIt can still be used when the data set is large.\nIt assumes that the data are drawn from a normally distributed population. There are various ways to test if this is plausibly the case, and you should try at least one of them, but with small samples, just where the t-test is most useful, it can be difficult to tell. In the end we can also appeal to reason: is there good reason to suppose that the data would or would not be normally distributed?\nWhen comparing the means of two samples, both samples should in principle have the same variance, which is a measure of the spread of the data, so in principle you need to check that this is at least approximately the case, or have reason to suppose that it should be. However, in an actual t-test done using R, the Welch variant of the t-test is carried out by default. This works even when the variances of the two sets are different, so in practice it is possible to ignore this equal variance requirement.\nWe only use it when we are comparing two samples, one for each of the two levels of a single factor. When we have samples for more than two levels and we use the t-test to look for a difference between any two of them, it becomes increasingly likely, the more pairs of samples we compare, that we will decide that we have found a difference because we got a p-value that was less than some pre-determined threshold (which could be anything, but is most often chosen to be 0.05) even if in reality there is none. This is the problem of high false positive rates arising from multiple pairwise testing and is where ANOVA comes in. t-tests are only used to detect evidence for a difference between two groups, not more. ANOVAs (or their non-parametric equivalent) are used when we are looking for differences between more than two groups.\n\n\n\n6.1.2 Motivation and example\nIn our example we will consider the impact of pesticide use on the masses of shells of garden snails (Cornu aspersum), as measured in gardens around a city, ten from randomly selected gardens that have used a range of pesticides for at least two years and ten that are from randomly selected gardens that have not ever used pesticides. We leave aside here the issue of how those gardens were identified and how randomisation was ensured.\n\n\n6.1.3 Questions and hypotheses\nOur question is:\nIs there evidence for a difference between snail shell masses in the gardens where pesticides were used compared to those where they were not used?\nFrom which a suitable null hypothesis is:\nThere is no difference between shell masses in the gardens, whether or not pesticides were used.\nand a suitable alternate, two-sided hypothesis is:\nThere is a difference between shell masses.\n\n\n6.1.4 The data\nSuppose we had our data arranged in a spreadsheet in three columns, one giving the garden ID, G1 to G20, one telling us whether pesticides were used in the garden, yes or no, and one telling us the masses in grams of the snail shells from each garden. Afficioados of R will see that this is ‘tidy’ data. Each variable (ID, pesticide use, shell mass) occurs in only one column, rather than being spread across several. It turns out that this way of storing your data makes it much easier to analyse.\n\n# there should be a 'garden_snails.csv' file in your data folder\n\nfilepath&lt;-here(\"data\",\"garden_snails.csv\")\nsnails&lt;-read_csv(filepath)\n\n# if not, you should be able to get it from Mike's github repo\n\n# file_url &lt;- \"https://raw.githubusercontent.com/mbh038/r-workshop/refs/heads/gh-pages/data/garden_snails.csv\"\n# snails&lt;-read_csv(file_url)\nhead(snails,20)\n\n# A tibble: 20 × 3\n   garden.ID pesticide shell_mass_g\n   &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt;\n 1 S1        Yes               1.37\n 2 S2        Yes               1.15\n 3 S3        Yes               0.73\n 4 S4        Yes               0.65\n 5 S5        Yes               1.03\n 6 S6        Yes               1.8 \n 7 S7        Yes               1.21\n 8 S8        Yes               1.41\n 9 S9        Yes               1.27\n10 S10       Yes               1.08\n11 S11       No                2   \n12 S12       No                3.99\n13 S13       No                1.99\n14 S14       No                1.75\n15 S15       No                2.81\n16 S16       No                2.15\n17 S17       No                1.87\n18 S18       No                3.46\n19 S19       No                2.8 \n20 S20       No                2.89\n\n\n\nIs this a tidy data set?\nIs the data in the pesticide column categorical?\nIf so, how many levels does it have and what are they?",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#the-process",
    "href": "tests_for_difference_two_levels.html#the-process",
    "title": "6  Tests for difference: one factor, two levels",
    "section": "6.2 The Process",
    "text": "6.2 The Process\n\n6.2.1 Step One: Summarise the data\nWith numerical data spread across more than one level of a categorical variable, we often want summary information such as mean values and standard errors of the mean for each level.\nHere we will calculate the number of replicates, the mean and the standard error of the mean for both levels of pesticide ie Yes and No:\n\nsnail.summary&lt;- snails |&gt;\ngroup_by(pesticide) |&gt;\nsummarise(n = n(),\n          mean.mass = mean(shell_mass_g),\n          se.mass = sd(shell_mass_g)/sqrt(n()))\nsnail.summary\n\n# A tibble: 2 × 4\n  pesticide     n mean.mass se.mass\n  &lt;chr&gt;     &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 No           10      2.57   0.236\n2 Yes          10      1.17   0.105\n\n\nFrom these data, does it look as though there is evidence for a difference between shell masses in the two types of garden? Clearly, the snails in the ten gardens that did not use pesticide had a higher mean shell mass than the ten from gardens that did use pesticide. But is this a fluke? How precisely do we think these sample means reflect the truth about the impact of the use of pesticides? That is what the standard error column tells us. You can think of the standard error as being an estimate of how far our sample means, drawn from just ten gardens of each type are likely to differ from the true shell masses for all gardens that did use pesticides and all gardens that did not.\nBottom line: the difference between the sample means is about ten times the size of the standard errors of each. It really does look as though snails shells in gardens where pesticides are not used are indeed heavier than in gardens where pesticides are used.\n\n\n6.2.2 Step Two: Plot the data\nRemember, before we do any statistical analysis, it is almost always a good idea to plot the data in some way. We can often get a very good idea as to the answer to our research question just from the plots we do.\nIn Figure 6.1, we will use ggplot() in R to plot a histogram of ozone levels, one for each side of the city. We will stack the histograms one above the other, all the better to help us spot any differences between east and west.\n\nsnails |&gt;\n  ggplot(aes(x=shell_mass_g)) +\n  geom_histogram(binwidth=0.2,fill=\"darkred\")+\n  facet_wrap(~pesticide,ncol=1) +\n  theme_classic()\n\n\n\n\n\n\n\nFigure 6.1: Stacked histograms\n\n\n\n\n\nInstead of histograms, we could have drawn box plots, as in:\n\nsnails |&gt;\n  ggplot(aes(x=pesticide,y=shell_mass_g))+\n  geom_boxplot()+\n  labs(x=\"Pestice use?\",\n       y=\"Shell mass (g)\") +\n  theme_classic()\n\n\n\n\n\n\n\nFigure 6.2: Side-byside box and whisker plots of the distribution of values in each snail sample. The lower and upper edges of each box show the 25th and 75th percentiles of each sample, and the thick black line between them shows the median value ie the 50th percentile\n\n\n\n\n\nor as a dot plot of the means with standard errors of the mean included as error bars, as in Figure 6.3\n\n# for this chart we will use the summary table that we created above.\n\nsnail.summary |&gt; \n  ggplot(aes(x=pesticide,y=mean.mass))+\n  geom_point(size=3) +\n  geom_errorbar(aes(ymin=mean.mass-se.mass,ymax=mean.mass+se.mass),width=0.1)+\n  ylim(0,4) + # try leaving this line out. What happens? Which is better?\n  labs(x=\"Pesticide use?\",\n       y=\"Shell mass (g)\") +\n  theme_classic()\n\n\n\n\n\n\n\nFigure 6.3: The data points show mean values, the error bars show plus or minus one standard error of the mean\n\n\n\n\n\n\nDo the data look as though they are inconsistent with the null hypothesis ?\nIn addition, do the data look as though each group is drawn from a normally distributed population? One of the types of graphs gives you no indication of that while the other two do. Which is the odd one out? Even when looking at the other two figures, when there are so few data it’s kind of hard to tell, no?\n\nLet’s now do some stats.\n\n\n6.2.3 Step Three: Check the validity of the data - are the data normally distributed?\nWe can go about establishing this in three ways: using an analytical test of normality, using a graphical method and by thinking about what kind of data we have. Let’s consider these in turn.\n\n6.2.3.1 Normality test - analytical method\nThere are several analytical tests one can run on a set of data to determine if it is plausible that it has been drawn from a normally distributed population. One is the Shapiro-Wilk test.\nFor more information on the Shapiro-Wilk test in R, type ?shapiro.test into the console window. For kicks, try it out on the examples that appear in the help window (which is the bottom right pane, Help tab). One example is testing a sample of data that explicitly is drawn from a normal distribution, the other tests a sample of data that definitely is not. What p-value do you get in each case? How closely do the histograms of each sample resemble a normal distribution?\n\n#first we create a data frame containing the two example data sets\nexample1&lt;-rnorm(500, mean = 5, sd = 3) # first example from the help pane\nexample2&lt;-runif(500, min = 2, max = 4) # second example from the help pane\n\ndf&lt;-tibble(data=c(example1,example2), distribution=c(rep(\"example 1: normal\",500),rep(\"example 2: not at all normal\",500)))\n\n# then we plot a histogram of each data set\nggplot(df,aes(x=data)) +\n  geom_histogram(bins=20,fill=\"cornflowerblue\") +\n  facet_wrap(~distribution) +\n  theme_classic()\n\n\n\n\n\n\n\n# and finally we run a Shapiro-Wilk normality test on each data set\nshapiro.test(example1) # 100 samples drawn from a normally distributed population\n\n\n    Shapiro-Wilk normality test\n\ndata:  example1\nW = 0.99842, p-value = 0.9353\n\nshapiro.test(example2) # 100 samples drawn from a uniformly (ie NOT normally) distributed population\n\n\n    Shapiro-Wilk normality test\n\ndata:  example2\nW = 0.94587, p-value = 1.519e-12\n\n\nFor the examples above, we see that Shapiro-Wilk test gave a high p-value for the data that we knew were drawn from a normal distribution, and a very low p-value for the data that we knew were not.\nThe Shapiro-Wilk test tests your data against the null hypothesis that it is drawn from a normally distributed population. It gives a p-value which, as always, is the probably of you having data as far from normality, or further, as yours are if the null hypothesis were true. If the p-value is less than 0.05 then we reject the null hypothesis and cannot suppose our data is drawn froma normally distributed population. In that case we would have to ditch the t-test for a difference, and choose another difference test in its place that could cope with data that was not normally distributed. For a two-sample t-test such as we are hoping to use here, the so-called non-parametric alternative that we could use instead is the Wilcoxon Rank Sum test, often called the Mann-Whitney U test.\nWhy don’t we do that in the first place, I hear you ask? Why bother with this finicky t-test that requires that we go through the faff of testing the data for normality before we can use it? The answer is that it is more powerful than other, so-called non-parametric tests that can cope with non-normal data. It is more likely than they are to spot a difference if there really is a difference. So if we can use it, that is what we would rather do.\nSo, onwards, let’s do the Shapiro-Wilk test on our data\nWe want to test each garden group for normality, so we group the data by location as before and and then summarise, this time asking for the p-value returned by the Shapiro-Wilk test of normality.\n\nsnails |&gt;\n  group_by(pesticide) |&gt;\n  summarise('Shapiro-Wilk p-value'=shapiro.test(shell_mass_g)$p.value)\n\n# A tibble: 2 × 2\n  pesticide `Shapiro-Wilk p-value`\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 No                         0.223\n2 Yes                        0.854\n\n\nFor both groups the p-value is more than 0.05, so at the 5% significance level we cannot reject the null hypothesis that the data are normally distributed, so we can go on and use the t-test. Yay!\n\n\n6.2.3.2 Graphical methods - the quantile-quantile or QQ plot.\nConfession: I don’t normally bother with numerical tests for normality such as Shapiro-Wilk. I usually use a graphical method instead.\nWe have already seen two ways of plotting the data that might help suggest whether it is plausible that the data are drawn from normally distributed populations. Histograms and box plots both indicate how data is distributed, and for normally distributed data both would be symmetrical. Well, they would be, more or less, if the data set was large enough but for small data sets it can be quite hard to tell from either type of plot whether the data are drawn from a normally distributed population.\nA better type of plot for making this judgement call is the quantile-quantile or ‘qq’ plot which basically compares the distribution of your data to that of a normal distribution. If your data are approximately normally distributed then a qq plot will give a straight(-ish) line. Even with small data sets, this is usually easy to spot.\n\nsnails |&gt;\n  ggplot(aes(sample=shell_mass_g)) +\n  stat_qq(colour=\"blue\") +\n  stat_qq_line() +\n  facet_wrap(~pesticide) +\n  theme_classic()\n\n\n\n\n\n\n\n\nNothing outrageously non-linear there, so that also suggests we can safely use the t-test.\nFor an overview of how normally distributed and non-normally distributed data looks when plotted in histograms, box plots and quantile-quantile plots, see this review\n\n\n6.2.3.3 The ‘thinking about the data’ normality test\nAs you might have guessed, this isn’t a test as such, but a suggestion that you think about what kind of data you have: is it likely to be normally distributed within its subgroups or not? If the data are numerical values of some physical quantity that is the result of many independent processes, and if the data are not bounded on either side (say by 0 and 100 as for exam scores) then it is quite likely that that they are. If they are count data, or ordinal data, then it is quite likely that they are not.\nThis way of thinking may be all you can do when data sets are very small and any of the more robust tests for normality presented here leave you not much the wiser.\n\n\n\n6.2.4 Do the actual two-sample t-test\nSo, it looks as though it is plausible that the data are drawn from normal distributions. That means we can go on to use a parametric test such as a two sample t-test and have confidence in its output.\nIf we were doing this in R we could use the t.test() function for this (other functions are available!). This needs to be given a formula and a data set as arguments. Look up t.test() in R’s help documentation, and see if you can get the t-test to tell you whether there is a significant difference between ozone levels in the east and in the west of the city.\n\nt.test(shell_mass_g~pesticide,data=snails)\n\n\n    Welch Two Sample t-test\n\ndata:  shell_mass_g by pesticide\nt = 5.4172, df = 12.442, p-value = 0.0001372\nalternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n95 percent confidence interval:\n 0.8397234 1.9622766\nsample estimates:\n mean in group No mean in group Yes \n            2.571             1.170 \n\n\n\n\n6.2.5 Interpret the output of the t-test.\nStudy the output of the t-test. Here are some questions to ask yourself.\n\nWhat kind of test was carried out?\n\nA Welch two sample t-test\n\nWhat data was used for the test?\n\nThe snail shell mass in g (the output variable) and pesticide use (the explanatory variable)\n\nWhat is the test statistic of the data?\n\nThis is t = 5.4172.\n\nHow many degrees of freedom were there? This number is the number of independent pices of information that were used to calculate the final result. It is usually one, two, or three or so less than the number of data points. Don’t overthink it at this stage, especially not the fact that here it is not an integer.\n\ndf = 12.442\n\nWhat is the p-value?\n\np = 0.0001372. You would most likely report this as p &lt; 0.001\n\nWhat does the p value mean?\n\nIt is the likelihood of seeing a difference between sample means as large or larger than the one we found if in fact pesticides made no difference to snail shell mass.\n\nWhat is the confidence interval for the difference between shell masses in gardens that use pesticides and in gardens that do not? Does it encompass zero? Remember that the confidence interval gives the range of values within which the true difference between mean shell masses might reasonably lie, given the data. If that range includes zero then the test is telling is that zero is a plausible value for the difference, and hence that we cannot reject the null hypothesis.\n\nThe 95% confidence interval has lower bound 0.8397 and upper bound 1.962.\n\nIs there sufficient evidence to reject the null hypothesis?\n\nYes. We see this in two ways. First the p value is much less than 0.05 and second, the 95% confidence interval does not encompass zero. In a way, the confidence interval is giving us more information than the p-value, since not only can we deduce whether there is evidence for a significant difference, we can also see how big that difference is and how precisely we know it.\n\nWhat does the word ‘Welch’ tell you - Google it or look it up in the help for t.test().\n\nIt tells us that a variant of the t-test is being used in which it does not matter if the two. samples have difference variances (spreads).",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#other-examples-where-a-two-sample-t-test-might-be-used.",
    "href": "tests_for_difference_two_levels.html#other-examples-where-a-two-sample-t-test-might-be-used.",
    "title": "6  Tests for difference: one factor, two levels",
    "section": "6.3 Other examples where a two sample t-test might be used.",
    "text": "6.3 Other examples where a two sample t-test might be used.\nRemember that t-tests in general are used when you have independent samples with multiple replicates drawn from populations corresponding to two levels of some factor (eg north coast, south coast; this beach, that beach; polluted place, clean place etc) and you have measured something numerical, like a length or a mass, temperature or concentration. You still have to do the tests for normality described above, but these are the basic criteria.\n\n6.3.1 Can you think of examples of where you might use a two-sample t-test?\nHere are a few suggestions:\n\nIs there a difference between the flight initiation distance of redstarts when confronted by dogs compared to when they are confronted by drones?\nIs the nitrate concentration of water in a river below a beaver dam different from the nitrate content above that dam?\n\nCan you think of another example?",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#what-if-i-cant-use-a-two-sample-t-test",
    "href": "tests_for_difference_two_levels.html#what-if-i-cant-use-a-two-sample-t-test",
    "title": "6  Tests for difference: one factor, two levels",
    "section": "6.4 What if I can’t use a two sample t-test?",
    "text": "6.4 What if I can’t use a two sample t-test?\n\nAssuming you have two independent samples, this might be because one or both sets failed the normality criterion, or your data are ordinal. In that case the likely alternative is the non-parametric equivalent of the t-test, variously known as the Wilcoxon Rank Sum test or the Mann Whitney U test.\nIf your data are in fact sets of paired values, for example because you measured some attribute of the same individuals before and after some treatment, or at two points in time, then you need to use the paired t-test.\nIf you only have one sample of replicates and want to compare its mean value to a threshold, then you use a one sample t-test. You might do this, for example, if you had collected sediment samples from an estuary, measured the concentration in those samples of some pollutant such as pathogens from sewage, or phosphates from farm runoff, and then wanted to see if the water was compliant with water quality thresholds as dictated by, say, the Water Framework Directive.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#the-non-parametric-case",
    "href": "tests_for_difference_two_levels.html#the-non-parametric-case",
    "title": "6  Tests for difference: one factor, two levels",
    "section": "6.5 The non-parametric case",
    "text": "6.5 The non-parametric case\nA common scenario is that we have two sets of measurements, and we want to see if there is evidence that they are drawn from different populations. For some data types we can use a t-test to do this, but for others we cannot.\nA t-test requires in particular that the two sets of data are normally distributed around their respective means. With ordinal data this makes no sense. The mean is undefined as a concept for such data.\nTo see this , reflect that for a collection \\(X\\) of numerical data, say, 5, 3, 3, 4, and 5 we would calculate the mean as:\n\\[\n\\bar{X} = \\frac{5+3+3+4+5}{5} = \\frac{20}{5}=4\n\\]\nBut trying doing the same to five responses of a Likert scale survey. Say the responses you had to five Likert items (individual questions) were “strongly disagree”, “strongly agree”, “mildly disagree”, “strongly disagree” and “don’t care either way”. If you tried to calculate a ‘mean’ response you would be attempting to add up all these responses and to divide the ‘sum’ by five, like this:\n\\[\\text{mean response}=\\frac{\\text{stongly disagree}+\\text{strongly agree}+\\text{mildly disagree}+\\text{stongly disagree}+\\text{don't care either way}}{5} = ?\n\\] This sum makes no sense, I hope you will agree. It makes no sense, not because we are using words to describe our responses, but because, as these are ordinal data, we do not know the size of the gaps between the different points on the scale. Is the difference in agreement between the lowest two, “strongly disagree” and “midly disagree” the same as the gap between the highest two, “mildly agree” and “strongly agree”? We don’t know, mainly because ‘agreement’ is not something that can be measured easily using something like a weighing machine. And if we don’t know, then we shouldn’t really be adding these responses up or dividing them by anything.\nNevertheless, ordinal data are very common, since they are typically what is generated by survey data, where for example repondents may answer a series of questions (‘items’), each with typically five possible responses, but maybe more or fewer, these responses being ordinal in the sense that there is a definite order to them. They might encompass responses like those above, say, or something similar like “very unhappy” to “very happy”. They are also common in clinical and veterinary practice where ordinal pain scores are widely used - patients being asked (if they are human) or assessed as to their level of pain on a scale of 1-10, for example. Note that even if the pain value is recorded as a number it is actually a label, that could just as well have been recorded as one of a series of letters, A, B, C etc or emojis, or any symbol you like. You can’t take the average of a set of faces!\nThus, formally, we need another kind of test for a difference. Broadly, we need to use some form of non-parametric test where we do not assume that the data has any form of distribution, and where, often, we do not use the actual values of the measurements in our dataset but instead use only their ranks. The smallest value would be given rank 1, the next rank 2 and so on.\nThere are many non-parametric tests out there. Here we will look at only one - the Wilcoxon Rank Sum Test, often referred to as a Mann-Whitney U test for a difference. We can use this for the scenario we have painted above, where we have two sets of data and we wish to know if these provide evidence that the populations from which the samples have been drawn are in fact different.\n\n6.5.1 Example\nThis example uses actual data gathered by a student at Newquay University Centre.\nThe student wished to assess peoples’ sense of wellbeing using two different sets of questions designed to assess this. The scales chosen were the Warwick–Edinburgh Mental Well-being Scale (WEMWBS) and the New Ecological Paradigm (NEP) Scale. The student wished in particular to determine whether this sense of well-being was affected by whether a person often and actively frequented the coast and made it and the sea a substantive part of their life in one way or another. ie to find out whether there was evidence to support the notion that it could be good for your mental wellbeing to be by the sea and to make it part of your life.\nEach scale used consists of 15 questions or ‘Likert items’, each of which is answered on a 5 point ordinal scale, where a score of 1 indicates lowest wellbeing and a score of 5 indicates highest wellbeing. Thus each respondent could score anything from 15 to 75.\nThe student got responses from 374 people, 86 of whom were not “marine” users, while the other 288 say that they were marine users. The total scores from each respondent were recorded for each type of survey and stored in the file wellness.csv which you should find on the module Moodle site / Teams page. Please put this file in the data folder of your R project.\n\n\n6.5.2 Script\nCode chunks for a script to carry out the analysis of this data are provided below. To use them you should create a new Quarto document using File/New File/Quarto document, from which you delete all the exemplar material below the yaml section at the top. The first few chunks of this script carry out the same old-same old that we see in script after script: load packages, load data, summarise data , plot data. Copy and paste any chunks you want to use into your own script then adapt them as necessary.\nYou can run your script by running each chunk in sequence, which you do by clicking the green arrow in the top-right corner of each chunk.\nTry also to ‘Render’ the script by clicking on the Render button at the top of the script pane.\n\n6.5.2.1 Load packages\n\nlibrary(tidyverse)\nlibrary(here)\n\n\n\n6.5.2.2 Load data\nOur data set is in a .csv file which we have placed in the data folder within our project folder.\nNote that this data set has been stored in ‘tidy’ form: each variable appears in only column, and each observation appears in only one row.\n\nfilepath&lt;-here(\"data\",\"wellness.csv\")\nwellness&lt;-read_csv(filepath)\nglimpse(wellness)\n\nRows: 748\nColumns: 4\n$ id          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ scale       &lt;chr&gt; \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\"…\n$ marine      &lt;chr&gt; \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\",…\n$ total_score &lt;dbl&gt; 48, 51, 37, 39, 38, 40, 54, 39, 54, 39, 51, 50, 49, 51, 54…\n\n\n\n\n6.5.2.3 Summarise the data\nWe’ll calculate the median score (50th percentile) and the 25th and 75th percentile scores. For ordinal data, these summary statistics are well defined, whereas means and standard deviations are not.\n\nwellness |&gt;\n  group_by(scale,marine) |&gt;\n  summarise(median.score=median(total_score),iqr_25=quantile(total_score,0.25),iqr_75=quantile(total_score,0.75))\n\n# A tibble: 4 × 5\n# Groups:   scale [2]\n  scale  marine median.score iqr_25 iqr_75\n  &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 WEMWBS No             42.5   36       49\n2 WEMWBS Yes            47     41       51\n3 nep    No             51     48.2     53\n4 nep    Yes            50     48       53\n\n\n\n\n6.5.2.4 Plot the data\nBox plots are particularly suitable for ordinal data since they show the 25th and 75th percentiles of the data (the bottom and top of the box) plus the 50th percentile aka the median, which is the thick line across each box. All of these percentiles are well defined quantities for ordinal data.\n\nwellness |&gt;\n  ggplot(aes(x = scale,y = total_score,fill = marine)) +\n  geom_boxplot() +\n  labs(x = \"Likert Scale\",\n       y = \"Wellbeing Score\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nLooking at the plot, what do you think each scale suggests about whether proximity to the sea makes a difference to wellbeing?\n\n\n6.5.2.5 Wilcoxon-Mann-Whitney U test\nFirst let’s pull out the scores as measured by the WEMWBS scale and do a test for a difference between the scores of marine users and those of non-marine users. We can use the filter() function to do this.\n\nWEMWBS&lt;-wellness |&gt; filter(scale==\"WEMWBS\") # save the WEMWBS data into a data frame called WEMWBS\nglimpse(WEMWBS)\n\nRows: 374\nColumns: 4\n$ id          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ scale       &lt;chr&gt; \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\"…\n$ marine      &lt;chr&gt; \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\",…\n$ total_score &lt;dbl&gt; 48, 51, 37, 39, 38, 40, 54, 39, 54, 39, 51, 50, 49, 51, 54…\n\n\nNow let’s do the actual Wilcoxon-Mann-Whitney U test:\n\nwilcox.test(total_score~marine,data=WEMWBS)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  total_score by marine\nW = 9077.5, p-value = 0.0001687\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe null hypothesis of this test is that there is no evidence that the data are drawn from different populations. In this case, the p-value is very small, so we can confidently reject that null hypothesis and assert that there is evidence, according to the WEMWBS scale that marine use makes a difference to peoples’ sense of wellbeing.\nDoes it make it worse or better? - we can see from the summary table and from the box plot that higher scores are associated with those people who were exposed to a marine environment.\nWe might report this results as follows, first using a plain English statement of the main finding, and then reporting the type of test use, the value of the test statistic that it calculated and the p value. In this case, because the p value is so small, we would not report its exact value, but simply give an indication of how small it is:\nWe find evidence, according to the WEMWBS scale, that the wellbeing score is 4.5 or about 10% higher for people exposed to a marine environment (Mann-Whitney U, W = 9077.5, p &lt; 0.001).\n\n\n\n6.5.3 Exercise\nAdapt the code of the last chunk so that you can do the same test but for data as recorded by the nep scale\n\n\n6.5.4 When should I use this Wilcoxon-Mann-Whitney U test?\nThe test we have used here is an example of a non-parametric test. This means that it does not assume that the data follow a known mathematical distribution and, further, that it can be used with ordinal data.\nWe used the Mann-Whitney U test in particular because we were testing for a difference, and because the factor of interest - marine exposure - had just two levels - Yes or No. This test is only suitable when there are just two levels, so you can think of it as as a non-parametric alternative to a t-test.\nIn another setting where we still had just one factor (eg zone of a rocky shore) but there were more than two levels (eg low, mid and high zones of the shore) and we decided that we wanted to do a non-parametric test for a difference, then we would probably use the Kruskal-Wallis test, which you can think of as the non-parametric alternative to a one-way ANOVA.\nIn this example we used the Mann-Whitney U test because the data were ordinal and thus not suitable to use with a parametric test (but see below!). Where we can, we usually try to use a parametric test as they are more powerful than their non-parametric equivalents, meaning, if there is a trend or a difference in the data, they are better able to detect it. However those parametric tests (t-test, ANOVA, pearson correlation, PCA, GLM to name but a few) typically require not only that the data are numerical but also a host of other things, including that they follow a particular distribution, usually (but not always) the normal distribution, and this is often not the case with real biological data. Often, especially with count data, there are lots of zeros, or the data distribution is heavily skewed, usually to the right. In these cases, providing the data are independent of each other, we can usually still use a non-parametric test such as we have here. it might not be the most powerful test we can use (GLMs are typically way better if you can use them), but it will work.\n\n\n6.5.5 Hang on!\nThe eagle eyed among you may have spotted a massive flaw in the line of argument presented above. We said that ordinal data can’t be added up, can’t be used to calculate averages and so on. Thus we can’t run parametric tests on them and have to look for alternatives, namely, non-parametric tests.\nAnd yet, these non-parametric tests are usually run on the output of Likert scales such as we have considered here, where for each person we have a number of Likert Items (ie individual questions) that together constitute the scale, that each generate a score 1-5, then we add up the scores to get a total score. But that means we are adding up ordinal data!!!\nIt turns out that you actually get much the same results with Likert scale data if you analyse them using supposedly inappropriate parametric tests such as a 2-sample t-test as you do if you use a non-parametric test such as the one we considered here, the Mann-Whitney test.\nA study by De Winter and Dodou (2010) shows this convincingly.\nde Winter, J. F. C., & Dodou, D. (2010). Five-Point Likert Items: t test versus Mann-Whitney-Wilcoxon (Addendum added October 2012). Practical Assessment, Research, and Evaluation,15, 1–16. https://doi.org/10.7275/bj1p-ts64\nFor an enlightening discussion of this paper, see this blog by Jim Frost",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#paired-data",
    "href": "tests_for_difference_two_levels.html#paired-data",
    "title": "6  Tests for difference: one factor, two levels",
    "section": "6.6 Paired data",
    "text": "6.6 Paired data\nOften one has a sample of replicated data where each element has a counterpart in another matched sample - paired data. A common scenario for this is when there are data for the same individual at two different points in time, for example before and after some event such as the application of a treatment.\nIn order to determine whether there is a difference between the two sets, one should take the paired aspect into account and not simply match the whole before-set against the whole after-set without doing this. That would be to throw away the information whereby there is likely to be a greater degree of correlation between the responses of an individual before and after the event than there is between any randomly chosen pairs of individuals before and after the event.\n\n6.6.1 Which test: paired t-test or Wilcoxon signed rank test?\nThere is a choice between at least two tests: the parametric paired t-test and the non-parametric Wilcoxon signed rank test. Ideally one would use the t-test since it is more powerful than the Wilcoxon test. This means several things, but in particular it means that, all else being equal, it can detect a small difference with higher probability than the Wilcoxon test can.\n\n\n6.6.2 The paired t-test\nWhere the data are numerical (ie not ordinal) and where the before and after data are both normally distributed around their respective mean values one would use the paired t-test in this scenario. One can test for normality using either a test such as the Shapiro-Wilk test, or graphically using either a histogram, a box plot, or (best), a quantile-quantile plot.\n\n\n6.6.3 The Wilcoxon Signed Rank test\nThe t-test, an example of a so-called parametric test, is actually pretty robust against departures from normality, but where one doubts its validity due to extreme non-normality or for other reasons such as the ordinal nature of the data, the Wilcoxon signed rank test is a useful non-parametric alternative. It is called non-parametric because it does not make any assumption about the distribution of the data values. It only uses their ranks, where the smallest value gets rank 1, the next smallest gets rank 2, and so on.\nSo, you typically use this test when you would like to use the paired t-test, but you cannot because one or both of the data sets is way off being normally distributed or is ordinal.\n\n6.6.3.1 Null Hypotheses\nIn both the t-test and the Wilcoxon signed rank tests, the null hypothesis is the usual ‘nothing going on’, ‘there is no difference’ scenario, but there is a subtle difference between them that reflects the different information that they use. In the Wilcoxon signed rank test the null is that the difference between the medians of pairs of observations is zero. This is different from the null hypothesis of the paired t–test, which is that the difference between the means of pairs is zero.\n\n\n6.6.3.2 Test output\nBoth tests will give a p value. This is the probability that the mean (t-test) or median (Wilcoxon signed rank) paired differences between the corresponding before and after sample elements would be equal to or greater than it actually is for the data if the null hypothesis were true. If the p value is less than some pre-decided ‘significance level’, usually taken to be 0.05, then we reject the null hypothesis. If it is not, then we fail to reject the null hypothesis.\n\n\n\n6.6.4 Example\nWe will use as an example a data set from Laureysens et al. (2004) that has measurements of metal content in the wood of 13 poplar clones growing in a polluted area, once from each clone in August and once again from each of them in November. The idea was to investigate the extent to which poplars could absorb metals from the soil and thus be useful in cleaning that up. Under a null hypothesis, there would be no change in the metal concentrations in the plant tissue of each clone between August and November. Under an alternate hypothesis, there would be.\nLaureysens, I. et al. (2004) ‘Clonal variation in heavy metal accumulation and biomass production in a poplar coppice culture: I. Seasonal variation in leaf, wood and bark concentrations’, Environmental Pollution, 131(3), pp. 485–494. Available at: https://doi.org/10.1016/j.envpol.2004.02.009.\nConcentrations of aluminum (in micrograms of Al per gram of wood) are shown below.\nLoad packages\n\nlibrary (tidyverse)\nlibrary(here)\nlibrary(cowplot) # to make the plots look nice\n\nLoad data\n\nfilepath &lt;- here(\"data\",\"poplars-paired_np.csv\")\npoplars &lt;- read_csv(filepath,show_col_types = FALSE)\nhead(poplars,20)\n\n# A tibble: 13 × 4\n      ID Clone          August November\n   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 Balsam_Spire      8.1     11.2\n 2     2 Beaupre          10       16.3\n 3     3 Hazendans        16.5     15.3\n 4     4 Hoogvorst        13.6     15.6\n 5     5 Raspalje          9.5     10.5\n 6     6 Unal              8.3     15.5\n 7     7 Columbia_River   18.3     12.7\n 8     8 Fritzi_Pauley    13.3     11.1\n 9     9 Trichobel         7.9     19.9\n10    10 Gaver             8.1     20.4\n11    11 Gibecq            8.9     14.2\n12    12 Primo            12.6     12.7\n13    13 Wolterson        13.4     36.8\n\n\nPlot the data\nBefore we do any test on some data to find evidence for a difference or a trend, it is a good idea to plot the data. This will reveal whatever patterns there are in the data and how likely they are to reveal a truth about the population from which they have been drawn.\nTidy the data\nIn this case there is work to do before we can plot the data. The problem is that the data is ‘untidy’. The two levels of the factor month are spread across two columns, August and November. For plotting purposes it will be useful to ‘tidy’ the data so that there is only one column containing both levels of month and another containing the aluminium concentrations. The function pivot_longer() can do this for us:\n\npoplars_tidy &lt;- poplars |&gt;\n  pivot_longer (August:November,names_to=\"month\",values_to=\"Al_conc\")\nhead(poplars_tidy,8)\n\n# A tibble: 8 × 4\n     ID Clone        month    Al_conc\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt;\n1     1 Balsam_Spire August       8.1\n2     1 Balsam_Spire November    11.2\n3     2 Beaupre      August      10  \n4     2 Beaupre      November    16.3\n5     3 Hazendans    August      16.5\n6     3 Hazendans    November    15.3\n7     4 Hoogvorst    August      13.6\n8     4 Hoogvorst    November    15.6\n\n\nNow we can plot the data as a box plot, with one box for August and one for November ie one for each level of the factor month. Had we not first tidied the data, we could not have done this.\n\npoplars_tidy |&gt;\n  ggplot(aes(x = month, y = Al_conc, fill = month, colour = month)) + \n  # alpa (= opacity) &lt; 1 in case any points are on top of each other\n  geom_boxplot(outlier.size=0,alpha=0.5) +\n  geom_point(alpha = 0.5) +\n  # group = ID makes the lines join elements of each pair\n  geom_line(aes(group=ID),colour = \"grey60\") +\n  labs(x = \"Month\",\n       y = \"Al conc.(mu g Al / g wood)\") +\n  theme_cowplot() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nDoes it look as though the difference between the medians could plausibly be zero for the population from which these samples were drawn? Or, put another way, if it was zero, how big a fluke would this sample be? That is what the p value actually tells us.\n\n\n6.6.5 Two sample paired t-test\nCheck for normality of differences\nBefore we use the t-test, we need to check that it is OK to do so. This means checking whether the paired differences are plausibly drawn from a normal distribution centred on zero.\nThe null hypothesis of the Shapiro-Wilk test is that the data set given to it is plausibly drawn from a normally distributed population. So let us give our sample of paired differences:\n\nshapiro.test(poplars$August-poplars$November)\n\n\n    Shapiro-Wilk normality test\n\ndata:  poplars$August - poplars$November\nW = 0.92667, p-value = 0.3081\n\n\nThe p value is very high. Thus we do not reject the null hypothesis and we can reasonably assume that the differences between the August and November aluminium concentrations in the sample could plausibly have been drawn from a normally distributed population, despite the outlier value in the November sample. Thus we can reasonably test for difference using a paired t-test.\nThe actual t-test\nWe can do this in R using the function t.test(), where we give to the function both the August and the November data, knowing that each August value has a counterpart November value, and we set the argument paired to TRUE.\n\nt.test(poplars$August, poplars$November, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  poplars$August and poplars$November\nt = -2.3089, df = 12, p-value = 0.03956\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -9.5239348 -0.2760652\nsample estimates:\nmean difference \n           -4.9 \n\n\nAll parts of the output have meaning and are useful, but here we will focus on just two:\n\nthe p value is equal to 0.040. Hence, if we have chosen the usual significance value of 0.05, we can take this to mean that there is evidence of a significant difference between the August and November values.\nthe lower and upper bounds of the 95% confidence interval are (-9.52, -0.28). YOu can think of this interval as the range of values within which the difference can plausibly lie, at the 95% confidence level. The key thing is that this range does not encompass zero. This means that we can be confident at the 95% level that there is a non-zero change on going from August to November, and, in particular, that the August value is lower than the November value.\n\n\n\n6.6.6 The non-parametric alternative: The Wilcoxon signed rank test\nTo be safe, because of that outlier, let us test for difference using the Wilcoxon signed rank test. In R this is done using the function wilcox.test(), with the argument paired set to TRUE.\n\nwilcox.test(poplars$August, poplars$November, paired = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  poplars$August and poplars$November\nV = 16, p-value = 0.03979\nalternative hypothesis: true location shift is not equal to 0\n\n\nWe see that the conclusion (in this case) is the same.\n\n\n6.6.7 Relation to one-sample paired test\nThe two-sample paired tests as we have done above are the same as doing a one-sample test to see if the differences between the August and November paired values is different from zero. This is true whether we do a t-test or a Wilcoxon signed rank test.\nIn either case, the first argument is the vector of differences, and the second mu is the threshold value against which we want to compare those differences, in this case zero.\n\nt.test(poplars$August - poplars$November, mu = 0, data = poplars)\n\n\n    One Sample t-test\n\ndata:  poplars$August - poplars$November\nt = -2.3089, df = 12, p-value = 0.03956\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -9.5239348 -0.2760652\nsample estimates:\nmean of x \n     -4.9 \n\n\n\nwilcox.test(poplars$August - poplars$November, mu = 0, data = poplars)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  poplars$August - poplars$November\nV = 16, p-value = 0.03979\nalternative hypothesis: true location is not equal to 0\n\n\nNote that the output from both these one-sample tests, where the one sample is the vector of differences and the threshold with which it is compared is zero, is exactly the same as the output of the two-sample tests where the two samples were the vectors between which we were interested in detecting a difference, ie the August and November values. This is not surprising since the two cases are just two ways of doing exactly the same thing, which is to ask if there is evidence from the sample for a difference in the population between the August and November concentrations of aluminium.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html",
    "href": "ANOVA_how_it_works_penguins.html",
    "title": "7  ANOVA explained",
    "section": "",
    "text": "7.1 The basic principles of ANOVA\nThere are many varieties of ANOVA but we will start with the simplest.\nWe would carry out a ‘One-way’ ANOVA (ANalysis Of VAriance) when we have independent replicates from three or more populations and we want to know if there is evidence for a difference between at least one pair of these in respect of the mean value of a numerical variable in which we ae interested. There almost certainly will be a difference between the sample means but these samples are just that - samples - randomly (we hope) drawn from their respective populations. If we drew three samples from the same population they would also almost certainly differ in their means! The differences are due to the random variation within the populations and our random method of choosing samples from them.\nWhat we want to know is whether the differences we see between our samples are big enough for us to reject the null hypothesis that there is no difference between the populations - that they are, in effect, the same population.\nHere we will go through an example in detail and work out all the mechanics, but once we have done that and seen how the output is derived from the input we will not need to do it again. We will use R to do the heavy lifting. We will just need to know when it is appropriate to use ANOVA, how to get R to do it and how to interpret the output that R produces.\nAn ANOVA analysis (bit like saying MOT test!) attempts to determine whether the differences between samples are significant by investigating the variability in the data. It investigates how the variability between samples compares to the variability within samples.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#the-basic-principles-of-anova",
    "href": "ANOVA_how_it_works_penguins.html#the-basic-principles-of-anova",
    "title": "7  ANOVA explained",
    "section": "",
    "text": "7.1.1 The Scenario\nAs our case study we consider the three species of penguin, Adelie, Chinstrap and Gentoo for which data has been gathered and made available in the palmerpenguins R package. For dozens of individuals of each species, records have been logged of the species, sex, year, bill length, bill depth, flipper length and body mass. Suppose we are interested in the shape of the bills and so create for each individual a new variable which is the ratio of bill length to bill depth. We wish to know if there is evidence from the data for whether this bill shape differs between the species. In the manner of the bills of the various species of finch spread across the islands of the Galapagos archipeligo, this might indicate a different food source for each species and more generally, indicate that each species occupies a different ecological niche from the others.\nWhen we plot the data as in Figure 7.1 we see that the species do differ in their bill shapes but that there is also a lot of variation between penguins of the same species.\n\n\n\n\n\n\n\n\nFigure 7.1: The distribution of bill shape for each of the three penguin species.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#summarise-the-data-calculate-means",
    "href": "ANOVA_how_it_works_penguins.html#summarise-the-data-calculate-means",
    "title": "7  ANOVA explained",
    "section": "7.2 Summarise the data: calculate means",
    "text": "7.2 Summarise the data: calculate means\nFirst step: calculate the grand mean\nFirst we calculate the ‘grand mean’, the mean of the bill shapes across all penguins in the data set:\n\n\n[1] 2.658853\n\n\nSecond step: calculate the group means\nNext, we calculate the mean bill shape for each species - these are what we will call the group means.\n\n\n# A tibble: 3 × 5\n  species       n pmean id_min id_max\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;int&gt;  &lt;int&gt;\n1 Adelie       10  2.06      1     10\n2 Chinstrap    10  2.66     11     20\n3 Gentoo       10  3.26     21     30",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#calculate-variabilities-around-the-means-sums-of-squared-differences",
    "href": "ANOVA_how_it_works_penguins.html#calculate-variabilities-around-the-means-sums-of-squared-differences",
    "title": "7  ANOVA explained",
    "section": "7.3 Calculate variabilities around the means: sums of squared differences",
    "text": "7.3 Calculate variabilities around the means: sums of squared differences\nNow we can calculate and plot the deviations of the data around these means. That will allow us to visualise and calculate what turn out to be the key quatities in carrying out an ANOVA analysis: the sums of squared differences. There are three of these and we will turn to them now:\n\n7.3.1 SST: the total sum of squares\nFRom the deviations of each individual arounnd the grand mean we can calculate what we will call SST: the total sum of squares. This is the sum of the squared differences between each individual penguin bill shape and the grand mean. In this and other measures of variability we square the differences so that we do not distinguish between positive and negative deviations.\nSST is a measure of the total variability of the data set. For our penguins we find that SST = 8.147\n\n\n\n\n\n\n\n\nFigure 7.2: The overall variability of the data (SST) is measured as the sum of the squared differences between each individual value and the grand mean, which is the mean of the whole data set. The individual differences are shown in the figure as vertical dotted lines. If there are n data points, then SST will have n-1 degrees of freedom.\n\n\n\n\n\nNow we determine two more measures of variability: the deviations of individual penguins from their group means, and the the deviations of those group means from the grand mean. For each of these measures, we will square them so that we do not distinguish between positive and negative deviations, and then sum these squares to get a measure of the total variabilities.\n\n\n7.3.2 SSE - Error sum of squares\nSSE is the error sum of squares. It is the sum of the squares of the deviations of the data around the three separate species means. This is a measure of the variation between individuals of the same species.\n\n\n7.3.3 SSG - group (species) sum of squares\nSSG is the group sum of squares. This is the sum of the squares of the deviations of the group (species, in this case) means from the grand mean. This is a measure of the variation between individuals of different species.\n\n\n\n\n\n\n\n\nFigure 7.3: a) Each species has a mean value for bill shape, shown by the three horizontal lines. Each individual penguin has a bill shape that differs from that by the length of the vertical dotted lines shown. When we square each of these lengths and then add up all these squares we get what we are calling here SSE: the error sum of squares. This is a measure of the variability between penguins of the same species. b) For each species, the mean bill shape differs from the grand mean by the length of the dotted lines shown. When we square and then sum these three lengths we get what we are calling here SSG: the squared sum of groups. This is a measure of the variability between penguins of different species.\n\n\n\n\n\nWhen the three species means are fitted, there is an obvious reduction in variability around the three means compared to that around the grand mean: SSE is less than SST, but it is not obvious if bill shape differs between the species.\nAt what point do we decide if the amount of variation explained by fitting the means is significant? By this, we mean, “When is the variability between the group means greater than we would expect by chance alone?\nFirst, we note that SSE and SSG partition between them the total variability SST in the data:",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#partitioning-the-sums-of-squares-sst-sse-ssg",
    "href": "ANOVA_how_it_works_penguins.html#partitioning-the-sums-of-squares-sst-sse-ssg",
    "title": "7  ANOVA explained",
    "section": "7.4 Partitioning the sums of squares: SST = SSE + SSG",
    "text": "7.4 Partitioning the sums of squares: SST = SSE + SSG\n\n\n\nSum of Squares\nValue\n\n\n\n\nSSE\n0.936\n\n\nSSG\n7.212\n\n\nSST\n8.147\n\n\nSSE + SSG\n8.148 = SST\n\n\n\nSo the total variability has been divided into two components. That due to differences between individuals of different species (SSG) and that due to differences between individuals of the same species. (SSE). Variability must be due to one or other of these components. Separating the total SS into its component SS is known as partitioning the sums of squares.\nA comparison of SSG and SSE is going to indicate whether fitting the three species means accounts for a significant amount of variability.\nHowever, to make a proper comparison, we really need to compare the variability per degree of freedom ie the variance, so first we need to discuss what we mean by degrees of freedom for each of the sums of squares calculated so far, and how we calculate them.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#partitioning-the-degrees-of-freedom",
    "href": "ANOVA_how_it_works_penguins.html#partitioning-the-degrees-of-freedom",
    "title": "7  ANOVA explained",
    "section": "7.5 Partitioning the degrees of freedom",
    "text": "7.5 Partitioning the degrees of freedom\nEvery sum of squares (SS) has been calculated using a number of independent pieces of information. In each, case, we call this number the number of degrees of freedom for the SS.\nFor SST this number is one less than the number of data points n. This is because when we calculate the deviations of each data point around a grand mean there are only n-1 of them that are independent, since by definition the sum of these deviations is zero, and so when n-1 of them have been calculated, the final one is pre-determined.\nSimilarly, when we calculate SSG, which measures the deviation of the \\(k\\) species means from the grand mean, we have \\(k\\)-1 degrees of freedom, (where in the present example \\(k\\), the number of species, is equal to three) since the deviations must sum to zero, so when \\(k\\)-1 of them have been calculated, the last one is pre-determined.\nFinally, SSE, which measures deviation around the group means will have n-k degrees of freedom, since the sum of each of the deviations around one of the group means must sum to zero, and so when all but one of them have been calculated, the final one is pre-determined. There are \\(k\\) group means, so the total degrees of freedom for SSE is n-k.\nThe degrees of freedom are additive:\n\\[\ndf(\\text{SST}) = df(\\text{SSE}) + df(\\text{SSG})\n\\]\nCheck: \\[\n\\begin{align*}\ndf(\\text{SST}) &= n-1\\\\\ndf(\\text{SSE}) &= k-1\\\\\ndf(\\text{SSG}) &= n-k\\\\\n\\therefore df(\\text{SSE}) + df(\\text{SSG}) &= k-1 + n-k\\\\\n&=n-1\\\\\n&=df(\\text{SST})\n\\end{align*}\n\\]",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#variances-per-degree-of-freedom-mean-squares",
    "href": "ANOVA_how_it_works_penguins.html#variances-per-degree-of-freedom-mean-squares",
    "title": "7  ANOVA explained",
    "section": "7.6 Variances per degree of freedom: Mean Squares",
    "text": "7.6 Variances per degree of freedom: Mean Squares\nNow we can calculate the variances which are a measure of the amount of variability per degree of freedom.\nIn this context, we call them mean squares. To find each one we divide each of the sums of squares (SS) by its corresponding degrees of freedom.\nGroup Mean Square (GMS) = SSG / k - 1. This is the variation per df between individuals of different species.\nError Mean Square (EMS) = SSE / n - k. This is the variation per df between individuals of the same species.\nTotal Mean Square (TMS) = SST / n - 1. This is the total variance per df of the dataset.\nUnlike the SS, the MS are not additive. That is, GMS + EMS \\(\\neq\\) TMS as we see in the table below where we list these values for our penguins sample:\n\n\n\nMean Square\nValue\n\n\n\n\nGMS\n2.73\n\n\nEMS\n0.0154\n\n\nTMS\n0.171",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#f-ratios",
    "href": "ANOVA_how_it_works_penguins.html#f-ratios",
    "title": "7  ANOVA explained",
    "section": "7.7 F-ratios",
    "text": "7.7 F-ratios\nIf species did not influence bill shape, we would expect as much variation between the penguins of the same species as between penguins of different species.\nWe can express this in terms of the mean squares: the mean square for groups (species) would be about the same as the mean square for error and so their ratio would be about equal to 1:\n\\[\nF=\\frac{\\text{GMS}}{\\text{EMS}}\\approx1\\quad{\\text{No difference between groups}}\n\\]\nWe call this ratio the F-ratio. This is the so-called ‘test statistic’ that ANOVA calculates from your data. F-ratios can never be negative since they are the ratio of two mean square values, both of which must be non-negative, but there is no limit to how large they can be. In fact,\n\\[\nF = \\frac{\\text{GMS}}{\\text{EMS}}\\gt 1\\quad{\\text{Possible difference between groups}}\n\\]\n\\[\nF = \\frac{\\text{GMS}}{\\text{EMS}}\\gg 1\\quad{\\text{Probable difference between groups}}\n\\]\nEven if the species were identical, the F-ratio is unlikely to be exactly 1 - it could by chance take a whole range of values. It turns out that F-ratios have a known distribution, the F-distribution, which represents the range and likelihood of all possible F ratios under the null hypothesis. ie when the species were identical.\nThe shape of the F distribution depends on the degrees of freedom of GMS and EMS, and we normally specify it by giving the values of each. In Figure 7.4 below we show F distributions for 2 and 27 degrees of freedom (ie 3 species, so k = 3, so the degrees of freedom of GMS = k-1 = 2, and 10 individuals per species, so n = 3 x 10 = 30, and hence the degrees of freedom of EMS = n-k = 30 - 3 = 27), and for 10 and 27 degrees of freedom.\n\n\n\n\n\n\n\n\nFigure 7.4: The F-distributions for (left) 2 and 27 degrees of freedom and (right) 10 and 27 degrees of freedom\n\n\n\n\n\nNote that, whatever the degrees of freedom, F-distributions are examples of so-called probability density functions. The area beneath them between any two values of F-ratio is equal to the probability of getting an F-ratio in that range. Hence the total area under the curves is equal to 1, since the F-ratio must take some value between zero and infinity, and the area under the tail to the right of any given F-ratio is the probability of getting an F-ratio bigger than that value.\nHence, the probability under the null hypothesis of getting an F-ratio as large or larger than the value we actually got is the area to the right of this F-ratio under the appropriate F distribution. We often call this probability the p-value. p for probability. p-values are the the probability of getting data as extreme (same F-ratio,) or more extreme (bigger F-ratio) as the data you got you got if the null hypothesis were true.\nIf the species were very different, then the GMS would be much greater than the EMS and the F-ratio would be greater than one. However it can be quite large even when there are no differences between the levels (here, Adelie, Chinstrap and Gentoo) of a factor (here, species). So how do we decide when the size of the F-ratio is due to a real difference between the levels rather than to chance?\nOur p-value (the probability that the F-ratio would have been as large as it is or larger under the null hypothesis) represents the strength of evidence against the null hypothesis. The smaller it is, the stronger the evidence, and, as a pragmatic choice, only when it is less than 0.05 do we regard the evidence as strong enough to reject the null. Note though that even if we had inside knowledge that the null hypothesis was in fact true, we would still get an F-ratio that large or larger and thus a p-value less than or equal to 0.05 5% of the time.\nWe find for our penguin data that the F ratio is 104\nIf we look at the F distribution in the left-hand figure in Figure 7.4 the one that corresponds to our case, with 2 and 27 degrees of freedom, we see that it has already fallen to near zero by the time F is equal to 5 or 6. There is essentially zero area beneath the curve to the right of the much larger F value we found for our data, F = 104. That means there is essentially zero chance of getting an F value this big or bigger if the null hypothesis were true (ie the p-value is esentially zero), and so we can confidently reject the null hypothesis.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#what-does-the-anova-result-tell-us",
    "href": "ANOVA_how_it_works_penguins.html#what-does-the-anova-result-tell-us",
    "title": "7  ANOVA explained",
    "section": "7.8 What does the ANOVA result tell us?",
    "text": "7.8 What does the ANOVA result tell us?\nThe null hypothesis of the ANOVA is that all the samples are drawn from populations with the same mean. If the F-value is so large that we reject this null hypothesis, then we infer that at least two of the population means differ. We not learn from the ANOVA where the difference or differences lie.\nTo find that out we typically need to continue the analysis with multiple pair-wise comparisons between the samples, taking care to control for the study-wide error rate.\n\n\nAnalysis of Variance Table\n\nResponse: bill_shape\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nspecies    2 7.2117  3.6058  104.05 2.05e-13 ***\nResiduals 27 0.9357  0.0347                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_one_way.html",
    "href": "ANOVA_one_way.html",
    "title": "8  One-way ANOVA",
    "section": "",
    "text": "8.1 Introduction\nFor this chapter you will find it useful to have this RStudio project folder if you wish to follow along and/or see how a complete script might look.\nIn this exercise we will carry out a method of analysis known as ANOVA - this is what is commonly used when you have one or more categorical variables, such as species, sex and so on, and a numerical response variable such as body mass and you want to know if there is a difference in the response variable between when the levels of the factors take different values.\nA one-way ANOVA is used where where we have one categorical variable with three or more levels (you could also use it where there are just two levels, but we use a t-test for that). For example if you want to see if a captive bird species prefers red, green or blue food pellets, then the factor would be be food colour, and three levels of that would be the three colours.\nA two-way ANOVA is used where we have two categorical variables, each of which has at least two levels. (Even if both have two levels, we still call it a two-way ANOVA. There is no such thing as a two-way t-test!). So, sticking with the captive birds, if were interested in whether they had a preference for colour (red, blue, green) and/or shape (round, square) of food pellets, then we could use a two way ANOVA to investigae the data, where the two factors or ‘ways’ would be food colour and food shape, with food colour having three levels (red, blue and green) and food shape having two levels (round and square).\nANOVAs involving three ways or more (the horror, the horror!) are rarely used since their interpretation is in practice difficult due to the multiplicity of possible so-called ‘interaction’ effects that commonly arise, whereby the impact on the output of the levels of one factor depend on the values of the levels of one or more other factors. Best avoided!\nANOVAs, whatever their flavour (one way, two way, repeated measures, ANCOVA etc) are examples of parametric tests. That is, they can only be used if the data at least approximately meet certain requirements such as equal variance across the data sets, normality of residuals etc. At the very least, the data should be numerical and not ordinal. Hence whenever we think of using an ANOVA we need to check that these requirements are at least approximately met. If they are not, then we may choose to turn to non-parametric alternatives.\nA non-parametric alternative to a one-way ANOVAs is commonly used, especially for studies that involve ordinal data such as Likert-type outputs from surveys. It is called a Kruskal-Wallis test.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_one_way.html#example-penguin-data-from-the-palmerpenguins-package",
    "href": "ANOVA_one_way.html#example-penguin-data-from-the-palmerpenguins-package",
    "title": "8  One-way ANOVA",
    "section": "8.2 Example: penguin data from the palmerpenguins package",
    "text": "8.2 Example: penguin data from the palmerpenguins package\nIn the following, we show how a one-way ANOVA might be carried out on a set of data on penguins, where we have a numerical output such as body mass, and categorical variables such as species (three levels: Adelie, Gentoo and Chinstrap) and sex (two levels: female and male). If we were interested in whether there was a difference in the output across the levels of species, then a one-way ANOVA might well be suitable.\n\n8.2.1 Follow along yourself:\nTo follow through this exercise yourself, you should have an RStudio project folder that contains:\nIn the Project/scripts folder:\nANOVA_one_way_template.Rmd (the script where you fill in the code chunks)",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_one_way.html#a-first-look-at-the-data",
    "href": "ANOVA_one_way.html#a-first-look-at-the-data",
    "title": "8  One-way ANOVA",
    "section": "8.3 A first look at the data",
    "text": "8.3 A first look at the data\n\n8.3.1 Load packages\n\nlibrary(tidyverse) # for data manipulation and plots, and more besides\nlibrary(ggfortify) # this is useful for diagnostics\nlibrary(palmerpenguins) # for the palmer penguin data\n\nThe palmerpenguins package comes with two in-built data sets on penguins. The simplest of them is called penguins and is the one we will use in this exercise:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n8.3.2 Remove observations with missing values\nWe can see from the first few values of the glimpse table that some rows have missing values (NAs). We need to decide what to do with them. Here we will simply remove them! Here is a way to remove any row that contains missing values in one column or another:\n\npenguins_clean &lt;- penguins |&gt;\n  drop_na()\nglimpse(penguins_clean)\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nThat has removed 11 rows of data, so we haven’t lost too much information.\n\n\n8.3.3 Summary - group by species and sex\nHere we use the famliar group_by() and summarise() construction to find the mean body mass for each combination of species and sex. We also calculate the standard error of those means and the number of individuals in each group.\n\npenguins_clean |&gt;\n  group_by(species, sex) |&gt;\n  summarise(n = n(), mean_bm = mean(body_mass_g), se_bm = sd(body_mass_g)/sqrt(n()) ) |&gt;\n  ungroup()\n\n# A tibble: 6 × 5\n  species   sex        n mean_bm se_bm\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    female    73   3369.  31.5\n2 Adelie    male      73   4043.  40.6\n3 Chinstrap female    34   3527.  48.9\n4 Chinstrap male      34   3939.  62.1\n5 Gentoo    female    58   4680.  37.0\n6 Gentoo    male      61   5485.  40.1\n\n\nLooking at this table, does it look as though females and males have different weights? If so, which is heavier? Is this true for all species? Do the different species weigh the same?\n\n\n8.3.4 Plot the data\nTo get further insight into these questions, we can plot the data. Here we will do a box plot\n\npenguins_clean  |&gt;\n  ggplot(aes(x=species, y = body_mass_g, fill = sex)) +\n  geom_boxplot() +\n  labs(x = \"Species\",\n       y = \"Body mass (g)\",\n       fill = \"Sex\") +\n  scale_colour_brewer(palette = \"Set1\") +\n  # theme_bw() +\n  theme(legend.position= c(0.1,0.8))\n\n\n\n\n\n\n\n\nWhat do you think now about size differences between species and the two sexes?\nThere is a lot going on here, so let’s approach this more simply to begin with and concentrate solely on the difference between the females of the species.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_one_way.html#one-way-anova",
    "href": "ANOVA_one_way.html#one-way-anova",
    "title": "8  One-way ANOVA",
    "section": "8.4 One-way ANOVA",
    "text": "8.4 One-way ANOVA\nLet’s ask the question: do the body weights differ between females of the different species?\nThere is just one factor here, species, and it has more than two levels - the three different species - and the reponse variable is numeric, so it is highly likely that the appropriate test to answer this question is a one-way ANOVA. ‘One way’ because there is one factor, and ‘ANOVA’ (instead of t-test) because there are more than two levels.\n\n8.4.1 Null hypothesis\nPretty much all of the commonly used statistics tests are asking the question: what is the probability that you would have got this data, or more extreme data, if the null hypothesis were true? Their job is to calculate that probability, which is called a p-value. There is a lot more besides, but what this means is that in carrying out any of these tests we at least need to have a hypothesis in mind and its corresponding null hypothesis. The null, remember, is typically the ‘nothing going on’, there is no effect, no difference scenario.\nSo in this case, a suitable null hypothesis would be that there is no difference in body mass between the females of the different penguin species.\nTo see if there is evidence from the data to reject this null, we will follow a sequence of steps that will be common to many analyses:\n\nget the data\nclean/prepare the data\nsummarise the data\nplot the data\nconstruct the model using whatever test is appropriate, in this case a one-way ANOVA\ncheck whether the model is valid\ninspect the model output\nreject or fail to reject the null hypothesis\nif we reject the null, carry out post-hoc tests\n(maybe) simplify the model and redo the analysis\n\nFor the penguin data, getting it was easy as it came with the palmerpenguins package.\nTo prepare the data, we start with the full data set and narrow it down to just the females, using the filter() function, and again make sure there are no lines with missing values, using drop_na(). We save this cleaned data set in an object called females.\n\n\n8.4.2 Filter and clean the data\n\nfemales &lt;- penguins |&gt;\n  filter(sex == \"female\") |&gt;\n  drop_na()\n\n\n\n8.4.3 Summarise the data\nThen let’s summarise these values to find the number of individuals, the mean body mass for each species, and the standard errors of those means:\n\nfemales |&gt;\n  group_by(species) |&gt;\n  summarise(n = n(), mean.mass_f = mean(body_mass_g), se.mass_f = sd(body_mass_g)/sqrt(n()))\n\n# A tibble: 3 × 4\n  species       n mean.mass_f se.mass_f\n  &lt;fct&gt;     &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 Adelie       73       3369.      31.5\n2 Chinstrap    34       3527.      48.9\n3 Gentoo       58       4680.      37.0\n\n\nWe should inspect this summary table and see what we already think about whether the null hypothesis is likely to be rejected, or not.\nNow let’s plot them, using a box plot (but choose your favourite plot type):\n\n\n8.4.4 Plot the data\n\nfemales  |&gt;\n  ggplot(aes(x=species, y = body_mass_g)) +\n  geom_boxplot(fill = \"#9ebcda\") +  # pick your favourite colour from https://colorbrewer2.org/\n  labs(x = \"Species\",\n       y = \"Body mass (g)\")\n\n\n\n\n\n\n\n  # theme_bw()\n\nFrom the summary table and the plot, what do you think? Do the masses differ between the species?\n\n\n8.4.5 Do the actual ANOVA\nYou probably have a good idea what the answer is, as to our question, but now we will move on to the actual statistics test, in this case a one-way ANOVA.\nAn ANOVA is one variant of a range of anlysis techniques known as ‘linear models’. If you were to look under the hood, you would see that mathematics behind it is exactly the same as that behind linear regression, which we use when we have a continuous explanatory variable and where we fit straight lines onto a scatter plot. Thus it is no surprise that the ANOVA is carried out in R in exactly the same way as linear regression would be:\nFirst, we use the lm() function to construct a linear model of the data:\n\n8.4.5.1 Construct the model\n\nfemales.model &lt;- lm(body_mass_g ~ species, data = females)\n\nHere the lm() function has done all the maths of the ANOVA, and we have saved the results of that in an object called females.model. Note the use of the formula body_mass_g ~ species as the first argument of the lm() function, where this means ‘body mass as a function of species’.\n\n\n8.4.5.2 Is the model valid?\nAll linear models are only valid if the data meet a number of criteria. Chief among these for an ANOVA is that the spread of the data should be roughly the same in each subset, and that the data within each subset should be normally distributed around their respective mean values. Only if these conditions are at least approximately met can we just go on and trust the output of the model. If they are not, we need to transform the data in some way until they are, or use a different test. A commonly used non-parametric alternative to the one-way ANOVA is the Kruskal-Wallis test.\nThere are various ways we can find out whether these conditions are met. A useful one is to do it graphically, and a useful way to do that is to use the autoplot() function from the ggfortify package. Let’s do it:\n\nautoplot(females.model) + theme_bw()\n\n\n\n\n\n\n\n\nAll four graphs presented here tell us something about the validity or not of our model. Here we will just focus on the upper two:\n\ntop-left: this shows the spread of the residual masses (diference between an individual’s mass and the mean mass for its species) for each species. We see that the spread of these values is aout the same for all three species. Check!\ntop-right: this is a quantile-quantile plot, often referred to as a qq-plot. This compares the distribution of the residuals for each species with a normal distribution. If the residuals are normally distributed, we will get a straight line. If not, we won’t. To get an idea of what qq-plots, histograms and box-plots look like for data that definitely are not normally distriuted, see this useful summary. In our case, there is a hint of a curve, but this qq-plot is really a pretty good approximation to linear for a real data set. No such data is ever perfectly normally distributed, so the best we are looking for, in practice is something approximating a straight line, often with some raggedness at either end. So, check again!\n\nOn both counts, we are good to go: we can reasonably trust the output of the ANOVA.\nSo what is this output? We find this in three steps\n\n\n8.4.5.3 Inspect the model\nThe overall picture\nFirst, we use the anova() function\n\nanova(females.model)\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n           Df   Sum Sq  Mean Sq F value Pr(&gt;F)    \nspecies     2 60350016 30175008     393 &lt;2e-16 ***\nResiduals 162 12430757    76733                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis gives us an overview of all the data and asks the question: how likely is it that you would have got your data if species made no difference to body mass. There are three things to note:\n\nthe test statistic, here called an F-value. This is a number calculated from the data. Roughly speaking, it is the ratio of the spread of values (aka variance) between the subgroups to that within the subgroups If the validity criteria for the test have been met by the data, then this has a known distribution. The bigger the F-value, the more likely it is that the null will be rejected.\nthe degrees of freedom, here denoted as Df and listed in the first column. These are the number of independent pieces of information in the data, which here means, how many species and how many penguins.\nthe p-value, which is the probability of getting an F value as big as or bigger than the one actually found, if the null hypothesis were true. This is is the number listed at the right as Pr(&gt;F).\n\nThe F value here is huge and the p-value is tiny, so tiny that it is essentially zero. Thus we can confidently reject the null hypothesis and assert that there is evidence from the data that body mass of females differs between at least one pair of species. Which two, or between all of them, and by how much we don’t yet know. This first step just tells us whether there is some difference somewhere. If there were no evidence of any difference we would stop the analysis right here.\nBut there is a difference in this case, so we continue.\nThe detailed picture\nWe use the summary() function for this:\n\nsummary(females.model)\n\n\nCall:\nlm(formula = body_mass_g ~ species, data = females)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-827.2 -193.8   20.3  181.2  622.8 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        3368.8       32.4  103.91   &lt;2e-16 ***\nspeciesChinstrap    158.4       57.5    2.75   0.0066 ** \nspeciesGentoo      1310.9       48.7   26.90   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 277 on 162 degrees of freedom\nMultiple R-squared:  0.829, Adjusted R-squared:  0.827 \nF-statistic:  393 on 2 and 162 DF,  p-value: &lt;2e-16\n\n\nThere is a lot in ths output, so let’s just consider the coefficient table, to begin with. Focus first on the top left value, in the Estimate column. This tells us the mean body mass of the reference or ‘Intercept’ species. In this case that is ‘Adelie’, purely because ‘Adelie’ comes alphabetically before the other two species names, ‘Chinstrap’ and ‘Gentoo’. By default, R will always order levels of a factor alpabetically. This is often a nuisance, and with many data sets we have to tell R to reorder the levels the way we want them, but here the order is OK.\nSo, the mean mass of female Adelie penguins in our sample is 3368 g. Cross check that with your initial summary table and the box plot. What about the other two species? Here’s the thing: for all rows except the first in the Estimate column we are not given the absolute value but the difference between their respective mean values and the reference mean in the first, ‘Intercept’ row.\nThus, we are being told that Chinstrap females in the sample have a mean body mass that is 158.37 g heavier than that of Adelie females, so that their mean body mass is 3368.84 + 158.37 = 3527.27g. Again, cross check that with your summary table and the box plot. Is it right?\nWhat about Gentoo females? Were they heavier than Adelie penguins, and if so, by how much? What was their mean body mass.\nWhy doesn’t summary() just tell us the actual body masses instead for all three species instead of doing it in this round about way? The reason is that ANOVA is concerned with detecting evidence of difference. This is why we are being told what the differences are between each of the levels and one reference level, which here is Adelie.\nAre those differences signifcant? We use the right hand p-value column for that. Look in the rows for Chinstrap and Gentoo penguins. In both cases the p values are much less than 0.05. This is telling us that in both cases there is evidence that females of these species are significantly heavier than those of the Adelie species.\nNote that we have only been told, so far, about the magnitude and significance of differences between all the levels and the reference level. We are not told the significance of any difference between any other pair of levels. So in particular, the ANOVA does not tell us whether there is a significant difference between the masses of Chinstrap and Gentoo females (although we may have a good idea what the answer is, from our initial summary table and plot).\nTo find the answer to that, we do post-hoc tests:\n\n\n\n8.4.6 Post hoc tests.\nA final step of most ANOVA analyses is to perform so-called post-hoc (‘after the fact’) tests which make pairwise comparisons between all possible pairs of levels, tell us what the differences are between those pairs and whether these differences are significant. Whatever method is used for this, it needs to take account of the danger of making Type-one errors that arises when multiple pair-wise tests are done.\nA commonly used function for doing this is Tukey’s Honest Signficant Difference: TukeyHSD()\n\nTukeyHSD(aov(body_mass_g ~ species, data = females))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = body_mass_g ~ species, data = females)\n\n$species\n                 diff    lwr  upr p adj\nChinstrap-Adelie  158   22.3  294 0.018\nGentoo-Adelie    1311 1195.6 1426 0.000\nGentoo-Chinstrap 1153 1011.0 1294 0.000\n\n\nIn each row of the output we see the difference between the mean masses of the females of two species, where a positive value tells us that the first named species has the heavier mass. So, we see that Gentoo females in the sample were on average 1310.9 g heavier than Adelie females.\nCompare these differences with your initial summary table and your box plot. Do they agree? They should!\nThe right-hand column ‘p adj’ tells us whether these difference are significant. If the p values are less than 0.05 then they are, at the 5% significance level. In this case they all are. The p values are so tiny for the differences between Gentoo and the other two species that that they are reported as zero.\n\n\n8.4.7 Reporting the Result.\nWe try to use plain English to report our results, while still telling the reader what test was used and the key outputs of the test. Try to report the name of the test, the test statistic, the degrees of freedom, and the p-value. if. the p-value is really small then it is common to report it as p&lt;0.01, or p&lt;0.001. No one cares if it is a billionth or a squillionth. It just matters that is t is really small, if that is the case. If it is only just below 0.05, then I would report it in full, so we might write p = 0.018. If p &gt; 0.05 then conventiallly it is not reported, except to say p &gt; 0.05.\nIn this case, we might say something like:\nWe find evidence that there is a difference between the body masses of females of the penguin species Adelie, Chinstrap and Gentoo (ANOVA, df = 2, 162, F = 393, p &lt; 0.001). In particular Gentoo are more than 1 kg heavier than the other two (p&lt; 0.001) while the difference between Chinstrap and Adelie is smaller, at 158 g, but still significant (p = 0.018).",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html",
    "href": "ANOVA_two_way_with_model_simplification.html",
    "title": "9  Two-way ANOVA with model simplification",
    "section": "",
    "text": "9.1 Factorial experiments and model simplification\nFor this chapter you will find it useful to have this RStudio project folder if you wish to follow along and try out the exercises at the end.\nThis exercise sheet is heavily indebted to Michael Crawley’s Statistics: An introduction using R, 2nd Ed, Wiley. Published in 2015 this emphasises statistics over R (in fact, much of the R he presents is written prior to the advent of the tidyverse dialect which we use here, and so may seem terse if that is what you are used to). It is very useful and is at a higher level than Beckerman, Childs and Petchey’s Getting Started in R: An introduction for biologists, 2nd Ed. OUP published in 2017. Their book also includes a simpler version of the example explored here.\nThe best model is the one that adequately explains the data with fewest parameters. This means with the smallest possible number of degrees of freedom.\nIf we have a very large number of parameters, a model can fit any data set but be of limited use in generalising beyond it (we will have overfitted the data). If we have too few we will not explain much of the variance of the data. A balance must be struck. Hence we want the minimal adequate model.\nAs Einstein almost said, a model should be as simple as possible, but no simpler. (Not to be outdone, the British statistician George Box also had a pithy saying about models: “All models are wrong, but some are useful”.)\nA factorial experiment has two or more factors, each with two or more levels, plus replication for each combination of factor levels. This means that we can investigate whether statistical interactions occur in which the effect of one factor depends on the value of another factor.\nWe take an example from a farm-trial of animal diets. There are two factors: diet and supplement. diet is a factor with three levels: barley, oats and wheat, where barley is the diet that has always been used and the other two are potential alternatives. The purpose of the trial is to see if their use makes a difference to growth outcomes. supplement is a factor with four levels: control, agrimore, supergain and supersupp, where control could mean the absence of any supplement or the supplement used up to now, whose effects we are hoping to improve upon through use of one of the others included in the trial. The response variable gain is weight gain after 6 weeks. There were 48 individual cows in total with 4 for each combination of diet and supplement. Having the same number of replicates for each combination of levels means that this is a balanced design.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#files-needed",
    "href": "ANOVA_two_way_with_model_simplification.html#files-needed",
    "title": "9  Two-way ANOVA with model simplification",
    "section": "9.2 Files needed",
    "text": "9.2 Files needed\nTo follow through this exercise, you should have an RStudio project folder that contains:\n\nIn the Project/scripts folder:\n\nANOVA_two_way_template.Rmd (the script where you fill in the code chunks)\n\nIn the Project/data folder\n\ngrowth.csv\n\n\nYou can download a complete project folder with these files in from here\nIn the following, we present the code you need to analyse this data together with explanatory text. Read the text closely so that you understand what each chunk of code is intended to do. In the accompanying template file, fill in the code as you go in the empty chunks, using this worksheet as a guide. As you complete each line of code, run it using Ctrl-Enter, or Cmd-Enter on a Mac. Alternatively, wait until you have completed the code for a chunk then run the whole chunk in one go by pressing the little green arrow at the top right of the chunk. Whichever way you choose, you are encouraged to view the code presented here as one way to do the analysis. Feel free to hack away at it and change things, to try different approaches and see what happens. That way you will learn. You may also wish to add your own text between the chunks.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#open-your-project",
    "href": "ANOVA_two_way_with_model_simplification.html#open-your-project",
    "title": "9  Two-way ANOVA with model simplification",
    "section": "9.3 Open your Project",
    "text": "9.3 Open your Project\nYou should be working within a folder that you have designated as what RStudio calls a ‘Project’. If you are, the name of your Project will appear at the top right of the RStudio window. Inside your Project folder you should have a notebooks folder for notrbooks like the one you are working from, and a data folder for all the data files. You will also see, at the top level of the Project, the .RProj file. You can see all this in the Files pane, bottom-right.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#load-packages",
    "href": "ANOVA_two_way_with_model_simplification.html#load-packages",
    "title": "9  Two-way ANOVA with model simplification",
    "section": "9.4 Load packages",
    "text": "9.4 Load packages\nI normally load all the packages in the chunk below into every script. The most important is the tidyverse package which is a goody bag containing several other packages. Loading this saves you from having to load each of those individually. The most often used among these is readr for reading and writing data from/to files, dplyr for data manipulation, and ggplot2 for plotting. Others will be used from time to time, and we don’t really need to be aware of that when it happens or to worry about it, so long as tidyverse has been loaded.\n\nlibrary(tidyverse)  # for data manipulation and plotting, and much else besides\nlibrary(here) # for finding our data easily\nlibrary(cowplot) # gives a nice theme for plots\nlibrary(ggfortify) # for diagnostic plots",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#read-in-the-data",
    "href": "ANOVA_two_way_with_model_simplification.html#read-in-the-data",
    "title": "9  Two-way ANOVA with model simplification",
    "section": "9.5 Read in the data",
    "text": "9.5 Read in the data\nThe growth.csv data file needs to be in the data folder within the Project folder.\nIn this chunk we read the growth.csv data into an R object to which we give the name weights.\n\nfilepath &lt;- here(\"data\", \"growth.csv\")\nweights &lt;- read_csv(filepath) # this function is from the readr package, part of tidyverse\nglimpse(weights)\n\nRows: 48\nColumns: 3\n$ supplement &lt;chr&gt; \"supergain\", \"supergain\", \"supergain\", \"supergain\", \"contro…\n$ diet       &lt;chr&gt; \"wheat\", \"wheat\", \"wheat\", \"wheat\", \"wheat\", \"wheat\", \"whea…\n$ gain       &lt;dbl&gt; 17.37125, 16.81489, 18.08184, 15.78175, 17.70656, 18.22717,…\n\n\nYou see from the output of the glimpse() function that weights has three columns and 48 rows. Two columns are of data type &lt;chr&gt; which is R-speak for text, and the other is data type &lt;dbl&gt; which is R-speak for numerical data with a decimal point.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#clean-the-data",
    "href": "ANOVA_two_way_with_model_simplification.html#clean-the-data",
    "title": "9  Two-way ANOVA with model simplification",
    "section": "9.6 Clean the data",
    "text": "9.6 Clean the data\nHaving read in the data some cleaning/wrangling/tidying or processing of the data is often required before we can go further with the analysis.\nWarning! This step can be the most time-consuming of the whole analysis, particularly if you are using a large data set obtained from a third party.\nHere there is not much to do, but we would like to make R recognise the categorical variables as factors, and order the levels.\n\n9.6.1 Force R to recognise supplement and diet as factors, and to reorder their levels.\nAt the moment, the contents within the variables supplement and diet are not being recognised as levels of factors. R is just thinking of them as text (or &lt;chr&gt; in R-speak), as we can see from the output of the glimpse() function in the chunk above. Let us fix that, as it will be useful for them to be recognized for what they are so that we can order the levels in a way that makes sense for our context, our plots and our analysis.\nSometimes levels of a factor have a natural order, such as Low, Mid and High as the levels of the factor Tidal Zone and sometimes they do not, for example Apples, Oranges and Pears as levels of the factor Fruit. Here, in the case of both our factors, we only wish to impose order among the levels in so far as we would like what we regard as the control or reference level to be first. By default, R puts the levels of a factor in alphabetical order. This is the order in which the boxes of a box plot would be displayed, reading left to right. In an ANOVA setting it means that differences of outcome (in this case, weight gain of the cows) are later calculated for each combination of levels with respect to the outcome for the combination of levels that are alphabetically first, in this case barley for diet and agrimore for supplement. In both the box-plot and the ANOVA output case this default ordering is not necessarily what we want. Normally, we want what we regard as the control levels to be the reference level and in this case that means barley for diet and control for supplement.\nTo ensure that a variable is regarded as a factor, and then to get its levels in the order we would like, we use the factor() function.\nIn the following chunk, factor() is used to designate both the supplement and diet columns of the data set as factors, and the level order of each is specified, with control coming first for supplement and barley coming first for diet.\n\n# This line of code designates the supplement and diet columns of weights as factors, orders the levels of these factors as required and saves the result under the original name.\nweights &lt;- weights |&gt;\n  mutate(supplement = factor(supplement, levels = c(\"control\",\"agrimore\", \"supergain\", \"supersupp\"))) |&gt;\n  mutate(diet = factor(diet, levels = c(\"barley\", \"oats\", \"wheat\")))\n\n# check that this worked\nglimpse(weights)\n\nRows: 48\nColumns: 3\n$ supplement &lt;fct&gt; supergain, supergain, supergain, supergain, control, contro…\n$ diet       &lt;fct&gt; wheat, wheat, wheat, wheat, wheat, wheat, wheat, wheat, whe…\n$ gain       &lt;dbl&gt; 17.37125, 16.81489, 18.08184, 15.78175, 17.70656, 18.22717,…\n\n# check the level order of each factor - does the 'reference' level come first?\nlevels(weights$supplement)\n\n[1] \"control\"   \"agrimore\"  \"supergain\" \"supersupp\"\n\nlevels(weights$diet)\n\n[1] \"barley\" \"oats\"   \"wheat\" \n\n\nDo you see how the variable types of the supplement and diet columns have been changed to &lt;fct&gt;? It worked!\nTo get control to be the reference level of supplement we needed to force the issue in this way. If we hadn’t then agrimore would have been regarded as such, since it is alphabetically the first among the levels of supplement. We didn’t need to do this for diet, since the stipulated ordering of the levels is just the alphabetical order and so we would have had that by default anyway. Sometimes, though, it doesn’t hurt to throw in a little redundancy for the sake of clarity.\nSo, now we have control as the reference level for supplement and barley as the reference level for diet. Now we can see more easily in our analysis what difference is made to weight gain when we change diet or supplement or both from a ‘business as usual’ combination of a barleydiet and the control supplement.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#summarise-the-data",
    "href": "ANOVA_two_way_with_model_simplification.html#summarise-the-data",
    "title": "9  Two-way ANOVA with model simplification",
    "section": "9.7 Summarise the data",
    "text": "9.7 Summarise the data\nOur question is a difference question: is there evidence from the data that using this or that diet in combination with this or that supplement makes a difference to growth? For an answer to this we will end up doing a 2-way ANOVA including the possibility of an interaction, then, as we will see, a simpler ANOVA that ignores the possibility of the interaction. All well and good, but before we go to those lengths, we do something more basic: we calculate the mean and standard error of the mean for each of the twelve combinations of diet and supplement.\nThere isn’t a function in base R with which we can calculate standard error of the mean directly, but we can do so knowing the standard deviation of the sample \\(\\text{SD}\\) (using sd()) and the sample size \\(n\\) (using n()) using this formula:\n\\[ \\text{SE}=\\frac{SD}{\\sqrt{n}}\\]\n\n# we use the group_by() and summarise() functions from dplyr (the package within tidyverse for data manipulation)\ngrowth_summary &lt;- weights |&gt;\n  group_by(diet, supplement) |&gt;\n  summarise(mean_gain = mean(gain), se_gain = sd(gain)/sqrt(n())) |&gt;\n  ungroup()\n\n# if we type the name of an object, it gets printed out for us\ngrowth_summary |&gt; kable()\n\n\n\n\n\n\ndiet\nsupplement\nmean_gain\nse_gain\n\n\n\n\nbarley\ncontrol\n23.29665\n0.7032491\n\n\nbarley\nagrimore\n26.34848\n0.9187479\n\n\nbarley\nsupergain\n22.46612\n0.7710644\n\n\nbarley\nsupersupp\n25.57530\n1.0599015\n\n\noats\ncontrol\n20.49366\n0.5056319\n\n\noats\nagrimore\n23.29838\n0.6131592\n\n\noats\nsupergain\n19.66300\n0.3489388\n\n\noats\nsupersupp\n21.86023\n0.4132292\n\n\nwheat\ncontrol\n17.40552\n0.4604420\n\n\nwheat\nagrimore\n19.63907\n0.7099260\n\n\nwheat\nsupergain\n17.01243\n0.4852821\n\n\nwheat\nsupersupp\n19.66834\n0.4746443\n\n\n\n\n\n\n\nNote the ordering of the diet and supplement levels in their respective columns: just what we have imposed!",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#plot-the-data",
    "href": "ANOVA_two_way_with_model_simplification.html#plot-the-data",
    "title": "9  Two-way ANOVA with model simplification",
    "section": "9.8 Plot the data",
    "text": "9.8 Plot the data\nThe next step, as so often before we launch into actual statistics, is to plot the data in a way that sheds light on the question we have. Here, we can use the use the means and standard errors of the mean that we have just calculated to produce a useful kind of line plot that in this context is often referred to as an interaction plot:\n\ngrowth_summary |&gt;\n  ggplot(aes(x = supplement,y = mean_gain, colour = diet, group = diet)) +\n  geom_point(size = 2) +\n  geom_line() +\n  geom_errorbar(aes(ymin = mean_gain - se_gain, ymax = mean_gain + se_gain), width = 0.1) +\n  labs(x = \"Supplement\",\n       y = \"Mean weight gain\") +\n  scale_fill_brewer() +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nNote that on this plot the error bars are standard errors of the mean. Any caption to a figure that contains error bars should explain what those error bars mean. In particular, it should say whether they are standard deviations of the sample, standard errors of the mean or confidence intervals. These are all different from each other. A good explanation of the difference is given by (Cumming, Fidler, and Vaux 2007)\nThis interaction plot is useful in that we see that both diet and supplement have an effect on growth and that the effect of one is altered little by the value of the other, the result of which is that the lines are more or less parallel. This suggests that we have main effects of both diet and supplement, but little or no interaction between them.\n\n9.8.1 Questions\nWhat could the line plot look like if:\n\nThere were no main effect of both diet and supplement, and no interaction\nThere were a main effect of diet, no main effect of supplement and no interaction?\nThere were no main effect of diet, a main effect of supplement and no interaction?\nThere were main effects of both and an interaction between them?\n\nThe plots tell you a great deal about what main effects and/or interactions there may be.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#anova",
    "href": "ANOVA_two_way_with_model_simplification.html#anova",
    "title": "9  Two-way ANOVA with model simplification",
    "section": "9.9 ANOVA",
    "text": "9.9 ANOVA\nNow for the actual statistical test. We will conduct a two-way ANOVA, which will look to see if there is evidence that either diet or supplement or both affect growth rate (the so-called main effects), and if the effect of one depends on the nature of the other (the so-called interaction).\nThe null hypothesis is that neither has any main effect and that there is no interaction.\nNow we can use either of the functions aov() or lm() to carry out a factorial ANOVA (the choice affects only whether we get an ANOVA table or a list of parameter estimates as the default output from summary().). Here, we will use lm(), partly because we would also use it for one-way ANOVAs and linear regression, and to do so here reminds of the common mathematical machinery that underlies all these methods.\nWe estimate parameters for the main effects of each level of diet and each level of supplement, plus terms for the interaction between diet and supplement.\nThe interaction degrees of freedom are the product of those for diet and supplement ie (3-1) x (4-1) = 6.\nThe model is:\ngain ~ diet + supplement + diet:supplement\nwhich can be written more simply using the asterisk notation as:\ngain ~ diet * supplement\n\n9.9.1 Construct the model\nFirst we construct the model using lm() and store the outputs of all the maths that `lm() does in an object called model0:\n\nmodel0 &lt;- lm(gain ~ diet * supplement, data = weights)\n\n\n\n9.9.2 Do we reject the null hypothesis?\nTo get an overall picture, we first use anova() to see if there is evidence to reject the null\n\nanova(model0)\n\nAnalysis of Variance Table\n\nResponse: gain\n                Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet             2 287.171 143.586 83.5201 2.999e-14 ***\nsupplement       3  91.881  30.627 17.8150 2.952e-07 ***\ndiet:supplement  6   3.406   0.568  0.3302    0.9166    \nResiduals       36  61.890   1.719                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA table shows that there a main effect of both diet and supplement (p&lt;0.001 in both cases), but that there is no hint of an interaction between diet and supplement (p = 0.917). Does that tally with what you see in the interaction plot? Clearly therefore, the effects of diet and supplement are merely additive (ie whichever level of one you have it does not affect the impact on growth of whichever level of the other you choose).\nThe ANOVA table does not show us effect sizes or allow us to work out which if any of the levels of the two factors are significantly different. For this, summary() is more useful:\n\nsummary(model0)\n\n\nCall:\nlm(formula = gain ~ diet * supplement, data = weights)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.48756 -1.00368 -0.07452  1.03496  2.68069 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   23.2966499  0.6555863  35.536  &lt; 2e-16 ***\ndietoats                      -2.8029851  0.9271390  -3.023  0.00459 ** \ndietwheat                     -5.8911317  0.9271390  -6.354 2.34e-07 ***\nsupplementagrimore             3.0518277  0.9271390   3.292  0.00224 ** \nsupplementsupergain           -0.8305263  0.9271390  -0.896  0.37631    \nsupplementsupersupp            2.2786527  0.9271390   2.458  0.01893 *  \ndietoats:supplementagrimore   -0.2471088  1.3111726  -0.188  0.85157    \ndietwheat:supplementagrimore  -0.8182729  1.3111726  -0.624  0.53651    \ndietoats:supplementsupergain  -0.0001351  1.3111726   0.000  0.99992    \ndietwheat:supplementsupergain  0.4374395  1.3111726   0.334  0.74060    \ndietoats:supplementsupersupp  -0.9120830  1.3111726  -0.696  0.49113    \ndietwheat:supplementsupersupp -0.0158299  1.3111726  -0.012  0.99043    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.311 on 36 degrees of freedom\nMultiple R-squared:  0.8607,    Adjusted R-squared:  0.8182 \nF-statistic: 20.22 on 11 and 36 DF,  p-value: 3.295e-12\n\n\nThis is a complex model as there are 12 estimated parameters: 6 main effects and 6 interactions. Notice that although the ‘controls’ for diet and supplement (barley and control) do not appear to be in the table, they are there really, in the first row.\nThe value 23.30 kg in the first row of the Estimate column on the left, labelled ‘Intercept()’ gives us the actual weight gain outcome for the combination of the two control levels, barley as diet and control as supplement. Check that this value tallies with what is shown in summary tables above, and in the interaction plot.\nThe weight gain values for all the other combinations of the levels of each factor are given as differences from this reference level.\nSo for example in row two, where diet is changed from barley to oats but supplement is still control, the value in the table is -2.8. This means that the weight gain when the diet is changed to oats but the supplement left as the control is 2.80 kg less than the reference value, and so must be 23.30-2.80 = 20.50 kg. This agrees with the value in the summary table of mean values that was calculated above, and tallies with the interaction plot.\nIn row seven we see that the effect of the interaction between the diet oats and the supplement agrimore is - 0.247. This means that on going from the reference levels of barley and control, for which the gain is 23.30, the change in gain is not just the sum of the two main effects (-2.80 for switch of diet to oats and +3.05 for switch of supplement to agrimore, but is modified by their interaction, of size - 0.247. Hence the mean gain for a diet of oats and a supplement of agrimore is the intercept value plus the sum of the two main effects, plus the interaction term: 23.297 - 2.803 + 3.052 - 0.247 = 23.299)\nSee if you can tally the other effect values in the summary table with the mean values given in table above and in the interaction plot for other combinations of diet and supplement.\nHere is a table to help you interpret the output of the summary() function.\n\n\n\n\n\nterm\nmeaning\ntype_of_effect\nestimate\nabsolute_value\np_value\nsignificance\n\n\n\n\n(Intercept)\nbarley + control\nMain effect\n23.30\n23.30\n&lt;0.001\n***\n\n\ndietoats\noats + control\nMain effect\n-2.80\n20.49\n&lt;0.01\n**\n\n\ndietwheat\nwheat + control\nMain effect\n-5.89\n17.41\n&lt;0.001\n***\n\n\nsupplementagrimore\nbarley + agrimore\nMain effect\n3.05\n26.35\n&lt;0.01\n**\n\n\nsupplementsupergain\nbarley + supergain\nMain effect\n-0.83\n22.47\nn.s.\n\n\n\nsupplementsupersupp\nbarley + supersupp\nMain effect\n2.28\n25.58\n0.019\n*.\n\n\ndietoats:supplementagrimore\noats + agrimore\nInteraction\n-0.25\n23.05\nn.s.\n\n\n\ndietwheat:supplementagrimore\nwheat + agrimore\nInteraction\n-0.82\n22.48\nn.s.\n\n\n\ndietoats:supplementsupergain\noats + supergain\nInteraction\n0.00\n23.30\nn.s.\n\n\n\ndietwheat:supplementsupergain\nwheat + supergain\nInteraction\n0.44\n23.73\nn.s.\n\n\n\ndietoats:supplementsupersupp\noats + supersupp\nInteraction\n-0.91\n22.38\nn.s.\n\n\n\ndietwheat:supplementsupersupp\nwheat + supersupp\nInteraction\n-0.02\n23.28\nn.s.\n\n\n\n\n\n\n\n\nThe output of the summary() function re-emphasises that none of the interaction terms are significant. It also suggests that a minimum adequate model will contain 5 parameters: an intercept, which just means that there is non-zero growth when the diet and supplement are the reference values, a difference from that growth due to changing the diet to oats, a difference due to changing it towheat, a difference due to changing the supplement to agrimore while keeping barley as the diet, and a difference due to changing the supplement instead to suppersupp..",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#model-simplification",
    "href": "ANOVA_two_way_with_model_simplification.html#model-simplification",
    "title": "9  Two-way ANOVA with model simplification",
    "section": "9.10 Model Simplification",
    "text": "9.10 Model Simplification\nGiven the results of the full interaction model, we begin model simplification by leaving out the interaction terms, to leave us with an additive model:\n\nmodel_1 &lt;- lm(gain ~ diet + supplement, data = weights)\nsummary(model_1)\n\n\nCall:\nlm(formula = gain ~ diet + supplement, data = weights)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.30792 -0.85929 -0.07713  0.92052  2.90615 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          23.4263     0.4408  53.141  &lt; 2e-16 ***\ndietoats             -3.0928     0.4408  -7.016 1.38e-08 ***\ndietwheat            -5.9903     0.4408 -13.589  &lt; 2e-16 ***\nsupplementagrimore    2.6967     0.5090   5.298 4.03e-06 ***\nsupplementsupergain  -0.6848     0.5090  -1.345 0.185772    \nsupplementsupersupp   1.9693     0.5090   3.869 0.000375 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.247 on 42 degrees of freedom\nMultiple R-squared:  0.8531,    Adjusted R-squared:  0.8356 \nF-statistic: 48.76 on 5 and 42 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#check-the-validity-of-the-additive-model",
    "href": "ANOVA_two_way_with_model_simplification.html#check-the-validity-of-the-additive-model",
    "title": "9  Two-way ANOVA with model simplification",
    "section": "9.11 Check the validity of the additive model",
    "text": "9.11 Check the validity of the additive model\nWe ought to pause here for a moment and just check that we are OK to go ahead and analyse our data using a general linear model (of which ANOVA is an example, linear regression and t-tests being others). We will use autoplot() from the ggfortify package, which gives us the standard four diagnostic plots.\n\nautoplot(model_1) + theme_cowplot() # autoplot() is from the ggfortify package.\n\n\n\n\n\n\n\n\nWell, that all looks fine. In particular, from the top-left figure we see that the variance of the residuals is more or less constant and from the top-right figure, the quantile-quantile plot, we get a pretty good approximation of a straight line which tells us that the residuals are more or less normally distributed. These are two key assumptions that must be at least approximately satisfied by data if it is going to make any sense to use a linear model to analyse it. We won’t discuss here the other two diagnostic plots, but they look fine too. So we are good to go using ANOVA with this data.\nBack to interpreting the output of the ANOVA:\nIt is clear that we need to retain all three levels of diet since the effect values of each differ from each other by an amount that is several times the standand errors, so that t &gt;&gt; 1. It is not clear that we need all the levels of supplement, however. supersupp is not obviously different from agrimore (difference = -0.727 with standard error = 0.509), yet both are clearly different from control. However supergrain is not obviously different from control (difference = -0.68, error = 0.509). Hence we are tempted to try a new model with just two levels of the factor supplement which we might sensibly call “best”, by which we mean agrimore or supersupp, and “worst” by which we mean control or supergrain. We’ll name this new factor supp2.\nThis code chunk amends the weights data frame by adding a new column to it called supp2 in which the values are either best if the supplement is agrimore or supersupp, or worst if the supplement is either of the other two\n\nweights &lt;- weights |&gt;\n  mutate(supp2 = ifelse(supplement %in% c(\"agrimore\", \"supersupp\"), \"best\", \"worst\"))\nglimpse(weights)\n\nRows: 48\nColumns: 4\n$ supplement &lt;fct&gt; supergain, supergain, supergain, supergain, control, contro…\n$ diet       &lt;fct&gt; wheat, wheat, wheat, wheat, wheat, wheat, wheat, wheat, whe…\n$ gain       &lt;dbl&gt; 17.37125, 16.81489, 18.08184, 15.78175, 17.70656, 18.22717,…\n$ supp2      &lt;chr&gt; \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"wors…\n\n\nIf we calculate the means and standard errors for weight gain under each diet for each of the two new classifications of supplement, and then plot them, we get this new interaction plot:\n\nweights |&gt;\n  group_by(diet, supp2) |&gt;\n  summarise(mean_gain = mean(gain), se_gain = sd(gain)/sqrt(n())) |&gt;\n  ungroup() |&gt;\n\n  ggplot(aes(x = supp2,y = mean_gain,colour = diet, group=diet)) +\n  geom_point(size=2) +\n  geom_line() +\n  geom_errorbar(aes(ymin = mean_gain-se_gain, ymax = mean_gain + se_gain), width=0.1) +\n  labs(x = \"Supplement\",\n       y = \"Mean weight gain\") +\n  scale_fill_brewer() +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nFrom this we can see that diet clearly makes a difference to weight gain, since the three lines are separated by a distance much larger than the standard errors, and also that the best supplement clearly makes a difference since there is a consistent drop on going from ‘best’ to ‘worst’, again by an amount that is much larger than the error bars, and there is clearly no interaction between diet and supplement, since the lines are parallel within the wiggle-room allowed by the error bars, which means that the effect of diet does not depend on supplement, and the effect of supplement does not depend on diet.\nNow we will make the simpler model, calling it model_2 (for comparison with the first additive model, model_1)\n\n# additive model whee the supplements have been condensed from four to two: best and worst\nmodel_2 &lt;- lm(gain ~ diet + supp2, data = weights)\n\nand then compare the two additive models:\n\nanova(model_1, model_2)\n\nAnalysis of Variance Table\n\nModel 1: gain ~ diet + supplement\nModel 2: gain ~ diet + supp2\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     42 65.296                           \n2     44 71.284 -2   -5.9876 1.9257 0.1584\n\n\nWhen we use anova() in this way it is testing the explanatory power of the second model against that of the first ie how much of the variance in the data does each explain. Its null hypothesis is that both models explain just as much of the variance as the other.\nThe simpler model has saved two degrees of freedom and is not significantly different in explanatory power than the more complex model (p = 0.158). Hence this is a better candidate as a minimal adequate model. All the parameters are significantly different from zero and from each other.\n\nsummary(model_2)\n\n\nCall:\nlm(formula = gain ~ diet + supp2, data = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6716 -0.9432 -0.1918  0.9293  3.2698 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  25.7593     0.3674  70.106  &lt; 2e-16 ***\ndietoats     -3.0928     0.4500  -6.873 1.76e-08 ***\ndietwheat    -5.9903     0.4500 -13.311  &lt; 2e-16 ***\nsupp2worst   -2.6754     0.3674  -7.281 4.43e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.273 on 44 degrees of freedom\nMultiple R-squared:  0.8396,    Adjusted R-squared:  0.8286 \nF-statistic: 76.76 on 3 and 44 DF,  p-value: &lt; 2.2e-16\n\n\nIn this table,\n\nline one (Intercept) tells us that the mean weight gain when on the barley diet and best supplement is 25.76 kg\nline two dietoats tells us that there is a significant drop in weight gain of 3.1 kg when diet is changed to oats.\nline three dietwheat tells us that there is a significant drop in weight gain of 5.99 kg when diet is changed to wheat.\nline four supp2worst tells us that there is a significant drop in wight gain of 2.68 kg when supplement is changed to worst.\n\nIn all cases, p&lt; 0.001, as indicated not only by the number in the Pr(&gt;|t|) column, but also by the ‘***’ in the right-most column of the table.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#reporting-the-results",
    "href": "ANOVA_two_way_with_model_simplification.html#reporting-the-results",
    "title": "9  Two-way ANOVA with model simplification",
    "section": "9.12 Reporting the results",
    "text": "9.12 Reporting the results\nWe have now reduced our initial 12 parameter model to a four parameter model that is much more tractable and easier to communicate. Our advice would be that for maximum weight gain a diet of barley with a supplement of agrimore or supersupp would be best.\nIf we were reporting this as a statistical test, we might say something like: A diet of barley with a supplement of agrimore or supersupp was to offer significant improvements over alternatives. There was no evidence of any interaction between diet and supplement. (ANOVA 2-way, F3,44 = 76.76, p &lt; 0.001)\n\n\n\n\nCumming, Geoff, Fiona Fidler, and David L. Vaux. 2007. “Error Bars in Experimental Biology.” Journal of Cell Biology 177 (1): 7–11. https://doi.org/10.1083/jcb.200611141.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "chi-square.html",
    "href": "chi-square.html",
    "title": "10  Chi-squared analysis of count data",
    "section": "",
    "text": "10.1 Chi-square goodness of fit test\nFor this chapter you will find it useful to have this RStudio project folder if you wish to follow along and try out the exercises at the end.\nA chi-square analysis is used when our data are in the form of raw counts for two or more categorical groups eg pea plants with either yellow peas or green peas, survival rate of mice if they took drug A or took drug B, etc. Each independent observation must definitely belong to either one group or the other, and there are no replicates. That is, for each category we just have one count.\nWhat we do is compare the counts we got to some expected value according either to chance or to some prior theory.\nFor example:\nIn a chi-square ‘goodness of fit’ test, we are testing data where we have a number of counts for each of two or more possible outcomes of some procedure (heads/tails, dice scores etc). We have an idea of how these counts should be distributed under some null hypothesis (the coin is fair, the dice is fair, genetic inheritance works in this or that way etc). The chi-square goodness of fit test tests how likely it is we would have got the counts we actually got if that null hypothesis were correct. We are testing how well our actual counts ‘fit’ the expected values.\nIn a typical software implementation of the test, such as in R, we give it the counts we actually got for each possible outcome and also the expected proportion for each outcome. The test then gives us a p-value, a probability, for how likely it is that we would have got the counts we actually got, or counts even further from the null hypothesis, if that null hypothesis were correct. If this p-value is too small, and by that we usually mean less than 0.05, then we reject the null hypothesis.",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#chi-square-goodness-of-fit-test",
    "href": "chi-square.html#chi-square-goodness-of-fit-test",
    "title": "10  Chi-squared analysis of count data",
    "section": "",
    "text": "10.1.1 Example\nSuppose we have crossed pea plants that were all heterozygous for yellow/green pea colour. In the F1 generation we get 176 offspring , of which 130 were yellow and 46 were green.\nThe data here are raw counts, and an individual pea plant offspring contributes either to the yellow count or to the green count, but not to both.\nOur expected counts of yellow and green are found by simply dividing the total count of offspring, 176, in the ratio 3:1, giving us an expected 132 yellow pea plants and an expected 44 green pea plants in the offspring F1 generation.\n\n\n10.1.2 Doing the chi-square test in R\nWhat we do in R is use the chisq.test() function to see how likely it is we would have got counts of 130 and 46 if the null hypothesis, with its expected counts in the ratio 3:1, were true.\nWe do it like this:\nchisq.test(c(130,46),p=c(0.75,0.25))\nThere are two arguments. The first is the counts we got, which we enter as a ‘vector’ c(z,y,....), so we write c(130,46). The second is a vector of the proportions we expect for the two counts, where these proportions should add up to one. So for our expected 3:1 ratio we enter c(0.75,0.25).\nLet’s do it: type the above function into the console window (bottom left). You will get an output something like this:\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  c(130, 46)\nX-squared = 0.12121, df = 1, p-value = 0.7277\n\n\nThis output is typical of tests done in R. We get the ‘test statistic’ whose name varies depending on the test. Here it is called X-squared, pronounced chi-squared. This is a number that the test calculates, based on the data you have given it. For the most part, we don’t need to worry about how it does that. Then there is the p-value, which is the probability of getting this test statistic if the null hypothesis were true.\nIn this case, we see that the p-value is 0.73, which is large. We could very plausibly have got yellow:green numbers of 130 and 46 if the null hypothesis were true, so we cannot reject that null hypothesis. In other words, our data are consistent at the 5% significance level with the predictions of simple Mendelian inheritance.\n\n\n10.1.3 Reporting the result in English\nIn English, we might report this result as:\nWe found counts of 130 yellow plants and 46 green plants, which are consistent at the 5% significance level with the predictions of Mendelian inheritance (chi-squared test, X-squared = 0.12, p=0.73).\nNote that we do not say we have proved Mendelian inheritance to be correct. We haven’t. We never prove things in science. We haven’t said anything about the truth of the null hypothesis. All we can say is whether our data are or are not consistent with the null hypothesis. In this case they are. We then report the test we used and the values of the test statistic and p-value. Other tests might give you other details to report too.\n\n\n10.1.4 Exercises\nExercise 1\nSuppose you tossed a fair coin 100 times and got 45 heads and 55 tails.\n\nUnder a null hypothesis that the coin is fair, what would the expected numbers of heads and tails be?\n\nYou use R to do a chi-square test of that null hypothesis. Here is the code to do that and the output it would give:\n\nchisq.test(c(45,55),p=c(0.5,0.5)) # we could leave out the second argument here\n\n\n    Chi-squared test for given probabilities\n\ndata:  c(45, 55)\nX-squared = 1, df = 1, p-value = 0.3173\n\n\n\nWhat do you conclude?\nHow would you report the result?\n\nExercise 2\nSuppose someone told you that the competence of scientists was linked to their astrological zodiac sign. I won’t name all of these, but there are twelve of them: Pisces, Scorpio, Cancer etc. To test this hypothesis, you spend a lot of time on Primo and identify 240 scientists, currently active, that have each published at least five papers in high impact journals in the last year. All of these people, you presume, are successful scientists. You write to each of them and ask them their date of birth. Amazingly(!), all of them respond. You then assign each of them to a zodiac sign according to their birth date and get the following counts for each sign:\nIn this code chunk we have typed out the counts and collected them as a vector, using the function `c()`. we have saved this under the name stars.\n\nstars&lt;-c(22,20,17,22,20,19,18,21,19,22,23,17)\n\n\nWhat would be a suitable null hypothesis in this investigation?\nWhat proportion of the total count would we expect for each star sign if this null were true?\nThe data meet the criteria required for use of a chi-square goodness of fit test. How can we tell?\nUse the chisq.test() function to implement this test.\nOn the basis of the output of the test, do you reject the null hypothesis?\nReport the result of the test in plain English.\n\n\n\n10.1.5 Solutions\nSolution 1\nThe expectation is that half the outcomes would be heads and half would be tails.\nThe null hypothesis of this test is that heads and tails are equally likely, ie that the coin is fair. Under this null hypothesis the expected outcome is 50 heads and 50 tails. From the output of the R code we see that the p-value, the probability of getting an outcome as far or further from that, is 0.317. That is pretty high. Would you do anything if you knew that the probability of a bad (or worse) outcome was 0.317? In particular, this p-value is greater than 0.05, so we cannot reject the null hypothesis that the coin is fair. That is, even with a fair coin it is not at all unlikely that you would get head/tail numbers as different from 50/50 as 45/55 if you tossed the coin 100 times. That will happen about 1/3 of the time if you repeatedly do trials where you toss the coin 100 times.\nTo report this result, you might say something like\nFrom 100 coin tosses we got 45 heads and 55 tails. These counts are consistent at the 5% significance level with the coin being fair (chi-squared test, X-squared = 1, p=0.317).\nSolution 2\n\nH0: There is no association between the astrological star sign of a researcher and their success in science (who knew?)\nOne twelfth for each sign ie a researcher is as likely to have one star sign as any other.\nThese are count data, there are at least five counts for every sign and the counts are independent - any individual researcher only contributes to one of the twelve counts.\nNote that we do not need to include the second p=... argument in this case since the default presumption, that all proportions are equal, is true here.\n\n\nchisq.test(stars)\n\n\n    Chi-squared test for given probabilities\n\ndata:  stars\nX-squared = 2.3, df = 11, p-value = 0.9971\n\n\n\nWe see that the p-value is almost one so we emphatically do not reject the null hypothesis.\nWe find no evidence that star sign affects success in science (X-sq=2.29, df = 11, p=0.997)\n\nNote the degrees of freedom that is reported: df = 11. The degrees of freedom is the number of independent pieces of information. Here, given that we know the total number of researchers, only eleven of the individual counts are independent. Once they are known, the twelfth can be calculated.",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#two-way-chi-square-analysis-test-of-independence",
    "href": "chi-square.html#two-way-chi-square-analysis-test-of-independence",
    "title": "10  Chi-squared analysis of count data",
    "section": "10.2 Two-way Chi square analysis: test of independence",
    "text": "10.2 Two-way Chi square analysis: test of independence\nAdapted from Chapter 5: Beckerman, Childs and Petchey: Getting Started with R\n\nFor R code to implement this section, scroll down to the next section *\n\nA common scenario where we have with count data is that there are two explanatory factors each with two or more levels that enable us to classify the data. This can happen when something is either true or not true, and a test for for this truth gives either a positive or a negative result. We might want to know if the test to determine truth was any better than flipping a coin: is there some association between what the truth is (eg I do or do not have a disease) and what the test says about that (testing positive or negative for the disease). In this example our data would be counts of people in each of four categories: have the disease / test positive, have the disease / test negative, do not have the disease / test positive and do not have the disease / test negative.\n\n10.2.1 Example - red and black ladybirds\nWe are going to analyse a scenario of this type to see if there is evidence for an association between two factors. Suppose we have some count data of ladybirds found in both an industrial and a rural location. In each location, some of the ladybirds are red and some are black. We would like to test for whether there is an association between the ladybird colour and its location. If there isn’t then we would expect the proportion of black to red ladybirds to be roughly the same in both habitats. If there is, then we would expect the proportions to be different, meaning that knowing the habitat would tell us something about the likelihood of a ladybird found there being black or red. That is way of saying that the colour of the ladybirds would not be independent of the location.\nBehind this the research purpose might be to investigate whether matching of morphological colour of the ladybirds to the prevalent colour of the environment confers an evolutionary advantage. If it does then we would expect there to be an association between morphological colour and environment so that the proportion of black to red ladybirds would be higher in a grimy industrial setting than in it would be in a rural setting.\nA Chi-Square contingency analysis can be used to investigate this. This type of analysis is used when you have\n\nCount data - for example, how many red ladybirds in a rural setting, how many in an industrial setting, how many black ladydbirds in each of the settings?\nEnough count data - typically at least 5 individuals for each combination of the levels in question, which would be rural/red, rural/black, industrial/red and industrial/black in this case.\nIndependent counts - each ladybird contributes to only one sub-total. For example, if it is red and found in a rural location, then it contributes to the count of red ladybirds found in a rural location, and not to any other sub-total, such as black ladybirds found in a rural location.\n\n\n10.2.1.1 Hypotheses\nWhat do you think a suitable hypothesis should be for this investigation, and what would the corresponding null hypothesis be?\n\nThe null hypotheses could be: H0: There is no association between habitat and ladybird colour. This means that whatever the proportion is of black to red ladybirds, it is the same in both habitats.\n\nThe alternate hypothesis could be: H1: There is an association between habitat and ladybird colour.\n\n\n# Load packages we need\nlibrary(tidyverse)\nlibrary(here)\nlibrary(cowplot) # this makes your plots look nicer\nlibrary(kableExtra)\ntheme_set(theme_cowplot()) # this sets the cowplot theme to be the default theme for any plots we make..\n\n\n# Import the data\nfilepath&lt;-here(\"data\",\"ladybirds_morph_colour.csv\")\nlady&lt;-read_csv(filepath)\n#glimpse(lady)\n\n\n\n10.2.1.2 The data\nThe data consist of counts of the number of ladybirds of each colour that were observed in 5 rural sites and 5 industrial sites. These data are in tidy form, with one variable per column. There are 20 rows, with each row containing the count of either red or black ladybirds found at a given site.\n\n\n# A tibble: 20 × 4\n   Habitat    Site  morph_colour number\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1 Rural      R1    black            10\n 2 Rural      R2    black             3\n 3 Rural      R3    black             4\n 4 Rural      R4    black             7\n 5 Rural      R5    black             6\n 6 Rural      R1    red              15\n 7 Rural      R2    red              18\n 8 Rural      R3    red               9\n 9 Rural      R4    red              12\n10 Rural      R5    red              16\n11 Industrial U1    black            32\n12 Industrial U2    black            25\n13 Industrial U3    black            25\n14 Industrial U4    black            17\n15 Industrial U5    black            16\n16 Industrial U1    red              17\n17 Industrial U2    red              23\n18 Industrial U3    red              21\n19 Industrial U4    red               9\n20 Industrial U5    red              15\n\n\nThe total counts for red and black ladybirds observed in industrial and rural settings are shown below:\n\n# Calculate the totals of each colour in each habitat.\ntotals&lt;- lady |&gt;\n  group_by(Habitat,morph_colour) |&gt;\n  summarise (total.number = sum(number))\n# totals |&gt;\n#   kbl() |&gt;\n#   kable_styling(full_width=FALSE)\ntotals\n\n# A tibble: 4 × 3\n# Groups:   Habitat [2]\n  Habitat    morph_colour total.number\n  &lt;chr&gt;      &lt;chr&gt;               &lt;dbl&gt;\n1 Industrial black                 115\n2 Industrial red                    85\n3 Rural      black                  30\n4 Rural      red                    70\n\n\n\n\n10.2.1.3 Plot the data.\nFrom these totals we can create a bar chart:\n\n# plot the data, with sensible colours\ntotals |&gt;\n  ggplot(aes(x = Habitat,y = total.number,fill=morph_colour))+\n  geom_col(position='dodge') +\n  labs(x=\"Habitat\",\n       y=\"Count\",\n       fill= \"Colour\") +\n  scale_fill_manual(values=c(black='#312626',red='#da1717')) + # this line manually sets the fill colours for us\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\n\n10.2.1.4 Interpret the graph before we do any ‘stats’\nLook at the plot - does it look as though the proportion of black to red ladybirds is the same in the two habitats? Do you expect to retain or to reject the null hypothesis, which says that there is no association between habitat and ladybird colour, and hence that the proportions are the same?\nA chi-square test of independence will enable us to determine how likely it is that we would have got proportions of black to red as different as or more different than they actually are if the null hypothesis were true.\n\n\n10.2.1.5 The Chi-square test\nTo do the chi square test, it helps to set out our count data as a 2 x 2 table of total counts:\n\nlady.mat&lt;-xtabs(number~Habitat + morph_colour, data=lady)\nlady.mat\n\n            morph_colour\nHabitat      black red\n  Industrial   115  85\n  Rural         30  70\n\n\nThis kind of table is sometimes called a contingency table.\nWhen we give these numbers to some statistical software such as R and ask it to carry out a ‘chi-square test’ it will use the data to calculate a ‘test statistic’ \\(X^2\\) by comparing the actual counts of the ladybirds in the table above with their expected counts under the null hypothesis. The further the actual counts are from their expected values, on the whole, the bigger this test statistic will be. For the gory (they are not that gory!) details on how this is done, see section 4 below but do note that, while these are interesting, if you find that kind of thing interesting, as I do, you do not need to be familiar with them to be able to apply a chi-square test. What you do need to know is when it is OK to use one, and when it is not, as is true for any statistical test.\nWe’ll turn to that issue now:\nProviding a number of conditions are met by the data (principally, that they are count data, that all the cell values are greater than or equal to about five and that they are all independent ie any ladybird contributes to the count of only one cell), this test statistic \\(X^2\\) has a so-called ‘chi-squared’ distribution. This is a known mathematical distribution, which makes it possible to calculate the probability that the statistic would be as big as it is, or bigger, if the null hypothesis were true. We call this probability the p-value.\nThis is generally how statistical tests work. They take your data and use it in a carefully chosen way to calculate some number that in general is called a test statistic but which is referred to by different names when calculated for particular tests. How it is calculated depends on the test and these days we never have to manually do the calculations ourselves. That’s taken care of by software like R. Providing the data meet certain criteria, the statistic will typically have a known probability distribution. This means that the probability that it will exceed a given value if the null hypothesis is true can be calculated. This probability is the p-value. If the p-value is very small, and by that we typically mean less than 0.05 or 0.01, then we can reject the null hypothesis.\nWhen we run a chi-square test in R on the data in the table above it gives us this as output:\n\nchisq.test(lady.mat)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  lady.mat\nX-squared = 19.103, df = 1, p-value = 1.239e-05\n\n\n\n\n10.2.1.6 Conclusion\nStudy the output of the chi-square test. Note that you are given a test-statistic (here called Chi-squared/X-squared) and a number of degrees of freedom (df) (in some tests you are given more than one of these). This is the number of independent pieces of information used to calculate the test statistic. Lastly, you are a given a p-value. This is the probability that you would have got a chi-squared value as big or bigger than the one you got if the null hypothesis were true. Here the null hypothesis is that there is no association between ladybird colour and location. Put another way, it is, roughly speaking the probability of getting the data you actually got if the null hypothesis were true.\nHere, the p-value is much less than 0.05, so we can safely reject the null hypothesis. The ladybird colour does not appear to be independent of the setting.\nAn appropriate way to report these data would be:\n‘Ladybird colour morphs are not equally common in the two habitats (Chi-sq =19.3, df = 1, p&lt;0.001)’\n\n\n10.2.1.7 Yates continuity correction\nThis is mentioned is the output of the test. What does it mean? It adjusts for the fact that our data are discrete and the chi-square distribution that we are using to calculate the p-values is continuous. That’s it.\n\n\n\n10.2.2 R script\nTo implement this analysis yourself, downloaod the project folder associated with this chapter. Unzip it, put it somewhere sensible on your machine, then, in RStudio, open the project by navigating to it, starting with File/Open Project, then open the chi_square_independence_template.qmd notebook. Use the code provided below to fill in the skeleton code chunks in that notebook, and run each chunk in turn.\nNote that the code provided below is not the only way to do what we want here. You are encouraged to play with it. For example, if you want to see what a particular line does, you can ‘comment it out’ by putting a # at the begining of the line, then running that line again. What difference does it make?\n\n10.2.2.1 Load packages we need\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(cowplot) # this makes your plots look nicer\n\n\n\n10.2.2.2 Import the data and inspect it\n\nfilepath&lt;-here(\"data\",\"ladybirds_morph_colour.csv\") \nlady&lt;-read_csv(filepath) # read the data int an object (in this case, a data frame) called lady\nglimpse(lady) # inspect it.\n\nRows: 20\nColumns: 4\n$ Habitat      &lt;chr&gt; \"Rural\", \"Rural\", \"Rural\", \"Rural\", \"Rural\", \"Rural\", \"Ru…\n$ Site         &lt;chr&gt; \"R1\", \"R2\", \"R3\", \"R4\", \"R5\", \"R1\", \"R2\", \"R3\", \"R4\", \"R5…\n$ morph_colour &lt;chr&gt; \"black\", \"black\", \"black\", \"black\", \"black\", \"red\", \"red\"…\n$ number       &lt;dbl&gt; 10, 3, 4, 7, 6, 15, 18, 9, 12, 16, 32, 25, 25, 17, 16, 17…\n\n\nAre those sensible names for the variables? Is the data tidy?\n\n\n10.2.2.3 Calculate the totals of each colour in each habitat.\nWe will save these totals into a new data frame called totals\n\ntotals&lt;- lady |&gt;\n  group_by(Habitat,morph_colour) |&gt;\n  summarise (total.number = sum(number))\ntotals\n\n# A tibble: 4 × 3\n# Groups:   Habitat [2]\n  Habitat    morph_colour total.number\n  &lt;chr&gt;      &lt;chr&gt;               &lt;dbl&gt;\n1 Industrial black                 115\n2 Industrial red                    85\n3 Rural      black                  30\n4 Rural      red                    70\n\n\nNow that we have these totals we use them to plot a bar chart of the data, using geom_col() withinggplot:\n\n\n10.2.2.4 Plot the data\n\ntotals |&gt;\n  ggplot(aes(x = Habitat,y = total.number,fill=morph_colour))+\n  geom_col(position='dodge') +\n  labs(x=\"Habitat\",\n       y=\"Count\",\n       fill= \"Colour\") +\n  theme_cowplot() # try leaving out this line (but if you do, leave out the final '+' in the line above). What happens?\n\n\n\n\n\n\n\n\n\n\n10.2.2.5 Fix the colours\nThe fill colours we got in the figure above are defaults from R, which does not realise that a factor of interest for us is the actual colour of the ladybirds. We would like the figure to reflect that, so let us make the bars red and black for red and black ladybirds respectively.\n\n# plot the data, with sensible colours\ntotals |&gt;\n  ggplot(aes(x = Habitat,y = total.number,fill=morph_colour))+\n  geom_col(position='dodge') +\n  labs(x=\"Habitat\",\n       y=\"Count\",\n       fill= \"Colour\") +\n  scale_fill_manual(values=c(black='#312626',red='#da1717')) + # this line manually sets the fill colours for us\n  theme_cowplot() +\n  theme(legend.position=\"none\") # this line removes the legend, since we no longer need it\n\n\n\n\n\n\n\n\n\n\n10.2.2.6 Interpret the graph before we do any ‘stats’\nLook at the plot - given these sample data, does it seem plausible that the proportions among the populations of black ladybirds and red ladybirdsare the same in both industrial and rural settings, or not? Do you expect to retain or to reject the null hypothesis?\n\n\n10.2.2.7 Making the contingency table from the data\nPreparation\nWe will use the function chisq.test() to carry out the chi square test. However, this requires a matrix of the total counts and our data is in one column of a data frame, spread over 20 rows, one count per colour per site. We need to convert this data frame into a 2x2 matrix of the total counts of each colour in each setting. We can use the xtabs() function to do this.\n\nlady.mat&lt;-xtabs(number~Habitat + morph_colour, data=lady)\nlady.mat\n\n            morph_colour\nHabitat      black red\n  Industrial   115  85\n  Rural         30  70\n\n\nA matrix of this type is sometimes called a contingency table.\n\n\n10.2.2.8 The actual Chi-square test\nLet’s do it…\n\nchisq.test(lady.mat)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  lady.mat\nX-squared = 19.103, df = 1, p-value = 1.239e-05\n\n\n\n\n10.2.2.9 Conclusion\nStudy the output of the chi-square test. Note that you are given a test-statistic (here called Chi-squared/X-squared), a number of degrees of freedom (df) (in some tests you are given more than one of these). This is the number of independent pieces of information used to calculate the test statistic. Lastly, you are a given a p-value. This is the probability that you would have got a chi-squared value as big or bigger than the one you got if the null hypothesis were true, where here the null hypothesis is that there is no association between ladybird colour and location. Put another way, the p-value is, roughly speaking, the probability of getting the data you got or more extreme data if the null hypothesis were true.\nIf the p-value is small (by which we usually mean less than 0.05) then we rejectthe null hypothesis.\nYou are also told that this test was done with Yates’ continuity correction. All this means is that an adjustment has been made for the fact that our data are discrete and the chi-square distribution that we are using to calculate the p-values is continuous. That’s it. Thank you, R, for doing this, but we don’t need to worry about it.\n\n\n10.2.2.10 Reporting the result\nSelect which of the two following statements would be an appropriate way to report these data, and fill in the missing values.\n\n\n10.2.2.11 Option 1\n‘Ladybird colour morphs are not equally common in the two habitats (Chi-sq =19.3, df = 1, p&lt;0.001)’\n\n\n10.2.2.12 Option 2\n‘We find insufficient evidence to reject the null hypothesis that Ladybird colour morphs are equally distributed in the two habitats (Chi-sq =, df = , p = )’\n\n\n\n10.2.3 Exercises\n\n10.2.3.1 Exercise One\nA researcher investigates whether two species A and B are associated with one another. If one is present at a site, does the other tend to be present, and if one is absent, does the other tend to be absent?. If the species were not associated with one another, then the presence of one would say nothing about the likely presence or absence of the other. Their occurrences would be independent.\nThe researcher goes to 100 sites and finds the following:\n\n\n\nWhat was found\nNumber\n\n\n\n\nA present, B present\n33.\n\n\nA present, B absent\n28.\n\n\nB present, A absent\n12.\n\n\nA absent, B absent\n27.\n\n\n\nThey enter these into a 2 x 2 contingency table in R, as follows:\n\nAB &lt;- matrix(c(33,28,12,27),nrow = 2)\nAB # R calls this a matrix - we will refer to it as a table.\n\n     [,1] [,2]\n[1,]   33   12\n[2,]   28   27\n\n\nThink about what each row and column in this table represents: as we have constructed it, columns relate to A and rows relate to B. The left hand column gives counts of sites where A was present, the right hand column gives counts where it was absent. The top row gives gives counts of sites where B was present, the bottom row gives counts of sites where it was absent.\n\nCreate this table yourself\nIs it valid here to use a chi square test for an association between species A and species B?\nSuppose the answer to 2. is ‘Yes’, Use the table to determine whether there is evidence for an association between species A and species B.\nWhat conclusion woud you reach if all four counts in the table were the same?\nWhat conclusion would you reach if neither species was ever seen in the absence of the other - meaning that the off-diagonal elements of the table would be zero?\n\n\n\n\n10.2.4 Solutions\n\n10.2.4.1 Exercise One\n\nYes, because the data are in the form of counts, each count is independent of the others and no count is less than 5.\nWe use the matrix AB as the argument for chisq.test().\n\n\nchisq.test(AB)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  AB\nX-squared = 4.3312, df = 1, p-value = 0.03742\n\n\nWe find p &lt; 0.05, so we reject the null hypothesis of no association and can say that there is evidence, at the 5% significance level, that species A and B are associated.\n\nLet us create a new matrix, AB_uniform, with all values equal to 25:\n\n\nAB_uniform &lt;- matrix(c(25,25,25,25),nrow = 2)\nAB_uniform\n\n     [,1] [,2]\n[1,]   25   25\n[2,]   25   25\n\nchisq.test(AB_uniform)\n\n\n    Pearson's Chi-squared test\n\ndata:  AB_uniform\nX-squared = 0, df = 1, p-value = 1\n\n\nHere , p = 1: there is no evidence from these data that species A and B are associated.\n\nNow we create a matrix wih both off-diagonal elements equal to zero\n\n\nAB_diagonal &lt;- matrix(c(25,0,0,25),nrow = 2)\nAB_diagonal\n\n     [,1] [,2]\n[1,]   25    0\n[2,]    0   25\n\nchisq.test(AB_diagonal)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  AB_diagonal\nX-squared = 46.08, df = 1, p-value = 1.135e-11\n\n\nIn this case p&lt;0.001. Hence the test tells us that in this case there is evidence that species A and B are strongly associated.",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#the-chi-square-test-explained-optional",
    "href": "chi-square.html#the-chi-square-test-explained-optional",
    "title": "10  Chi-squared analysis of count data",
    "section": "10.3 The Chi-Square Test explained (optional)",
    "text": "10.3 The Chi-Square Test explained (optional)\nYou can skip this section if you are not interested in how the chi-square test works. If you are, read on.\nLet’s recall the number of sightings of each colour of ladybird in each habitat:\n\nlady.mat\n\n            morph_colour\nHabitat      black red\n  Industrial   115  85\n  Rural         30  70\n\n\nIf there were no association between colour and habitat, then we would expect the relative proportions of colour to be independent of habitat. That is they should be the same in both rural and industrial habitats. Looking at the table above we see that two thirds (200 out of 300) of all sightings, regardless of colour, were in an industrial habitat. Hence we would expect that two thirds of all 145 black sightings would be in an industrial habitat. We thus arrive at an ‘expected’ number for sightings of black ladybirds in an industrial habitat to be \\((200/300) \\times 145 = 96.7\\). Similarly, we would expect one third (100 out of 300) of all 155 red sightings to be in a rural habitat, giving an ‘expected’ number for this combination of levels to be \\((100/300)\\times 155 = 51.7\\). More generally, the expected number in an any cell of the table, under the null hypothesis of no association between the two factors is given by\n\\[\\text{expected number}=\\frac{\\text{row total}\\times\\text{column total}}{\\text{grand total}}\\] Using this method, these are the four expected numbers for each combination of levels of the two factors we have:\n\n\n      [,1]   [,2]\n[1,] 96.67 103.33\n[2,] 48.33  51.67\n\n\nTo get a measurement of how far the actual table numbers are from their expected values we can, for each cell, square the difference between the expected and actual values, then divide the result by the expected value, and finally sum the four results that we get. This has the effect of giving equal weight to positive or negative deviations of the observed values from the expected values, and scales each squared deviation so that all four have equal weight in the final sum. The result is the chi-squared test statistic:\n\\[\n\\begin{align*}\nX^2&=\\sum_{i=1}^4\\frac{(O_i-E_i)^2}{E_i}\\\\\n&=\\frac{(115-96.67)^2}{96.67} + \\frac{(85-103.33)^2}{103.33} + \\frac{(30-48.33)^2}{48.33} + \\frac{(70-51.67)^2}{51.67}\\\\\n&=20.189\n\\end{align*}\n\\] Yates continuity correction (digression)\nYou may have noticed that the \\(X^2 = 20.189\\) value calculated above is slightly larger than the value calculated by the chisq.test() function, which found \\(X^2 = 19.096\\). This is because the function uses Yates’ continuity correction. This was suggested by Yates in 1934 to correct for the fact that the \\(X^2\\) statistic we calculate is actually discrete (because we have categorical data) whereas the chi-square distribution is continuous. This means that the value we calculate tends to be too big so that our p-values are too small. The problem is most apparent where we have small numbers and one degree of freedom, which is what you have in a \\(2 \\times 2\\) contigency table such s in the example above. Yates’ fix is quite simple: just amend the \\(X^2\\) statistic to the following:\n\\[\nX^2=\\sum_{i=1}^4\\frac{(\\lvert O_i-E_i \\rvert - 0.5)^2}{E_i}\n\\]\nwhere the vertical lines mean ‘take the absolute value of’, so that |85-103.33| = |-18.33| = 18.33\nThis gives us\n\\[\n\\begin{align*}\nX^2&=\\sum_{i=1}^4\\frac{(\\lvert O_i-E_i \\rvert - 0.5)^2}{E_i}\\\\\n&=\\frac{(|115-96.67|-0.5)^2}{96.67} + \\frac{(|85-103.33|-0.5)^2}{103.33} + \\frac{(|30-48.33|-0.5)^2}{48.33} + \\frac{(|70-51.67|-0.5)^2}{51.67}\\\\\n&=19.096\n\\end{align*}\n\\] which is exactly what the chisq.test() function gives.\nUnder a null hypothesis of no association between habitat and colour all the counts would be the ‘expected’ values, and \\(X^2\\) would be zero. The further away from these values the actual results are, the bigger \\(X^2\\) will be and the more likely it is that we can reject the null. For a sufficiently large value of \\(X^2\\) we will reject the null. To sum up, in general we will reject the null when \\(X^2\\) is large, and fail to reject it when it is small.\nBut how large is large enough to reject the null?\nTo answer this we use the fact that the sampling distribution of \\(X^2\\) is a chi-squared distribution with (in this case) \\((2-1) \\times (2-1) = 1\\) degrees of freedom. The sampling distribution is the distribution you would get if you were to repeat the study multiple times, each time calculating the \\(X^2\\) statistic from the four counts of black/red, industrial/rural. Each time you would get a slightly different value of \\(X^2\\). The spread of those hypothetical values is what we call the sampling distribution. Providing none of the cell values in the tables are too samll (see below) it turns out that this distribution has a known mathematical form, known as a chi-square distribution.\nThe p-value given in a chi-square test is the probability of getting a chi-squared statistic \\(X^2\\) as big as or bigger than the one you actually got. This is the area under the chi-squared distribution with the appropriate number of degrees of freedom to the right of the test-statistic value \\(X^2\\).\n\nxsquared&lt;-19.1\nggplot(data.frame(x = c(0,20)), aes(x = x)) + \n  stat_function(fun = dchisq, args = list(df = 1)) + \n  stat_function(fun = dchisq, args = list(df = 1), xlim = c(xsquared, 20),\n                  geom = \"area\", fill = \"#84CA72\", alpha = .2) +\n  scale_x_continuous(name = \"\", breaks = seq(0, 20,2)) +\n  geom_vline(xintercept=xsquared,linewidth=0.2,colour=\"gray80\") +\n  theme_cowplot()\n\n\n\n\n\n\n\n\n\n10.3.0.1 Why does the test statistic have this distribution?\nTo answer this, let us recognise that in our observed contingency table, where we have N observations altogether, we can think of there being a probability \\(P_i\\) that any individual observation will end up in cell i, with the actual observed frequency in that cell, \\(O_i\\) being the product of this probability and the total number of observations: \\(O_i = P_i\\times N\\). If we repeated the study again and again we would get slightly different numbers in each cell each time. The distribution of the numbers in a particular cell would follow similar rules as that of the number of heads we might get each time if we tossed a coin N times, then did the same again and again. This distribution is called a binomial distribution.\nIn other words, our observed frequencies have been obtained by sampling from a binomial distribution, where \\(O_i \\sim \\text{Binomial}(P_i,N)\\). Now, providing N is large enough and providing too that the probabilities \\(P_i\\) are not too close to 0 or 1, then the binomial distribution resembles a normal distribution. Thus, providing \\(P_i\\times N\\), that is providing the observed frequencies \\(O_i=P_i\\times N\\) are large enough, then the \\(O_i\\) will be approximately normally distributed.If this is the case, then so too is \\(\\frac{(O_i-E_i)}{\\sqrt{E_i}}\\) since the expected values \\(E_i\\) are fixed quantities and all this transformation does is turn our normal distribution into a standard normal distribution, with mean = 0 and standard deviation = 1. ie it shifts and squishes the distribution.\nHence in our expression for our test statistic \\(X^2\\) what we are doing is adding up k (= 4 in this case) squared standard normal distributions. This is the definition of a chi-squared distribution with four degrees of freedom. Thus we see that the sampling distribution of our test statistic is a chi-squared distribution.\nThe one final slightly odd detail is that when we run a chi-square test for a 2 x 2 contingency table we are told that there is one degree of freedom. In general, for \\(m \\times  n\\) table, there will be \\((m-1)\\times(n-1)\\) degrees of freedom. This is because the test presumes that the row and column totals are already known. If that is the case then if one value of a 2x2 table is known, the other 3 values can be found by deduction. Hence only one of the four values (it doesn’t matter which one) can be chosen freely.",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "fishers-exact-test.html",
    "href": "fishers-exact-test.html",
    "title": "11  Fisher’s Exact Test",
    "section": "",
    "text": "11.1 The Hypergeometric Function\nFisher’s Exact Test can be used in place of a chi-square test for independence in cases where counts are too low to make the chi-square approximation appropriate for calculating a p-value. A rule of thumb often used for this is that a chi-square test is no longer reliable when there are fewer than five or so counts in every cell of the contingency table.\nRonald Fisher devised an exact test now named after him that could be used for this scenario in situations where the marginal totals (ie row totals and column totals) of the table were fixed by the experimenter.\nSay we were devising a phone app that we hoped could distingush between two visually similar species of grass given a photo. In an experiment, we show the app 10 cases of species A and 10 of species B and in each case we record the identification decision of the app. That will give four possibilities.\nThe counts of these decisions can be recorded in a two-way, in this case \\(2 \\times 2\\), table, in which each cell contains the counts recorded for each combination of the levels of each factor. In a test run, suppose the numbers recorded for each case were as follows:\nHere we see that the row totals are set by the experimenter while the column totals are set by the app. Thus we see that the app is slightly more likely to choose A than B, regardless of the actual identity of the plant, since it went for A 11 times out of 20, and for B only 9 times.\nOur null hypothesis might be:\nThe app has no ability to correctly distinguish species A from species B.\nWe should not use a chi-square test for independence here since the numbers in two of the cells are less than 5.\nIn the Fisher’s Exact Test, we accept that the marginal totals are as observed. With this constraint we then determine the probability of getting every possible table of cells, under the null hypothesis that the two factors are independent of each other. The p-value for (lack of) independence of the two variables is then the cumulative sum of the probabilities of getting the table we actually got, plus those of tables that are even less likely. That is just what p-values are - the probabiliy of gettting the data you got, or more extreme data, if the null hypothesis is true.\nTo do this, we notice that if the column and row totals have been fixed in advance, then only one of the cell values of the table is independent. We can choose any cell. Once this is known, all the others can be determined, using this first value and the marginal totals. It turns that we can treat this number as a random variable that varies with probabilities governed by the hypergeometric distribution. We can thus use this distribution to calculate the probability of getting any possible table, given the row and column totals.\nLet’s see how this works.\nImagine an urn with N balls, K of them white, the rest, N-K of them, black.\nDraw a sample of n balls without replacement - that is, don’t put a ball back in the urn once it has been taken out.\nLet x be the number of white balls in the sample.\nx is a random variable that follows a hypergeometric (N, K, n) distribution:\n\\[\n\\text{P}(X=x)=\\frac{{K \\choose x} {{N-K} \\choose {N-x}}}{N\\choose n}\n\\]\nHere, terms like \\(K\\choose x\\) refer to the number of ways that n objects can be chosen from a collection of N objects. For details, see any book on proability, but the deatils need not concern us here. This tells us the probability that we will get x white balls if we draw n balls without replacement (each ball that is drawn is not put back) from an urn that at first contains N balls, K of them white.\nFor example, if we have an urn with N = 20 balls, K = 11 of them white, the remaining N - K = 9 of them black, and draw n = 10 balls without replacement, the probability that x = 7 of these will be white is given by\n\\[\n\\begin{align*}\nN=20, K&=11, n=10, x=7\\\\\n\\\\\n\\text{P}(X=7)& =\\frac{{K \\choose x} {{N-K} \\choose {N-x}}}{N\\choose n}\\\\\n&=\\frac{{11 \\choose 7} {{20-11} \\choose {20-7}}}{20\\choose 10}\\\\\n&= 0.014\n\\end{align*}\n\\] Here, we calculated the probability of getting \\(x\\) given the number \\(K\\) of white balls in the urn, which is one column total of the table, the number \\(n\\) of balls that were sampled, which is one row total, and the total number \\(N\\) of balls in the urn. Given these three numbers we know the other column total, which must be \\(N-K\\), and the other row total which must be \\(N-n\\). These are the numbers of balls that were not sampled and the number of black balls in the urn, respectively.\nFurther, given that these row and column totals are all known, if we get \\(x\\) white balls among the \\(n\\) balls that we take from the urn, we also know the other three values in the table. That is we know the number of black balls that came out, and the numbers of white and black balls that must still be in the urn.\nIn other words, when we calculate the probability of getting a particular number \\(x\\) of white balls from the urn, out of \\(n\\) that we drew altogether, given that the urn contains \\(K\\) white balls and \\(N\\) balls altogether, we are calculating the probability of getting a particular set of four values in the \\(2 \\times 2\\) table.\nNote too that when balls are sampled from the urn we do not preferentially sample white or black balls: we would expect the ratio of colours sampled to be roughly equal to the ratio of colours actually in the urn.\nThis is precisely the scenario for our app data under the null hypothesis of no association between the truth and the app’s decision. We know how many times it decided on Species A (ie we know one column total of our table, let’s call it \\(K = 11\\)), we know how many seedlings of species A we actually had (so that’s one row total, let’s call it \\(n = 10\\)) and we know how many seedlings we had altogether: \\(N = 20\\). Therefore, just as with the urn, we also know the other row and column totals, which must be 10 and 9 respectively. We find that the app decided on species A just 3 times. Convince yourself that knowing all these numbers means that the other entries in the table (top right, and the bottom row) are now also known.\nThus if we want to calculate the probability of getting precisely this table, that will be the same as the probability of deciding there were 3 white seedlings since that number fixes the other three entries in the table.\nAnd finally, and more precisely, if we want to calculate the probability of our app getting this table of results given that its decision for any seedling is independenty of the truth for that seedling, then we have exactly the same set-up asfor the urn, and we can use the hypergeometric function to calculate the probability of getting any particular table of results.\nIn case you are worried, we will not normally use the hypergeometric function directly, nor de we even need to know it is being used. We can use the R runction fisher.test(): see below.",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "fishers-exact-test.html#hypergeometric-distribution-in-r",
    "href": "fishers-exact-test.html#hypergeometric-distribution-in-r",
    "title": "11  Fisher’s Exact Test",
    "section": "11.2 Hypergeometric Distribution in R",
    "text": "11.2 Hypergeometric Distribution in R\nHowever if you are interested in the hypergeometric function, it is catered for in R like many other distributions by a family of four functions:\n\ndhyper(x, m, n, k).\nphyper(q, m, n, k).\nqhyper(p, m, n, k).\nrhyper(nn, m, n, k).\n\nwhere, in R:\n\nm = number of white balls in urn\n\nn = number of black balls in urn.\nk = number of balls sampled (without replacement).\nx = number of white balls in sample.\nnn= number of balls in urn = m + n\n\nFor further information on the use of any of these, type the name of the function preceded by a question mark into the console pane, bottom left. Help will then appear in the Help pane, bottom right.\nIn our example, to calculate the probability of a given table of results,\ndhyper(r1c1, col1, nn-col1, row1)",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "fishers-exact-test.html#back-to-fishers-exact-test",
    "href": "fishers-exact-test.html#back-to-fishers-exact-test",
    "title": "11  Fisher’s Exact Test",
    "section": "11.3 Back to Fisher’s Exact Test",
    "text": "11.3 Back to Fisher’s Exact Test\nFirst we will go through all the steps manually just so you can see what is going on under the hood of a Fisher’s Exact Test, then we will do the test in R.\nIn this test we\n\nAssume the null hypothesis (independence) to be true.\nConstrain the marginal counts to be as observed\n\nCalculate the probability of the observed table, given the null hypothesis\n\nIn the case of the example, that means that we should calculate\n\\[\nP(\\text{observed table}|H_0) = P(X=7|H_0)\\\\\nX ∼ \\text{Hypergeometric} (N=20, K=11, n=10)\n\\] and then\n\nIn the same way we calculate the probabilities of all possible tables, given the constraints of the marginal totals, then we\nSum the probabilities of all tables that are as or more ‘extreme’ than the observed table ie whose probability is less than or equal to that of the observed table.\nThe resulting sum is the p-value of the observed table, given the marginal totals.\n\nIf this p-value is less than a predetermined threshold value (the value chosen is usually 0.05) then we reject the null hypothesis and regard the data as providing evidence for an association between the factors. That is we reject the idea that they are independent of each other. In the case presented above, that means we reject the idea\n\n\n11.3.1 Example\nIn the figure below we show all possible tables, along with their respective p-values, calculate using the hypergeometric function. The table of the actual results obtained is picked out in colour. We see that tables\n\n\n\n\n\n\n\n\n\nWe see that tables a), b), c), h) (the actual results), i) and j) all have p-values equal to or less than that of the actual results. The combined total of these p-values is 0.06978.\nThis p-value is bigger than 0.05 so we would normally fail to reject the null hypothesis that there is no association between the two factors. There is no evidence from these data that the app does any better than guessing whether a plant is species A or species B!",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "fishers-exact-test.html#fishers-exact-test-in-r",
    "href": "fishers-exact-test.html#fishers-exact-test-in-r",
    "title": "11  Fisher’s Exact Test",
    "section": "11.4 Fisher’s Exact Test in R",
    "text": "11.4 Fisher’s Exact Test in R\nIf we already know the four counts for our \\(2 \\times 2\\) table we can create a \\(2 \\times 2\\) matrix of them like this:\n\napp.mat &lt;- matrix(c(3,8,7,2), nrow = 2)\napp.mat\n\n     [,1] [,2]\n[1,]    3    7\n[2,]    8    2\n\n\nIf we had the data in a tidy data frame like we had for the ladybird data we can convert it into a matrix using the xtabs() command in the same way as we did there.\nWe use this matrix as the argument of the fisher.test() function:\n\nfisher.test(app.mat)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  app.mat\np-value = 0.06978\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.007870555 1.133635839\nsample estimates:\nodds ratio \n 0.1226533 \n\n\nThat’s it! It’s a one liner, just like many statistical tests in R. You will see that this gives the same p-value that we have calculated manually.",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "fishers-exact-test.html#what-if-we-just-use-a-chi-square-test",
    "href": "fishers-exact-test.html#what-if-we-just-use-a-chi-square-test",
    "title": "11  Fisher’s Exact Test",
    "section": "11.5 What if we just use a chi-square test?",
    "text": "11.5 What if we just use a chi-square test?\nIf we use a chi-square test on this same data, R gives us a warning, because of the low values in the table:\n\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  app.mat\nX-squared = 3.2323, df = 1, p-value = 0.0722\n\n\nThis is a cautionary tale: you can use R to run this or that test on data even when the data are unsuitable for the test, and you will get an output, but the output may not be reliable. Here, at least, we are given a warning that alerts us to this possibility, and in fact our conclusion would be the same as it was when using the Fisher’s Exact Test, but that is not always the case. You need to think carefully as to whether a given test is the right one before you use it.",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "fishers-exact-test.html#why-not-always-use-fishers-exact-test-instead-of-a-chi-square-test",
    "href": "fishers-exact-test.html#why-not-always-use-fishers-exact-test-instead-of-a-chi-square-test",
    "title": "11  Fisher’s Exact Test",
    "section": "11.6 Why not always use Fisher’s Exact Test instead of a chi-square test?",
    "text": "11.6 Why not always use Fisher’s Exact Test instead of a chi-square test?\nFisher’s Exact Test becomes computationally expensive when the number of tables for which a p-value has to be calculated becomes very large. This happens very quickly when each of the two factors has more than two levels, so that the contingency table is no longer \\(2 \\times 2\\) but \\(3 \\times 4\\) or \\(4\\times 6\\) and so on. However for \\(2 \\times 2\\) tables it works well and quickly for table cell values into the 1000s and is a perfectly valid alternative to the chi-squared test. But then, in that case, why not just use a chi-squared test? For large cell values this works well and anyone reading your work is more likely to be familiar with it than with the Fisher’s Exact Test.",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "fishers-exact-test.html#another-cautionary-note",
    "href": "fishers-exact-test.html#another-cautionary-note",
    "title": "11  Fisher’s Exact Test",
    "section": "11.7 Another Cautionary Note",
    "text": "11.7 Another Cautionary Note\nThis leads us to think that the Fisher’s Exact Test is only really a better alternative to the chi-squared test when sample sizes are very small and the chi-squared approximation becomes invalid. However, think about it. If you have a small sample then it is likely that it is not very representative of the population from which it has been drawn, and that you will fail to detect any association that there may be between the two factors. The solution therefore, when you have a small sample, is likely not to say Hallelujah! and grasp the Fisher’s Test with glee, but, if you can, to get a bigger sample. However, if that is not possible and you are stuck with a small sample size, then a Fisher’s Exact Test of independence is more reliable than a chi-squared test.",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "12  Correlation",
    "section": "",
    "text": "12.1 The correlation coefficient\nPart of this guide to correlation draws heavily on the very helpful chapter in Statistics, by Freedman, Pisani and Purves (1991).\nFor this chapter you will find it useful to have this RStudio project folder if you wish to follow along and try out the exercises at the end.\nThe notable statistician Karl Pearson and Alice Lee carried out a study to investigate the resemblance between children and their parents. As part of the study, they measured the heights of 1078 parents and of their children at maturity. Here we use a subset of of their data, that comprises 746 parent-child pairs, avaialble in the HistData package. The heights of the children are plotted against the heights of the parents in the plots below, where we distinguish between father-son and mother-daughter pairs.\nThe taller a father, the taller his sons tend to be. It is the same with mothers and daughters.\nThere is a positive association between a father’s height and the height of his sons.\nBut there is a lot variation - the association is weak.\nIf you know the height of a father, how much does that tell you about the height of his sons?\nConsider fathers who are about about 67 inches tall, and look at the wide variation in the heights of their sons - - all the points between the two vertical dotted lines. The same is true for the daughters of mother who are about 63 inches tall.\nIf there is a strong association between two variables, then knowing one helps a lot in predicting the other. But when there is a weak association, information about one variable does not help much in guessing the other. When there is no association, it does not help at all.\nSuppose we are looking at the relationship between two variables and have already plotted the scatter plot. The graph looks like a cloud of points.\nHow can we summarise it numerically?\nThe first thing we can do is to mark a point that shows the average of the x-values and the average of the y-values. This is the point of averages. It marks the centre of the cloud.\nThe next step is to measure the width of the cloud from side to side, in both the x and the y directions. This can be done using the standard deviations (SD) of the x and y values. Remember that if both x and y are normally distributed, then 95% of the data will lie within about 2 (1.96 if we want to be pernickety) standard deviations of the mean, in each direction.\nSo far, our summary statistics are:\nThese statistics tell us where the centre of the cloud is and how far spread out it is both vertically and horizontally, but they do not tell the whole story.\nConsider the following two sets of data plotted below. Both have the same centre and the same spread.\nHowever the points in the first cloud are tightly clustered around a line - there is a strong linear association between the two variables. In the second cloud, the clustering is much looser. The strength of the association is different in the two diagrams. To measure the association, one more summary statistic is needed - the correlation coefficient.\nThis coefficient is usually abbreviated as r, for no good reason.\nThe correlation coefficient is a measure of linear association or clustering around a line. The relationship between two variables can be summarized by:",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#the-correlation-coefficient",
    "href": "correlation.html#the-correlation-coefficient",
    "title": "12  Correlation",
    "section": "",
    "text": "mean of the x values, SD of the x values.\nmean of the y values, SD of the y values.\n\n\n\n\n\n\n\n\n\nthe average of the x-values, the SD of the x-values.\nthe average of the y-values, the SD of the y-values.\nthe correlation coefficient r",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#different-values-of-r.",
    "href": "correlation.html#different-values-of-r.",
    "title": "12  Correlation",
    "section": "12.2 Different values of r.",
    "text": "12.2 Different values of r.\nLet us see how this looks graphically. In the Figure below we show six scatter plots for hypothetical data. In all six pictures the average is 3 and the standard deviation is 1 for both x and y. The correlation coefficient is printed in each case.\n\n\n\n\n\n\n\n\n\nThe one top left shows a correlation of 0 and the cloud is completely formless. As x increases, y shows no tendency to increase or decrease. It just straggles around.\nThe next diagram has r = 0.4 and a linear pattern is just starting to emerge. The next has r = 0.6 with a stronger linear pattern, and so on. The closer r is to 1 the stronger is the linear association and the more tightly clustered are the points around a line.\nA correlation of 1, which does not appear in the Figure is often referred to as a perfect correlation. It means that all the points lie exactly on a line so there is a perfect linear correlation between the two variables. Correlation coefficients are always between -1 and 1.\nThe correlation between the heights of identical twins is around 0.95. A scatter diagram for the heights of twins would thus look like the bottom right diagram in the Figure. We see that even with a coefficient this big there is a still a fair degree of scatter. The heights of identical twins are far from being equal all the time.\nReal data in the life sciences never shows perfect correlation and rarely does it even show strong correlation. It is more common for it to look like Pearson’s father-son data, with weak associations and r values in the range 0.3 to 0.7. This is even more true for data from the social sciences which concern human behaviour.\nWe can also have negative associations between variables. For example women with more education tend to have fewer children. Animals with higher body weight tend to have lower metabolic rates. As one variable increases, the other decreases. When there is negative association, the correlation coefficient has a negative sign.\nBelow we show six examples of negative correlation. As in the previous figure, all the data sets have a mean of 3 and a standard deviation of 1.\n\n\n\n\n\n\n\n\n\n\nCorrelations are always between -1 and 1, but can take any value in between. A positive correlation means that the cloud slopes up: as one variable increases, so does the other. A negative correlation means that the cloud slopes down. As one variable increases, the other decreases.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#using-r-to-find-the-correlation-coefficient.",
    "href": "correlation.html#using-r-to-find-the-correlation-coefficient.",
    "title": "12  Correlation",
    "section": "12.3 Using R to find the correlation coefficient.",
    "text": "12.3 Using R to find the correlation coefficient.\nFirst, let us try to find the correlation between two sets of data where we know what the correlation coefficient is, because we created the data ourselves. Let us suppose that we have columns called x and y within a data frame called df, and that the correlation coefficient between these data is in fact 0.8.\n\ncor.test(x, y, method = \"pearson\", data = df)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = 9.1467, df = 48, p-value = 4.319e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6667273 0.8801895\nsample estimates:\n      cor \n0.7971401 \n\n\nThere are different ways to calculate the correlation coefficient. Which of them is appropriate depends mainly on the type of data, and if they are numeric, whether they are normally distributed. If they are, then we use the Pearson method. If they are not, for example because they are ordinal data, then we use the Spearman’s Rank method and write method=“spearman” instead. In this case we can relax the requirement that there is a linear association between the data sets, but there does still need to be a monotonic relationship.\nIt is important to be able to interpret and report the output.\nFirst, understand that R is carrying out a test, the null hypothesis of which is that there is no correlation between the values of x and the values of y among the populations from which the x and y data were drawn, so that the correlation coefficient between x and y within those populations is zero. It then reports a p-value: how likely it is that you would have got the data you got for this sample of data if that null hypothesis were true. As with most tests, to do this it uses the data to calculate a so-called test-statistic. How it does this need not concern us here. The details will differ from test to test, and the name given will differ. Here it is called t. It also reports the number of independent pieces of information used to calculate that statistic. This is called the degrees of freedom, here denoted df. This usually (but not necessarily) has a value that is 1,2 or 3 less than the number of data points.\nThen it reports the p-value. A tiny (close to zero) value here means that it thinks it very unlikely that the samples would be as they are if the x and y variables were not correlated in the populations from which the samples were drawn. A high (by which we usually mean greater than 0.05) value means that there is a reasonable chance that the actual non-zero correlation coefficient could have been found between x and y in the samples, even though those values were not correlated in the wider populations from which the samples were drawn. In that case we would have found no evidence that the x and y data within the population were correlated. This doesn’t mean that they aren’t, just that we have insufficient evidence to reject the null hypothesis that they are not.\nThe p-value reported here is 4.3e-12. That is R’s way of saying what in standard form would be written 4.3 x 10-12. This is a really tiny value. It is 0.0000000000043, which is a very inconvenient way to write such a small number. Hence R’s way of doing it or the standard form way of doing it. In the context of a statistical test and when p is is that small we don’t care about its exact value, we simply note that it is very, very small. We thus can confidently reject the null hypothesis and assert that the data provide evidence that x and y are correlated, in this case positively.\nFurther, it reports the actual correlation coefficient. Here it finds r = 0.797, which we happen to know to be correct because we created this data set ourselves, and a 95% confidence interval for the coefficient. The precise meaning of the confidence interval is subtle, but it is a kind of error bar for the correlation coefficient r. It means that if we drew sample after sample from the population and calculated the confidence interval for r for each sample, then 95% of the time that interval would capture the true value of r. Thus, you can reasonably think of the confidence interval as being the range of values within which the true population correlation coefficient plausibly lies, given the value that was found for the sample.\nIf the p-value is small enough that we reject the null-hypothesis, then this confidence interval should not encompass zero. Why? Because any value inside the confidence interval is a plausible value for the population correlation coefficient andif we are going to reject the null hypothesis, then zero should not be a plausible value for the population correlation coefficient, given the data.\nIf the p-value is large enough that we do not reject the null hypothesis then this confidence interval will encompass zero. Why? Becuase if the confidence interval encompasses zero, then zero is a plausible value for the correlation coefficient and so we should not reject the null.\nHere, the confidence interval is from 0.67 to 0.88. This does not encompass zero. In fact it is far from zero, so is consistent with our finding a really small p-value. On both groundss, we reject the null.\nTo report the result of this test we would say something like:\n\nWe find evidence for a strong positive correlation between x and y (Pearson r =0.80, t=9.1, df=48, p&lt;0.001)\n\nNote that when the p-value is much less than 0.05 as it is here we do not normally report its exact value, but simply write p&lt;0.01, or p&lt;0.001, and so on. The point is that these ways of reporting it tell the reader that p is way less than 0.05. This is all they need to know to see that we can confidently reject the null hypothesis.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-iris-correlation",
    "href": "correlation.html#sec-iris-correlation",
    "title": "12  Correlation",
    "section": "12.4 Correlations for real data",
    "text": "12.4 Correlations for real data\nLet us look at the Iris data set that is built into R. It contains values for the Sepal Width, Sepal Length, Petal Width and Petal Length for samples of 50 plants from each of three species of Iris, setosa, versicolor and virginica. Here are the first few rows:\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nWe will look to see if the data allow us to reject the idea that sepal width and sepal length are not correlated within the wider populations of each of these species:\nFirst, let’s plot the data\n\niris |&gt;\n  ggplot(aes(x = Sepal.Width, y = Sepal.Length, colour = Species)) +\n  geom_point() +\n  labs(x = \"Sepal Width (mm)\",\n       y = \"Sepal Length (mm)\") +\n  facet_wrap(~Species, nrow = 1, scales = \"free\") +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\n\n\n12.4.1 What we can tell from plotting the data\nHaving seen the plots, do you think it plausible that there is a linear relationship between sepal width and length? Only in this case can you use a Pearson correlation test (see below). If not linear, do you think there is at least a monotonic relationship between petal width and length (ie no peaks or troughs)? That, at least, would enable you to use a Spearman’s Rank correlation test. If neither are true, then you can’t use either test.\n\n\n12.4.2 Test for normality\nIt does look as though there is a plausibly linear relationship between sepal width and length, so we might be able to use the Pearson method for calculating correlation coefficient. This is the ‘parametric’ method that is more powerful than the ‘non-parametric’ alternative, the Spearman’s rank method.\nIn principle, however, this method requires that each group of the data be approximately normally distributed around its repective mean (that is what the word parametric is getting at), so we ought to test for this. We can do this either graphically or using a normality test such as the Shapiro wilk test. Let us do the latter here:\nFirst we do it for one species, for example the setosa. We can use the filter function from the dplyr packages within tidyverse to separate the data for this species from that for the others. We will save it as a new data frame called setosa:\n\nsetosa &lt;- iris |&gt;\n  filter(Species == \"setosa\")\n\nNow we use the shapiro.test() function to assess the normality of the Sepal.Length column data\n\n1shapiro.test(setosa$Sepal.Length)\n\n\n1\n\nNote the use of the dollar notation to pick out a particular column of a data frame. Here we pick out the Sepal.Length column of the setosa data frame.\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  setosa$Sepal.Length\nW = 0.9777, p-value = 0.4595\n\n\nand then of the Sepal.Width column of data:\n\nshapiro.test(setosa$Sepal.Width)\n\n\n    Shapiro-Wilk normality test\n\ndata:  setosa$Sepal.Width\nW = 0.97172, p-value = 0.2715\n\n\nWHen we do it for all species, for both sepal width and sepal length, we find\n\n\n# A tibble: 3 × 3\n  Species    Sepal.Length_p.val Sepal.Width_p.val\n  &lt;fct&gt;                   &lt;dbl&gt;             &lt;dbl&gt;\n1 setosa                  0.460             0.272\n2 versicolor              0.465             0.338\n3 virginica               0.258             0.181\n\n\nAll the p-values for this test are comfortably greater than 0.05 so we can reasonably presume that our data are drawn from normally distributed populations. This, plus the plausibly linear realtionships we have seen in the graphs means that can go ahead and use the Pearson method to calculate the correlation coefficient between sepal length and width for each species!\n\n\n12.4.3 Calculate the correlation coefficients\nLooking at each graph, it appears that there is a positive correlation for each species, but that this is weaker for versicolar and virginica than it is for setosa. Knowing the sepal width for that species gives you a much better idea of the sepal length, and vice-versa, than is true for the other two species.\nLet us find out:\n\niris |&gt;\n  group_by(Species) |&gt;\n  summarise(r=cor.test(Sepal.Width,Sepal.Length)$estimate,\n            lower.bound95 = cor.test(Sepal.Width,Sepal.Length)$conf.int[1],\n            upper.bound95 = cor.test(Sepal.Width,Sepal.Length)$conf.int[2],\n            \"p value\" = cor.test(Sepal.Width,Sepal.Length)$p.value)\n\n# A tibble: 3 × 5\n  Species        r lower.bound95 upper.bound95 `p value`\n  &lt;fct&gt;      &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1 setosa     0.743         0.585         0.846  6.71e-10\n2 versicolor 0.526         0.290         0.702  8.77e- 5\n3 virginica  0.457         0.205         0.653  8.43e- 4\n\n\nThe table gives the estimated value for the Pearson correlation coefficient in each case, the lower and upper bound of the 95% confidence interval for that coefficient and the p-value.\nDo these output provide evidence for a correlation between sepal length and sepal width in each case?",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#the-problem-of-missing-confounding-variables",
    "href": "correlation.html#the-problem-of-missing-confounding-variables",
    "title": "12  Correlation",
    "section": "12.5 The problem of missing confounding variables",
    "text": "12.5 The problem of missing confounding variables\nSuppose in the above analysis we had not distinguished between the three species. If we had plotted the speal length and width data we would have seen this:\n\niris |&gt;\n  ggplot(aes(x = Sepal.Width, y = Sepal.Length)) +\n  geom_point() +\n  labs(x = \"Sepal Width (mm)\",\n       y = \"Sepal Length (mm)\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThis looks like a weak negative correlation, as is confirmed by calculating the Spearman’s (in this case) rank correlation coefficient.\nIf we had in fact accounted for species and plotted the data separately for each species, or together but distinguishing each species by colour we would have seen the falsity of that conclusion:\n\niris |&gt;\n  ggplot(aes(x = Sepal.Width, y = Sepal.Length, colour = Species)) +\n  geom_point() +\n  labs(x = \"Sepal Width (mm)\",\n       y = \"Sepal Length (mm)\",\n       colour = \"Species\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nThe message here is that failure to spot important ‘missing’ variables, in the case species, can lead to grossly misleading ideas as to whether two variables are correlated, and if so, how.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#the-pearson-correlation-coefficient-measures-the-degree-of-linear-association.",
    "href": "correlation.html#the-pearson-correlation-coefficient-measures-the-degree-of-linear-association.",
    "title": "12  Correlation",
    "section": "12.6 The Pearson correlation coefficient measures the degree of linear association.",
    "text": "12.6 The Pearson correlation coefficient measures the degree of linear association.\n\n12.6.1 Pearson correlation coefficient\nSometimes the Pearson correlation coefficient r is a poor measure of the degree of association within a data set. Outliers and non-linearity are two problem cases.\nConsider first a data set where there is a very strong association between variables, but where the data set contains an outlier, and then a data set where there is a strong but non-linear association between variables. Here we mean by ‘strong’ that knowing the value of one variable gives you a very good idea of the value of the other.\n\n\n\n\n\n\n\n\n\nThe outlier in the left-hand figure above brings the correlation coefficient down to 0.08, which is close to zero. The correlation coefficient in the right-hand figure is similarly small at 0.158, despite that there is a strong association between the x and y data. The reason is that the association is non-linear.\n\n\n12.6.2 Spearman’s rank correlation coefficient rSp\nThe Spearman’s rank correlation coefficient is a valid measure of the association between two variables providing their relationship is monotonic - that is, continuously rising or flat, or continuously falling or flat. Linear relationships are monotonic, so the Spearman’s rank correlation coefficient is always a useful (if not necessarily the most powerful - Pearson would trump it if it could be used) measure of association for such cases, but it will still be valid when the relationship is monotonic but non-linear, whereas the Pearson correlation coefficient would not be.\n\nSpearman’s rank correlation coefficient rSp can also be be used for ordinal data, whereas Pearson’s r coefficient cannot be. This makes it very useful in much of ecology, animal behaviour and environmental studies where ordinal scales are commonly used.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#when-is-it-appropriate-to-calculate-a-correlation-coefficient",
    "href": "correlation.html#when-is-it-appropriate-to-calculate-a-correlation-coefficient",
    "title": "12  Correlation",
    "section": "12.7 When is it appropriate to calculate a correlation coefficient?",
    "text": "12.7 When is it appropriate to calculate a correlation coefficient?\nSo, to sum up, we note that the Pearson correlation coefficient is a measure of linear association, not of association in general. At least, this is true if you are calculating the Pearson correlation coefficient. If your data are not suitable for that and you decide to calculate the Spearman’s Rank correlation coefficient, then the condition is relaxed somewhat: there might be but there no longer needs to be a linear relationship between the two variables, but there must at least be a monotonic one. That means that, as one variable increases, the other should either increase or remain constant, or decrease or remain constant. There should be no peaks or troughs in the data.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#association-is-not-the-same-thing-as-causation",
    "href": "correlation.html#association-is-not-the-same-thing-as-causation",
    "title": "12  Correlation",
    "section": "12.8 Association is not the same thing as causation",
    "text": "12.8 Association is not the same thing as causation\nA very important and often repeated point to note is that correlation measures the degree of linear (Pearson) or monotonic (Spearman) association. But association is not the same as causation. Two entities can have a strong degree of monotonic or linear association without there being any causal relation between the two. There could be a confounding variable that underlies the association.\nFor example you would probaly find a strong positive correlation between the number of pubs in towns and the nubmer of churches. You would be mistaken, however, in thinking that this provides evidence that drink drives people to religion, or the reverse, since there is a very obvious confounding variable underlying this association, which is the population of the towns: the more people, the more pubs, the more churches, the more of pretty much anything.\nThat said, if you spot a strong association beween two variables, it may be an indicator of a causal relationship, so it is at least worth pausing for thought to decide whether this is worthy of further investigation.\nBe careful though, as spurious correlations can easily arise. See Spurious Correlations for many amusing examples of these.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#examples",
    "href": "correlation.html#examples",
    "title": "12  Correlation",
    "section": "12.9 Examples",
    "text": "12.9 Examples\n\n12.9.1 Adult human heights\nHere we use data from Our World in Data: Human Height\nThe figure below shows the mean heights of adult men vs adult women in many different countries who were born in 1996. There is a strong positive and linear correlation between these heights (rsp = 0.941, p &lt; 0.001). The Spearman’s rank correlation coefficient is calculated because the male mean heights are not normally distributed among themselves.\n\n\n\n\n\n\n\n\n\n\n\n12.9.2 Cyclones\nCyclones are areas of low atmospheric pressure around which steep gradients in air pressure can cause strong winds to develop, which in turn may create large waves if the cylone is over the oceans. Depending on where they occur, these storms are variously also known as hurricanes and typhoons. Here we consider whether the peak wind speeds are associated with the depth of the low pressure at the eye of the storm, and whether the peak wave sizes are associated with how wide the storm is. If the answer to either of these questions is yes, then an outcome of practical importance - wind speed, wave height - can be predicted at least in part by a variable that can be measured easily - pressure, distance.\n\n12.9.2.1 South West Indian Ocean intense tropical cyclones 1973 - 2024\nPressure gradients cause winds so it is reasoable to ask whether there a correlation between the peak wind speed and minimum pressure at the eye of cyclones. Here we look at data for for intense tropical cyclones in the south west Indian Ocean over the period 1973-2024. The data are taken (scraped using the R package rvest!) from: https://en.wikipedia.org/wiki/List_of_South-West_Indian_Ocean_intense_tropical_cyclones . The original data sources are available on that site.\n\n\n\n\n\n\n\n\n\nIn this figure we see that there is a weak but significant negative correlation between the peak recorded wind speed and the minimum recorded pressure at the centre of intense tropical cyclones that occurred in the south west Indian Ocean between 1973 and 2024. Neither wind speed nor pressure were normally distributed, so the Spearman’s rank correlation coefficient rSp has been calculated, and not the Pearson r coefficient.\n\n\n12.9.2.2 Significant wave height vs size of cyclone\nHere we look at results displayed in Figure 3f of a paper by Oh et al (2023). In this paper the authors seek to determine whether there is an association between the size of a cyclone, measured by the ‘R50’ distance measured outward from the storm centre to where the wind speeds have subsided to 50 kph, and the maximum ‘significant wave height’ of the swell created by the storm. Significant wave height is a widely used measure that is the average height of the heighest 1/3 of the waves, these being the ones that impact most on practical matters like the fuel consumption of ships, the erosion of shores, and so on.\n\n\n\n\n\n\n\n\n\nThe plot shows that there is a monotonically rising relationship between the R50 radius of the cyclones included in the study and the maximum significant wave height of swell created by them. The relationship is not linear however, so the only appropriate measure of correlation coefficient is the Spearman’s rank, which gives \\(r_{sp} = 0.948, p&lt;0.001\\), indicating a very strong positive association between the size of a cyclone and the height of the waves it creates.\n\nWhy do you think this relationship flattens off for larger storm sizes?\n\n\n\n\n12.9.3 Lichen abundance\nIn a (2008) paper, Jovan et al. investigated the utility of using lichen as bioindicators of air quality. Is there, they asked, an association between air quality and the abundance of this or that species of lichen?\n\n\n\n\n\n\n\n\n\n\nWe note that the relationship between air quality score and proportion of nitrophyte abundance is plausibly linear.\n\n\nThere is a strong negative correlation (rSp=-0.78, p&lt;0.001) between air quality score and proportion of nitrophyte lichen. This suggests that this proportion can be used as a bioindicator of air quality.\n\n\nA Spearman’s rank correlation coefficient was calculated in this case rather than a Pearson correlation coefficient, despite the fact that both the x and y variables are plausibly drawn from normally distributed populations of values, according to a Shapiro-Wilk test. The problem is that both the air quality score and the proportion of nitrophyte abundance are ordinal variables - something we learn from reading the paper in which this figure appears. This means that analysis using parametric tests such as the Pearson method for determining linear correlation are not appropriate. A non-parametric method such as Spearman’s rank method can be used instead.\n\n\n\n12.9.4 Soil bacteria\nHere we look at figures published in a paper by Lauber and co-workers (2009) in which they investigate the evidence for an association between soil pH and the abundance of various species of bacteria.\n\n\n\n\n\n\n\n\n\nIn the Figure above, note that the Pearson r-values for C and E are close to zero, and the p-values are greater than 0.1, meaning that at this significance level there is no evidence from these data that there is any linear association between soil pH and the relative abundances of Alphaproteobacteria or Beta/Gammaproteobacteria. From the plots, it looks in C as if there no assocation at all, whereas in E it looks as though there might be, but if so then not a linear or even monotonic association, for which the correlation coefficient (Pearson r or Spearman rSp) would be a poor measure.\n\n\n12.9.5 Birds\n\n\n\n\n\n\n\n\n\nThis figure is from a study by Pain et al (2019) on the impact on bird populations of ingestion of lead from spent lead ammunition arising from hunting using rifles and shot guns. For several species of wetland birds, the population trend (as measured by a proxy scale) is plotted against the prevalence of carcasses found to contain lead shot.\nThere is a clear negative trend here that is plausibly linear, or at least monotonic. Shapiro-Wilk tests show that neither data set is plausibly drawn from a normally distributed population, so a Spearman’s rank correlation coefficient is calculated. The result is \\(r_{\\text{Sp}} = -0.697, p &lt; 0.01\\) so we can say that there is evidence of a significant and strong negative correlation between lead shot ingestion of waterbird species and their population trends.\n\n\n12.9.6 Heart rate vs life expectancy\n\n\n\n\n\n\n\n\n\nHere we see a strong negative linear correlation between the life expectancy of different species and the log of the mean heart rate in beats per minute. On this plot, humans are almost an outlier. In this case, use of a Shapiro-Wilk test showed that both life expectancy and log of the heart rate were found to be consistent with being drawn from normally distributed populations, so the Pearson method was used to calculate the correlation coefficient.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#exercise-1",
    "href": "correlation.html#exercise-1",
    "title": "12  Correlation",
    "section": "12.10 Exercise 1",
    "text": "12.10 Exercise 1\n\n\n\n\n\n\n\n\n\nPlots A to F above show scatter plots of different data sets Y against X.\n\nWhich of them show linear behaviour?\nWhich of them show monotonic behaviour?\nFor which of them might it be appropriate to calculate the following correlation coefficients between X and Y?\n\nPearson r\nSpearman rank rsp",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#exercise-2",
    "href": "correlation.html#exercise-2",
    "title": "12  Correlation",
    "section": "12.11 Exercise 2",
    "text": "12.11 Exercise 2\nMeasurements were made on female Adelie penguins on a series of islands in the Antarctic. The bill lengths and depths of 73 individuals were recorded and are shown in the plot below.\n\n\n\n\n\n\n\n\n\nFrom this information and plot, decide\n\nWhether it is plausible that there is a linear relationship between the bill depths and lengths within the data set\nWhether the correlation coefficient within the data set is likely to be positive or negative\nWhether the correlation is ‘strong’ or ‘weak’ ie is the absolute value of the correlation coefficient likely to be close to 1 or close to zero\nWhether there might be evidence from this data that there is any correlation between bill depth and bill length in the population from which this data set was drawn.\n\n\n12.11.0.1 Tests for normality\nShapiro-Wilk tests are carried out to check for normality of the two sets of data:\n\nshapiro.test(bill_depths)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bill_depths\nW = 0.9831, p-value = 0.4364\n\n\n\nshapiro.test(bill_lengths)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bill_lengths\nW = 0.99117, p-value = 0.8952\n\n\nOn this basis, we see that we can use the Pearson method to calculate the correlation coefficient between bill depth and bill length. What is telling us this?\n\n\n12.11.0.2 Calculate correlation coefficient\nFor these data, we calculate the correlation coefficient using the Pearson method.\n\n# Pearson\ncor.test(bill_depths,bill_lengths, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  bill_depths and bill_lengths\nt = 1.3714, df = 71, p-value = 0.1746\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07209557  0.37677877\nsample estimates:\n      cor \n0.1606361 \n\n\nWhich part(s) of this output tell us that:\n\nThere is a weak positive correlation between bill length and depth within the sample?\nThere is no evidence that this correlation exists in the wider population from which this data set was drawn?\n\nHow would you report this result?",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#exercise-3",
    "href": "correlation.html#exercise-3",
    "title": "12  Correlation",
    "section": "12.12 Exercise 3",
    "text": "12.12 Exercise 3\n\nIf you haven’t done so already, click on this link to download the practice project for correlation. You may then need to unzip it.\nOpen the project in RStudio using File/Open Project then navigating to the project and clicking on the correlation.Rproj file\nOpen the notebook correlation.qmd that you will find in the notebooks subfolder.\nRun the initial code chunks to load packages and the penguin and iris data\nCheck in the Environment pane that you have created data frames called penguins and iris. Note the variable (column) names in each case.\nUsing the code in section Section 12.4 as a guide, create a faceted plot of petal length against petal width for each species. To do this you need to create a new code chunk, paste the code from Section 12.4 into it and then adapt it a necessary.\nAgain using Section 12.4 as a guide, calculate the Pearson correlation coefficient between petal length and petal width for each species, and display this, plus the lower and upper bounds of the confidence interval and the p-value for each species in a table.\n\n\nDoes it appear that the petal length and petal width are correlated for each species?\n\nIs the correlation positive or negative?\n\nFor which species is the correlation strongest?\n\nDo the correlation coefficients make sense, given the plots?\n\nMoving on to the penguin data….\n\nUse the filter() function from the dplyr package in tidyverse to filter the penguin data down to Gentoo males. Save the data in a data frame called gentoo_m. You can do this by including the following code chunk in your notebook:\n\n```{r}\ngentoo_m &lt;- penguins |&gt;\nfilter(sex == \"male\") |&gt;\nfilter(species == \"Gentoo\")\n```\n\nPlot the bill depth vs bill length using code similar to this:\n\n```{r}\ngentoo_m |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point() +\n  labs(x = \"Bill depth (mm)\",\n       y = \"Bill length (mm)\") +\n  theme_classic()\n```\n\nLook at the plot and decide if it is plausible that the association between bill depth and bill length is plausibly monotonic and if so, if it is plausibly linear. On this basis, before even considerinrg the issue of normality, decide if it is appropriate to use Pearson or Spearman’s rank measures of correlation with this data.\nIf your answer to 10. was Yes, use the shapiro.test() function to determine if it is OK to use the Pearson measure of correlation. To do this you must separately test both the bill_length_mm and bill_depth_mm columns of the gentoo_m for normality. Remember that you can pick a particular column of a data frame by using the dollar notation. For example, df$X would pick out the column X from the data frame df.\nUse the cor.test() function with the appropriate method argument (method = \"pearson\" or method = \"spearman\"), as appropriate, to determine if the bill length and bill depth variables are correlated. This means using an expression of the form cor.test(df$X, df$Y, method = \"pearson\") if you wanted to determine the Pearson correlation coefficient between the X and Y columns of the data frame df.\n\n\n\n\n\nFreedman, David, Robert Pisani, Roger Purves, and Ani Adhikari. 1991. Statistics. 2nd ed. W. W. Norton & Company.\n\n\nJovan, Sarah. 2008. “Lichen Bioindication of Biodiversity, Air Quality, and Climate: Baseline Results from Monitoring in Washington, Oregon, and California.” http://gis.nacse.org/lichenair/doc/Jovan2008.pdf.\n\n\nLauber, Christian L., Micah Hamady, Rob Knight, and Noah Fierer. 2009. “Pyrosequencing-Based Assessment of Soil pH as a Predictor of Soil Bacterial Community Structure at the Continental Scale.” Applied and Environmental Microbiology 75 (15): 5111–20. https://doi.org/10.1128/aem.00335-09.\n\n\nOh, Youjung, Sang Myeong Oh, Pil-Hun Chang, and Il-Ju Moon. 2023. “Optimal Tropical Cyclone Size Parameter for Determining Storm-Induced Maximum Significant Wave Height.” Frontiers in Marine Science 10 (February). https://doi.org/10.3389/fmars.2023.1134579.\n\n\nPain, Deborah J., Rafael Mateo, and Rhys E. Green. 2019. “Effects of Lead from Ammunition on Birds and Other Wildlife: A Review and Update.” Ambio 48 (9): 935–53. https://doi.org/10.1007/s13280-019-01159-0.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "simple-linear-regression.html",
    "href": "simple-linear-regression.html",
    "title": "13  Simple linear regression",
    "section": "",
    "text": "13.1 Introduction\nFor this chapter you will find it useful to have this RStudio project folder if you wish to follow along and try out the exercises at the end.\nA class of analytical models that you will use often go under the name General Linear Models. They include linear regression, multiple regression, ANOVA, ANCOVA, Pearson correlation and t-tests.\nDespite appearances, these models are all fundamentally linear models. They share a common framework for estimation (least squares) and a common set of criteria that the data must satisfy before they can be used. These criteria centre around the idea of normally distributed residuals. An important stage of any analysis that uses linear models is that these assumptions are checked, as part of the Plot -&gt; Model -&gt; Check Assumptions -&gt; Interpret -&gt; Plot again workflow.\nHere, we will go through an example of simple linear regression - suitable for trend data where we wish to predict a continuously varying response, given a value of a single continuous explanatory variable. As we go we show code snippets from a Quarto notebook that does this job, and, at the bottom, an example complete notebook that you could adapt to your own needs.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple-linear-regression.html#simple-linear-regression---plant-growth",
    "href": "simple-linear-regression.html#simple-linear-regression---plant-growth",
    "title": "13  Simple linear regression",
    "section": "13.2 Simple Linear Regression - plant growth",
    "text": "13.2 Simple Linear Regression - plant growth\nAs a first example, we ask the question: does plant growth rate depend on soil moisture content?\nWe predict that more moisture will probably allow higher growth rates. We note that this means there will be a clear relationship between the variables, one that should be apparent if we plot the response (dependent) variable - plant growth rate - against the explanatory (independent) variable - soil moisture content. We note that both the explanatory variable and the dependent variables are continuous - they do not have categories.\nWhat we might want to do in linear regression is be able to predict the value of the dependent variable, knowing the value of the independent variable, using a linear model. Just as ofen, we simply want to determine the sensitivity of the dependent variable to changes in the independent variable. In practice, this means drawing a ‘best fit’ straight line through the data and determining the intercept and gradient of this line. It is the gradient that tells us the sensitivity of the dependent variable to chnges in the independent variable, and that is what we most often look out for in the analysis.\n\n13.2.1 Load packages\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggfortify)\nlibrary(cowplot)\n\n\n\n13.2.2 Get the data\nWe have a data set to explore our question: The plant.growth.rate.csv data set is available as a csv file in the data subfolder of the skeleton project folder you have hopefullly downloaded to go along with this session.\n\nfilepath &lt;- here(\"data\",\"plant.growth.rate.csv\")\nplants &lt;- read_csv(filepath)\nglimpse(plants)\n\nRows: 50\nColumns: 2\n$ soil.moisture.content &lt;dbl&gt; 0.4696876, 0.5413106, 1.6979915, 0.8255799, 0.85…\n$ plant.growth.rate     &lt;dbl&gt; 21.31695, 27.03072, 38.98937, 30.19529, 37.06547…\n\n\nWe see that the data set contains two continuous variables, as expected.\n\n\n13.2.3 Plot the data\nWe can use the package ggplot2, which is part of tidyverse to do this:\n\nplants |&gt;\n  ggplot(aes(x=soil.moisture.content, y=plant.growth.rate)) +\n  geom_point() +\n  labs(x=\"Soil moisture content\",\n       y=\"Plant growth rate (mm/week)\") +\n  xlim(0,2) +\n  ylim(0,50) +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nFrom the plot, we note that:\n\nthere is an upward trend that is plausibly linear within this range of soil misture content. The more moisture there is in the soil, the greater the growth rate of the plants appears to be.\nthe variance of the data, that is the range of vertical spread is approximately constant for the whole range of soil moisture content. This is one of the key criteria that data must satisfy if we are to analyse them using a linear model such as simple linear regression.\nwe can estimate the intercept and gradient of a best fit line just by looking at the plot. Roughly speaking, the moisture content varies from 0 to 2, while the growth rate rises from 20 to 50, a rise of about 30. Hence the gradient is about 30/2 = 15 mm/week, while the intercept is somewhere between 15 mm and 20 mm / week.\n\nIt is always good practice to examine the data before you go on to do any statistical analysis. For all but the smallest data sets, that means plotting them.\n\n\n13.2.4 Make a simple model using linear regression\nWe use the function lm() to do this, and we save the results in an object to which we give the name model_pgr. This function needs a formula and some data as its arguments:\n\nmodel_pgr&lt;-lm(plant.growth.rate ~ soil.moisture.content, data = plants)\n\nThis reads: ‘Fit a linear model, where we hypothesize that plant growth rate is a function of soil moisture content, using the variables from the plants data frame.’\n\n\n13.2.5 Check assumptions\nBefore we rush into interpreting the output of the model, we need to check whether it was valid to use a linear model in the first place. Whatever the test within which we are using a linear model, we should do the necessary diagnostic checks at this stage.\nYou can do this using tests designed for the purpose, but I prefer to do it graphically, using a function autoplot() from the package ggfortify. You give this the model we have just created using lm() and it produces four very useful graphs. I suggest that, after once installing ggfortify you include the line library(ggfortify) at the start of every script.\nHere is how you use it:\n\nautoplot(model_pgr, smooth.colour=NA) + theme_cowplot()\n\n\n\n\n\n\n\n\nThe theme_cowplot() part is not necessary, but it gives the plots a nice look, so why not?\nThese plots are all based around the `residuals’, which is the vertical distance between observed values and fitted values ie between each point and the best fit line through the points - the line which the linear model is finding for us, by telling us its intercept and gradient.\nIn simple linear regression, the best fit line is the one that minimises that sum of the squared residuals.\nSo what do these plots mean?\n\nTop-left: This tells you about the structure of the model. Was it a good idea to try to fit a straight line to the data? If not, for example because the data did follow a linear trend, then there will be humps or troughs in this plot.\nTop-right: This evaluates the assumption of normality of the residuals. The dots are the residuals and the dashed line is the expectation under the normality assumption. This is a much better way to check normality than making a histogram of the residuals, especially with small samples.\nBottom-left: This examines the assumption of equal variance of the residuals. A linear model assumes that the variance is constant over all predicted values of the response variables. There should be no pattern. Often, however, there is. With count data, for example, the variance typically increases with the mean.\nBottom-right: This detects leverage - which means points that have undue influence on the gradient of the fitted line, and outliers. If you have outliers in your data, you need to decide what to do with them.\n\nIn the case of these data, we are good to go! There is no discernible pattern in either of the left-hand plots, the qq-plot is about as straight as you ever see with real data, and there are no points exerting undue high influence.\n\n\n13.2.6 Interpretation of the model\nNow that we have established that the data meet the criteria required for the model to be valid, we can go ahead and inspect its output. We will do this using two tools that we also use for every other general linear model we implement (t-test, ANOVA etc). These are anova() and summary()\nLet us first use anova():\n\nanova(model_pgr)\n\nAnalysis of Variance Table\n\nResponse: plant.growth.rate\n                      Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nsoil.moisture.content  1 2521.15 2521.15  156.08 &lt; 2.2e-16 ***\nResiduals             48  775.35   16.15                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe F value here is an example of a ‘test statistic’, a number that a test calculates from the data, from which it is possible to further calulate how likely it is that you would have got the data you got if the null hypothesis were true. This particular test statistic is the ratio of the variation in the data that is explained by the explanatory variable to the leftover variance. The bigger it is, the better the job that the explanatory variable is doing at explaining the variation in the dependent variable. The p value, which here is effectively zero, is the chance you would have got an F value this big or bigger from the data in the sample if in fact there were no relationship between plant growth rate and soil moisture content. If the p value is small (and by that we usually mean less than 0.05) then we can reject the null hypothesis that there is no relationship between plant growth rate and soil moisture content.\nHence, in this case, we emphatically reject the null: there is clear evidence that plant growth rate is at least in part explained by soil moisture content.\nNow we use the summary() function:\n\nsummary(model_pgr)\n\n\nCall:\nlm(formula = plant.growth.rate ~ soil.moisture.content, data = plants)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9089 -3.0747  0.2261  2.6567  8.9406 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             19.348      1.283   15.08   &lt;2e-16 ***\nsoil.moisture.content   12.750      1.021   12.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.019 on 48 degrees of freedom\nMultiple R-squared:  0.7648,    Adjusted R-squared:  0.7599 \nF-statistic: 156.1 on 1 and 48 DF,  p-value: &lt; 2.2e-16\n\n\nThis gives us estimates of the intercept (19.348) and gradient (12.750) of the best fit line through the data. The null hypothesis is that both these values are zero, and the p-value is our clue as to whether we can reject this null. Here, in both cases, we clearly can.\nWe also see the Adjusted R-squared value of 0.7599. This is the proportion of the variance in the dependent variable that is explained by the explanatory variable. Thus it can vary between 0 and 1. A large value like this indicates that soil moisture content is a good predictor of plant growth rate.\n\n\n13.2.7 Back to the figure\nTypically, a final step in our analysis involves including the model we have fitted into the original figure, if that is possible in a straightforward way. In the case of simple linear regression, it is. It means adding a straight line with the intercept and gradient displayed by the summary() function. We do this by adding a line geom_smooth(method = \"lm\") to our plot code:\n\nplants |&gt;\n  ggplot(aes(x=soil.moisture.content, y=plant.growth.rate)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x=\"Soil moisture content\",\n       y=\"Plant growth rate (mm/week)\") +\n  xlim(0,2) +\n  ylim(0,50) +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nThis gives both a straight line and the ‘standard error’ of that line - meaning, roughly speaking, the wiggle room within which the ‘true’ line , for the population as opposed to this sample drawn from it, probably lies.\n\n\n13.2.8 Report the result\nWe would likely want to include this plot in our report, along with a statement like:\nWe find evidence for a linear increase in plant growth rate with soil moisture content (p&lt;0.001), with an additional 12.75 mm of growth per unit increase in soil moisture content.\n\n\n13.2.9 Conclusion\nWe have carried out a simple linear regression on continuous data. This is an example of a general linear model. We first plotted the data, then we used lm() to fit the model. Next we inspected the validity of the model using autoplot. We then inspected the model itself using first anova() then summary(). Finally we included the output of the model on the plot, in this case by adding to it a straight line with the intercept and gradient determined by the regression model, and reported the result in plain English.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple-linear-regression.html#simple-linear-regression---ocean-ph",
    "href": "simple-linear-regression.html#simple-linear-regression---ocean-ph",
    "title": "13  Simple linear regression",
    "section": "13.3 Simple linear regression - ocean pH",
    "text": "13.3 Simple linear regression - ocean pH\nIn this second example we provide the script but leave you to interpret the outcome of each step. Use the code snippets below to help you complete the notebook ocean-pH.qmd that is provided for you in the notebooks subfolder of the project lined to this chapter (if you haven’t aleady, you can download this using the link at the hed of this chapter.) If you do this then your notebook should follow exactly the same steps as for the plant growth example, but using different names.\nHere we use data from Figure 5.20 of AR6, WG1 from the IPCC. It shows ocean pH measurements from a location (137\\(^{\\circ}\\)E, 5\\(^{\\circ}\\)N) in the western Pacific between 1980 and 2020.\nYour task is to assess whether there is a significant linear trend in pH with time and if so to determiine the change in pH per year or per decade during the forty year period from 1980 to 2020.\n\n13.3.1 Load packages\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggfortify)\nlibrary(cowplot)\n\n\n\n13.3.2 Load data\n\npH_filepath &lt;- here(\"data\",\"ipcc_AR6_WGI_Figure_5_20-pH.csv\")\npH&lt;-read_csv(pH_filepath,skip=6) # we have to skip the first 6 lines because of meta-data included in the file - check it out!\nglimpse(pH)\n\nRows: 81\nColumns: 2\n$ year &lt;dbl&gt; 1984.121, 1985.171, 1986.066, 1987.060, 1987.488, 1988.058, 1989.…\n$ pH   &lt;dbl&gt; 8.100000, 8.097917, 8.086458, 8.102604, 8.071875, 8.098437, 8.095…\n\n\n\n\n13.3.3 Plot data\n\npH |&gt;\n  ggplot(aes(x = year, y = pH)) +\n  geom_point() +\n  labs(x = \"Year\",\n       y = \"Ocean pH\") +\n  scale_y_continuous(limits=c(8.0,8.12), breaks=seq(8.0,8.2,0.02)) + #&gt; 1\n  theme_cowplot()\n\n\n\n\n\n\n\n\nIs there a linear trend?\n\n\n13.3.4 Fit linear model\n\npH_model &lt;- lm(pH ~ year, data= pH)\n\n\n\n13.3.5 Check model validity\n\nautoplot(pH_model) + theme_cowplot()\n\n\n\n\n\n\n\n\nIs it reasonable to apply a linear model to these data? Remember that each of these plots tell you something about whether this is the case.\n\n\n13.3.6 Inspect model\nANOVA\n\nanova(pH_model)\n\nAnalysis of Variance Table\n\nResponse: pH\n          Df    Sum Sq   Mean Sq F value    Pr(&gt;F)    \nyear       1 0.0169414 0.0169414  199.87 &lt; 2.2e-16 ***\nResiduals 79 0.0066962 0.0000848                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAre we OK to reject the null hypothesis that pH does not change over this time period?\nSummary\n\nsummary(pH_model)\n\n\nCall:\nlm(formula = pH ~ year, data = pH)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0247092 -0.0049642  0.0002185  0.0060392  0.0168809 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.234405   0.224330   50.08   &lt;2e-16 ***\nyear        -0.001583   0.000112  -14.14   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.009207 on 79 degrees of freedom\nMultiple R-squared:  0.7167,    Adjusted R-squared:  0.7131 \nF-statistic: 199.9 on 1 and 79 DF,  p-value: &lt; 2.2e-16\n\n\nWhat is the change in ocean pH per decade over the last four decades? Is this change statistically significant? Does the linear model account for much of the variance in the data?\n\n\n13.3.7 Replot the data, model included\n\npH |&gt;\n  ggplot(aes(x = year, y = pH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", colour = \"blue\") + # add the line. Method =\"lm\" for a straight line.\n  labs(x = \"Year\",\n       y = \"Ocean pH\") +\n  scale_y_continuous(limits=c(8.0,8.12), breaks=seq(8.0,8.2,0.02)) + # fix limits and break points of y-axis\n  annotate(geom=\"text\", x = 2015, y = 8.11, label = \"137W, 5N\", size = 5) + # measurement site\n  theme_cowplot()\n\n\n\n\n\n\n\n\nHow would you report this result?",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple-linear-regression.html#multivariate-data",
    "href": "simple-linear-regression.html#multivariate-data",
    "title": "13  Simple linear regression",
    "section": "13.4 Multivariate data",
    "text": "13.4 Multivariate data\n\n13.4.1 Tree volume from height and girth\nWe now consider the common case where an outcome might arise as the combined result of more than one explanatory variable.\nSuppose we wanted to know the potential timber yield from a tree. The direct but destructive way to find ot would be to chop it down and weigh it, but a more useful and non-destructive route would be if we could estimate it from variables that we could easily measure that would be reasonable predictors of timber yield, for example the girth of the trunk as measured by its diameter at breast height, and its height, which can easily be found using a protractor, a tape measure and simple trigonometry.\n\n  here(\"data\", \"tree-volume.csv\") |&gt;\n  read_csv() |&gt;\n  # filter(diameter &lt; 1.6) |&gt;\n  glimpse() -&gt; \n  trees\n\nRows: 31\nColumns: 3\n$ diameter &lt;dbl&gt; 0.6917, 0.7167, 0.7333, 0.8750, 0.8917, 0.9000, 0.9167, 0.916…\n$ height   &lt;dbl&gt; 70, 65, 63, 72, 81, 83, 66, 75, 80, 75, 79, 76, 76, 69, 75, 7…\n$ volume   &lt;dbl&gt; 10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9, 2…\n\n\nWe would like to know if there is correlation between the potential predictors. If there is, we call this collinearity. We need to be aware of it since it can make i dificult to decide on the influence of any individual predictor.\nLet us look at the correlations between all three variables:\n\nGGally::ggpairs(trees)\n\n\n\n\n\n\n\n\nTree volume is strongly correlated with tree girth and moderately correllated with tree height.\nWe see that there is correlation between girth and height, but it is not catastrophic. Since girth and height are our predictors, we sy that there is moderate collinearity between them.\nFirst attempt at analysis: naive multiple linear regression\n\nvol_dh &lt;- lm(volume ~  diameter + height, data = trees)\n\n\nautoplot(vol_dh) + theme_cowplot()\n\n\n\n\n\n\n\n\n\nanova(vol_dh)\n\nAnalysis of Variance Table\n\nResponse: volume\n          Df Sum Sq Mean Sq  F value  Pr(&gt;F)    \ndiameter   1 7581.9  7581.9 503.2967 &lt; 2e-16 ***\nheight     1  102.4   102.4   6.7953 0.01448 *  \nResiduals 28  421.8    15.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(vol_dh)\n\n\nCall:\nlm(formula = volume ~ diameter + height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4042 -2.6496 -0.2853  2.1998  8.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -57.9876     8.6370  -6.714 2.74e-07 ***\ndiameter     56.4992     3.1708  17.819  &lt; 2e-16 ***\nheight        0.3392     0.1301   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.881 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: &lt; 2.2e-16\n\n\nThis analysis is telling us that girth is a highly significant predictor of volume (p&lt;0.001) but that height is only weakly signficant (p = 0.0145).\nThis may not be true. The effect of height may bebeing masked by that of girth, which was given to the model first.\nVariance Inflation Factor (VIF) scores as way of detecting co-linearity\nVariable inflation factor (VIF) scores tell us if there is co-linearity. The higher the score, the more likely that variable is affected by co-linearity.\n\n\n\nVIF\nCo-liniarity\n\n\n\n\n1 -5\nno collinearity\n\n\nVIF &gt; 5\nconcerning\n\n\nVIF &gt; 10\nserious\n\n\n\n\nlibrary(car)\nvif(vol_dh)\n\ndiameter   height \n1.369214 1.369214 \n\n\nOur VIF scores are both low, suggesting there is no need to take account of co-linearity.\nA better statistical model: transformation\nTHe above analysis shows that we could predict tree volume adequately using both diameter and height as predictors in an additive model. However a better approach might be to look more carefully at how volume arises from a combinaion of height and diameter.\nTree volume scales approximately with cross-sectional area × height, so a log–log model is more realistic: logarithms turn mutliplication into addition: log(xy) = log(x) +log(y), so if volume = diamter x height, then log(volume) = log(diameter) x log(height):\n\nvol_dh_log &lt;- lm(log(volume) ~ log(diameter) + log(height), data = trees)\n\n\nautoplot(vol_dh_log)\n\n\n\n\n\n\n\n\n\nanova(vol_dh_log) + theme_cowplot()\n\nNULL\n\n\n\nsummary(vol_dh_log)\n\n\nCall:\nlm(formula = log(volume) ~ log(diameter) + log(height), data = trees)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.168549 -0.048520  0.002473  0.063630  0.129170 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -1.70475    0.88184  -1.933   0.0634 .  \nlog(diameter)  1.98271    0.07501  26.434  &lt; 2e-16 ***\nlog(height)    1.11708    0.20442   5.465  7.8e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08138 on 28 degrees of freedom\nMultiple R-squared:  0.9777,    Adjusted R-squared:  0.9761 \nF-statistic: 613.3 on 2 and 28 DF,  p-value: &lt; 2.2e-16\n\n\nSO now both predictors ae highly significant.\n\n\n13.4.2 Environmental Indicators data set\nThe dataset For this exerecise, we make use of the enviro_indicators dataset from the Kaggle Global Environmental Indicators data. The dataset comprises socio-economic and environmental data collected from various countries, focusing on factors related to sustainable development. It includes information such as forest coverage, biodiversity index, protected areas, rural population, and deforestation rates, aimed at understanding the relationship between human activities and land degradation.\n\nenv_data &lt;- read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/regression_sprint/enviro_indicators.csv')\nenv_data &lt;-env_data |&gt;\n  dplyr::select(Country, biodiversity_index, forest_coverage, deforestation_rate, soil_erosion, population_density)\nglimpse(env_data)\n\nRows: 32\nColumns: 6\n$ Country            &lt;chr&gt; \"Vietnam\", \"Guinea-Bissau\", \"Bosnia and Herzegovina…\n$ biodiversity_index &lt;dbl&gt; 6.505159, 94.888554, 96.563203, 80.839735, 30.46137…\n$ forest_coverage    &lt;dbl&gt; 36.21781, 76.55000, 61.23958, 51.90609, 20.92130, 2…\n$ deforestation_rate &lt;dbl&gt; 36.58311, 123.38494, 115.74374, 111.75237, 46.45033…\n$ soil_erosion       &lt;dbl&gt; 7.987880, 13.013811, 13.037065, 11.179719, 2.715506…\n$ population_density &lt;dbl&gt; 468.71584, 351.05460, 289.32997, 57.61648, 311.3535…\n\n\n\nGGally::ggpairs(env_data[,2:6]) \n\n\n\n\n\n\n\n\nOnly deforestion rate appears to be correlated with biodiversity index or to have a clear linear relation with it. However forest coverage is positively correlated and may have a weak linear relationship with biodiversity index, so let us include both as predictors in a linear regression model to predict biodiversity index\nFirst we make an additive model\n\nbi.model.additive &lt;- lm(biodiversity_index ~ deforestation_rate + forest_coverage, data= env_data)\n\nThen we make an interactive model, where we explicityl include the possibility that the effect of deforestation rate might depend on the value of forest coverage, and vice-versa.\n\nbi.model.interactive &lt;- lm(biodiversity_index ~ deforestation_rate * forest_coverage, data= env_data)\n\nNow we use the anova() function to see if the siimpler model, which is nested within the first, has equally good explanatory power. The null hypothesis of this test is that it does.\n\nanova(bi.model.interactive,bi.model.additive)\n\nAnalysis of Variance Table\n\nModel 1: biodiversity_index ~ deforestation_rate * forest_coverage\nModel 2: biodiversity_index ~ deforestation_rate + forest_coverage\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     28 2073.5                           \n2     29 2133.3 -1   -59.824 0.8078 0.3764\n\n\nThe p value is high which suggest that there is no need for the interactive model, so we stick with the simpler additive model\nNow we chwck that it is OK to believe the results of a linear regression model\n\nautoplot(bi.model.additive)\n\n\n\n\n\n\n\n\nThis all looks fine so on we go\n\nanova(bi.model.additive)\n\nAnalysis of Variance Table\n\nResponse: biodiversity_index\n                   Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ndeforestation_rate  1 27353.1 27353.1 371.829 &lt; 2.2e-16 ***\nforest_coverage     1  2658.2  2658.2  36.134 1.545e-06 ***\nResiduals          29  2133.3    73.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis tells us that both deforestation rate ands forest cover are significan explanatory variables.\n\nsummary(bi.model.additive)\n\n\nCall:\nlm(formula = biodiversity_index ~ deforestation_rate + forest_coverage, \n    data = env_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.7119  -5.4799   0.4557   5.5636  21.8836 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        -9.40170    4.11993  -2.282     0.03 *  \ndeforestation_rate  1.19271    0.06158  19.369  &lt; 2e-16 ***\nforest_coverage    -0.57052    0.09491  -6.011 1.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.577 on 29 degrees of freedom\nMultiple R-squared:  0.9336,    Adjusted R-squared:  0.9291 \nF-statistic:   204 on 2 and 29 DF,  p-value: &lt; 2.2e-16\n\n\nThe null hypothesis here is that the values of te parameters are zero. We can reject that null for both forest cover and deforestation rate.\nThe estimate column tells us the amount by which the biodiversity index will change if either of the factors increases by one unit, while keeping the other factor constant.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "GLM.html",
    "href": "GLM.html",
    "title": "14  GLM introduction",
    "section": "",
    "text": "14.1 What to do when the linear model is not applicable?\nThis chapter draws on Michael Crawley’s Statistics: An introduction using R, 2nd Ed, Wiley.\nIn many settings we have a response variable such as a weight, length or concentration that is a continuous real number.\nFor many common analyses of those variables, such as t-test, regression, ANOVA, or in fact any manifestation of the linear model, the data needs to have the following attributes:\nIf one or more of these assumptions is wrong, the analyses become unreliable in that the claims that they make will be based on assumptions that are false.\nIn ecology and many other fields, this is very commonly the case.\nConsider for example the variance of some commonly encountered types of data:\nCount data\nThese are whole numbers that can never be negative. That straight away gives a problem with the assumptons of the linear model if the counts are typically low. To have ‘normally distributed errors’ would require that some of the counts be negative, which is nonsense.\nRegardless of that, it is the case with many examples of count data - examples where we know how often something did happen, but not how often it did not happen - that the higher the counts, the higher the variance of the counts: think say of a situation where you were recording the numbers of individuals of some species per unit time or area. If the mean number were 5 then you might find counts that varied, say, between 3 and 7, but if the mean number were 500 then would not expect the individual counts to be constrained between 498 and 502. You would expect a greater variance than when the mean was only 5. This is common for count data but it breaks a key assumption of the linear model.\nCount data also often has the ‘zero inflation’ problem whereby the number of zeros recorded is high.\nWe said that this increase in variance with mean count value is found for count data where we know how often something did happen but not how often it did not happen. Examples of this are counts of lightning strikes, meiofauna on a petri dish, birds in a churchyard,\nProportion data\nSometimes, however, as well as knowing how often something did happen, we also know how often it did not happen. We can express each count as a poportion of the total. We could characterise the first counts as ‘successes’ and the second as ‘failures’, although this can sometimes be somewhat macabre in a medical context. Both counts are important to a full analysis of the data.\nExamples are:\nAs another example you might be studying the behaviour of civets in a zoo in time blocks of 20 minutes. For every minute within each block you record whether the civets appeared agitated. At the end of each block you will know the proportion p of one minute slots within which they were agitated and the proportion 1-p when they were not.\nThis type of data has low variance when proportions are very low, around zero, or very high, around one, since all or nearly all individuals are alike in either case. In between, the variance peaks when the proportion is around 0.5 (50:50) and there are many successes and failures.\nHence, unlike count data where the variance increases monotonically with the mean count, with proportion data the variance is a humped function of the mean. For this reason, proportion data is often analysed using the Binomial distribution. If p is the probability of a success and (1-p) is the probability of a failure, the mean number of successes is np and the variance of this number of success is np(1-p). Hence this variance will be zero if p = 0 or if p = 1, and is maximised if p = 0.5.\nPercentage cover data\nThis is a type of proportion data where the data are continuous but bounded, ranging from 0% to 100%, with values outside that range being impossible and meaningless. You might encounter this when doing flora and fauna surveys with quadrats, and are recording a species for which individual counts are not possible, for example barnacles or many macroalgae on a rocky shore, or grasses in a meadow. Its variance will vary in a similar way to proportion data derived from count data: tendding towards zero for either 0% or 100% cover, and maximal when the cover is around 50%. In this case, a common approach is to arcsine transform the data, and then analyse it using linear models, but consider usin the GLM approaches discussed below.\nA common approach to try to solve at least part of the problem is to ‘transform’ the response data such that its distribution more resembles that of a ‘normal’ distribution. This usually means operating on it with functions such as square root or log that have the effect of pulling in the long tails of highly skewed distributions. This approach has its limitations. For one thing, it is the errors (ie the difference between the actual and the predicted values) that should be normally distributed, not the the data themselves.\nA more powerful and versatile approach is to use generalized linear models, commonly referred to as GLMs.",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>GLM introduction</span>"
    ]
  },
  {
    "objectID": "GLM.html#introduction-to-genralized-linear-models-glms",
    "href": "GLM.html#introduction-to-genralized-linear-models-glms",
    "title": "14  GLM introduction",
    "section": "14.2 Introduction to Genralized Linear Models (GLMs)",
    "text": "14.2 Introduction to Genralized Linear Models (GLMs)\nA generalized linear model has three important properties:\n\nthe error structure\nthe linear predictor\nthe link function\n\nWhile none of these concepts is likely to be familiar, the ideas behind each of them are straightforward and it is worth getting to know what each one involves.",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>GLM introduction</span>"
    ]
  },
  {
    "objectID": "GLM.html#the-error-structure",
    "href": "GLM.html#the-error-structure",
    "title": "14  GLM introduction",
    "section": "14.3 The Error Structure",
    "text": "14.3 The Error Structure\nIn the linear model, which underpins such analyses as the various forms of t-test, ANOVA, linear regression or pearson correlation, the errors are assumed to be normally distributed. By ‘error’ we means the difference between the observed value and the mean of a population, or the difference between an observed or a fitted value, as in regression.\nOften, this is not the case, certainly in ecology and in many studies within the life sciences. It is common to have data for which the errors do not have a normal distribution.\nThe errors may be:\n\nstrongly skewed - a few errors are much larger (most common) or smaller than the rest\nstrongly kurtotic - much more scrunched up or spread out than would be consistent with a normal distribution\nstrictly bounded - they can’t imply values beyond the realms of what is phyiscally possible, as in proportion data.\nstrictly positive - as with count data, where negative values are meaningless.\n\nBefore the advent of GLMs, any of these error issues would have required the use either of a transformation of the response data or the use of non-parametric methods, whereby the response data were ranked and only those ranks were then used in subsequent analysis, the actual values being ignored, with consequent loss of power as a result of not using this information. A GLM allows one to specify a variety of commonly encountered types of error distribution:\n\nPoisson errors - useful with count data\nbinomial errors - useful with proportion data\ngamma errors - useful with data that show a constant coefficient of variation\nexponential errors - useful with data on time to death (survival analysis)\n\nIn R the error structure is defined by means of the family argument, thus:\nglm(y ~ z, family = poisson)\nif the response variable has Poisson errors\nor\nglm(y ~ z, family = binomial)\nif the response is binary and has binomial errors.\nThe response variable z can be continuous, leading to a regression analysis, or categorical, leading to an ANOVA-like procedure called analysis of deviance.",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>GLM introduction</span>"
    ]
  },
  {
    "objectID": "GLM.html#the-linear-predictor",
    "href": "GLM.html#the-linear-predictor",
    "title": "14  GLM introduction",
    "section": "14.4 The linear predictor",
    "text": "14.4 The linear predictor\nIn a GLM, each observed value y is related to a predicted value, which is obtained by transformation of the value emerging from the linear predictor \\(\\eta\\). This linear predictor is a linear sum of the effects of one or more explanatory variables \\(x_j\\). It is what you would see if you asked in R for summary.lm.\n\\[\n\\eta_i = \\sum_{j=1}^px_{ib}\\beta_j\n\\] The \\(x\\)s are the values of the p different explanatory variables and the \\(\\beta\\)s are the (usually) unknown poarameters to be estimated from the data. The right-hand side of this equation is called the linear structure. Thre are as many terms in this as there are parameters to be estimated, that is p.",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>GLM introduction</span>"
    ]
  },
  {
    "objectID": "GLM.html#fitted-values",
    "href": "GLM.html#fitted-values",
    "title": "14  GLM introduction",
    "section": "14.5 Fitted values",
    "text": "14.5 Fitted values\n\n14.5.1 Deviance: a general measure of variability\nDeviance is a generalisation of the concept of variance: it is a measure of the lack of fit of the model to the data.\n\\[\n\\text{deviance} = -2\\times\\text{log likelihood}\n\\]\nwhere the log likelihood depends on the model, given the data. Deviance is the same as variance when we have constant variance and normally distributed errors, as with linear regression, ANOVA and ANCOVA. That is, the appropriate measure of lack of it is \\((y-\\hat{y})^2\\). For count data, however, we need another measure of lack of fit, which turns out to be \\(y\\log(\\frac{y}{\\hat{y}})\\). Other types of data need yet other measures of lack of it. Here is a summary table of them:\n\n\n\n\n\nModel\nDeviance\nError\nLink\n\n\n\n\nlinear\n$\\sum \\left(y-\\hat{y}\\right)^2$\nGaussian\nidentity\n\n\nlog linear\n$2\\sum y\\log{\\left(\\dfrac{y}{\\hat{y}}\\right)}$\nPoisson\nlog\n\n\nlogistic\n$2\\sum y\\log{\\left(\\dfrac{y}{\\hat{y}}\\right)} +(n-y)\\log{\\left(\\dfrac{n-y}{n-\\hat{y}}\\right)}$\nbinomial\nlogit\n\n\ngamma\n$2\\sum {\\dfrac{(y-\\hat{y})}{y} - \\log{\\left(\\dfrac{y}{\\hat{y}}\\right)} }$\ngamma\nreciprocal",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>GLM introduction</span>"
    ]
  },
  {
    "objectID": "GLM.html#the-link-function",
    "href": "GLM.html#the-link-function",
    "title": "14  GLM introduction",
    "section": "14.6 The link function",
    "text": "14.6 The link function\nThe predicted value of y is related to the linear predictor value \\(\\eta\\) via the link function g:\n\\[\n\\eta = g(\\mu)\n\\] What we mean by this is that the value of \\(\\eta\\) is obtained by transforming y by the link function, and the predicted value of y is obtained by applying the inverse of the link function to \\(\\eta\\).\nIn the case of the identity link, the linear predictor is the predicted value of y. Hence, in simple linear regression, we are in effect using a special case of the GLM, one where the link function is the identity ie do nothing.",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>GLM introduction</span>"
    ]
  },
  {
    "objectID": "GLM.html#canonical-link-functions",
    "href": "GLM.html#canonical-link-functions",
    "title": "14  GLM introduction",
    "section": "14.7 Canonical link functions",
    "text": "14.7 Canonical link functions\nThe most commonly used link functions are shown below. A key requirement of any link function is to ensure that fitted values stay within reasonable bounds. Thus if the response is a count, the fitted values should all be zero or positive, and if a proportion, should be between zero and one, and so on.\nThus for count data, the log function is an appropriate link function, since the fitted values are then antilogs of the linear predictor, and so are guaranteed to be greater than or equal to zero. For proportion data, the logit function is appropriate since the fitted value is then the antilog of the log odds \\(\\log(\\frac{p}{q})\\), ie \\(\\frac{p}{q}\\), which is a proportion, guaranteed to be between zero and one since \\(p\\leq q\\) and both \\(p\\) and \\(q\\) are non-negative.\n\n\n\nError\nCanonical Link\n\n\n\n\ngaussian\nidentity\n\n\npoisson\nlog\n\n\nbinomial\nlogit\n\n\nGamma\nreciprocal",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>GLM introduction</span>"
    ]
  },
  {
    "objectID": "glm_poisson_with_count_data.html",
    "href": "glm_poisson_with_count_data.html",
    "title": "\n15  GLM poisson with count data\n",
    "section": "",
    "text": "15.1 Preliminaries\nThis exercise is taken from Chapter 7 of Beckerman, Childs and Petchey: Getting Started with R\nWe study variation of number of offspring of Soay sheep with mother’s body weight. These are count data. They are not normally distributed since they are discrete and bounded by zero - a ewe cannot have a negative number of births. Moreover, the variance of the data set is not constant - the spread of values tends to increase the more offspring a ewe has. This is common (but not universal) with count data.\nLoad packages\nlibrary(tidyverse)\nlibrary(ggfortify) \nlibrary(here)\nlibrary(cowplot)\nLoad sheep data\nfilepath &lt;- here(\"data\", \"soay_sheep_fitness.csv\")\nsoay &lt;- read_csv(filepath)\n\n# check its structure\nglimpse(soay)\n\nRows: 50\nColumns: 2\n$ fitness   &lt;dbl&gt; 4, 3, 2, 14, 5, 2, 2, 5, 8, 4, 12, 6, 3, 2, 3, 0, 5, 3, 5, 6…\n$ body.size &lt;dbl&gt; 6.373546, 7.183643, 6.164371, 8.595281, 7.329508, 6.179532, …\nMake the first plot to explore the data\nsoay |&gt;\n ggplot(aes(x = body.size, y = fitness)) +\n geom_point() +\n geom_smooth(method = 'lm', se = FALSE, linewidth = 0.5) +\n xlab('Body mass (kg)') +\n ylab('Lifetime fitness') +\n theme_cowplot()\nA few things stand out from this plot:",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>GLM poisson with count data</span>"
    ]
  },
  {
    "objectID": "glm_poisson_with_count_data.html#preliminaries",
    "href": "glm_poisson_with_count_data.html#preliminaries",
    "title": "\n15  GLM poisson with count data\n",
    "section": "",
    "text": "There is a clear trend: as the ewe’s body mass increases, so does the number of offspring tend to increase. That makes sense: a heavier ewe would have more resources to support one lamb after another.\nThe trend is not linear. There is a clear upwards curve.\nThe spread of values increases as the mother’s body mass increases. This is common with count data.\nThere are no negative response values - while a ewe could have no lambs, it definitely cannot have a negative number of lambs.",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>GLM poisson with count data</span>"
    ]
  },
  {
    "objectID": "glm_poisson_with_count_data.html#the-wrong-way-to-analyse-the-data---do-a-linear-model",
    "href": "glm_poisson_with_count_data.html#the-wrong-way-to-analyse-the-data---do-a-linear-model",
    "title": "\n15  GLM poisson with count data\n",
    "section": "\n15.2 The wrong way to analyse the data - do a linear model",
    "text": "15.2 The wrong way to analyse the data - do a linear model\nWe should have an inkling that this is the wrong way since the variation of offspring with body weight is clearly not linear!\nFor insight, let’s do it anyway using lm(), and look at the diagnostic plots we get when we give the resulting model soay.linear to autoplot()\n\nsoay.linear &lt;- lm(fitness ~ body.size, data = soay)\n\n\n15.2.1 Diagnostic plots of linear model\n\nautoplot(soay.linear, smooth.colour = NA) + theme_cowplot()\n\n\n\n\n\n\n\nThese plots show us that these data violate all the properties required of data before they can be sensibly analysed using a linear model.\n\n15.2.1.1 Top-left: residuals vs fitted values\nThis has a clear U-shape, when, for a linear model to be appropriate, there should be no systematic pattern. This is a consequence of trying to fit a straight lines through a data cloud that has a clear non-linear trend. We underestimate fitness at small body sizes, then over-estimate at middling body sizes and finally underestimate it again at the largest body sizes\n\n15.2.1.2 Top-right: the normal Q-Q plot\nThis is not right: for a linear model to be valid the points should be scattered around a straight line. Here, there is systematic deviation from that at both ends, in a way that indicates over-dispersion of the residuals compared to what they would be if they were normally distributed see this for an explanation of Q-Qplots. Over dispersion means bring more spread out, in particular with thicker tails, than would be the case with normally distributed data.\n\n15.2.1.3 Bottom left: Scale location plot\nThis plot shows us that there is a positive relationship between the absolute size of the residuals and the fitted values. This reflects the increasing vertical spread of the data. For a linear model to be appropriate, the spread of residuals values should be approximately constant across the whole range of fitted values.\n\n15.2.1.4 Bottom-right: residuals leverage plot\nThis is actually not too bad. There are no obvious outliers, so no single poiunt will have an undue influence on the analysis.\nHowever, one out four is not good enough, as Meatloaf didn’t quite say. The linear model clearly does not do a very good job of describing the data, so we need a different approach.",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>GLM poisson with count data</span>"
    ]
  },
  {
    "objectID": "glm_poisson_with_count_data.html#the-right-way---using-a-glm",
    "href": "glm_poisson_with_count_data.html#the-right-way---using-a-glm",
    "title": "\n15  GLM poisson with count data\n",
    "section": "\n15.3 The right way - using a GLM",
    "text": "15.3 The right way - using a GLM\nThe linear model is based on the errors of the data being normally distributed, but this is very often not the case for count data, for at least these obvious reasons:\n\nThe normal; distribution concerns continuous variables that may be fractional, whereas count data are discrete 0, 1, 2… Ewes cannot give birth to 2.5 offspring.\nThe normal distribution allows negative values, whereas count data cannot be negative. They can be zero, but not negative. You can’t find -2 limpets in a quadrat, witness -3 instances of a behaviour or give birth to -1 offspring!\nThe normal distribution is symmetrical whereas count data very often are not symmetrically distributed, in part because negative values are not possible but occasional very large values are.\n\nHence, the normal distribution is just not a very good model for many kinds of count data. The Poisson distribution, on the other hand is a good starting point for the analysis of certain kinds of count data. We mean by that the kind where, while we may know how often something did happen, we do not know how often it did not happen, and where occasional large count values are possible ie there is no upper bound to possible count values. This might include, say, counting meiofauna on a microscope slide, offspring (!), goals per game in football, decays per given time interval in a radioactive sample, mutations per unit length in a strand of DNA, and many, many more.\nInstances where we do also know how often something did not happen, as well as how often it did, such as the occupancy or not among a fixed total of nest boxes, are best analysed as proportion data using the binomial distribution. This can also be handled within a GLM framework, but we shall not discuss it here.\n\n\n\n\n\n\nThe Poisson distribution\n\n\n\nIn case you want to know, here is the mathematical form of the Poisson distribution for a random non-negative, discrete variable x that is distributed according to this distribution. It is specified entirely by its mean value \\lambda, which is also the value of its variance. Hence, the variance increases with the mean.\n\nP(x) = \\frac{e^{-\\lambda}\\lambda^x}{x!}\n\nWe won’t here go into the details of the Poisson distribution, but will limit ourselves to presenting three examples of it, for mean count values of 2, 4 and 6. The x axes show the range of possible counts, and the y axis shows the probability of each value.\nThese examples illustrate why the Poisson distribution is a good choice for use with unbounded (That is, no upper bound. There is a bound of zero at the lower end) count data since it overcomes all the issues discussed above:\n\nonly discrete values 0, 1, 2,… are possible. These are bounded at zero. High count values are possible but are increasingly unlikely.\nthe variance of the the distribution increases as the mean value increases - in fact, the variance and the mean are equal. We see this as a fattening of the distribution as the mean count value increases.\n\n\n\n\n\n\n\n\n\n\n\n\nsoay.glm &lt;- glm(fitness ~ body.size, data = soay, family = poisson(link = log))\nautoplot(soay.glm, smooth.colour = NA) + theme_cowplot()\n\n\n\n\n\n\n\nThese are much better than they were with the linear model. The autoplot() function ensures that these will look right if the error distribution of the model does indeed follow a Poisson distribution.",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>GLM poisson with count data</span>"
    ]
  },
  {
    "objectID": "glm_poisson_with_count_data.html#interpret-the-glm",
    "href": "glm_poisson_with_count_data.html#interpret-the-glm",
    "title": "\n15  GLM poisson with count data\n",
    "section": "\n15.4 Interpret the GLM",
    "text": "15.4 Interpret the GLM\nTo do this we use the same anova() and summary() functions that we are used to suing with lm(). For something produced by lm(), anova() produces an ANOVA table, but for someting produced by glm() it produces an Analysis of Deviance table.\n\n# use anova() and summary() to interpret the model\nanova(soay.glm) # sequential sums of squares table\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: fitness\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                         49     85.081              \nbody.size  1   37.041        48     48.040 1.157e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn GLMs\n\nsummary(soay.glm) # coefficients table\n\n\nCall:\nglm(formula = fitness ~ body.size, family = poisson(link = log), \n    data = soay)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.42203    0.69432  -3.488 0.000486 ***\nbody.size    0.54087    0.09316   5.806 6.41e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 85.081  on 49  degrees of freedom\nResidual deviance: 48.040  on 48  degrees of freedom\nAIC: 210.85\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>GLM poisson with count data</span>"
    ]
  },
  {
    "objectID": "glm_poisson_with_count_data.html#remake-the-figure-and-add-fitted-line",
    "href": "glm_poisson_with_count_data.html#remake-the-figure-and-add-fitted-line",
    "title": "\n15  GLM poisson with count data\n",
    "section": "\n15.5 Remake the figure and add fitted line",
    "text": "15.5 Remake the figure and add fitted line\nFind min and max of the x data\n\nmin.size &lt;- min(soay$body.size)\nmax.size &lt;- max(soay$body.size)\n\nMake the new.x values.\nWe use the ‘body.size’ variable name to name the column just as it is in the orginal data\n\nnew.x &lt;- expand.grid(body.size = seq(min.size, max.size, length.out = 1000))\n\nGenerate fits and standard errors\n\nnew.y &lt;- predict(soay.glm, newdata = new.x, se.fit = TRUE)\nnew.y &lt;- data.frame(new.y)\nhead(new.y)\n\n        fit    se.fit residual.scale\n1 0.1661991 0.2541777              1\n2 0.1682619 0.2538348              1\n3 0.1703247 0.2534919              1\n4 0.1723874 0.2531491              1\n5 0.1744502 0.2528063              1\n6 0.1765130 0.2524635              1\n\n\nHousekeeping to bring new.x and new.y together\n\naddThese &lt;- data.frame(new.x, new.y)\naddThese &lt;- rename(addThese, fitness = fit)\nhead(addThese)\n\n  body.size   fitness    se.fit residual.scale\n1  4.785300 0.1661991 0.2541777              1\n2  4.789114 0.1682619 0.2538348              1\n3  4.792928 0.1703247 0.2534919              1\n4  4.796741 0.1723874 0.2531491              1\n5  4.800555 0.1744502 0.2528063              1\n6  4.804369 0.1765130 0.2524635              1\n\n\nCalculate confidence intervals and include them in addThese\n\naddThese &lt;- mutate(addThese, \n lwr = fitness-1.96*se.fit, \n upr = fitness +1.96*se.fit)\n\nPlot the final figure\n\nggplot(soay, aes(x = body.size, y = fitness)) +\n # first show the raw data\n geom_point(size = 3, alpha = 0.5) +\n # now add the fit and confidence intervals\n # we don't need to specify body.size and fitness as they are inherited from above\n geom_smooth(data = addThese, aes(ymin = lwr, ymax = upr), stat = 'identity') +\n theme_cowplot()\n\n\n\n\n\n\n\nThis did not work! We need to account for the link function. For a poisson model, this is the log(). Hence, the predictions are the log of the expected fitness. Here is the fix:\nStart again with addThese\n\naddThese &lt;- data.frame(new.x, new.y)\n# then:\naddThese &lt;- mutate(addThese, \n fitness = exp(fit), \n lwr = exp(fit-1.96*se.fit), \n upr = exp(fit +1.96*se.fit))\nhead(addThese)\n\n  body.size       fit    se.fit residual.scale  fitness       lwr      upr\n1  4.785300 0.1661991 0.2541777              1 1.180808 0.7174951 1.943300\n2  4.789114 0.1682619 0.2538348              1 1.183246 0.7194600 1.946004\n3  4.792928 0.1703247 0.2534919              1 1.185690 0.7214303 1.948712\n4  4.796741 0.1723874 0.2531491              1 1.188138 0.7234059 1.951425\n5  4.800555 0.1744502 0.2528063              1 1.190591 0.7253869 1.954141\n6  4.804369 0.1765130 0.2524635              1 1.193050 0.7273732 1.956861\n\n\nRedo the final plot\n\nggplot(soay, aes(x = body.size, y = fitness)) +\n # first show the raw data\n geom_point(size = 3, alpha = 0.5) +\n # now add the fit and confidence intervals\n # we don't need to specify body.size and fitness as they are inherited from above\n geom_smooth(data = addThese, aes(ymin = lwr, ymax = upr), stat = 'identity') +\n theme_cowplot()",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>GLM poisson with count data</span>"
    ]
  },
  {
    "objectID": "glm_with_count_data_ancova.html",
    "href": "glm_with_count_data_ancova.html",
    "title": "16  GLM with count data",
    "section": "",
    "text": "16.1 Why is simple linear regression often inappropriate for count data?\nSimple linear regression assumes constant variance and normal distrivution of the errors (residuals)\nAll the issues can be handled if instead we use a generalised linear models (GLM)\nThis example is taken from Crawley (2014), p247.\nSuppose we are investigating the effect on species richness in some plots that have different total biomass and one of three different levels of soil acidity: low, medium and high pH. Richness values have been counted on each of 90 plots, 30 with each level of soil pH.\n# these are the packages we will need for this script\nlibrary(tidyverse)\nlibrary(here)\nlibrary(cowplot)\nlibrary(ggfortify)\nspecies &lt;- here(\"data\",\"species.csv\") |&gt; # species.csv needs to be in the data folder within your project folder.\n  read_csv() |&gt;\n  glimpse() \n\nRows: 90\nColumns: 3\n$ pH      &lt;chr&gt; \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\"…\n$ Biomass &lt;dbl&gt; 0.46929722, 1.73087043, 2.08977848, 3.92578714, 4.36679265, 5.…\n$ Species &lt;dbl&gt; 30, 39, 44, 35, 25, 29, 23, 18, 19, 12, 39, 35, 30, 30, 33, 20…\nWe have one response variable Species which is a count, and two predictors: Biomass which is a continuous numerical variable and pH which has been recorded as an ordinal variable with three levels (low, mid, high).\nFirst, a bit of housekeeping. We ensure that the levels of pH are logged in R’s brain in the ordering that makes sense, that is, “low”, then “medium” then “high”, and not as R would unless we did something about it, which is to do so alphabetically” “high” then “low” then “medium”.\nThis minor but important task often comes up when dealing with a categorical variable that has several levels, so this small chunk of code is worth remembering:\nspecies &lt;- species |&gt;\n  mutate(pH = fct_relevel(pH, \"low\", \"mid\", \"high\"))\nNow we’ll plot the data as species richness vs biomass, with a best-fit straight line for each class of pH.\nspecies |&gt;\n  ggplot(aes(x = Biomass, y = Species, colour = pH)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(aes(colour = pH),method = \"lm\", se = F, alpha = 0.2, linewidth = 0.7) +\n  scale_colour_brewer(palette = \"Set1\") +\n  theme_cowplot()",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>GLM with count data</span>"
    ]
  },
  {
    "objectID": "glm_with_count_data_ancova.html#why-is-simple-linear-regression-often-inappropriate-for-count-data",
    "href": "glm_with_count_data_ancova.html#why-is-simple-linear-regression-often-inappropriate-for-count-data",
    "title": "16  GLM with count data",
    "section": "",
    "text": "the linear model might lead to the prediction of negative counts\nthe variance of the response variable is likely to increase with the mean\nthe errors will not be normally distributed\nzeros are difficult to handle in transformations",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>GLM with count data</span>"
    ]
  },
  {
    "objectID": "glm_with_count_data_ancova.html#analysis-using-linear-model-ancova",
    "href": "glm_with_count_data_ancova.html#analysis-using-linear-model-ancova",
    "title": "16  GLM with count data",
    "section": "16.2 Analysis using linear model: ANCOVA",
    "text": "16.2 Analysis using linear model: ANCOVA\nHow did we get these straight lines? We can find the intercept and gradients for these lines by using a linear model, in the form of an ANCOVA (ANalysis of COVAriance), which is just like a two-way ANOVA except that one of the explanatory variables is continuous.\nFirst we try an interactive model, remembering that Biomass*pH is shorthand for Biomass + pH + Biomass:pH, where the Biomass and pH terms are the main effects of each variable, and Biomass:pH is the interaction between them.\n\nmodel_interaction &lt;- lm(Species ~ Biomass * pH, data = species)\n\nIn this model, a non-zero main effect of biomass means that the lines have non-zero gradient - the species richness depends on biomass, irrespective of pH class. A non-zero main-effect of pH means that the lines for each pH class are separate from each other - pH makes a difference to species richness, for all values of biomass. A non-zero interaction between biomass and pH class means that the gradients of the lines are different - the degree to which the species richness varies with one factor depends on the value of the other factor.\nTo see if any of these terms are significant, we use the anova() function:\n\nanova(model_interaction)\n\nAnalysis of Variance Table\n\nResponse: Species\n           Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nBiomass     1  838.5  838.51  57.5126 4.105e-11 ***\npH          2 6268.9 3134.43 214.9878 &lt; 2.2e-16 ***\nBiomass:pH  2    6.3    3.13   0.2147    0.8073    \nResiduals  84 1224.7   14.58                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis tells us that there is no evidence for an interaction between biomass and pH class, but that there is a main effect of biomass and of pH. Species richness does vary with biomass, and for any given value of biomass the richness will differ depending on the pH class.\nHow big are these effects?\n\nsummary(model_interaction)\n\n\nCall:\nlm(formula = Species ~ Biomass * pH, data = species)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.290 -2.554 -0.124  2.208 15.677 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    17.84740    1.22510  14.568  &lt; 2e-16 ***\nBiomass        -2.82778    0.45357  -6.235 1.74e-08 ***\npHmid          11.18359    1.76813   6.325 1.17e-08 ***\npHhigh         22.75667    1.83564  12.397  &lt; 2e-16 ***\nBiomass:pHmid   0.26268    0.54557   0.481    0.631    \nBiomass:pHhigh  0.02733    0.51248   0.053    0.958    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.818 on 84 degrees of freedom\nMultiple R-squared:  0.8531,    Adjusted R-squared:  0.8444 \nF-statistic: 97.58 on 5 and 84 DF,  p-value: &lt; 2.2e-16\n\n\nAccording to this model there is evidence that species richness declines with increasing biomass and is substantially greater at higher pH values, but there is no evidence that pH affects the relationship between species richness and biomass.\nMake sure you can see how the intercepts and gradients of the three lines in the plot above can be got from the model summary.\nBut, is it really true? Do we believe what it is telling us, that pH has no effect on the relationship between species and biomass?\nThat there is a problem here with the linear model becomes clear if we extropolate its predictions (ie the lines in the plot) to higher values of biomass. For each pH class it predicts negative species counts above some value of biomass, for example above about 6 at low pH. But this is nonsense: counts are strictly bounded below by zero and any sensible model should take account of this.\nSo we refit using a GLM and Poisson errors instead of a linear model.",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>GLM with count data</span>"
    ]
  },
  {
    "objectID": "glm_with_count_data_ancova.html#fit-using-glm",
    "href": "glm_with_count_data_ancova.html#fit-using-glm",
    "title": "16  GLM with count data",
    "section": "16.3 Fit using GLM",
    "text": "16.3 Fit using GLM\n\nmodel_glm_poisson_interaction &lt;- glm(Species ~ Biomass*pH, family = \"poisson\", data = species)\nsummary(model_glm_poisson_interaction)\n\n\nCall:\nglm(formula = Species ~ Biomass * pH, family = \"poisson\", data = species)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     2.95255    0.08240  35.833  &lt; 2e-16 ***\nBiomass        -0.26216    0.03803  -6.893 5.47e-12 ***\npHmid           0.48411    0.10723   4.515 6.34e-06 ***\npHhigh          0.81557    0.10284   7.931 2.18e-15 ***\nBiomass:pHmid   0.12314    0.04270   2.884 0.003927 ** \nBiomass:pHhigh  0.15503    0.04003   3.873 0.000108 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 452.346  on 89  degrees of freedom\nResidual deviance:  83.201  on 84  degrees of freedom\nAIC: 514.39\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe residual deviance is not greater than the number of degrees of freedom so we do not need to correct for overdispersion. Do we need to retain the interaction term? Let us test this by fitting an additive model without it:\n\nmodel_glm_poisson_NO_interaction &lt;- glm(Species ~ Biomass + pH, data = species)\nanova(model_glm_poisson_interaction, model_glm_poisson_NO_interaction, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: Species ~ Biomass * pH\nModel 2: Species ~ Biomass + pH\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1        84       83.2                          \n2        86     1230.9 -2  -1147.7 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSo we do need to retain the interaction. There is a highly significant difference between the slopes at different levels of pH. So Model 1 is the minimally adequate model.\nNow we plot fitted curves through the data\n\nspecies |&gt;\n  ggplot(aes(x = Biomass, y = Species, colour = pH)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, method.args = list(family = \"poisson\")) +\n  scale_colour_brewer(palette = \"Set1\") +\n  theme_cowplot()\n\n\n\n\n\n\n\n\n\n\n\n\nCrawley, Michael J. 2014. Statistics: An Introduction Using r. 2nd ed. Wiley Blackwell.",
    "crumbs": [
      "Generalised Linear Models (GLM)",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>GLM with count data</span>"
    ]
  },
  {
    "objectID": "community_ecology.html",
    "href": "community_ecology.html",
    "title": "17  Community Ecology",
    "section": "",
    "text": "17.1 Preliminaries\nAs an ecologist you might visit several sites and at each one record either presence/absence or abundance data for whatever flora and/or fauna species you find there, together, possibly, with a range of environmental variables that characterise the site on that day in some way, such as properties of the soil, meteorological conditions, the site designation, aspect and so on.\nYou could have questions about each site, such as its species richness or its diversity, but you might also have questions about what links or differentiates the sites. Which are most and least similar to each other in terms of species assemblages, and how is this driven by the environmental variables?\nIn this tutorial we introduce some elementary techniques that enable us to address these questions.\nThe vegan package used here provides tools for descriptive community ecology. It can be found on CRAN.\nSome of the data used here is from Mark Gardener’s book: Statistics for Ecologists Using R and Excel.\nYou will need to add the data sets plant_species_lists.csv and bird.csv to your data folder inside your project folder.\nThis worksheet assumes that you are using RStudio and have it open on your laptop.\nYou should have a project folder on your laptop where you are doing the R work related to this module. You will have given it a name that is meaningful and useful to you. In what follows, I will refer to it as your RStuff folder.\nYou should be working within this Project. If you are not doing so already, go to File/Open Project then navigate to your RStuff (or whatever you have called your project) folder and click on the RStuff.Rproj file within it. RStudio will then restart and you will see the name of your project folder at the top-right-hand corner of the RStudio window.\nUsing File/New File/R Notebook, start a new R Notebook in your RStuff/scripts folder. Delete all the explanatory material beneath the yaml, amend the title in the yaml to something sensible then add author and date lines to the yaml, so that you end up with something like the yaml at the top of this script.",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Community Ecology</span>"
    ]
  },
  {
    "objectID": "community_ecology.html#load-packages",
    "href": "community_ecology.html#load-packages",
    "title": "17  Community Ecology",
    "section": "17.2 Load packages",
    "text": "17.2 Load packages\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(vegan)\nlibrary(kableExtra)\n# the cowplot package provides an excellent theme that helps plots to look good, and functions to arrange multiple plots within a figure\nlibrary(cowplot) \n\nSuppose we have visited several sites and listed the different species present at each site. We have not recorded how many of each species are present",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Community Ecology</span>"
    ]
  },
  {
    "objectID": "community_ecology.html#read-in-data",
    "href": "community_ecology.html#read-in-data",
    "title": "17  Community Ecology",
    "section": "17.3 Read in data",
    "text": "17.3 Read in data\n\nfilepath&lt;-here(\"data\",\"plant_species_lists.csv\")\nplrich&lt;-read_csv(filepath)\nglimpse(plrich)\n\nRows: 183\nColumns: 2\n$ Site    &lt;chr&gt; \"ML1\", \"ML1\", \"ML1\", \"ML1\", \"ML1\", \"ML1\", \"ML1\", \"ML1\", \"ML1\",…\n$ Species &lt;chr&gt; \"Achillea millefolium\", \"Centaurea nigra\", \"Lathyrus pratensis…\n\n\nThere are two columns of data, one containing site names and one containing species names.\nHere are the first few rows:\n\nhead(plrich)\n\n# A tibble: 6 × 2\n  Site  Species             \n  &lt;chr&gt; &lt;chr&gt;               \n1 ML1   Achillea millefolium\n2 ML1   Centaurea nigra     \n3 ML1   Lathyrus pratensis  \n4 ML1   Leucanthemum vulgare\n5 ML1   Lotus corniculatus  \n6 ML1   Plantago lanceolata",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Community Ecology</span>"
    ]
  },
  {
    "objectID": "community_ecology.html#species-richness",
    "href": "community_ecology.html#species-richness",
    "title": "17  Community Ecology",
    "section": "17.4 Species richness",
    "text": "17.4 Species richness\nSuppose that initially we just want to find how many species were recorded at each site. There are several ways we could do this:\n\n17.4.1 Species richness the classic R way\nOld-school R is terse and sometimes opaque, but it is also very powerful.\nFirst we create a table indicating the presence/absence of each of the species at each of the sites.\n\nps&lt;-table(plrich$Species,plrich$Site)\nps # - uncomment this line if you want to see the table we have just created -it is big!\n\n                          \n                           ML1 ML2 MU1 MU2 PL2 PU2 SL1 SL2 SU1 SU2\n  Achillea millefolium       1   1   1   1   0   1   0   0   0   0\n  Aegopodium podagraris      0   0   0   0   0   0   0   1   0   0\n  Agrostis capillaris        0   1   1   1   1   0   1   1   1   0\n  Agrostis stolonifera       0   0   0   0   0   1   0   1   0   0\n  Anthriscus sylvestris      0   0   0   0   0   0   0   0   1   1\n  Arctium minus              0   0   0   0   0   0   0   0   0   1\n  Arrhenatherum elatius      0   0   0   0   1   0   0   1   1   1\n  Bidens cernua              0   0   0   0   1   0   0   0   0   0\n  Brachythecium rutabulum    0   0   0   0   1   0   1   1   1   0\n  Bromus hordeaceus          0   0   0   0   0   0   1   0   1   0\n  Calystegia sepium          0   0   0   0   0   0   0   1   0   0\n  Capsella bursa-pastoris    0   0   0   0   1   1   0   0   0   0\n  Cardamine pratensis        0   0   0   0   1   0   0   0   0   0\n  Centaurea nigra            1   1   1   1   0   0   0   0   0   0\n  Cerastium fontanum         0   0   1   0   0   1   0   0   1   1\n  Chamerion angustifolium    0   0   0   0   0   0   0   0   1   0\n  Chenopodium album          0   0   0   0   0   0   0   0   1   0\n  Cirsium arvense            1   0   1   1   1   1   1   1   1   1\n  Cirsium palustre           0   0   0   0   1   0   0   0   0   0\n  Crataegus monogyna (s)     0   0   0   0   0   0   0   0   1   0\n  Cynosurus cristatus        1   0   0   1   0   0   0   0   0   0\n  Dactylis glomerata         0   0   0   0   0   0   0   1   1   1\n  Deschampsia flexuosa       1   0   1   0   0   0   0   0   0   0\n  Elytrigia repens           0   0   0   0   0   0   0   1   0   0\n  Epilobium hirsutum         0   0   0   0   0   0   0   1   0   1\n  Epilobium montanum         0   0   0   0   1   0   0   0   0   1\n  Fallopia convolvulus       0   0   0   0   0   0   0   0   1   1\n  Festuca rubra              1   0   1   0   0   0   0   0   0   0\n  Filipendula ulmaria        0   0   0   0   0   0   0   1   0   0\n  Fraxinus excelsior (s)     0   0   0   0   0   0   0   0   1   0\n  Galium aparine             0   0   0   0   0   0   1   1   1   1\n  Galium verum               0   1   1   0   0   0   0   0   0   0\n  Geranium columbinum        0   0   0   0   0   0   1   0   0   0\n  Geranium molle             0   0   0   0   0   0   1   0   0   0\n  Glechoma hederacea         0   0   0   0   0   0   0   0   0   1\n  Hedera helix (g)           0   0   0   0   0   0   0   0   0   1\n  Heracleum sphondylium      0   0   0   0   0   0   0   0   1   1\n  Holcus lanatus             1   1   1   0   0   1   1   1   1   1\n  Impatiens glandulifera     0   0   0   0   0   0   0   1   0   0\n  Juncus effusus             0   0   0   0   1   0   0   0   0   0\n  Juncus inflexus            0   0   0   0   0   0   0   0   0   1\n  Lathyrus pratensis         1   0   0   0   0   0   0   1   0   0\n  Leucanthemum vulgare       1   1   1   1   0   0   0   0   0   0\n  Lolium perenne             0   0   0   0   0   1   1   0   0   1\n  Lotus corniculatus         1   1   1   1   0   1   0   0   0   0\n  Phalaris arundinacea       0   0   0   0   0   0   0   1   0   0\n  Plantago lanceolata        1   1   1   1   0   0   0   0   0   0\n  Plantago major             0   1   1   1   0   0   0   0   0   0\n  Poa pratensis              0   0   0   0   0   0   0   0   1   0\n  Poa trivialis              0   0   0   0   0   0   0   1   0   0\n  Prunella vulgaris          1   1   0   1   0   0   0   0   0   0\n  Quercus robur (s)          0   1   0   0   0   0   0   0   1   0\n  Quercus seedling/sp        0   0   1   0   0   0   0   0   0   0\n  Ranunculus acris           1   0   1   0   0   0   0   0   0   0\n  Ranunculus repens          0   1   1   1   1   1   1   1   1   0\n  Rubus fruticosus agg (g)   0   0   0   0   0   0   1   1   1   1\n  Rumex acetosa              0   1   1   1   0   0   0   0   0   0\n  Rumex crispus              0   0   0   0   0   0   1   0   1   0\n  Rumex obtusifolius         0   0   1   0   0   0   0   1   1   1\n  Rumex sanguineus           0   0   0   0   0   0   1   0   0   0\n  Salix caprea (s)           0   0   0   0   0   0   0   0   0   1\n  Salix fragilis (s)         0   0   0   0   0   0   0   1   0   0\n  Silene dioica              0   0   0   0   0   0   0   0   0   1\n  Sonchus arvensis           0   0   0   0   0   0   1   0   1   1\n  Sonchus asper              0   0   0   0   0   0   0   0   0   1\n  Stachys sylvatica          0   0   0   0   0   0   0   1   1   1\n  Symphytum officinale       0   0   0   0   0   0   0   1   0   0\n  Tanacetum vulgare          0   0   0   0   0   0   0   0   0   1\n  Taraxacum seedling/sp      0   0   1   1   0   0   0   0   0   0\n  Thuidium tamariscinum      0   0   0   0   0   0   1   0   0   0\n  Trifolium dubium           0   0   0   0   0   1   0   0   0   0\n  Trifolium pratense         1   1   1   1   0   0   0   0   0   0\n  Trifolium repens           1   1   1   0   1   1   0   0   0   0\n  Urtica dioica              0   1   0   0   1   0   1   1   1   1\n  Veronica arvensis          0   0   0   0   0   0   0   0   1   0\n  Vicia hirsuta              0   0   0   0   0   0   0   0   1   1\n\n\nNote that in this table, most of the entries are zeros. This is common in data recording of this kind. At any one site, most of the species are not found.\nThen we calculate the sum of each column. This will give us the number of species found at each site.\n\npsr&lt;-colSums(ps)\npsr\n\nML1 ML2 MU1 MU2 PL2 PU2 SL1 SL2 SU1 SU2 \n 15  16  21  14  13  11  16  24  27  26 \n\n\nNote that the actual number of species at each site will be greater than these values. How much greater, do you think, and how could we estimate the true richness from the observed richness?\n\n\n17.4.2 Species richness the tidyverse way\nWe can use the group_by() and summarise() combo yet again. In this code chunk we finish off with the arrange() function from the dplyr package (the sub-package of tidyverse that is used for data manipulation) to present the sites in descending order of species richness.\n\nplr&lt;-plrich |&gt;\n  group_by(Site) |&gt;\n  summarise(species.richness=n()) |&gt;\n  arrange(Site)\nplr\n\n# A tibble: 10 × 2\n   Site  species.richness\n   &lt;chr&gt;            &lt;int&gt;\n 1 ML1                 15\n 2 ML2                 16\n 3 MU1                 21\n 4 MU2                 14\n 5 PL2                 13\n 6 PU2                 11\n 7 SL1                 16\n 8 SL2                 24\n 9 SU1                 27\n10 SU2                 26\n\n\nTry tweaking the code so that you get this table in ascending order of richness, or in alphabetical order of site. Hint: try adjusting the argument of the arrange() function.\nPlot of species richness\n\nplrich |&gt;\n  group_by(Site) |&gt;\n  summarise(species.richness=n()) |&gt;\n\n  #ggplot(aes(x=Site,y=species.richness)) +\n  ggplot(aes(x=fct_reorder(Site,species.richness),y=species.richness)) +\n  geom_point(size=3) +\n  labs(x=\"Site\",\n       y=\"Species richness\") +\n  theme_cowplot()\n\n\n\n\n\n\n\n\n\n\n17.4.3 Species richness the vegan way\nvegan is a package that brings with it a lot of functions intended for analysis of community ecology data. It’s style is ‘old-school’ R rather than tidyverse R, which is what we have mainly been using.\n\nps&lt;-table(plrich$Species,plrich$Site)\nspecnumber(ps,MARGIN=2) #MARGIN=2 means summing over columns (not the usual vegan way!)\n\nML1 ML2 MU1 MU2 PL2 PU2 SL1 SL2 SU1 SU2 \n 15  16  21  14  13  11  16  24  27  26 \n\n\n\n\n17.4.4 Species richness of the dune data set.\nThis data set is built into the vegan package. It contains cover class values of 30 species at 20 sites, with the sites arranged by row and the species by column. This is the default expectation of functions in vegan. The species names are abbreviated to 4+4 letters.\n\ndata(dune)\nhead (dune)\n\n  Achimill Agrostol Airaprae Alopgeni Anthodor Bellpere Bromhord Chenalbu\n1        1        0        0        0        0        0        0        0\n2        3        0        0        2        0        3        4        0\n3        0        4        0        7        0        2        0        0\n4        0        8        0        2        0        2        3        0\n5        2        0        0        0        4        2        2        0\n6        2        0        0        0        3        0        0        0\n  Cirsarve Comapalu Eleopalu Elymrepe Empenigr Hyporadi Juncarti Juncbufo\n1        0        0        0        4        0        0        0        0\n2        0        0        0        4        0        0        0        0\n3        0        0        0        4        0        0        0        0\n4        2        0        0        4        0        0        0        0\n5        0        0        0        4        0        0        0        0\n6        0        0        0        0        0        0        0        0\n  Lolipere Planlanc Poaprat Poatriv Ranuflam Rumeacet Sagiproc Salirepe\n1        7        0       4       2        0        0        0        0\n2        5        0       4       7        0        0        0        0\n3        6        0       5       6        0        0        0        0\n4        5        0       4       5        0        0        5        0\n5        2        5       2       6        0        5        0        0\n6        6        5       3       4        0        6        0        0\n  Scorautu Trifprat Trifrepe Vicilath Bracruta Callcusp\n1        0        0        0        0        0        0\n2        5        0        5        0        0        0\n3        2        0        2        0        2        0\n4        2        0        1        0        2        0\n5        3        2        2        0        2        0\n6        3        5        5        0        6        0\n\n\nTo find the species richness of the sites we use the specnumber() function to find the number of non-zero values in each row:\n\nspecnumber(dune,MARGIN=1)\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \n 5 10 10 13 14 11 13 12 13 12  9  9 10  7  8  8  7  9  9  8 \n\n\nNote that we use MARGIN=1 in this case, which instructs vegan to sum along rows since the sites are arranged by row, rather than by column. In fact, we could have left it out, since as we can see from the help for specnumber, the default value for MARGIN is 1, ie sites in rows, species in columns, which is the default expectation of vegan. If we wanted the opposite, sites in columns and species in rows, then we would have specified MARGIN=2.\n\n\n17.4.5 Species richness of the dune data set - doing it the tidyverse way\nYou can take this section or leave it - I’ll take it!. If you find the vegan talk of MARGIN confusing, then you might prefer to use the tidyverse style to find the species richness of the sites in the dune data set.\nIn the code chunk below we note that the dune data set is not at all tidy. To be so, all 30 columns in the original data set should be collapsed into two, which here we will call “species” and “abundance”. We use the (I find) repeatedly useful function pivot_longer() to do this. Before we do this it will be useful to use the mutate() function to add a column that gives an ID number for each site.\n\ndune |&gt;\n  mutate(id=seq(1:n())) |&gt;# add in a site ID column\n  pivot_longer(1:30,names_to=\"species\",values_to=\"abundance\") |&gt; #tidy the abundance data\n  filter(abundance&gt;0) |&gt; # remove the lines where the the abundance is zero\n  group_by(id) |&gt; # group by site\n  summarise(species.richness=n()) # count the number of species (rows) left for each site.\n\n# A tibble: 20 × 2\n      id species.richness\n   &lt;int&gt;            &lt;int&gt;\n 1     1                5\n 2     2               10\n 3     3               10\n 4     4               13\n 5     5               14\n 6     6               11\n 7     7               13\n 8     8               12\n 9     9               13\n10    10               12\n11    11                9\n12    12                9\n13    13               10\n14    14                7\n15    15                8\n16    16                8\n17    17                7\n18    18                9\n19    19                9\n20    20                8",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Community Ecology</span>"
    ]
  },
  {
    "objectID": "community_ecology.html#diversity-index",
    "href": "community_ecology.html#diversity-index",
    "title": "17  Community Ecology",
    "section": "17.5 Diversity Index",
    "text": "17.5 Diversity Index\n‘Better stories can be told about Simpson’s index than about Shannon’s index, and still grander narratives about rarefaction (Hurlbert 1971). However, these indices are all very closely related (Hill 1973), and there is no reason to despise one more than others (but if you are a graduate student, don’t drag me in, but obey your Professor’s orders).’\nIf we have abundance data. or each species at a site, for example percentage cover, we have more information than if had simply had presence-absence data. Hence, not surprisingly, w can Given the proportion \\(p_i\\) of each of several species \\(i\\) within a population, we can estimate the so-called diversity of the population in one of several ways from proportions \\(\\hat p_i\\) of each species within a sample. The proportions can be of total counts or of total percentage cover.\n\n17.5.1 Shannon’s Diversity Index\nThis calculates \\(H\\) where\n\\[H=-\\sum{p_i\\log_b p_i}\\]\nwhere \\(b\\) is the base of the log. Most commonly, natural logs to base \\(e^1\\) are used. Those are the ones often denoted ln on calculators and in some books. Or sometimes log_e, or sometimes just log. Confusing, eh? It is usually clear from the context when natural logs are being used.\n\n\n17.5.2 Simpson’s and Inverse Simpson’s Diversity Index\nThese are based on the quantity\n\\[D=\\sum_ip_i^2\\]\nSimpson’s index is \\(1-D\\) and varies from 0 to 1 while the inverse Simpson’s index is \\(1/D\\)\nNote how this measure \\(D\\) makes sense as a diversity index. If a site had one species that comprised 100% of the cover of the site, ie \\(p = 1\\) for that species and \\(p = 0\\) for every other species, then the diversity of that site would be low, right? The value of \\(D\\) would be 1 and the Simpson’s index value would thus be zero.\nOn the other hand, if we had a more diverse site with each of ten species having a percentage cover of 10%, so that each of the \\(p_i\\) would be 0.1, then the value of \\(D\\) would be \\(\\sum_ip_i^2=10\\times 0.1^2=0.1\\) This would give a Simpson’s diversity index value of 0.9.\nTry this out for a site with two species, or five, or 100, each with equal cover \\(p_n\\). What values do you get in these cases for the Simpson’s index?\n\n\n\n\n\n\n\n\n\n\\(N\\) species\n\\(p\\)\n\\(D\\)\nSimpson’s Index = \\(1-D\\)\n\n\n\n\n2\n0.5\n\\(p_1^2 + p_2^2 = 0.5\\)\n\\(1-0.5 = 0.5\\)\n\n\n5\n0.2\n\\(p_1^2 + \\cdots p_5^2 = 0.2\\)\n\\(1-0.2 = 0.8\\).\n\n\n100\n0.01\n\\(p_1^2 + \\cdots p_{100}^2 = 0.01\\)\n\\(1-0.01 = 0.99\\).",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Community Ecology</span>"
    ]
  },
  {
    "objectID": "community_ecology.html#calculation-of-bird-species-diversities-in-a-range-of-habitats",
    "href": "community_ecology.html#calculation-of-bird-species-diversities-in-a-range-of-habitats",
    "title": "17  Community Ecology",
    "section": "17.6 Calculation of bird species diversities in a range of habitats",
    "text": "17.6 Calculation of bird species diversities in a range of habitats\nAs an example, let us suppose we have gathered count data on the numbers of unique individuals of different species of birds observed in a range of habitats. What are the diversities of bird species in each of the habitats?\nOur data collection sheet might look like this:\n\nbird&lt;-read_csv(here(\"data\",\"bird.csv\"))\nbird\n\n# A tibble: 6 × 6\n  Species       Garden Hedgerow Parkland Pasture Woodland\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 Blackbird         47       10       40       2        2\n2 Chaffinch         19        3        5       0        2\n3 Great Tit         50        0       10       7        0\n4 House Sparrow     46       16        8       4        0\n5 Robin              9        3        0       0        2\n6 Song Thrush        4        0        6       0        0\n\n\nThis is nice for us to read, but not the best for analysis. It is not ‘tidy’. Each of the habitats surveyed is really a different level of the single variable ‘habitat’. Hence, this variable is appearing across five columns. A tidy data set has any variable appear only once in any one row. When this is the case, analysis becomes much easier. We can tidy this data set using the command pivot_longer(), which we use here to produce a new version of the data set called tidy.bird, in which we have just two columns, habitat and abundance. This data set is tidy.\nNote that tidy data sets tend to be longer and thinner than untidy data sets, with more rows and fewer columns.\nIn the chunk below, we start with the data frame bird, tidy it, then write the result into an object called tidy.bird.\n\ntidy.bird&lt;- bird |&gt;\n  pivot_longer(Garden:Woodland,names_to=\"habitat\",values_to=\"abundance\") |&gt;\n  arrange(habitat) # sort in order of habitat name\n\ntidy.bird # have a look at it\n\n# A tibble: 30 × 3\n   Species       habitat  abundance\n   &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt;\n 1 Blackbird     Garden          47\n 2 Chaffinch     Garden          19\n 3 Great Tit     Garden          50\n 4 House Sparrow Garden          46\n 5 Robin         Garden           9\n 6 Song Thrush   Garden           4\n 7 Blackbird     Hedgerow        10\n 8 Chaffinch     Hedgerow         3\n 9 Great Tit     Hedgerow         0\n10 House Sparrow Hedgerow        16\n# ℹ 20 more rows\n\n\nIn this next chunk we use the tidy data set to calculate the three diversity indices - Shannon, Simpson and Inverse Simpson, for each of the habitats. First we do this in a ‘home-made’ way, typing in the formulae for each index ourselves\n\ntidy.bird |&gt;\n  group_by(habitat) |&gt;\n  filter(abundance&gt;0) |&gt;\n  summarise(N=sum(abundance),\n            shannon.di=-sum((abundance/sum(abundance))*log(abundance/sum(abundance))),\n            simpson.di=1-sum((abundance/sum(abundance))^2),\n            inv.simpson.di=1/sum((abundance/sum(abundance))^2)) |&gt;\n  arrange(-shannon.di) |&gt;\n  kbl(digits=3) |&gt;\n  kable_styling(full_width=FALSE)\n\n\n\n\nhabitat\nN\nshannon.di\nsimpson.di\ninv.simpson.di\n\n\n\n\nGarden\n175\n1.542\n0.762\n4.205\n\n\nParkland\n69\n1.248\n0.617\n2.609\n\n\nHedgerow\n32\n1.154\n0.635\n2.738\n\n\nWoodland\n6\n1.099\n0.667\n3.000\n\n\nPasture\n13\n0.984\n0.592\n2.449\n\n\n\n\n\n\n\nNow let’s do that using the diversity() function in vegan. I have left in the MARGIN and base arguments, but we could have left them out since we are happy to use their default values.\n\n# MARGIN =2 means sum along rows\n\ntidy.bird |&gt;\n  group_by(habitat) |&gt;\n  summarise(N=sum(abundance),\n            shannon.di=diversity(abundance, index = \"shannon\", MARGIN = 2,base=exp(1)),\n            simpson.di=diversity(abundance, index = \"simpson\", MARGIN = 2),\n            inv.simpson.di=diversity(abundance, index = \"invsimpson\", MARGIN = 2)) |&gt;\n  arrange(-shannon.di) |&gt; # sort rows in descending order of shannon diversity index\n  kbl(digits=3) |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\n\nhabitat\nN\nshannon.di\nsimpson.di\ninv.simpson.di\n\n\n\n\nGarden\n175\n1.542\n0.762\n4.205\n\n\nParkland\n69\n1.248\n0.617\n2.609\n\n\nHedgerow\n32\n1.154\n0.635\n2.738\n\n\nWoodland\n6\n1.099\n0.667\n3.000\n\n\nPasture\n13\n0.984\n0.592\n2.449\n\n\n\n\n\n\n\nIn the next chunk we use the diversity() function alone to give us the Shannon diversities for each of the habitats. This line of code uses the original untidy data set bird, and asks that the counts in columns 2 to 6 be used to caclulate the diversities of each of the habitats. MARGIN=2 is the instruction to look down the columns (MARGIN=1 would mean look along the rows) , while [,2:6] is an example of how data fames can be spliced and diced. bird[2,3] would mean take the element at row 2, column 3, while [,2:6] means take the block of data in all the rows, and columns 2 to 6\n\ndiversity(bird[,2:6],MARGIN=2)\n\n   Garden  Hedgerow  Parkland   Pasture  Woodland \n1.5422709 1.1538939 1.2483919 0.9839614 1.0986123 \n\n\nIn this example we see that the vegan package provides functions that do useful things for us that we sometimes could have done in another way, but where that way might have been more complicated. In this example, use of the diversity() function from vegan means that we don’t have to know or type in the actual formulae for whichever index we are after.\nNow let us find the Shannon diversity index of the 20 sites in the dune dataset. That is, species in columns, sites in rows.\n\ndata(dune)\ndiversity(dune)\n\n       1        2        3        4        5        6        7        8 \n1.440482 2.252516 2.193749 2.426779 2.544421 2.345946 2.471733 2.434898 \n       9       10       11       12       13       14       15       16 \n2.493568 2.398613 2.106065 2.114495 2.099638 1.863680 1.979309 1.959795 \n      17       18       19       20 \n1.876274 2.079387 2.134024 2.048270 \n\n\nSee how easy this is to implement when we have arranged our data as in the dune dataset?\n\n17.6.1 Are these diversities different?\nIf we have a number of habitats, and for each of them we have arrived at one or other of the diversity indices considered above, we might reasonably ask if there is a significant difference between the indices.\nThis is difficult. For each habitat we have a single number, so we cannot use a t-test/ANOVA, and the number is not a count, so we cannot use a chi-squared test.",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Community Ecology</span>"
    ]
  },
  {
    "objectID": "community_ecology.html#dissimilarity",
    "href": "community_ecology.html#dissimilarity",
    "title": "17  Community Ecology",
    "section": "17.7 (Dis)similarity",
    "text": "17.7 (Dis)similarity\nThe last problem, of telling how similar or dissimilar sites are, based on the species assemblages found at them or on a clutch of environmental variables measured at each site, or both, is a common task in ecology.\nIn this and the following sections we will explore the concept of ‘dissimilarity’ and the various ways it can be calculated and displayed.\nThe vegan package has the function vegdist() which permits dissimilarity indices to be calculated for both presence-absence data and for data where abundances have been recorded in some form (eg count, frequency, percentage cover).\nFor more information on vegdist, particularly on the specifics of what the possible indices are that can be calculated, type ?vegdist into the console window and look at the Help information that appears in the bottom-right pane of RStudio.\nLet us calculate the Jaccard index of dissimilarity between the sites of the dune data set. This data set, remember, gives cover class values of 30 species on 20 sites. Note that there is an accompanying data set dune.env which contains environmental data for each of the sites. This is a common scenario, whereby you gather both species and environmental data for a given site.\nThe Jaccard index of dissimilarity between two sites is the ratio of the number of species that they have in common to the total number of species contained in either one or both sites.\nThis can be calculated as follows:\n\nNumber of species in site A = A\nNumber of species in site B = B\nNumber of species common to both sites = C\nThus, total number of species across both sites = A + B - C\n\nFrom which Jaccard Index J = C / (A + B - C)\nIf each site has exactly the same species assemblage, so that they are identical in this respect, then this index is 1. If there are no shared species between the sites so that there is no point of similarity, then this index is 0. Hence the Jaccard index J between two sites is always between 0 and 1.\nFor example if clifftop site A contains Armeria maritima, Jasione montana and Silene uniflora while clifftop site B contains Armeria maritima, Jasione montana and Anthyllis vulneraria, the number of species they have in common is two, while the total number of species between them is four so that the Jaccard index of dissimilarity between them \\(J(A,B) = J(B,A) = \\frac{2}{4}= 0.5\\)\nTHe Sorensen index is similar\nSince we have 20 sites, so that each site has to be compared with 19 others, and since the dissimilarity of site 1 and site 2 is the same as that of site 2 and site 1 ie J(1,2) = J(2,1), just as the distance from Bristol to Bath is the same as the distance from Bath to Bristol, this gives 190 distinct Jaccard indices for this data set.\n\ndune.dist&lt;-round(vegdist(dune,method=\"jaccard\",binary=FALSE),2)\ndune.dist\n\n      1    2    3    4    5    6    7    8    9   10   11   12   13   14   15\n2  0.64                                                                      \n3  0.62 0.51                                                                 \n4  0.69 0.53 0.43                                                            \n5  0.78 0.58 0.64 0.67                                                       \n6  0.78 0.68 0.72 0.78 0.46                                                  \n7  0.71 0.61 0.64 0.67 0.37 0.37                                             \n8  0.79 0.70 0.49 0.58 0.78 0.74 0.69                                        \n9  0.75 0.65 0.51 0.55 0.67 0.75 0.66 0.48                                   \n10 0.73 0.45 0.64 0.65 0.52 0.48 0.43 0.70 0.75                              \n11 0.72 0.70 0.71 0.74 0.77 0.62 0.62 0.69 0.75 0.58                         \n12 0.96 0.83 0.61 0.69 0.82 0.78 0.77 0.61 0.52 0.84 0.80                    \n13 0.91 0.75 0.60 0.68 0.81 0.86 0.78 0.54 0.58 0.85 0.86 0.52               \n14 1.00 0.88 0.86 0.89 0.94 0.89 0.93 0.72 0.86 0.86 0.90 0.82 0.79          \n15 1.00 0.95 0.83 0.85 0.92 0.89 0.91 0.60 0.80 0.92 0.85 0.77 0.81 0.53     \n16 0.96 0.94 0.80 0.80 0.94 0.92 0.94 0.60 0.79 0.94 0.93 0.74 0.75 0.70 0.53\n17 0.94 0.90 0.94 0.95 0.77 0.81 0.80 0.94 0.94 0.77 0.82 0.96 0.93 0.95 0.94\n18 0.88 0.75 0.76 0.80 0.70 0.66 0.71 0.78 0.81 0.65 0.49 0.85 0.89 0.91 0.84\n19 1.00 0.89 0.91 0.88 0.83 0.84 0.85 0.85 0.88 0.83 0.71 0.82 0.90 0.92 0.88\n20 1.00 0.97 0.87 0.87 0.94 0.92 0.94 0.66 0.82 0.94 0.89 0.82 0.84 0.62 0.46\n     16   17   18   19\n2                     \n3                     \n4                     \n5                     \n6                     \n7                     \n8                     \n9                     \n10                    \n11                    \n12                    \n13                    \n14                    \n15                    \n16                    \n17 1.00               \n18 0.93 0.86          \n19 0.95 0.72 0.71     \n20 0.51 0.95 0.82 0.85",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Community Ecology</span>"
    ]
  },
  {
    "objectID": "community_ecology.html#visualising-similarity",
    "href": "community_ecology.html#visualising-similarity",
    "title": "17  Community Ecology",
    "section": "17.8 Visualising similarity",
    "text": "17.8 Visualising similarity\n\n17.8.1 Heatmaps\nOne way to visualise this dissimilarity matric is to use a heat map, in which the darker colours indicate greater ‘dissimilarity’.\n\nheatmap(as.matrix(dune.dist))\n\n\n\n\n\n\n\n\n\n\n17.8.2 Dendrograms\nAs we have seen, the function vegdist() produces a matrix of distance values between every pair of sites. We used a heat map to give a visual indication of the overall pattern of dissimilarity between the sites. A dendrogram is another useful way to visualize this to help us identify where the greatest similarities and differences lie.\n\ndune.hc1&lt;-hclust(dune.dist)\ndune.hc2&lt;-hclust(dune.dist,method=\"ward.D2\")\nopar &lt;- par(mfrow = c(1, 1))\nplot(dune.hc2,hang=-1)\n\n\n\n\n\n\n\n#plot(dune.hc2, hang=-1)\npar(opar)\n\nThe dendrograms are a useful, but for most real data, approximate summary of the distance matrix. The key way to interpret them is that the height at which two sites are joined is an indication of their dis-similarity. The higher that is, the less similar they are. These dendrograms sometimes suggest which clusters may exist in your data, but they can also sometimes be misleading about that. They are most accurate near the bottom, so in this example the suggestion from the dendrogram that sites 6 and 7 are most similar to each other, similarly 3 and 4, is probably correct.\nNote also that dendrograms become very hard to interpret when the number of data points (in this case sites) becomes very large. This is less often an issue in the field of ecology where we are typically dealing within a samll number of sites, as here, but in other settings such as molecular biology of the gene where the data may consist of thousands of DNA sequences, they become impossible to decipher, and also very expensive computationally.\nExercise\nFor the dune data, find the species richness and Simpson diversity index for each site, then create a dendrogram that illustrates which sites are most similar/dissimilar to each other\nWe suppose that we have already the data into a data frame called dune from a .csv file, which is arranged the vegan way, with sites as rows, species as columns.\n\n# species richness\nspecnumber(dune)\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \n 5 10 10 13 14 11 13 12 13 12  9  9 10  7  8  8  7  9  9  8 \n\n# Simpson's diversity index\ndiversity(dune, index = \"simpson\")\n\n        1         2         3         4         5         6         7         8 \n0.7345679 0.8900227 0.8787500 0.9007407 0.9140076 0.9001736 0.9075000 0.9087500 \n        9        10        11        12        13        14        15        16 \n0.9115646 0.9031909 0.8671875 0.8685714 0.8521579 0.8333333 0.8506616 0.8429752 \n       17        18        19        20 \n0.8355556 0.8614540 0.8740895 0.8678460 \n\n# Dissimilarity dendrogram\ndune.dist&lt;-vegdist(dune)\ndune.hc&lt;-hclust(dune.dist)\nplot(dune.hc)",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Community Ecology</span>"
    ]
  },
  {
    "objectID": "ordination.html",
    "href": "ordination.html",
    "title": "18  Ordination",
    "section": "",
    "text": "18.1 Reading List\nFor this chapter you will find it useful to have this RStudio project folder if you wish to follow along and try out the exercises at the end. ## Introduction\nIn ecology, genetics and other fields data is often gathered across a great many variables - many sites and species, many environmental variables. To see patterns and relationships in this multidimensional data can be tricky. Which sites are most similar or dissimilar, which environmental variables are most important in driving these differences? It can be hard to tell.\nOrdination is the general term for several techniques that try to find order in this chaos. A main task they tackle in order to do this is to reduce the dimensionality of a problem down to two or three such that the patterns that exist in the full data set are at least approximately preserved in this reduced number of dimensions, yet can be depicted in a two-dimensional plot and made sense of. It is a bit like shining a bright light at an elephant and looking at the shadow cast onto a screen behind the elephant. The shadow is a depiction of the elephant in which the full three-dimensional object is viewed in only two dimensions. Some details of the real object are lost, but enough remains in the two dimensions for real insights to be possible. You would still recognise the shadow as being that of an elephant, no?\nIn one of our examples in this chapter we have a data set where many geological samples were taken and in each one measurements were made of eight different variables - the concentrations of two radioisotopes and of six different elements. We can think of each of these variables as representing an axis in eight dimensional space (don’t think too long over this or your head might pop!). If we could draw this space (we can’t!) then each sample would be at a point specified by its values of these eight variables, just as a point on a 2 dimensional graph might be specified by its coordinates, say (3,2), meaning the point where the variable x had the value 3 and the variable y had the value 2. The whole data set would be a cloud of points in this space.\nIn this space, samples that were similar to each other would lie close to each other and samples that were very different would be far apart. Samples that had high concentrations of aluminium would be far along the aluminium concentration axis, and also far along the calcium axis if in addition they had high concentrations of calcium, and so on. These are the patterns and environmental drivers of those patterns that we would see in the raw data if we could plot it in full, but we cannot because we cannot visualize, let alone draw in eight dimensions.\nOrdination techniques try to solve this problem, in one way or another, by reducing the dimensionality of the data set in such a way that the patterns of similarity / dissimilarity present in the full set of dimensions are still there, at least approximately in two, sometimes three, dimensions where we can see them and make sense of them.\nTo determine these patterns an ordination technique has to find some way of\nOksanan (2004) is a comprehensive set of lecture notes from one of the main architects of the vegan package. This gives succinct explanations of all the main multivariate analysis methods including PCA and NMDS.\nShlens (2014) is a mathematical but clear treatment, which gets to the core of what PCA is doing.",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Ordination</span>"
    ]
  },
  {
    "objectID": "ordination.html#reading-list",
    "href": "ordination.html#reading-list",
    "title": "18  Ordination",
    "section": "",
    "text": "Oksanen, Jari. 2004. “Multivariate Analysis in Ecology - Lecture Notes.” https://web.archive.org/web/20181024083948/http://cc.oulu.fi:80/~jarioksa/opetus/metodi/notes.pdf.\n\n\nShlens, Jonathon. 2014. “A Tutorial on Principal Component Analysis.” https://doi.org/10.48550/arXiv.1404.1100.",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Ordination</span>"
    ]
  },
  {
    "objectID": "PCA.html",
    "href": "PCA.html",
    "title": "19  Principal Components Analysis (PCA)",
    "section": "",
    "text": "19.1 Example One: Geological samples\nPCA is useful where it makes sense to determine dissimilarity between observations using Euclidean distance. That is the case where the variables (eg concentrations, temperatures, pH, masses, weights, heights etc) are measured using a continuous numerical scale. Thus, PCA is often useful for multivariate analysis of environmental data. In the geological example we shall explore below those variables are the concentrations of various elements and radioisotopes. In the penguins example after that, they are the lengths and thicknesses of bills and flippers, and the body weight of the penguins.\nThis exercise is adapted slightly from that of Holland (2019)\nBefore we do anything else, let’s load the packages we are going to use, just like we always do:\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(here)\nlibrary(ggfortify)\nlibrary(GGally)\nlibrary(cowplot)\ntheme_set(theme_minimal())\nThis data set has geochemical analyses of 200 limestone samples, including the major elements Al, Ca, Fe, K, Mg, Mn, and Si, aswell as stable isotope ratios of carbon and oxygen (d13C and d18O). It also records the stratigraphic position of the samples (ie, the depth beneath the surface).\nThis data set is in the file purdin_pca.csv:\nfilepath &lt;- here::here(\"data\",\"purdin_pca.csv\")\npurdin&lt;-read_csv(filepath, show_col_types = FALSE)\nglimpse(purdin)\n\nRows: 200\nColumns: 10\n$ ID            &lt;chr&gt; \"Ae\", \"Ad\", \"Ac\", \"Ab\", \"Aa\", \"A1\", \"A2\", \"A3\", \"A4\", \"A…\n$ StratPosition &lt;dbl&gt; 35.080, 34.745, 34.660, 34.555, 34.285, 34.150, 34.050, …\n$ d13C          &lt;dbl&gt; 1.95, 1.82, 1.91, 0.93, -0.34, 0.46, 0.55, 0.86, 0.82, 0…\n$ d18O          &lt;dbl&gt; -4.66, -4.57, -4.77, -4.58, -4.54, -3.86, -3.94, -4.10, …\n$ Al            &lt;dbl&gt; 1.49, 2.73, 4.26, 4.69, 4.80, 3.89, 3.11, 3.35, 2.71, 7.…\n$ Ca            &lt;dbl&gt; 297.16, 275.89, 328.11, 329.66, 285.67, 346.72, 291.68, …\n$ Fe            &lt;dbl&gt; 1.52, 2.70, 3.13, 9.11, 3.46, 3.79, 3.02, 3.26, 2.69, 2.…\n$ Mg            &lt;dbl&gt; 2.22, 2.84, 3.12, 2.88, 3.69, 8.77, 5.46, 5.69, 3.89, 4.…\n$ Mn            &lt;dbl&gt; 0.34, 0.33, 0.40, 0.47, 0.45, 0.32, 0.21, 0.11, 0.10, 0.…\n$ Si            &lt;dbl&gt; 8.61, 7.32, 9.65, 23.07, 9.97, 7.98, 6.91, 7.50, 5.79, 2…\nAs we are most interested in how the geo-chemical composition varies, we will pull those variables (columns 2–10) off into a separate data frame called geochem for our analysis\ngeochem &lt;- purdin |&gt;\n  dplyr::select(d13C:Si) # means select all columns from d1C to Si, inclusive.\nglimpse(geochem)\n\nRows: 200\nColumns: 8\n$ d13C &lt;dbl&gt; 1.95, 1.82, 1.91, 0.93, -0.34, 0.46, 0.55, 0.86, 0.82, 0.91, 0.86…\n$ d18O &lt;dbl&gt; -4.66, -4.57, -4.77, -4.58, -4.54, -3.86, -3.94, -4.10, -4.23, -4…\n$ Al   &lt;dbl&gt; 1.49, 2.73, 4.26, 4.69, 4.80, 3.89, 3.11, 3.35, 2.71, 7.33, 3.50,…\n$ Ca   &lt;dbl&gt; 297.16, 275.89, 328.11, 329.66, 285.67, 346.72, 291.68, 295.68, 3…\n$ Fe   &lt;dbl&gt; 1.52, 2.70, 3.13, 9.11, 3.46, 3.79, 3.02, 3.26, 2.69, 2.88, 2.89,…\n$ Mg   &lt;dbl&gt; 2.22, 2.84, 3.12, 2.88, 3.69, 8.77, 5.46, 5.69, 3.89, 4.70, 3.28,…\n$ Mn   &lt;dbl&gt; 0.34, 0.33, 0.40, 0.47, 0.45, 0.32, 0.21, 0.11, 0.10, 0.12, 0.09,…\n$ Si   &lt;dbl&gt; 8.61, 7.32, 9.65, 23.07, 9.97, 7.98, 6.91, 7.50, 5.79, 22.11, 7.7…\nThinking ahead to how we want to do an exploratory plot of these measurements, and possibly summaries, grouping by variable, we realise that this will be difficult unless we ‘tidy’ the data. What we mean by this is that we could think of this data set as having one ‘factor’ = geochemical variable, within which there are many levels = d13C, d18O, Al, Ca etc. If the data were tidy, then there would be just one column for the names of all these levels, and another column for their values. At the moment the data set is not tidy as the levels of the variable are spread across several columns, but we can make it so by using the pivot_longer() function:\ngeochem_tidy&lt;-geochem |&gt;\n  pivot_longer(1:8,names_to=\"Variable\",values_to=\"Value\")\nglimpse(geochem_tidy)\n\nRows: 1,600\nColumns: 2\n$ Variable &lt;chr&gt; \"d13C\", \"d18O\", \"Al\", \"Ca\", \"Fe\", \"Mg\", \"Mn\", \"Si\", \"d13C\", \"…\n$ Value    &lt;dbl&gt; 1.95, -4.66, 1.49, 297.16, 1.52, 2.22, 0.34, 8.61, 1.82, -4.5…\nDo you see what that has done? We started with a short, fat data set and have ended up with a long, thin one.\nNow we have tidied the data, we can easily look at it for each level:\ngeochem_tidy |&gt;\n  ggplot(aes(x=Value)) +\n  geom_histogram() +\n  facet_wrap(~Variable,scales=\"free\")\n\n\n\n\n\n\n\n  # theme_cowplot()\nNote the very different scales for each of these variables, in both the x and y directions.\nSeveral of the major elements among these (Al, Fe, Mg, Mn, Si) seem to be right-skewed, while the two isotopic ratio variables (d18O and d13C) are left skewed to a lesser degree.\nFor PCA to work, the data need to be normally distributed, just like in all the variations of the linear model (regression, t-test, ANOVA, Pearson correlation). To make this more approximately the case for this data set, we can transform them using a log10 transformation.\ngeochem_trans &lt;- geochem |&gt; mutate(across(c(\"Al\",\"Si\"),log10))\nOther restrictions on the data apply to the use of PCA just as in linear model analyses. In particular, outliers should be removed since they will have a disproportionate influence on the outcome.\nNow we create a PCA model from the transformed data. We scale the data so that all variables are given equal weight. If we did not do this, then the variables for which values are very large (Ca, in this case) would dominate the analysis.\npca &lt;- prcomp(geochem_trans, scale.=TRUE)\nFrom this, we get these outputs:\n# The variance vector shows how much of the variance in the data is explained by each PC. A variance is the square of a standard deviation.\nvariance &lt;- (pca$sdev)^2\n\n# We divide each element of the vector by the total variance to find the proportion of variance explained by each PC\nvpc &lt;-variance/sum(variance) * 100\n\n# Here we find the cumulative proportin explained by the first n PCs. \ncpc &lt;- cumsum(vpc)\n\n# gather these vectors together in a data frame called scree.\nscree&lt;-tibble(PC = seq(1:length(pca$sdev)), vpc = vpc, cpc = cpc)\n\n# loadings show how much each variables contributes to each PC\nloadings &lt;- pca$rotation\nrownames(loadings) &lt;- colnames(geochem)\n\n# These show the values each data point has for each of the principal components\nscores &lt;- pca$x",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Principal Components Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "PCA.html#example-one-geological-samples",
    "href": "PCA.html#example-one-geological-samples",
    "title": "19  Principal Components Analysis (PCA)",
    "section": "",
    "text": "19.1.1 Scree Plot\nA scree plot help us see how the variance in the data is distributed among the principal components. We see that each successive PC explains less variance than the one before. We can use this plot to help us decide how many PCs we need to retain. The horizontal line in the plot shows the total variance divided by the number of PCs. Hence we could decide to retain only those PCs that explain more variance than this, since only they explain more than one variable’s worth of data.\n\n### Basic scree plot\nscree |&gt;\n  ggplot(aes(x = PC,y = vpc)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(yintercept = mean(scree$vpc), linetype = \"dashed\", colour = \"red\") +\n  labs(x = \"Principal component\",\n       y = \"% variance explained\") +\n  theme_cowplot()\n\n\n\n\n\n\n\n\n\nscree\n\n# A tibble: 8 × 3\n     PC   vpc   cpc\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1 33.0   33.0\n2     2 25.8   58.8\n3     3 13.2   72.0\n4     4 11.4   83.4\n5     5  7.62  91.0\n6     6  4.72  95.7\n7     7  3.27  99.0\n8     8  1.01 100  \n\n\nIn this data set we see that there is a big drop off in variance explained on going from the second to the third PC but that the third nevertheless just about accounts for more than one variables worth of variance and so may be worth retaining. Even if we do, we will still have reduced the dimensionality of our data from eight to three.\n\n\n19.1.2 Loadings\nLet’s look at the loadings of the first three PCs.\nFrom this we can see which variables have high loadings, positive or negative, on each principal component, and so contribute most to it. This helps us work out what each principal component represents. A positive loading means that the variable correlate positively with the principal component, whereas a negative loading means that it correlates negatively.\n\nround(loadings, 2)[ , 1:3] # this means give me all the row,and columns 1 to 3, rounded to 2 decimal places.\n\n       PC1   PC2   PC3\nd13C -0.11 -0.23  0.69\nd18O -0.05  0.62  0.05\nAl   -0.56  0.18  0.07\nCa    0.33  0.08  0.37\nFe   -0.42 -0.33 -0.31\nMg   -0.28  0.47  0.34\nMn   -0.11 -0.44  0.42\nSi   -0.55 -0.04 -0.01\n\n\nHere we see that Al, Fe and Si all have strong negative loading on axis 1, meaning that high values of PC1 correspond to low values of the variables. On axis 2 we see that d180 has a strong positive loading while Mn has a strong negative loading. Thus, large values of PC2 correspond to large values of d180 and to low values of Mn.\n\n\n19.1.3 Biplot\nScores and loadings are often shown together on what is known as a biplot. The autoplot() function from the package ggfortify provides a good way to do this, offering you lots of control over the details.\n\ncoord.system &lt;- coord_fixed(ratio = 1, xlim = c(-0.3,0.3), ylim = c(-0.3,0.3))\nautoplot(pca, data = geochem,\n         alpha = 0.5,\n         loadings = TRUE,\n         loadings.label = TRUE,\n         ) + \n  coord.system +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nA biplot shows you which samples are similar to one another, and how the variables control that similarity. Samples that are plotted close together on the biplot are most similar, those that are far apart are least similar.\nFrom the distribution of the data in the biplot we can identify underlying patterns. In this biplot we see a dense cluster of data in the upper-right, and a separate, more diffuse cluster towards the lower left.\nIt turns out too that the rock samples come from one of two layers, the Carters and the Hermitage formations, depending on whether they are above or below a major non-conformity in sediment deposition that occurs at a stratigraphic position of 34.2 m. We can colour the points in the biplot accordingly\n\ngeochem &lt;- geochem |&gt;\n  mutate(formation = ifelse(purdin$StratPosition&lt;34.2,\"Carters\",\"Hermitage\"))\n\n\ncoord.system &lt;- coord_fixed(ratio=1, xlim = c(-0.3,0.3),ylim = c(-0.3,0.3))\nautoplot(pca, data = geochem,colour='formation',\n         loadings = TRUE, loadings.colour = 'blue',\n         loadings.label = TRUE, loadings.label.size = 3) + coord.system + theme_cowplot()\n\n\n\n\n\n\n\n\n\n\n19.1.4 What to Report\nWhen reporting a principal components analysis, always include at least these items:\n\nA description of any data culling or data transformations that were used prior to ordination. State these in the order that they were performed.\nWhether the PCA were scaled (scale .= TRUE) or not (scale .= FALSE).\nA scree plot that shows the explained variance of each of the principal components and that illustrates the criteria used for selecting the number of principal components to be studied.\nA table of loadings of all variables for each of the principal components that was studied. The table should highlight (e.g., with boldface) those loadings that are considered the most important for each principal component.\nOne or more plots of sample scores that emphasizes the interpretation of the principal components, such as color-coding samples by an external variable. It is often useful to show the vectors corresponding to the loadings on these plots",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Principal Components Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "PCA.html#example-two-palmer-penguins",
    "href": "PCA.html#example-two-palmer-penguins",
    "title": "19  Principal Components Analysis (PCA)",
    "section": "19.2 Example Two: Palmer Penguins",
    "text": "19.2 Example Two: Palmer Penguins\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(GGally)\nlibrary(cowplot)\ntheme_set(theme_minimal())\n\n\ndata(penguins)\npenguins &lt;- penguins |&gt; drop_na()\nglimpse(penguins)\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\npenguins |&gt;\n  dplyr::select(species, body_mass_g, ends_with(\"_mm\")) |&gt;\n  GGally::ggpairs(aes(color = species),\n          columns = c(\"flipper_length_mm\", \"body_mass_g\", \n                      \"bill_length_mm\", \"bill_depth_mm\")) +\n  scale_colour_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\"))\n\n\n\n\n\n\n\n\n\npenguins_num &lt;- penguins |&gt;\n  dplyr::select(bill_length_mm:body_mass_g)\n\n\npenguins_num |&gt;\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"values\") |&gt;\n  summarise(n = n())\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  1332\n\n\n\npenguins_pca &lt;- prcomp(penguins_num, scale.=TRUE)\n\n\n# The variance vector shows how much of the variance in the data is explained by each PC. A variance is the square of a standard deviation.\nvariance &lt;- (penguins_pca$sdev)^2\n\n# We divide each element of the vector by the total variance to find the proportion of variance explained by each PC\nvpc &lt;-variance/sum(variance) * 100\n\n# Here we find the cumulative proportin explained by the first n PCs. \ncpc &lt;- cumsum(vpc)\n\n# gather these vectors together in a data frame called scree.\nscree&lt;-tibble(PC = seq(1:length(penguins_pca$sdev)), vpc = vpc, cpc = cpc)\n\n# loadings show how much each variables contributes to each PC\nloadings &lt;- penguins_pca$rotation\nrownames(loadings) &lt;- colnames(penguins_num)\n\n# These show the values each data point has for each of the principal components\nscores &lt;- penguins_pca$x\n\n\n### Basic scree plot\nscree |&gt;\n  ggplot(aes(x = PC,y = vpc)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(yintercept = mean(scree$vpc), linetype = \"dashed\", colour = \"red\") +\n  labs(x = \"Principal component\",\n       y = \"% variance explained\") +\n  theme_cowplot()\n\n\n\n\n\n\n\n\n\nscree\n\n# A tibble: 4 × 3\n     PC   vpc   cpc\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1 68.6   68.6\n2     2 19.5   88.1\n3     3  9.22  97.3\n4     4  2.70 100  \n\n\n\nround(loadings, 2)# this means give me all the row,and columns 1 to 3, rounded to 2 decimal places.\n\n                    PC1   PC2   PC3   PC4\nbill_length_mm     0.45 -0.60 -0.64  0.15\nbill_depth_mm     -0.40 -0.80  0.43 -0.16\nflipper_length_mm  0.58 -0.01  0.24 -0.78\nbody_mass_g        0.55 -0.08  0.59  0.58\n\n\n\ncoord.system &lt;- coord_fixed(ratio = 1, xlim = c(-0.2,0.2), ylim = c(-0.2,0.2))\nautoplot(penguins_pca, data = penguins_num,\n         alpha = 0.5,\n         loadings = TRUE,\n         loadings.label = TRUE,\n         ) + \n  coord.system\n\n\n\n\n\n\n\n\n\ncoord.system &lt;- coord_fixed(ratio=1, xlim = c(-0.2,0.2),ylim = c(-0.2,0.2))\nautoplot(penguins_pca, data = penguins,\n         x = 1, y = 2,\n         colour='species',\n         alpha = 0.5,\n         loadings = TRUE, \n         loadings.colour = 'blue',\n         loadings.label = TRUE, \n         loadings.label.size = 3) +\n  coord.system\n\n\n\n\n\n\n\n\n\ncoord.system &lt;- coord_fixed(ratio=1, xlim = c(-0.2,0.2),ylim = c(-0.2,0.2))\nautoplot(penguins_pca, data = penguins,colour='species',\n         x = 2, y = 3,\n         alpha = 0.5,\n         loadings = TRUE,\n         loadings.colour = 'blue',\n         loadings.label = TRUE,\n         loadings.label.size = 3) +\n  coord.system\n\n\n\n\n\n\n\n\n\n\n\n\nHolland, Steven M. 2019. “Principal Components Analysis (PCA).” http://strata.uga.edu/8370/handouts/pcaTutorial.pdf.",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Principal Components Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "NMDS.html",
    "href": "NMDS.html",
    "title": "20  NMDS",
    "section": "",
    "text": "20.1 Ordination plot of disimilarites using NMDS\nIn the dune case, the multiple dimensions are the many species. What patterns exist among them? Which seem to like similar conditions and tend to appear at the same sites, and vice versa. Also, which sites are similar to each other, in respect of what grows there, and which are different? These are the questions that ordination plots are intended to shed light on. For data based on the presence/absence/abundance of species, Non-Metric Multidimensional Scaling (NMDS) is a widely used type of ordination technique.\nHere, we will entirely skate over the details, but will carry out the analysis using the vegan function metaMDS() then plot the result.\nWe will use as an example the dune data set that comes as part of the vegan package. This has ordinal plant species abundance data in columns and sites in rows. There are 20 sites and 30 species altogether, although most sites only have a few of these.\nlibrary(tidyverse)\nlibrary(vegan)\ndata(dune)\nglimpse(dune)\n\nRows: 20\nColumns: 30\n$ Achimill &lt;dbl&gt; 1, 3, 0, 0, 2, 2, 2, 0, 0, 4, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0\n$ Agrostol &lt;dbl&gt; 0, 0, 4, 8, 0, 0, 0, 4, 3, 0, 0, 4, 5, 4, 4, 7, 0, 0, 0, 5\n$ Airaprae &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0\n$ Alopgeni &lt;dbl&gt; 0, 2, 7, 2, 0, 0, 0, 5, 3, 0, 0, 8, 5, 0, 0, 4, 0, 0, 0, 0\n$ Anthodor &lt;dbl&gt; 0, 0, 0, 0, 4, 3, 2, 0, 0, 4, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0\n$ Bellpere &lt;dbl&gt; 0, 3, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0\n$ Bromhord &lt;dbl&gt; 0, 4, 0, 3, 2, 0, 2, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ Chenalbu &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0\n$ Cirsarve &lt;dbl&gt; 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ Comapalu &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0\n$ Eleopalu &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 4, 5, 8, 0, 0, 0, 4\n$ Elymrepe &lt;dbl&gt; 4, 4, 4, 4, 4, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ Empenigr &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0\n$ Hyporadi &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 5, 0\n$ Juncarti &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 4\n$ Juncbufo &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 2, 0, 4, 0, 0, 4, 3, 0, 0, 0, 0, 0, 0, 0\n$ Lolipere &lt;dbl&gt; 7, 5, 6, 5, 2, 6, 6, 4, 2, 6, 7, 0, 0, 0, 0, 0, 0, 2, 0, 0\n$ Planlanc &lt;dbl&gt; 0, 0, 0, 0, 5, 5, 5, 0, 0, 3, 3, 0, 0, 0, 0, 0, 2, 3, 0, 0\n$ Poaprat  &lt;dbl&gt; 4, 4, 5, 4, 2, 3, 4, 4, 4, 4, 4, 0, 2, 0, 0, 0, 1, 3, 0, 0\n$ Poatriv  &lt;dbl&gt; 2, 7, 6, 5, 6, 4, 5, 4, 5, 4, 0, 4, 9, 0, 0, 2, 0, 0, 0, 0\n$ Ranuflam &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 4\n$ Rumeacet &lt;dbl&gt; 0, 0, 0, 0, 5, 6, 3, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0\n$ Sagiproc &lt;dbl&gt; 0, 0, 0, 5, 0, 0, 0, 2, 2, 0, 2, 4, 2, 0, 0, 0, 0, 0, 3, 0\n$ Salirepe &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 5\n$ Scorautu &lt;dbl&gt; 0, 5, 2, 2, 3, 3, 3, 3, 2, 3, 5, 2, 2, 2, 2, 0, 2, 5, 6, 2\n$ Trifprat &lt;dbl&gt; 0, 0, 0, 0, 2, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ Trifrepe &lt;dbl&gt; 0, 5, 2, 1, 2, 5, 2, 2, 3, 6, 3, 3, 2, 6, 1, 0, 0, 2, 2, 0\n$ Vicilath &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0\n$ Bracruta &lt;dbl&gt; 0, 0, 2, 2, 2, 6, 2, 2, 2, 2, 4, 4, 0, 0, 4, 4, 0, 6, 3, 4\n$ Callcusp &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 3, 0, 0, 0, 3\nord &lt;- metaMDS(dune)\n\nRun 0 stress 0.1192678 \nRun 1 stress 0.1183186 \n... New best solution\n... Procrustes: rmse 0.02026934  max resid 0.06495756 \nRun 2 stress 0.1192678 \nRun 3 stress 0.1192678 \nRun 4 stress 0.1183186 \n... New best solution\n... Procrustes: rmse 4.436103e-06  max resid 1.374397e-05 \n... Similar to previous best\nRun 5 stress 0.1183186 \n... Procrustes: rmse 2.30823e-06  max resid 7.005743e-06 \n... Similar to previous best\nRun 6 stress 0.1183186 \n... Procrustes: rmse 4.379328e-06  max resid 1.40192e-05 \n... Similar to previous best\nRun 7 stress 0.1886532 \nRun 8 stress 0.1183186 \n... Procrustes: rmse 7.579409e-06  max resid 2.422446e-05 \n... Similar to previous best\nRun 9 stress 0.1183186 \n... Procrustes: rmse 2.441043e-06  max resid 8.2769e-06 \n... Similar to previous best\nRun 10 stress 0.1192679 \nRun 11 stress 0.1809579 \nRun 12 stress 0.1183186 \n... Procrustes: rmse 1.531721e-06  max resid 4.827847e-06 \n... Similar to previous best\nRun 13 stress 0.1192679 \nRun 14 stress 0.1192679 \nRun 15 stress 0.1183186 \n... New best solution\n... Procrustes: rmse 3.536066e-06  max resid 1.052042e-05 \n... Similar to previous best\nRun 16 stress 0.1183186 \n... Procrustes: rmse 8.688538e-06  max resid 2.756482e-05 \n... Similar to previous best\nRun 17 stress 0.1183186 \n... Procrustes: rmse 2.887226e-06  max resid 8.6588e-06 \n... Similar to previous best\nRun 18 stress 0.1183186 \n... Procrustes: rmse 4.157663e-06  max resid 1.387561e-05 \n... Similar to previous best\nRun 19 stress 0.1808911 \nRun 20 stress 0.1192678 \n*** Best solution repeated 4 times\nplot(ord, type = \"t\")\npoints(ord, display = \"sites\", cex = 0.2, pch=21, col=\"red\", bg=\"yellow\")\n\n\n\n\n\n\n\n#text(ord, display = \"spec\", cex=0.7, col=\"blue\")\nThe plot shows in two dimensions the sites, here numbered 1-18, and in red, the species. Sites that appear close together on this plot are more similar than sites that are far apart. Species that are close together on the plot tend to occur together, while species that are far apart do so less often.\nCheck that this plot accords with the dendrogram above.\nImprovements to this plot can be made using ggplot - plenty of blogs exist that tell you how to do this.",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>NMDS</span>"
    ]
  },
  {
    "objectID": "NMDS.html#how-well-does-the-nmds-plot-reflect-true-patterns-in-the-data",
    "href": "NMDS.html#how-well-does-the-nmds-plot-reflect-true-patterns-in-the-data",
    "title": "20  NMDS",
    "section": "20.2 How well does the NMDS plot reflect true patterns in the data?",
    "text": "20.2 How well does the NMDS plot reflect true patterns in the data?\nThis can be quantified by calculating the ‘stress’ of the process. A low value indicates that the NMDS ordination plot is capturing the true patterns in the data well. Anything above about 0.2 means that it is doing a poor job and that you may need to transform the data in some way and try again.\n\nord$stress\n\n[1] 0.1183186\n\n\nWe find here a stress value of less than 0.2, so we can infer that the NMDS plot gives a useful indication of patterns in the data.\nA Shephard Plot is another way to visually indicatye the degree of stress. If the data ate tightly grouped around a diagonal straight line then there is low strees. If they are, the stres is higher.\n\nstressplot(ord)\n\n\n\n\n\n\n\n\nFor our data, the plot indicates low stress, in accordance with the direct measurement of the stress value.",
    "crumbs": [
      "Multivariate Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>NMDS</span>"
    ]
  },
  {
    "objectID": "qq_plots.html",
    "href": "qq_plots.html",
    "title": "21  Quantile-quantile plots",
    "section": "",
    "text": "21.0.1 Introduction\nAdapted from an exercise by Jon Yearsley (School of Biology and Environmental Science, UCD)\nQ-Q plots can play a useful role when trying to decide whether a dataset is normally distributed, and if it is not, then how it differs from normality.\nWe will investigate the types of quantile-quantile plots you get from different types of distributions.\nWe will look at data distributed according to",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#what-is-a-q-q-plot",
    "href": "qq_plots.html#what-is-a-q-q-plot",
    "title": "21  Quantile-quantile plots",
    "section": "21.1 What is a Q-Q plot?",
    "text": "21.1 What is a Q-Q plot?\nQuantiles partition a dataset into equal subsets. For example, if we wished to partition a standard normal (mean = 0, standard deviation = 1) population into 4 equal subsets, the 3 quantiles (ie the three values of x) that would do this are -0.675, 0 and 0.675. In this way, 25% of the population would have a value greater than 0.675, 25% between 0 and 0.675, 25% between -0.675 and 0 and the final 25% would have a value less than -0.675. When we draw the distribution, the areas under the curve between neighbouring quantiles will be equal:",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#normally-distributed-data.",
    "href": "qq_plots.html#normally-distributed-data.",
    "title": "21  Quantile-quantile plots",
    "section": "21.2 Normally distributed data.",
    "text": "21.2 Normally distributed data.\nBelow we show an example of 150 observations that are drawn from a normal distribution. The normal distribution is symmetric, so has no skew. Its mean is equal to its median.\nOn a Q-Q plot data that are approximtely normally distributed data lie roughly on a straight line, perhaps looking a bit ragged at each end. The box plot is symmetric with few or no outliers.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#right-skewed-data.",
    "href": "qq_plots.html#right-skewed-data.",
    "title": "21  Quantile-quantile plots",
    "section": "21.3 Right-skewed data.",
    "text": "21.3 Right-skewed data.\nRight skewed distributions are non-symmetric and have a long tail heading towards extreme values on the right-hand side of the distribution. The mean is more positive than the median.\nIn the example we show an exponential distribution.\nIn the Q-Q plot, such distributions give a distinctive convex curvature. The box-plot may show outliers out towards large values.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#left-skewed-data.",
    "href": "qq_plots.html#left-skewed-data.",
    "title": "21  Quantile-quantile plots",
    "section": "21.4 Left-skewed data.",
    "text": "21.4 Left-skewed data.\nLeft skewed distributions are non-symmetric and have a long tail heading towards extreme values on the left-hand side of the distribution. The mean is more negative than the median. The box plot may show outliers down towards small values.\nIn the example we show a negative exponential distribution.\nIn the Q-Q plot, such distributions give a distinctive concave curvature.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#under-dispersed-data",
    "href": "qq_plots.html#under-dispersed-data",
    "title": "21  Quantile-quantile plots",
    "section": "21.5 Under-dispersed data",
    "text": "21.5 Under-dispersed data\nUnder-dispersed data are data whose distribution is more concentrated around a central value than is the case for normally distributed data. There are fewer outliers and the tails of the distribution are lighter. As an example here we show 150 points drawn from a uniform distribution.\nNote the distinctive curvature of the Q-Q plot. The ‘box’ of the boxplot is bigger than for a normal distribution, since the interquartile range covers a larger range of values.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#over-dispersed-data",
    "href": "qq_plots.html#over-dispersed-data",
    "title": "21  Quantile-quantile plots",
    "section": "21.6 Over-dispersed data",
    "text": "21.6 Over-dispersed data\nOver-dispersed data are data whose distribution is more widely spread around a central value than is the case for normally distributed data. There are more outliers and the tails of the distribution are fatter. As an example here we show 150 points drawn from a laplace distribution.\nNote the distinctive curvature of the Q-Q plot - like the previous one but curving the other way. The ‘box’ of the boxplot is smaller than for a normal distribution, since the interquartile range covers a smaller range of values.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#caution",
    "href": "qq_plots.html#caution",
    "title": "21  Quantile-quantile plots",
    "section": "21.7 Caution",
    "text": "21.7 Caution\nWith small data sets, the scatter in the data can make it difficult to tell from the histogram (especially) or even the Q-Q plot whether the dataset is plausibly drawn from a normally distributed population. In that case you might choose to combine use of the plots with a normality test, such as Kolmogorov-Smirnov or Shapiro-Wilk. The null hypothesis of these is that the data ARE drawn from normally distributed populations, so the smaller the p-value when they are applied to a dataset, the less likely it is that this is true.\nWith large data sets,the Kolmogorov-Smirnov and Shapiro-Wilk tests become very sensitive to even small deviations from normality and might give a p-value that would lead you to suppose that a dataset was not drawn from a normally distributed population. Since no data set is ever truly normal, even when we consider the whole population, all we really need to know is whether the data are close enough to normal that the various tests (eg t-test, ANOVA, correlation, least square regression) that require it are going to work well enough. For these large data sets, histograms and Q-Q plots can be very useful indicators of approximate normality.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#who-cares-about-normality-anyway-the-central-limit-theorem.",
    "href": "qq_plots.html#who-cares-about-normality-anyway-the-central-limit-theorem.",
    "title": "21  Quantile-quantile plots",
    "section": "21.8 Who cares about normality anyway? The central limit theorem.",
    "text": "21.8 Who cares about normality anyway? The central limit theorem.\nLastly, for large enough data sets, we don’t actually need the data to be normally distributed for the tests that require normality to work! This is because what they require is not that the dataset itself be normal, but that the distribution of the means of many such data sets, the so-called sampling distribution, be normal. A very important mathematical result known as the Central Limit Theorem guarantees that this will be the case whatever the distribution of each of the data sets, as long as these datasets are large enough!\nHow large is large enough? There’s the rub! A common rule of thumb is that if the dataset has size N&gt;30 or so, then it is safe to use tests that require normality. Indeed, one does find that sampling distributions for data drawn from uniform or mildly skewed distributions such as the exponential distribution are roughly normal when N exceeds 30 or so, but for more skewed datasets, a larger dataset can be needed - it depends on how far from normality the distribution is. The further from normal it is the larger the dataset needs to be before the Central Limit Theorem applies to a good approximation. For a highly skewed dataset, for example one distributed according to something like a log-normal distribution, it can require N&gt;200 or so, or even more before it is OK to use t-tests and the like.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "power-analysis.html",
    "href": "power-analysis.html",
    "title": "22  Power Analysis",
    "section": "",
    "text": "22.1 Simulation of one trial\nThis topic is all about how to design our studies such that they are likely to detect an effect, whether that is a correlation or a difference, if there really is one there to be detected. If there were, and we didn’t that would be a shame (and a waste of time and money and possibly an ethical hoohah).\nIn what follows we focus on a simple study that seeks to detect a difference between two populations, but the ideas generalise to other designs.\nSuppose we have a magic soil supplement that we hope will enhance the growth rate of tomato plants, make us rich and pay for a comfortable retirement.\nBefore riches, however, we have to be sure that it has the desired effect. For the supplement to be a money spinner let’s suppose that we need it to enhance the growth rate of plants by 10% and so we must do a study to see if this is in fact the case.\nIn a properly randomised design, we take N plants and give them ‘normal’ plant food, and another N plants and give them the normal food plus our supplement. We keep all other conditions the same for the two groups of plants.\nAfter 30 days, the plants grown with the usual food grow with a mean mass of 300g and a standard deviation of 30g. The other plants will need to have a mean mass of at least 330g, and we will suppose that they too will have a standard deviation of 30g.\nSuppose we chose N = 100. Then we can simulate the masses of the individual plants in the two samples, supposing there were a 10% effect of supplement. We do this by using the rnorm() function to draw samples of 100 replicates each from a normally distributed population, in one case with a mean of 300g and in the other with a mean of 330g, both with standard deviations of 30g\nN&lt;-100\ndelta&lt;-0.05 # ie 10% ie the mean of the treatment population is 10% greater than that of the control population\n\n# set up the sample parameters: means and standard deviations\nm1&lt;-300 \nsd1&lt;-30\nm2&lt;-(1+delta)*m1\nsd2&lt;-sd1\n\nuntreated&lt;-rnorm(N,m1,sd1) # the control sample\ntreated&lt;-rnorm(N,m2,sd2) # the treated sample\n\n# put these samples  in a tidy tibble\ntrial_data &lt;- tibble(treatment = c(rep(\"untreated\", N), rep(\"treated\", N)),\n                     mass = c(untreated, treated))\n# plot these simulated data\ntrial_data |&gt;\n  ggplot(aes(x=mass,colour = treatment, fill=treatment)) +\n  geom_density(bins=20, alpha = 0.4) +\n  # geom_jitter(colour=\"gray50\",width=0.2) +\n  labs(x=\"Plant mass (g)\",\n       y=\"Frequency\") +\n  theme_bw()\nSuppose the supplement really did work like this, and really did improve growth rates by 10%. How many plants, ie what value of N would we need in each group in order to have an 80% chance of correctly spotting this difference? This probability of detection of an effect that is there is what we call the power of a study. If the effect is smaller than 10% (or whatever boost we deemed sufficient) and we do not detect it we do not mind, since that means that our supplement had not worked as we had hoped, but it would be a waste of our time and money if there really were a sizeable effect but we did not spot it because our sample size was too small. Equally, we do not want a sample size that is far larger than is necessary to achieve sufficient power. Doing so would incur unnecessary time and money costs and possibly have ethical implications.\nIn thinking about how we might analyse our data we formulate a null hypothesis:\nThe supplement has no effect.\nIn that case we would expect the difference between the masses of the treated plants and untreated plants to be zero.\nIf in fact the supplement has an effect on growth, then the data should force us to reject this null hypothesis. In order that there be at least an 80% chance that it does this, there has to be an at least 80% chance that the mean value of our treated plants lies outside the rejection regime of the null hypothesis.\nWith one pair of samples of plants, we might or might not detect the effect, depending on which individuals ended up being included in the samples that we drew from their respective populations, since this would determine whether the mean of the treated plants was or was not in the rejection regime of the null. Thus, if we ran a t-test on our two simulated samples, we might or might not get a p-value that is less than our chosen significance level,\nt.test(mass~treatment,data=trial_data)$p.value\n\n[1] 0.03042862\nWht we want to know is: what is the chance that we would get the ‘correct’ result, which in our case is that there is a difference and which would be indicated to us by the p-value being less than our chosen significance level (for brevity’s sake, let’s just assume from now on that we chose this to be 0.05), so that we (correctly) reject the null hypothesis of there being no difference.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Power Analysis</span>"
    ]
  },
  {
    "objectID": "power-analysis.html#simulation-of-many-trials",
    "href": "power-analysis.html#simulation-of-many-trials",
    "title": "22  Power Analysis",
    "section": "22.2 Simulation of many trials",
    "text": "22.2 Simulation of many trials\nA way to find out is to carry out this simulation many times and see in what fraction of our trials we see a significant effect (that is, p &lt; 0.05), supposing that the supplement really does work. That will give us an idea of the power of our study.\nThat is what we will now do:\nWe write a function that will return TRUE or FALSE depending on the p-value of a trial in which we specifiy the mean value of the control group, the standard deviation of the control group (assumed to be the same in the treatment group), the size of the effect delta, where if delta is 0.1, say, then we mean that the effect size is a 10% increase in growth mass, N is the sample size for each group and alpha is the significance level that we choose (most likely, but not necessarily 0.05).\nIn the function we carry out a t-test for each pair of samples to determine whether we can reject the null hypothesis that the treated plants are drawn from a population with the same mass as the untreated plants.\nWe know that the null hypothesis is false because we have drawn our samples from populations that do differ in their mean values. We want to see if our test correctly rejects the null so that we detect the effect, which here is the mass difference between the means of the two groups of plants.\nIf the p-value is less than our chosen significance level then we reject the null, if not, we do not.\nRejecting the null in this case means we have a ‘True Positive’ and so we make our function return the logical value TRUE in that case. Failing to reject the null in this case is a mistake. We call this kind of mistake a ’False Negative” and when this happens me make out function return the logical value FALSE.\n\neffect_detected&lt;-function(m1,sd1,delta,N,alpha){\n  pop1&lt;-rnorm(N,m1,sd1)\n  pop2&lt;-rnorm(N,m1*(1+delta),sd1)\n  return(t.test(pop1,pop2)$p.value&lt;alpha) #return TRUE if p &lt;0.05, return FALSE if not.\n}",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Power Analysis</span>"
    ]
  },
  {
    "objectID": "power-analysis.html#calculate-the-power-of-the-study-for-a-range-of-sample-and-effect-sizes",
    "href": "power-analysis.html#calculate-the-power-of-the-study-for-a-range-of-sample-and-effect-sizes",
    "title": "22  Power Analysis",
    "section": "22.3 Calculate the power of the study for a range of sample and effect sizes",
    "text": "22.3 Calculate the power of the study for a range of sample and effect sizes\nNow we run this trial as many times as we want. Let us run it 10,000 times, for a range of sample sizes and effect sizes. In any one trial we may or may not reject the null. We want to see, for a given set of conditons, in what fraction of trials, in the long run, we do reject the null. That will be an estimate of the power of our study: the likelihood that we will detect an effect if the effect really exists, as it does here.\n\ntrials&lt;-10000\nm1&lt;-300\nsd1&lt;-30 # population standard deviation\ndeltas&lt;-c(0.01,0.02,0.03,0.04,0.05,0.1) # effect size where, for example, 0.05 means that the effect is 5% the size of the control mean\nN&lt;-100 # sample size\nalpha&lt;-0.05 # chosen significance level\n\nNs&lt;-c(seq(2,9,1),seq(10,50,10),seq(60,200,20),seq(240,480,40))\n\npower_vals_raw&lt;-matrix(rep(0,length(Ns)*length(deltas)),ncol=length(deltas))\nfor(i in 1:length(Ns)){\n  for (j in 1:length(deltas)){\n    trial_results &lt;- replicate(trials,effect_detected(m1,sd1,deltas[j],Ns[i],alpha))\n    power_vals_raw[i,j]&lt;-mean(trial_results)\n  }\n}\n\npower_vals &lt;- as.tibble(power_vals_raw)\nnames(power_vals) &lt;- deltas\npower_vals &lt;- power_vals |&gt;\n  mutate(N=Ns) |&gt;\n  pivot_longer(-N,names_to = \"effect_size\", values_to = \"power\")\n\nPlotted, this looks like:\n\npower_vals |&gt;\n  mutate(ok=power&gt;0.8) |&gt;\n  ggplot(aes(x=N,y=power,colour=effect_size)) +\n  # geom_point() +\n  geom_smooth(method = \"loess\", span=0.25, linewidth=0.6, se=FALSE) +\n  # geom_line() +\n  scale_x_continuous(breaks=seq(0,max(Ns),50)) +\n  scale_y_continuous(breaks=seq(0,1,0.2), limits=c(0,1)) +\n  geom_hline(yintercept=0.8,linetype=\"dashed\",colour=\"darkblue\", linewidth = 0.1) +\n  geom_hline(yintercept=0.9,linetype=\"dashed\",colour=\"darkblue\", linewidth = 0.1) +\n  labs(x = \"Sample size N\",\n       y = \"Power\",\n       colour =\"Effect size\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIn this plot we show how the power varies with sample size N, for different effect sizes (where 0.05, say, means a difference of 5% in populations means), with a population standard devation of 30 g on a control mean of 300g, and a significance level of 0.05.\nWhat is a suitable sample size for a power of 80%?\nWe see that we need a sample size of about 65 to get a power of 80%.\nWhat sample size would be needed for a power of 90%, or 99%?\nWe need sample sizes of 85 for a power of 90% and of 150 for a power of 99%. There are progressively diminshing returns once you try to make the power much greater than 80% or 90% or so. The sample sizes needed become very large.\nNow try varying the effect size, the population variation and the chosen significance level and see how they affect the power.\nWe should find that, all else being equal:\n\nIf the effect size is reduced, the power decreases. It is harder to tell apart two populations that do not differ by much.\nIf the population variation goes down the power increases. It is easier to tell apart two populations if there is little variation within each population.\nIf the signicance level is reduced, say to 0.01 from 0.05, the power will go down. We are reducing the type 1 error rate, but at the same increasing the type 2 error rate. It is les likely, even if there is an effect, that we will detect it.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Power Analysis</span>"
    ]
  },
  {
    "objectID": "power-analysis.html#factors-that-affect-the-power-of-a-study",
    "href": "power-analysis.html#factors-that-affect-the-power-of-a-study",
    "title": "22  Power Analysis",
    "section": "22.4 Factors that affect the power of a study",
    "text": "22.4 Factors that affect the power of a study\nWhat factors about a study force you to have larger sample sizes for a given power? \nIf the effect size is small, the population variation is large or the significance level required is small.\nHow would you know what the variation of your population is? How could you reduce this?.\nYou might look at the literature. Very likely, someone has done a study similar to yours. What variation did they see? Alternatively, you could do a pilot study and with relatively little effort get an idea yourself of the variation within your population of interest of the attrivute you want to measure.\nWhat downside might there be to reducing the population variation, supposing you could do it, in order to increase power?.\nYou can sometimes reduce the variation within the population that you are studying by restricting variation in causal factors that do not interest you for that particular study, but which might also be contributing to variation in the outcome variable that does interest you. So while your study might focus on the impact of soil treatments on growth rates of seedlings, for example, it may be that soil moisture also affects growth rate. Hence if you ensure that all seedlings are grown under the same moisture conditions, you will remove any variation in growth rates due to that, and hence reduce the total varation. This will increase the power of your study - where by that we mwan the likelihood that you will detect any difference that your soil supplement made to growth rate.\nThe downside to this approach is that you will restrict the applicability of your study. In the example above, having applied the supplement and observed some difference or not compared to a control sample, you could only make inferences to populations of seedlings that were grown under precisely the soil moisture conditions you chose for your study. You could no longer make statements about the supplement preferences of seedlings grown under any old soil moisture conditions.\nAn alternative approach, which preserves both power and range of applicability is to change the design of the study. In this case, instead of making it a one-factor study in which growth rate is measured against supplement presence or absence, we could also measure the soil moisture and include this in the analysis. If we had measured discrete levels of soil moisture, we would now have a 2-way ANOVA, and if we had measured it as a continuous variable we would have an ANCOVA.\nHow would you know what the effect size was, and could you increase it?\nThe same applies here as for the variation. You could look at the literature. Someone else has very likely done a similar study. What effect size did they observe? Or, you could do a quick and dirty pilot study. Unlike the population variation thing, however, it is easier to detect an effect if the effect size is bigger. Is there anything you can do to increase it? In an observational study in the wild, perhaps not but in a manipulative study perhaps you can. In our example we might use large doses of supplement rather than small ones, hoping to see a larger effect as a result.\nWhat assumptions have gone into this power calculation?\nWe assumed here that our samples were drawn from normally distributed populations with equal variances. In our simulations we knew that was the case because me made it so by design, but we could have given them any distribution or variance we wanted. Simulations are a very powerful tool.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Power Analysis</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Books on using R\n(Beckerman et al. 2017)\nThis book is the one whose approach to the use of R aligns most closely with this one. It has very litle statistics\n(Crawley 2014)\nThis is a statistics book that focuses on the needs of ecologists, with R code throughout.\n\n\nBeckerman, Andrew, Dylan Childs, Owen Petchey, Andrew P. Beckerman,\nDylan Z. Childs, and Owen L. Petchey. 2017. “Introducing\nStatistics in r.” In. https://doi.org/10.1093/acprof:oso/9780198787839.003.0005.\n\n\nCrawley, Michael J. 2014. Statistics: An Introduction Using r.\n2nd ed. Wiley Blackwell.\n\n\nCumming, Geoff, Fiona Fidler, and David L. Vaux. 2007. “Error Bars\nin Experimental Biology.” Journal of Cell Biology 177\n(1): 7–11. https://doi.org/10.1083/jcb.200611141.\n\n\nFreedman, David, Robert Pisani, Roger Purves, and Ani Adhikari. 1991.\nStatistics. 2nd ed. W. W. Norton & Company.\n\n\nGilbert, Francis, Peter McGregor, and Chris Barnard. 2017. Asking\nQuestions in Biology; a Guide to Hypothesis Testing, Experimental Design\nand Presentation in Practical Work and Research Projects. 5th ed.\nBenjamin Cummings.\n\n\nHolland, Steven M. 2019. “Principal Components Analysis\n(PCA).” http://strata.uga.edu/8370/handouts/pcaTutorial.pdf.\n\n\nJovan, Sarah. 2008. “Lichen Bioindication of Biodiversity, Air\nQuality, and Climate: Baseline Results from Monitoring in Washington,\nOregon, and California.” http://gis.nacse.org/lichenair/doc/Jovan2008.pdf.\n\n\nLauber, Christian L., Micah Hamady, Rob Knight, and Noah Fierer. 2009.\n“Pyrosequencing-Based Assessment of Soil pH as a Predictor of Soil\nBacterial Community Structure at the Continental Scale.”\nApplied and Environmental Microbiology 75 (15): 5111–20. https://doi.org/10.1128/aem.00335-09.\n\n\nOh, Youjung, Sang Myeong Oh, Pil-Hun Chang, and Il-Ju Moon. 2023.\n“Optimal Tropical Cyclone Size Parameter for Determining\nStorm-Induced Maximum Significant Wave Height.” Frontiers in\nMarine Science 10 (February). https://doi.org/10.3389/fmars.2023.1134579.\n\n\nOksanen, Jari. 2004. “Multivariate Analysis in Ecology - Lecture\nNotes.” https://web.archive.org/web/20181024083948/http://cc.oulu.fi:80/~jarioksa/opetus/metodi/notes.pdf.\n\n\nPain, Deborah J., Rafael Mateo, and Rhys E. Green. 2019. “Effects\nof Lead from Ammunition on Birds and Other Wildlife: A Review and\nUpdate.” Ambio 48 (9): 935–53. https://doi.org/10.1007/s13280-019-01159-0.\n\n\nShlens, Jonathon. 2014. “A Tutorial on Principal Component\nAnalysis.” https://doi.org/10.48550/arXiv.1404.1100.",
    "crumbs": [
      "References",
      "References"
    ]
  }
]