[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "r4nqy",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works.html",
    "href": "ANOVA_how_it_works.html",
    "title": "3  ANOVA principles",
    "section": "",
    "text": "3.1 What is ANOVA?\nMaterial used from Chapter One of Grafen and Hails: Modern Statistics for the Life Sciences",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ANOVA principles</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works.html#what-is-anova",
    "href": "ANOVA_how_it_works.html#what-is-anova",
    "title": "3  ANOVA principles",
    "section": "",
    "text": "3.1.1 The basic principles of ANOVA\nIn a simple case we consider the comparison of three means. This is done by the analysis of variance (ANOVA). In this case we will go through an example in detail and work out all the mechanics, but once we have done that and seen how the output is derived from the input we will not need to do it again. We will use R to do the heavy lifting. We will just need to know when it is appropriate to use ANOVA, how to get R to do it and how to interpret the output that R produces.\n\n\n3.1.2 The Scenario\nSuppose we have three fertilizers and wish to compare their efficacy. This has been done in a field experiment where each fertilizer is applied to 10 plots and the 30 plots are later harvested, with the crop yields being calculated. We end up with three groups of 10 figures and we wish to know if there are any differences between these groups.\nWhen we plot the data we see that the fertilizers do differ in the amount of yield produced but that there is also a lot of variation between the plots that were given the same fertilizer.\n\n\n\n\n\n\n\n\n\n\n\n3.1.3 What does an ANOVA do?\nAn ANOVA (ANalysis Of VAriance) analysis attempts to determine whether the differences between the effect of the fertilizers is significant by investigating the variability in the data. We investigate how the variability between groups compares to the variability within groups.\n\n\n3.1.4 Grand Mean\nFirst we calculate the ‘grand mean’, the mean of the yields across all 30 plots:\n\n\n[1] 4.643667\n\n\n\n3.1.4.1 Deviations from the grand mean\n\n\n\n\n\n\n\n\n\n\n\n3.1.4.2 Mean value of yield for each fertilizer\n\n\n# A tibble: 3 × 2\n  FERTIL fmean\n  &lt;fct&gt;  &lt;dbl&gt;\n1 1       5.44\n2 2       4.00\n3 3       4.49\n\n\n\n\n\n3.1.5 Measures of variability\n\n3.1.5.1 SST - Total sum of squares\n\n\n[1] 36.4449\n\n\nSST is the total sum of squares. It is the sum of squares of the deviations of the data around the grand mean. This is a measure of the total variability of the data set.\n\n\n3.1.5.2 SSE - Error sum of squares\nSSE is the error sum of squares. It is the sum of the squares of the deviations of the data around the three separate group means. This is a measure of the variation between plots that have been given the same fertilizer.\n\n\n3.1.5.3 SSF - Fertilizer sum of squares\nSSF is the fertilizer sum of squares. This is the sum of the squares of the deviations of the group means from the grand mean. This is a measure of the variation between plots given different fertilizers.\n\n\n\n\n\n\n\n\n\nWhen the three group means are fitted, there is an obvious reduction in variability around the three means compared to that around the grand mean, but it is not obvious if the fertilizers have had an effect on yield.\nAt what point do we decide if the amount of variation explained by fitting the means is significant? By this, we mean, “When is the variability between the group means greater than we would expect by chance alone?\nFirst, we note that SSF and SSE partition between them the total variability in the data:\n\n\n\n3.1.6 SST = SSF + SSE\n\n\n[1] 36.4449\n\n\n[1] 10.82275\n\n\n[1] 25.62215\n\n\n[1] 36.4449\n\n\nSo the total variability has been divided into two components. That due to differences between plots given different treatments and that due to differences between plots given the same treatment. Variability must be due to one or other of these components. Separating the total SS into its component SS is known as partitioning the sums of squares.\nA comparison of SSF and SSE is going to indicate whether fitting the three fertilizer means accounts for a significant amount of variability.\nHowever, to make a proper comparison, we really need to compare the variability per degree of freedom ie the variance.\n\n\n3.1.7 Partitioning the degrees of freedom\nEvery sum of squares (SS) has been calculated using a number of independent pieces of information. In each, case, we call this number the number of degrees of freedom for the SS.\nFor SST this number is one less than the number of data points n. This is because when we calculate the deviations of each data point around a grand mean there are only n-1 of them that are independent, since by definition the sum of these deviations is zero, and so when n-1 of them have been calculated, the final one is pre-determined.\nSimilarly, when we calculate SSF, which measures the deviation of the group means from the grand mean, we have \\(k\\)-1 degrees of freedom, (where in the present example \\(k\\), the number of treatments, is equal to three) since the deviations must sum to zero, so when \\(k\\)-1 of them have been calculated, the last one is pre-determined.\nFinally, SSE, which measure deviation around the group means will have n-k degrees of freedom, since the sum of each of the deviations around one of the group means must sum to zero, and so when all but one of them have been calculated, the final one is pre-determined. There are \\(k\\) group means, so the total degrees of freedom for SSE is n-k.\nThe degrees of freedom are additive: \\[\ndf(\\text{SST}) = df(\\text{SSE}) + df(\\text{SSF})\n\\] Check:\n\\[\\begin{align*}\ndf(\\text{SST}) &= n-1\\\\\ndf(\\text{SSE}) &= k-1\\\\\ndf(\\text{SSF}) &= n-k\\\\\n\\therefore df(\\text{SSE}) + df(\\text{SSF}) &= k-1 + n-k\\\\\n&=n-1\\\\\n&=df(\\text{SST})\n\n\\end{align*}\\]\n\n\n3.1.8 Mean Squares\nNow we can calculate the variances which are a measure of the amount of variability per degree of freedom.\nIn this context, we call them mean squares. To find each one we divided each of the sums of squares (SS) by their corresponding degrees of freedom.\nFertiliser Mean Square (FMS) = SSF / k - 1. This is the variation per df between plots given different fertilisers.\nError Mean Square (EMS) = SSE / n - k. This is the variation per df between plots given the same fertiliser.\nTotal Mean Square (TMS) = SST / n - 1. This is the total variance per df of the dataset.\nUnlike the SS, the MS are not additive. That is, FMS + EMS \\(\\neq\\) TMS.\n\n\n3.1.9 F-ratios\nIf none of the fertilizers influenced yield, we would expect as much variation between the plots treated with the same fertilizer as between the plots treated with different fertilizers.\nWe can express this in terms of the mean squares: the mean square for fertilizer would be the same as the mean square for error:\n\\[\n\\frac{\\text{FMS}}{\\text{EMS}}=1\n\\] We call this ratio the F-ratio. It is the end result of ANOVA. F-ratios can never be negative since they are the ratio of two mean square values, both of which must be non-negative, but there is no limit to how large they can be.\nEven if the fertilizers were identical, the F-ratio is unlikely to be exactly 1 - it could by chance take a whole range of values. The F-distribution represents the range and likelihood of all possible F ratios under the null hypothesis. ie when the fertilizers were identical.\nThe shape of the F distribution depends on the degrees of freedom of FMS and EMS, and we normally specify it by giving the values of each. Below we show F distributions for 2 and 27 degrees of freedom (ie 3 plots, so k = 3, so the degrees of freedom of FMS = k-1 =2, and 10 plants per plot, so n = 3 x 10 =30, and hence the degrees of freedom of EMS = n-k = 30 - 3 = 27), and for 10 and 27 degrees of freedom.\n\n\n\n\n\n\n\n\n\nNote that, whatever the degrees of freedom, F-distributions are examples of so-called probability density functions. The area beneath them between any two values of F-ratio is equal to the probability of getting an F-ratio in that range. Hence the total area under the curves is equal to 1, since the F-ratio must take some value between zero and infinity, and the area under the tail to the right of any given F-ratio is the probability of getting an F-ratio bigger than that value.\nHence, the probability under the null hypothesis of getting an F-ratio as large or larger than the value we actually got is the area to the right of this F-ratio under the appropriate F distribution. We often call this probability the p-value. p for probability. p-values are the the probability of getting data as extreme (same F-ratio,) or more extreme (bigger F-ratio) as the data you got you got if the null hypothesis were true.\nIf the fertilizers were very different, then the FMS would be much greater than the EMS and the F-ratio would be greater than one. However it can be quite large even when there are no treatment differences. So how do we decide when the size of the F-ratio is due to treatment rather than to chance?\nTraditionally, we decide that it sufficiently larger than one to be due to treatment differences if it would be this large or larger under the null hypothesis only 5% or less of the time. If we had inside knowledge that the null hypothesis was in fact true then we would still get an F-ratio that large or larger 5% of the time.\nOur p-value ie the probability that the F-ratio would have been as large as it is or larger, under the null hypothesis, represents the strength of evidence against the null hypothesis. The smaller it is, the stronger the evidence, and only when it is less than 0.05 do we regard the evidence as strong enough to reject the null.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ANOVA principles</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works.html#anova-example-1",
    "href": "ANOVA_how_it_works.html#anova-example-1",
    "title": "3  ANOVA principles",
    "section": "3.2 ANOVA example 1",
    "text": "3.2 ANOVA example 1\nWe will carry out the ANOVA analysis of the fertilizer data discussed on the previous tab.\nOur question is whether yield depends on fertilizer.\nWhat is our null hypothesis?\nStart a new notebook with these two code chunks to begin with:\n```{r global-options, include=FALSE}\nknitr::opts_chunk$set(fig.width=12, fig.height=8, warning=FALSE, message=FALSE,echo=FALSE)\n```\n\n```{r load packages, message=FALSE,warning=FALSE,echo=FALSE}\nlibrary(tidyverse)\nlibrary(here)\nlibrary(cowplot)\nlibrary(gridExtra)\nlibrary(ggfortify)\n```\nLoad the fertilizer.csv data into an object call `fertilizer\nIs it tidy data? If not, tidy it.\nConvert the FERTIL column to a factor, using this code:\n```{r make_factor}\nfertilizer &lt;- fertilizer |&gt;\n  mutate(FERTIL=as.factor(FERTIL))\n```\nMake a box plot of yield vs fertilizer, like the one on the previous tab.\nNow use the lm() function to create the anova model (There are several ways to do this in R - this is just one)\n\nfertil.model&lt;-lm(YIELD~FERTIL,data=fertilizer)\n\nNow inspect the model:\n\nanova(fertil.model)\n\nAnalysis of Variance Table\n\nResponse: YIELD\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nFERTIL     2 10.823  5.4114  5.7024 0.008594 **\nResiduals 27 25.622  0.9490                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSo we find that we can reject the null hypothesis. There is thus evidence that fertilizer does affect yield (ANOVA, df = 2,27, F=5.7, p&lt; 0.01)\nWhat this test has not done so far is show us where the differences lie. An ANOVA is a holistic test that tells you whether or not there is evidence for a difference between at least one pair of groups being compared. To identify which gruopd, if any, are differeny, we need to do so-called post-hoc tests.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ANOVA principles</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works.html#anova-example-2",
    "href": "ANOVA_how_it_works.html#anova-example-2",
    "title": "3  ANOVA principles",
    "section": "3.3 ANOVA example 2",
    "text": "3.3 ANOVA example 2\nAn experiment was performed to compare four melon varieties. It was designed so that each variety was grown in six plots, but two plots growing variety 3 were accidentally destroyed.\nWe wish to find out if there is evidence for a difference in yield between the varieties.\n\n3.3.1 Null hypothesis\nWhat is the null hypothesis of this study?\nThe data are in the melons.csv dataset.\nWrite code chunks to\n\n\n3.3.2 Load data and inspect the data\n\n\nRows: 22\nColumns: 2\n$ YIELDM  &lt;dbl&gt; 25.12, 17.25, 26.42, 16.08, 22.15, 15.92, 40.25, 35.25, 31.98,…\n$ VARIETY &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4,…\n\n\n\n\n3.3.3 Prepare the data\nEnsure that the VARIETY column is a factor\n\n\n3.3.4 Plot the data\nCreate a scatter plot of the yield.\n\n\n\n\n\n\n\n\n\n\n\n3.3.5 Summarise the data\nCreate a summary table that shows the mean yield for each variety.\n\n\n# A tibble: 4 × 3\n  VARIETY     N  Mean\n  &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt;\n1 1           6  20.5\n2 2           6  37.4\n3 3           4  20.5\n4 4           6  29.9\n\n\n\n\n3.3.6 1-way ANOVA\n\n3.3.6.1 Create the model\n\nmelons.model&lt;-lm(YIELDM~VARIETY,data=melons)\n\n\n\n3.3.6.2 Check the validity of the model\n\nautoplot(melons.model,smooth.colour=NA) + theme_cowplot()\n\n\n\n\n\n\n\n\nDO the data look as though they meet the criteria for an ANOVA? - the variance of the residuals is roughly constant across all groups and the qq-plot is fairly straight. We could confirm with a normality test if we like. For example, we could use a Shapiro-Wilk test.\nWhat is the null hypothesis of this test?\n\nshapiro.test(melons.model$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  melons.model$residuals\nW = 0.94567, p-value = 0.2586\n\n\nWhat do we conclude from this test?\n\n\n3.3.6.3 Inspect the model\n\nanova(melons.model)\n\nAnalysis of Variance Table\n\nResponse: YIELDM\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nVARIETY    3 1115.28  371.76  23.798 1.735e-06 ***\nResiduals 18  281.19   15.62                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhat conclusions would you draw from the output of this model and the table of mean yields for each variety?\n\n\n\n3.3.7 Confidence intervals of the means\nLet us find the 95% confidence intervals for each mean\nThese we calculate as\n\\[\n\\text{Mean}\\pm t_{\\text{crit}}\\text{SE}_{\\text{mean}}\n\\] For a 95% confidence interval and 18 degrees of freedom, \\(t_{\\text{crit}}\\) is 2.1, so we find that the intervals are:\n\n\n# A tibble: 4 × 4\n  VARIETY  Mean    LB    UB\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1        20.5  17.1  23.9\n2 2        37.4  34.0  40.8\n3 3        20.5  16.3  24.6\n4 4        29.9  26.5  33.3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ANOVA principles</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works.html#anova-example-2-solution",
    "href": "ANOVA_how_it_works.html#anova-example-2-solution",
    "title": "3  ANOVA principles",
    "section": "3.4 ANOVA example 2 solution",
    "text": "3.4 ANOVA example 2 solution\nAn experiment was performed to compare four melon varieties. It was designed so that each variety was grown in six plots, but two plots growing variety 3 were accidentally destroyed.\nWe wish to find out if there is evidence for a difference in yield between the varieties.\n\n3.4.1 Null hypothesis\nWhat is the null hypothesis of this study?\nThe data are in the melons.csv dataset.\nWrite code chunks to\n\n\n3.4.2 Load data and inspect the data\n\nfilepath&lt;-here(\"data\",\"melons.csv\")\nmelons&lt;-read_csv(filepath)\nglimpse(melons)\n\nRows: 22\nColumns: 2\n$ YIELDM  &lt;dbl&gt; 25.12, 17.25, 26.42, 16.08, 22.15, 15.92, 40.25, 35.25, 31.98,…\n$ VARIETY &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4,…\n\n\n\n\n3.4.3 Prepare the data\nEnsure that the VARIETY column is a factor\n\nmelons &lt;- melons |&gt;\n  mutate(VARIETY=as.factor(VARIETY))\n\n\n\n3.4.4 Plot the data\nCreate a scatter plot of the yield.\n\nmelons |&gt;\n  ggplot(aes(x=VARIETY,y=YIELDM)) +\n  geom_point() +\n  scale_y_continuous(limits=c(0,45),breaks=seq(0,45,5))+\n  labs(x = \"Melon variety\", y=\"Yield\") +\n  theme_cowplot()\n\n\n\n\n\n\n\n\n\n\n3.4.5 Summarise the data\nCreate a summary table that shows the mean yield for each variety.\n\nmelons |&gt;\n  group_by(VARIETY) |&gt;\n  summarise(\n    N=n(),\n    Mean=round(mean(YIELDM),2)\n  )\n\n# A tibble: 4 × 3\n  VARIETY     N  Mean\n  &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt;\n1 1           6  20.5\n2 2           6  37.4\n3 3           4  20.5\n4 4           6  29.9\n\n\n\n\n3.4.6 1-way ANOVA\n\n3.4.6.1 Create the model\n\nmelons.model&lt;-lm(YIELDM~VARIETY,data=melons)\n\n\n\n3.4.6.2 Check the validity of the model\n\nautoplot(melons.model,smooth.colour=NA) + theme_cowplot()\n\n\n\n\n\n\n\n\nThe data look as though they meet the criteria for an ANOVA - the variance of the residuals is roughly constant across all groups and the qq-plot is fairly straight. We could confirm with a normality test if we like. For example, we could use a Shapiro-Wilk test.\nThe null hypothesis of this test is that the residuals are drawn from a population that is normally distributed\n\nshapiro.test(melons.model$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  melons.model$residuals\nW = 0.94567, p-value = 0.2586\n\n\nSince p&gt;0.05 we conclude that there is no reason to reject the null hypothesis that the residuals are normally disributed.\n\n\n3.4.6.3 Inspect the model\n\nanova(melons.model)\n\nAnalysis of Variance Table\n\nResponse: YIELDM\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nVARIETY    3 1115.28  371.76  23.798 1.735e-06 ***\nResiduals 18  281.19   15.62                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(melons.model)\n\n\nCall:\nlm(formula = YIELDM ~ VARIETY, data = melons)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4233 -2.2781 -0.5933  2.6694  5.9300 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.4900     1.6136  12.699 2.02e-10 ***\nVARIETY2     16.9133     2.2819   7.412 7.14e-07 ***\nVARIETY3     -0.0275     2.5513  -0.011  0.99152    \nVARIETY4      9.4067     2.2819   4.122  0.00064 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.952 on 18 degrees of freedom\nMultiple R-squared:  0.7986,    Adjusted R-squared:  0.7651 \nF-statistic:  23.8 on 3 and 18 DF,  p-value: 1.735e-06\n\n\nWhat conclusions would you draw from the output of this model and the table of mean yields for each variety?\nWe see that the null hypothesis is rejected with a p-value of less than 0.001. We conclude that there are significant differences in the mean yield of melons across the varieties. We estimate that variety 2 has the highest mean yield and varieties 1 and 3 have the lowest mean yields.\nThe unexplained variance ie the error s for each group is 15.6 with 18 degrees of freedom. So the standard error for each group is \\(\\frac{s}{\\sqrt{n}}\\) where s=\\(\\sqrt{15.6}\\) = 3.95 divided by the number of elements in each group, giving us standard errors of 1.97 for variety 3, and 1.61 for the other varieties.\n\n\n\n3.4.7 Confidence intervals of the means\nLet us find the 95% confidence intervals for each mean\nThese we calculate as\n\\[\n\\text{Mean}\\pm t_{\\text{crit}}\\text{SE}_{\\text{mean}}\n\\] For a 95% confidence interval and 18 degrees of freedom, \\(t_{\\text{crit}}\\) is 2.1, so we find that the intervals are:\n\n\n# A tibble: 4 × 4\n  VARIETY  Mean    LB    UB\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1        20.5  17.1  23.9\n2 2        37.4  34.0  40.8\n3 3        20.5  16.3  24.6\n4 4        29.9  26.5  33.3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ANOVA principles</span>"
    ]
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "2  A recommended analysis workflow",
    "section": "",
    "text": "2.1 Are you working within your Project?\nThis is intended as a rough outline of the sequence of steps one commonly goes through when working on scripts:\nDetails will differ from script to script, but this sequence of steps is very common.\nBefore we even think of the script, we need to make sure that we are working within our Project. If we are not doing this, bad things will happen. If you are, the Project name will be at the top right of the RStudio window. If you are not, save the script you are working on, and go to File/Open Project and open your Project. If you haven’t even got a ‘Project’ or don’t know what that means then just make sure that everything you need for whatever you are working on is in one folder and then turn that folder into a Project. (So a ‘Project’ is just a regular folder that has been given superpowers.) You do that by going to File/New Project/Existing Directory. Then you navigate to your folder and click on Create Project. RStudio will then restart and you will see the name of your newly anointed Project folder at the top-right of the RStudio window. You know that a folder is a ‘Project’ because it will have a .Rproj file inside it.\nIf all this sounds complicated, don’t worry. It really isn’t. Just get someone to show you how to do it and you will be fine.\nNow, to the script itself:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#statement-of-the-questions-to-be-investigated",
    "href": "workflow.html#statement-of-the-questions-to-be-investigated",
    "title": "2  A recommended analysis workflow",
    "section": "2.2 Statement of the question(s) to be investigated",
    "text": "2.2 Statement of the question(s) to be investigated\nWithout thinking this through, you won’t know what your script is for…\nWhat is the analysis that will follow for? What question are you trying to answer? What hypotheses are you trying to test?\nSuppose we were trying to test the hypothesis that there is no difference between the petal widths of the setosa, versicolor and virginica species of iris. All we have to go on are the petal widths of the plants we happened to measure. From these measurements we want to make a statement about these three species in general.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#open-a-notebook",
    "href": "workflow.html#open-a-notebook",
    "title": "2  A recommended analysis workflow",
    "section": "2.3 Open a notebook",
    "text": "2.3 Open a notebook\nIn RStudio, go to File/New File/ R Notebook. Delete everything below the yaml section at the top. This strangeley named section is the bit between the two lines with three dashes in. For the most part, we will not need to worry about this section. We just should not delete it entirely. What is useful to do is to amend the title to something sensible, and to add author: \"your name\" and date: \"the date in any old format\" lines, so that your yaml will look something like this:\n---\ntitle: \"A typical workflow\"\nauthor: \"Who wrote this?\"\ndate: \"Today's date\"\noutput:\n  html_document:\n    df_print: paged\n---\nDelete everything beneath this yaml section. The big empty space that then leaves you with is where you write your code. Remember that in notebooks, the code goes in ‘chunks’ that are started and finished with by lines with three backticks. Any other text goes between the chunks and you can format this text using the simple rule of Markdown, available in the RStudio Help menu. Thus your script will end up looking something like this:\n---\ntitle: \"A typical workflow\"\nauthor: \"Who wrote this?\"\ndate: \"Today's date\"\noutput:\n  html_document:\n    df_print: paged\n---",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#set-up-chunk",
    "href": "workflow.html#set-up-chunk",
    "title": "2  A recommended analysis workflow",
    "section": "2.6 Set-up chunk",
    "text": "2.6 Set-up chunk\nJust put this chunk in at the top. Worry about it later. Or don’t worry about it at all, if you prefer. It is there to suppress warnings and messages from appearing in the rendered version of your script.\n```{r, include=FALSE}\n# makes the rendered version look prettier\nknitr::opts_chunk$set(message=FALSE,warning=FALSE)\n```",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#load-packages",
    "href": "workflow.html#load-packages",
    "title": "2  A recommended analysis workflow",
    "section": "2.7 Load Packages",
    "text": "2.7 Load Packages\nYou will nearly always want the first five packages, and often you will appreciate the sixth, janitor. Others, such as vegan will be useful from time to time, depending on what you are doing. If any of these lines throw an error, it is most likely because you have not yet installed that package. Do so in the console pane (not in this script!) using the function install.packages(\"name of package\"). Then run this whole chunk again.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggfortify)\nlibrary(readxl)\nlibrary(cowplot)\nlibrary(janitor)\nlibrary(vegan)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#load-data",
    "href": "workflow.html#load-data",
    "title": "2  A recommended analysis workflow",
    "section": "2.8 Load data",
    "text": "2.8 Load data\nThere are several ways to do this, so details will differ depending on what file type your data is saved in and where it is stored.\nHere are two examples. In each case code here presumes that the data is stored in a subfolder called ‘data’ within the Project folder, and we use the function here() from the here package. In my experience this dramatically simplifies the business of finding your data, wherever your script is. It makes it easier for you to share your script with others and be confident that what worked for you will work for them. It does require that you are working within your project.\n\n2.8.1 If from a csv file\nIf you have your data in a data subfolder within your project, this chunk will work. Just substitute the name of your data file\n\nfilepath&lt;-here(\"data\",\"iris.csv\")\niris&lt;-read_csv(filepath)\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n\n\n\n\n2.8.2 If from an Excel file\nYou will need to use read_excel() from the readxl package, and you have to specify the name of the worksheet that holds the data you want. You can, if you want, specify the exact range that is occupied by the data. However I suggest you avoid doing this unless it turns out that you need to do so. If your data is a nice, neat, rectangular block of rows and columns, you should find that you don’t need to specify the range.\n\nfilepath&lt;-here(\"data\",\"difference_data.xlsx\")\niris&lt;-read_excel(path = filepath,\n                 sheet = \"iris\", # delete the comma if you choose not to specify the range in the line below\n                 range= \"A1:F151\" # optional - try leaving it out first. Only include if necessary.\n                 ) |&gt;\n  clean_names()\nglimpse(iris)\n\nRows: 150\nColumns: 6\n$ id           &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ sepal_length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n\n\n\n\n2.8.3 If from a URL\nYou can load data into R directly from a URL if you are given one, and that is in fact how you will mainly access data to be used in this statistics text.\nhere, we load data from a file stored in a ‘’repo’ on my github account:\n\niris&lt;-read_csv(\"https://raw.githubusercontent.com/mbh038/r4nqy/refs/heads/main/data/iris.csv\") |&gt;\n  clean_names()\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ sepal_length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#clean-manipulate-the-data",
    "href": "workflow.html#clean-manipulate-the-data",
    "title": "2  A recommended analysis workflow",
    "section": "2.9 Clean / Manipulate the data",
    "text": "2.9 Clean / Manipulate the data\nOften we need to do some sort of data ‘wrangling’ to get the data into the form we want. For example we may wish to tidy it (this has a particular meaning when applied to data sets), to remove rows with missing values, to filter out rows from sites or time periods that we don’t want to include in our analysis, to create new columns and so on.\nFor example, lets create a new data frame for just the setosa species of iris:\n\nsetosa &lt;- iris |&gt;\n  filter(species == \"setosa\") # filter picks out rows according to criteria being satisfied in some column\nglimpse(setosa)\n\nRows: 50\nColumns: 5\n$ sepal_length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n\n\nor maybe we just want the column that contain numeric data and not the one containing the species identifiers, which is text:\n\niris_numeric &lt;- iris |&gt;\n  select(-species) # select() retains or leaves out particular columns. Here, we leave out the species column.\nglimpse(iris_numeric)\n\nRows: 150\nColumns: 4\n$ sepal_length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#summarise-the-data",
    "href": "workflow.html#summarise-the-data",
    "title": "2  A recommended analysis workflow",
    "section": "2.10 Summarise the data",
    "text": "2.10 Summarise the data\nHow big is the difference between the mean of this group over here and that group over there, and how big is that difference compared to the precision with which we know those means? We nearly always want to do this as a first way to get insight into whether we will or will not reject our hypothesis. For example, let’s find the mean petal widths of the three species, the standard errors of those means and save the results to a data frame called petal_summary\n\npetal_summary&lt;-iris |&gt;\n  group_by(species) |&gt;\n  summarise(mean.Pwidth = mean(petal_width),\n            se.Pwidth = sd(petal_width/sqrt(n())))\npetal_summary\n\n# A tibble: 3 × 3\n  species    mean.Pwidth se.Pwidth\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 setosa           0.246    0.0149\n2 versicolor       1.33     0.0280\n3 virginica        2.03     0.0388\n\n\nWe can look at this table and already get an idea as to whether the petal widths are the same or are different for the three species.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#plot-the-data",
    "href": "workflow.html#plot-the-data",
    "title": "2  A recommended analysis workflow",
    "section": "2.11 Plot the data",
    "text": "2.11 Plot the data\nThe next step is usually to plot the data in some way. We would typically use the ggplot2 package from tidyverse to do this.\n\n2.11.1 Bar plot with error bars\nWe could plot a bar plot with error bars, working from the summary data frame that we created:\n\npetal_summary |&gt;\n  ggplot(aes(x = species, y = mean.Pwidth)) +\n  geom_col(fill=\"#a6bddb\") + # this is the geom that gives us a bar plot when we have already done the calculations\n  geom_errorbar(aes(ymin = mean.Pwidth - se.Pwidth, ymax = mean.Pwidth + se.Pwidth), width = 0.15) +\n  labs( x = \"species\",\n        y = \"Mean petal width (mm)\",\n        caption = \"Error bars are ± one standard error of the mean\") + # important to say what these error bars denote\n  theme_cowplot()\n\n\n\n\n\n\n\n\nNote that we have given the bars a fill colour - we got this color from this site due to the cartographer Cynthia Brewer, who is behind the various incarnations of the Brewer package in R, which is great for getting colours that work well. We have used the same colour for each species since the x-axis labels already tell us which bar relates to which species. To use a different colour for each bar would imply there is some extra information encoded by colour. Since there is not, it serves no purpose to have different colours, and potentially confuses the reader. Remember always that a plot is intended to convey a message. Anything that detracts from that message should be avoided, however pretty you think it is.\nA couple of points could be made about this type of plot:\nFirst, what about those error bars? Three types of error bar are in common usage and there are arguments in favour and against the use of each of them:\n\nthe standard deviation tells us about the spread of values in a sample, and is an estimate of the spread of values in a population;\n\nthe standard error of the mean, as used here, is an estimate of the precision with which the sample means estimates the respective population means for each of the species.\nthe confidence interval, typically a 95% confidence interval, gives us the region within which we are (say) 95% confident that the true species mean petal width might plausibly lie.\n\nWhich type of error bar is best to use depends on what story you want to tell. Here, because we are interested in whether there is evidence of a difference in the mean petal width of different species, we have gone for the standard eror of the mean.\nRegardless of which error bar you use and why, you should always tell the reader which one you have gone for, as we have in the caption to the figure.\nA second point about this bar plot is that it doesn’t tell us very much, and indeed nothing that we didn’t already know. It only conveys the mean and standard error values for each species, which is information we already have, arguably more compactly and in more easily readable form, in the table we created. Further, it potentially obscures information that might come from knowing the distribution of the data.\nHere are three other plot types that do show the distribution of petal widths for each species and thus add extra information to what we already know from the summary table\n\n\n2.11.2 Box plot\n\niris |&gt;\n  ggplot(aes(x=species, y=petal_width)) + # what we want to plot\n  geom_boxplot(fill=\"#a6bddb\",notch=FALSE) + # what kind of plot we want\n  geom_jitter(width=0.1, colour = \"#f03b20\",alpha=0.5) +\n  labs (x = \"species\",\n        y = \"Petal Width (mm)\") +\n  theme_cowplot() # choose a theme to give the plot a 'look' that we like\n\n\n\n\n\n\n\n\nHere, we have added the points themselves on top of the box plot. When there are not too many data points, this can be useful. The ‘jitter’ adds some horizontal or vertical jitter, or both, so that the points do not lie on top of each other. In this case we see that the variability of petal widths is not the same for each species and that the data are roughly symmetrically distributed around the median values in each case. This information is useful in helping us determine which statistical test might be appropriate for these data.\n\n\n2.11.3 Violin plot\nA useful alternative to the box plot, especially when the data set is large, is the violin plot:\n\niris |&gt;\n  ggplot(aes(x = species, y = petal_width)) + # what we want to plot\n  geom_violin(fill=\"#a6bddb\",notch=TRUE) + # what kind of plot we want\n  #geom_jitter(width=0.1, colour = \"#f03b20\",alpha=0.5) +\n  labs (x = \"species\",\n        y = \"Petal Width (mm)\") +\n  theme_cowplot() # choose a theme to give the plot a 'look' that we like\n\n\n\n\n\n\n\n\nThe widths of the blobs (I am probably supposed to call them ‘violins’!) show us the distribution of the data - where they are widest is where the data are concentrated, while the height of the blobs shows us the range of variation of the data. The positions of the blobs tells us the mean petal widths of the different species and gives us an idea of the differences between them.\n\n\n2.11.4 Ridge plot\nA bit like a violin plot. This needs the package ggridges to be installed.\n\nlibrary(ggridges)\niris |&gt;\n  ggplot(aes(x = petal_width,y = species)) + # what we want to plot\n  geom_density_ridges(fill=\"#a6bddb\") + # what kind of plot we want\n  #geom_jitter(width=0.1, colour = \"#f03b20\",alpha=0.5) +\n  labs (x = \"Petal Width (mm)\",\n        y = \"species\") +\n  theme_cowplot() # choose a theme to give the plot a 'look' that we like\n\n\n\n\n\n\n\n\nHaving seen the summary and one of these plots of the data, would you be inclined to reject, or fail to reject, a null hypothesis that said that there was no difference between the petal widths of the three species?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#statistical-analysis",
    "href": "workflow.html#statistical-analysis",
    "title": "2  A recommended analysis workflow",
    "section": "2.12 Statistical analysis",
    "text": "2.12 Statistical analysis\nOnly now do we move on to the statistical analysis to try to answer our intial question(s). But by now, after the summary and plot(s), we may already have a pretty good idea what that answer will turn out to be.\nThe exact form of the analysis could take many forms. In a typical ecology project you might carry out several types of analysis, each one complementing the other. Here, an appropriate analysis might be to use the linear model in the form of a one-way ANOVA, since we have one factor (species) with three levels (setosa, versicolor and virginica) and an output variable that is numeric and likely to be normally distributed. We can use the lm() function for this.\n\n2.12.1 Create the model object\n\npw.model &lt;-lm (petal_width ~ species, data = iris)\n\n\n\n2.12.2 Check the validity of the model\nWe won’t go into this here, but an important step is to check that the data satisfy the often finicky requirements of whatever statistical test we have decided to use. The autoplot() function form the ggfortify package is great for doing this graphically.\n\nautoplot( pw.model) + theme_cowplot()\n\n\n\n\n\n\n\n\nHere we note in particular that although the spread of data within each level is not roughly the same (top left figure)), the QQ plot is pretty straight (top-right figure). This means that the data are approximately normally distributed around their respective means. Taken together, this means that these data satisfy reasonably well the requirements of a linear model, so the output of that model should be reliable.\n\n\n2.12.3 The overall picture\nTypically, statistical tests are testing the likelihood of the data being as they are, or more ‘extreme’ than they are, if the null hypothesis were true. Thus, the null hypothesis is central to statistical testing.\nThe null hypothesis is typically that the ‘nothing going on’, ‘no difference’ or ’ no association’ scenario is true. In this case, it would be that there is no difference between the petal widths of the the three species of iris being considered here.\nTypically too, a test will in the end spit out a p-value which is the probability that we would have got the data we got, or more extreme data, if the null hypothesis were true. Being a probability, it will always be a value between 0 and 1, where 0 means impossible, and 1 means certain. The closer the p-value is to zero, the less likely it is we would have got our data if the null hypothesis were true. At some point, if the p-value is small enough, we will decide that the probability of getting the data we actually got if the null hypothesis were true is so small that we reject the null hypothesis. Typically, the threshold beyond which we do this is when p = 0.05, but we could choose other thresholds. (Sounds arbitrary - yes, it is, but the choice of 0.05 is a compromise value that makes the risk of making each of two types of error - rejecting the null when we should not, and failing to reject it when we should, both acceptably small. This is a big topic which we won’t explore further here.)\nIn the end, whatever other information we get from it, the outcome of a statisical test is typically that we either reject the null hypothesis or we fail to reject it. If we reject it then we are claiming to have detected evidence for an ‘effect’ and we go on to determine how big that effect is and whether it is scientifically interesting. If we fail to reject the null, that does not necessarily mean that there is no ‘effect’ (difference, trend, association etc). That might be the case, but it might also just mean that we didn’t find evidence for one from our data.\nIt is all a bit like in a law court where the ‘null hypothesis’ is that the defendant is innocent, and at the end of the proceedings this null is either rejected (Guilty!) because the evidence is such as to make it untenable to hold onto the null hypothesis, or not rejected, because the evidence is not strong enough to convict, in which case the defendant walks free - but is not declared innocent. Formally, the court has simply found insufficient evidence to convict. In the latter case, the court would have failed to reject the null hypothesis. Crucially, it would not have declared that the defendant was innocent. In the same way, in a scientific study, we either reject or fail to reject a null hypothesis. We never ever accept the null hypothesis as true.\nActually, many researchers are unhappy wih this so-called ‘frequentist’ narrative and have sought to use an alternative ‘Bayesian’ approach to testing hypotheses. In this approach we can accept hypotheses and we can bring in prior knowledge. This is an interesting topic, but a very big one so we will not pursue it further here.\nWith all that behind us, we are in a better place to understand what the output of the test is telling us.\nFor the 1-way ANOVA, as with other examples of the linear model, this output comes in two stages:\n\n\n2.12.4 Overall picture\nIs there evidence for a difference between at least two of the mean values?\nTo see if there is evidence for this, an ANOVA test calculate the ratio between the dfference betweeN the groups compared to the differences within the groups. it calls this ratio \\(F\\). The bigger \\(F\\) is, the more likely we are to reject the null hypothesis that there is no difference between he groups.\n\nanova(pw.model)\n\nAnalysis of Variance Table\n\nResponse: petal_width\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nspecies     2 80.413  40.207  960.01 &lt; 2.2e-16 ***\nResiduals 147  6.157   0.042                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe F value is huge. The null hypothesis of this test, as with many tests, is that there is no difference between the petal widths of the three populations from which these samples have been drawn. In that case, the F value would be one. The p-value is telling us how likely it is that we would get an F value as big or bigger than the one we got for our samples if the null hypothesis were true. Since the p-value is effectively zero here, we reject the null hypothesis: we have evidence from our data that there is a significant varation of petal width between species.\nThe degrees of freedom Df tells us the number of independent pieces of information that were used to calculate the result. Let’s not dwell on this here, but there are two that we have to report in this case: the number of levels minus one ie 3-1 = 2, and the number of individual data points in each level minus one, times the number of levels ie (50-1) x 3 = 147.\n\n\n2.12.5 Effect size\nNow that we have established that at least two species of iris have differing petal widths, we go onto investigate where the differences lie, and how big they are. This is important: effect sizes matter. It is one thing to establish that a difference is statistically significant (and typically even the tiniest difference can show up as significant in a study if the sample size is big enough), it is quite another to establish whether the difference is big enough to be scientifically interesting.\n\nsummary(pw.model)\n\n\nCall:\nlm(formula = petal_width ~ species, data = iris)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-0.626 -0.126 -0.026  0.154  0.474 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.24600    0.02894    8.50 1.96e-14 ***\nspeciesversicolor  1.08000    0.04093   26.39  &lt; 2e-16 ***\nspeciesvirginica   1.78000    0.04093   43.49  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2047 on 147 degrees of freedom\nMultiple R-squared:  0.9289,    Adjusted R-squared:  0.9279 \nF-statistic:   960 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nThe output here is typical of that from a 1-way ANOVA analysis in R. Each line refers to one of the three levels of the factor being investigated, which is petal width in this case. By default, those levels are arranged alphabetically, so in this case the order is setosa, versicolor then virginica. The first row is always labelled (Intercept), so here that row is referring to setosa. This level is used as the ‘control’ or reference level- the one with which the others are compared. If we are happy to have setosa as that control then we can just carry on, but if we are not, then we have to tell R which level we want to play that role. We’ll go through how to do that later on.\nIn the Estimate column the value 0.246 cm in the first row refers to the actual mean petal width of the setosa plants in the sample. If we go back to the summary table we created earlier on, or look at one of the plots we created, we see that that is the case.\nFor all other rows, the value in the Estimate column is not referring to the absolute mean petal width but to the difference between the mean petal width for that species and the mean petal width of the control species. So we see that the mean petal width of the versicolor in our sample is 1.08 cm greater than that of setosa and so is equal to .246 + 1.08 = 1.326 cm, while that of the virginica is 1.78 cm greater and so is equal to 2.026 cm. Check from the table of mean values we created and the plots that this is correct.\nHere though, we are not interested in absolute values so much as we are in differences, which is why that is what the summary table here gives us. Look again at the differences between the mean petal widths for versicolor and virginica and that for setosa and compare them with the standard erros of those differences, which are given in the second column of the table. These standard errors are much smaller than the differences, meaning that we can have confidence that the differences are statistically significant.\nThis is borne out by the p-value in the right hand column of the table. The null hypothesis of this table is that there is no difference in petal width between populations of the different species from which these samples have been drawn.\nLastly, the adjusted \\(R^2\\) tells us the proportion of variation of petal width that is accounted for by taking note of the species. Here, the value is 0.93, which tells us hat little else besides species determines the relative petal widths. Ther are no other variables that we need to have taken into account.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#report-in-plain-english",
    "href": "workflow.html#report-in-plain-english",
    "title": "2  A recommended analysis workflow",
    "section": "2.13 Report in plain English",
    "text": "2.13 Report in plain English\nYou would say something like\nWe find evidence that petal widths are not the same acros thhree species of iris, with virginica &gt; versicolor &gt; setosa. (ANOVA, df = 2, p &lt; 0.001)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "t-test-2-sample.html",
    "href": "t-test-2-sample.html",
    "title": "3  Two-sample t-test",
    "section": "",
    "text": "3.1 Preliminaries\nIn this exercise we find out how to use R to run a two-sample t-test, to determine whether there is evidence to reject the hypothesis that two samples are drawn from the same population.\nTwo-sample t-tests are used when we have two independent sets of numerical data, and we want to know whether the data provide evidence that the sets are drawn from different populations.\nThe exercise is based on Chapter 5: Beckerman, Childs and Petchey: Getting Started with R.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Two-sample *t*-test</span>"
    ]
  },
  {
    "objectID": "t-test-2-sample.html#motivation-and-example",
    "href": "t-test-2-sample.html#motivation-and-example",
    "title": "3  Two-sample t-test",
    "section": "3.2 Motivation and example",
    "text": "3.2 Motivation and example\nIn our example we will consider concentrations of airborne ozone (O3) at ground level, as measured in gardens around a city. This is of interest because ozone levels can affect how well crops grow, and can impact on human health.\nWe have measurements of airborne ozone levels in ppb taken at two samples of locations in the city: some randomly selected from among gardens in the eastern residential sector and some randomly selected from among gardens in the western sector, close to a zone of heavy industry.\nOur question is:\nIs there evidence for a difference between airborne ozone concentrations in the east and the west of the city?\nFrom which our null hypothesis is:\nThere is no difference between airborne ozone concentrations in the east and the west of the city.\nand our alternate, two-sided hypothesis is:\nThere is a difference between airborne ozone concentrations in the east and the west of the city.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Two-sample *t*-test</span>"
    ]
  },
  {
    "objectID": "t-test-2-sample.html#the-two-sample-t-test",
    "href": "t-test-2-sample.html#the-two-sample-t-test",
    "title": "3  Two-sample t-test",
    "section": "3.3 The two-sample t-test",
    "text": "3.3 The two-sample t-test\nThis can be used when we have two independent sets of numerical data, and our question is whether the data provide evidence that the sets are drawn from different populations.\n\n3.3.1 Pros of the t-test\n\nIt can be used when the data set is small.\nIt can still be used when the data set is large. So…if in doubt, just use the t-test, (Kind of, the data do need to fulfil some criteria, but being few in number is fine. See below).\n\n\n\n3.3.2 Cons of the t-test\n\nIt assumes that the data are drawn from a normally distributed population. There are various ways to test if it is plausible tha this is the case, and you should try at least one of them, but with small samples, just where the t-test is most useful, it can be difficult to tell. In the end we can also appeal to reason: is there good reason to suppose that the data would or would not be normally distributed?\nWhen comparing the means of two samples both samples should have approximately the same variance, which is a measure of the spread of the data. You need to check that this is at least approximately the case, or have reason to suppose that it should be. (Note: in an actual t-test, it is possible to ignore this requirement - see below).\nWhen we have more than two samples and we use the t-test to look for a difference between any two of them, it becomes increasingly likely, the more pairs of samples we compare, that we will decide that we have found a difference because we got a p-value that was less than some pre-determined threshold (which could be anything, but is most often chosen to be 0.05) even if in reality there is none. This is the problem of high false positive rates arising from multiple pairwise testing and is where ANOVA comes in. t-tests are only used to detect evidence for a difference between two groups, not more. ANOVAs (or their non-parametric equivalent) are used when we are looking for differences between more than two groups.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Two-sample *t*-test</span>"
    ]
  },
  {
    "objectID": "t-test-2-sample.html#the-workflow",
    "href": "t-test-2-sample.html#the-workflow",
    "title": "3  Two-sample t-test",
    "section": "3.4 The workflow",
    "text": "3.4 The workflow\n\n3.4.1 Open your project\nOpen your RStuff (or whatever you have called it) project using File/Open Project, navigating to the project folder, then clicking on the ... .Rproj file you will find there.\nIf your Rstuff folder is not already a Project, then make it one using File/New Project/Existing Directory - then navigate to your Rstuff folder.\n\n\n3.4.2 Create a new script\nCreate a nw notebook script using File/New File/R Notebook Delete everything from below the yaml section at the top. This is the bit between the pair of lines with three dashes. In the yaml, amend the title and add lines author: \"&lt;your name&gt;\" and date: \"&lt;the date&gt;\". Inside the quotes, add your name and the date.\nNow add code chunks to carry out the steps listed below. In between the chunks, add as much explanatory text as you want so that next time you come back, you understand what each code chunk is doing. You can format this text using the simple markdown rules to be found in Help/markdown Quick Reference\n\n\n3.4.3 Load packages\nWe typically include a chunk at or near the top of a script that loads any packages we are going to use. If we load all of them in this one chunk it is easy to see at a glance which ones have been loaded.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/michaelhunt/git_repos/r4nqy\n\nlibrary(mbhR)\n# if that last line doesn't work, uncomment the next line by deleting the # and run it to install the mbhR package. \n# remotes::install_github(“mbh038/mbhR”)\n\n\n\n3.4.4 Read in and inspect the data\n\n# there should be an 'ozone.csv' file in your data folder\n# if not, you should be able to get it from the data folder on Teams or Moodle\nfilepath&lt;-here(\"data\",\"ozone.csv\")\nozone&lt;-read_csv(filepath)\n\nRows: 20 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): garden.id, garden.location\ndbl (1): ozone\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#glimpse(ozone)\n\nWhat kind of data have we got?\nYou might also wish to inspect the data using summary(). If so, include a code chunk to do this.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Two-sample *t*-test</span>"
    ]
  },
  {
    "objectID": "t-test-2-sample.html#step-one-summarise-the-data",
    "href": "t-test-2-sample.html#step-one-summarise-the-data",
    "title": "3  Two-sample t-test",
    "section": "3.5 Step One: Summarise the data",
    "text": "3.5 Step One: Summarise the data\nWith numerical data spread across more than one level of a categorical variable, we often want summary information such as mean values and standard errors of the mean for each level. We can do this by using the group_by() and then summarise() combination. This first group the data however you want to, then calculates whatever summary information you have requested for each group.\nHere we will calculate the number of replicates, the mean and the standard error of the mean for both levels of garden.location ie east and west, then store the result in a data frame called ozone.summary\n\nozone.summary&lt;-ozone |&gt;\ngroup_by(garden.location) |&gt;\nsummarise(n = n(),\n          mean.ozone = mean(ozone),\n          se.ozone = sd(ozone)/sqrt(n()))\nozone.summary\n\n# A tibble: 2 × 4\n  garden.location     n mean.ozone se.ozone\n  &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 East               10       77.3     2.49\n2 West               10       61.3     2.87\n\n\nFrom these data, does it look as though there is evidence for a difference between ozone levels in the East and the West? Clearly, the ten gardens in the east had a higher mean ozone concentration than the ten in the west. But is this a fluke? How precisely do we think these sample means reflect the truth about the east and the west of the city? That is what the standard error column tells us. You can think of the standard error as being an estimate of how far from the true ozone concentrations for the whole of the east and the whole of the west our sample means, drawn from just ten locations in each part of the city, are likely to be.\nBottom line: the difference between the sample means is about six times the size of the standard errors of each. It really does look as thought east of the city has a higher ozone concentration than the west.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Two-sample *t*-test</span>"
    ]
  },
  {
    "objectID": "t-test-2-sample.html#step-two-plot-the-data",
    "href": "t-test-2-sample.html#step-two-plot-the-data",
    "title": "3  Two-sample t-test",
    "section": "3.6 Step Two: Plot the data",
    "text": "3.6 Step Two: Plot the data\nRemember, before we do any statistical analysis, it is almost always a good idea to plot the data in some way. We can often get a very good idea as to the answer to our research question just from the plots we do.\nHere, we will\n\nuse ggplot() to plot a histogram of ozone levels\nuse the facet_wrap() function to give two copies of the histogram, one for east and one for west, and to stack the histograms one above the other.\nmake the histogram bins 10 ppm wide.\n\n\nozone |&gt;\n  ggplot(aes(x=ozone)) +\n  geom_histogram(binwidth=10,fill=\"darkred\")+\n  facet_wrap(~garden.location,ncol=1) +\n  theme_classic()\n\n\n\n\n\n\n\n\nInstead of histograms, we could have drawn box plots:\n\nozone |&gt;\n  ggplot(aes(x=garden.location,y=ozone))+\n  geom_boxplot()+\n  labs(x=\"Garden Location\",\n       y=\"Ozone concentration (ppb)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nor as a dot plot with standard errors of the mean included:\n\n# for this chart we will use the summary table that we created above.\n\nozone.summary |&gt; \n  ggplot(aes(x=garden.location,y=mean.ozone))+\n  geom_point(size=3) +\n  geom_errorbar(aes(ymin=mean.ozone-se.ozone,ymax=mean.ozone+se.ozone),width=0.1)+\n  ylim(0,100) + # try leaving this line out. What happens? Which is better?\n  labs(x=\"Garden Location\",\n       y=\"Ozone concentration (ppb)\",\n       caption=\"The data points show mean values, the error bars show plus or minus one standard error of the mean \") +\n  theme_classic()\n\n\n\n\n\n\n\n\nDo the data look as though they support the null hypothesis or not?\nIn addition, do the data look as though each group is drawn from a normally distributed population? One of the types of graphs gives you no indication of that while the other two do. Which is the odd one out? Even when looking at the other two figures, when there are so few data it’s kind of hard to tell, no?\nLet’s now do some stats.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Two-sample *t*-test</span>"
    ]
  },
  {
    "objectID": "t-test-2-sample.html#step-three-carry-out-statistical-analysis",
    "href": "t-test-2-sample.html#step-three-carry-out-statistical-analysis",
    "title": "3  Two-sample t-test",
    "section": "3.7 Step Three: Carry out statistical analysis",
    "text": "3.7 Step Three: Carry out statistical analysis\n\n3.7.1 Are the data normally distributed?\nWe can go about establishing this in three ways: using an analytical test of normality, using a graphical method and by thinking about what kind of data we have. Let’s consider these in turn.\n\n\n3.7.2 Normality test - analytical method\nThere are several analytical tests one can run on a set of data to determine if it is plausible that it has been drawn from a normally distributed population. One is the Shapiro-Wilk test.\nFor more information on the Shapiro-Wilk test, type ?shapiro.test into the console window. For kicks, try it out on the examples that appear in the help window (which is the bottom right pane, Help tab). One example is testing a sample of data that explicitly is drawn from a normal distribution, the other tests a sample of data that definitely is not. What p-value do you get in each case? How closely do the histograms of each sample resemble a normal distribution?\n\n#first we create a data frame containing the two example data sets\nexample1&lt;-rnorm(100, mean = 5, sd = 3) # first example from the help pane\nexample2&lt;-runif(100, min = 2, max = 4) # second example from the help pane\n\ndf&lt;-tibble(data=c(example1,example2), distribution=c(rep(\"normal\",100),rep(\"not at all normal\",100)))\n\n# then we plot a histogram of each data set\nggplot(df,aes(x=data)) +\n  geom_histogram(bins=10,fill=\"cornflowerblue\") +\n  facet_wrap(~distribution) +\n  theme_classic()\n\n\n\n\n\n\n\n# and finally we run a Shapiro-Wilk normality test on each data set\nshapiro.test(example1) # 100 samples drawn from a normally distributed population\n\n\n    Shapiro-Wilk normality test\n\ndata:  example1\nW = 0.99024, p-value = 0.6842\n\nshapiro.test(example2) # 100 samples drawn from a uniformly (ie NOT normally) distributed population\n\n\n    Shapiro-Wilk normality test\n\ndata:  example2\nW = 0.96263, p-value = 0.006219\n\n\nFor the examples above, we see that Shapiro-Wilk test gave a hig p-value for the data that we knew were drawn from a normal distribution, an a very low p-value for the data that we knew were not.\nThe Shapiro-Wilk test tests your data against the null hypothesis that it is drawn from a normally distributed population. It gives a p-value. If the p-value is less than 0.05 then we reject the null hypothesis and cannot suppose our data is normally distributed. In that case we would have to ditch the t-test for a difference, and choose another difference test in its place that could cope with data that was not normally distributed.\nWhy don’t we do that in the first place, I hear you ask? Why bother with this finicky t-test that requires that we go through the faff of testing the data for normality before we can use it? The answer is that it is more powerful than other, so-called non-parametric tests that can cope with non-normal data. It is more likely than they are to spot a difference if there really is a difference. So if we can use it, that is what we would rather do.\nSo, onwards, let’s do the Shapiro-Wilk test on our data\nWe want to test each garden group for normality, so we group the data by location as before and and then summarise, this time asking for the p-value returned by the Shapiro-Wilk test of normality.\n\nozone |&gt;\n  group_by(garden.location) |&gt;\n  summarise('Shapiro-Wilk p-value'=shapiro.test(ozone)$p.value)\n\n# A tibble: 2 × 2\n  garden.location `Shapiro-Wilk p-value`\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 East                            0.0953\n2 West                            0.599 \n\n\nFor both groups the p-value is more than 0.05, so at the 5% significance level we cannot reject the null hypothesis that the data are normally distributed, so we can go on and use the t-test. Yay!\n\n\n3.7.3 Graphical methods - the quantile-quantile or QQ plot.\nConfession: I don’t normally bother with numerical tests for normality such as Shapiro-Wilk. I usually use a graphical method instead.\nFor an overview of how normally distributed and non-normally distributed data looks when plotted in histograms, box plots and quantile-quntile plots, see this review\nWe have already seen two ways of plotting the data that might help suggest whether it is plausible that the data are drawn from normally distributed populations. Histograms and box plots both indicate how data is distributed, and for normally distributed data both would be symmetrical. Well, they would be, more or less, if the data set was large enough but for small data sets it can be quite hard to tell from either type of plot whether the data are drawn from a normally distributed population.\nA better type of plot for making this judgement call is the quantile-quantile or ‘QQ’ plot which basically compares the distribution of your data to that of a normal distribution. If your data are approximately normally distributed then a qq plot will give a straight(-ish) line. Even with small data sets, this is usually easy to spot.\n\nozone |&gt;\n  ggplot(aes(sample=ozone)) +\n  stat_qq(colour=\"blue\") +\n  stat_qq_line() +\n  facet_wrap(~garden.location) +\n  theme_classic()\n\n\n\n\n\n\n\n\nNothing outrageously non-linear there, so that also suggests we can safely use the t-test.\n\n\n3.7.4 The ‘thinking about the data’ normality test\nAs you might have guessed, this isn’t a test as such, but a suggestion that you think about what kind of data you have: is it likely to be normally distributed within its subgroups or not? If the data are numerical values of some physical quantity that is the result of many independent processes, and if the data are not bounded on either side (say by 0 and 100 as for exam scores) then it is quite likely that that they are. If they are count data, or ordinal data, then it is quite likely that they are not.\nThis way of thinking may be all you can do when data sets are very small and any of the more robust tests for normality presented here leave you not much the wiser.\n\n\n3.7.5 Now for the actual two-sample t-test\nSo, it looks as though it is plausible that the data are drawn from normal distributions. That means we can go on to use a parametric test such as a t-test and have confidence in its output.\nWe can use the t.test() function for this. This needs to be given a formula and a data set as arguments. Look up t.test() in R’s help documentation, and see if you can get the t-test to tell you whether there is a significant difference between ozone levels in the east and in the west of the city.\n\nt.test(ozone~garden.location,data=ozone)\n\n\n    Welch Two Sample t-test\n\ndata:  ozone by garden.location\nt = 4.2363, df = 17.656, p-value = 0.0005159\nalternative hypothesis: true difference in means between group East and group West is not equal to 0\n95 percent confidence interval:\n  8.094171 24.065829\nsample estimates:\nmean in group East mean in group West \n             77.34              61.26 \n\n\nNote the ~ tilda symbol. This means ‘is a function of’. So this line means: do a t-test to see if there is a significant difference between the ozone levels in the two garden locations.\n\n\n3.7.6 Interpret the output of the t-test.\nStudy the output of the t-test.\n\nWhat kind of test was carried out?\nWhat data was used for the test?\nWhat is the test statistic of the data?\nHow many degrees of freedom were there? Does the number make sense? In a t-test the ’degrees of freedom is one less than the number of data points.\nWhat is the p-value?\nWhat does the p value mean?\nWhat is the confidence interval for the difference between ozone levels in east and west? Does it encompass zero?\nIs there sufficient evidence to reject the null hypothesis?\nWhat does the word ‘Welch’ tell you - look it up in the help for t.test().",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Two-sample *t*-test</span>"
    ]
  },
  {
    "objectID": "workflow.html#first-header",
    "href": "workflow.html#first-header",
    "title": "2  A recommended analysis workflow",
    "section": "2.4 First header",
    "text": "2.4 First header\nAny text we want to add. Note that a code chunk starts with {r} and ends with\n\nlibrary(tidyverse) # some actual R code",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#second-header",
    "href": "workflow.html#second-header",
    "title": "2  A recommended analysis workflow",
    "section": "2.5 Second header",
    "text": "2.5 Second header\nAny text we want to add to explain what this next chunk does\n```{r, include=FALSE}\nlibrary(tidyverse) # some actual R code\n```",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "t-test-2-sample.html#pros-and-cons-of-the-t-test",
    "href": "t-test-2-sample.html#pros-and-cons-of-the-t-test",
    "title": "3  Two-sample t-test",
    "section": "3.3 Pros and cons of the t-test",
    "text": "3.3 Pros and cons of the t-test\n\n3.3.1 Pros\n\nIt can be used when the data set is small.\nIt can still be used when the data set is large. So…if in doubt, just use the t-test, (Kind of, the data do need to fulfil some criteria, but being few in number is fine. See below).\n\n\n\n3.3.2 Cons\n\nIt assumes that the data are drawn from a normally distributed population. There are various ways to test if it is plausible tha this is the case, and you should try at least one of them, but with small samples, just where the t-test is most useful, it can be difficult to tell. In the end we can also appeal to reason: is there good reason to suppose that the data would or would not be normally distributed?\nWhen comparing the means of two samples both samples should have approximately the same variance, which is a measure of the spread of the data. You need to check that this is at least approximately the case, or have reason to suppose that it should be. (Note: in an actual t-test, it is possible to ignore this requirement - see below).\nWhen we have more than two samples and we use the t-test to look for a difference between any two of them, it becomes increasingly likely, the more pairs of samples we compare, that we will decide that we have found a difference because we got a p-value that was less than some pre-determined threshold (which could be anything, but is most often chosen to be 0.05) even if in reality there is none. This is the problem of high false positive rates arising from multiple pairwise testing and is where ANOVA comes in. t-tests are only used to detect evidence for a difference between two groups, not more. ANOVAs (or their non-parametric equivalent) are used when we are looking for differences between more than two groups.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Two-sample *t*-test</span>"
    ]
  },
  {
    "objectID": "t-test-2-sample.html#step-three-check-that-the-data-meet-the-criteria-required-for-a-t-test",
    "href": "t-test-2-sample.html#step-three-check-that-the-data-meet-the-criteria-required-for-a-t-test",
    "title": "3  Two-sample t-test",
    "section": "3.7 Step Three: Check that the data meet the criteria required for a t-test",
    "text": "3.7 Step Three: Check that the data meet the criteria required for a t-test\n\n3.7.1 Are the data normally distributed?\nWe can go about establishing this in three ways: using an analytical test of normality, using a graphical method and by thinking about what kind of data we have. Let’s consider these in turn.\n\n\n3.7.2 Normality test - analytical method\nThere are several analytical tests one can run on a set of data to determine if it is plausible that it has been drawn from a normally distributed population. One is the Shapiro-Wilk test.\nFor more information on the Shapiro-Wilk test, type ?shapiro.test into the console window. For kicks, try it out on the examples that appear in the help window (which is the bottom right pane, Help tab). One example is testing a sample of data that explicitly is drawn from a normal distribution, the other tests a sample of data that definitely is not. What p-value do you get in each case? How closely do the histograms of each sample resemble a normal distribution?\n\n#first we create a data frame containing the two example data sets\nexample1&lt;-rnorm(100, mean = 5, sd = 3) # first example from the help pane\nexample2&lt;-runif(100, min = 2, max = 4) # second example from the help pane\n\ndf&lt;-tibble(data=c(example1,example2), distribution=c(rep(\"normal\",100),rep(\"not at all normal\",100)))\n\n# then we plot a histogram of each data set\nggplot(df,aes(x=data)) +\n  geom_histogram(bins=10,fill=\"cornflowerblue\") +\n  facet_wrap(~distribution) +\n  theme_classic()\n\n\n\n\n\n\n\n# and finally we run a Shapiro-Wilk normality test on each data set\nshapiro.test(example1) # 100 samples drawn from a normally distributed population\n\n\n    Shapiro-Wilk normality test\n\ndata:  example1\nW = 0.98844, p-value = 0.5414\n\nshapiro.test(example2) # 100 samples drawn from a uniformly (ie NOT normally) distributed population\n\n\n    Shapiro-Wilk normality test\n\ndata:  example2\nW = 0.92306, p-value = 2.054e-05\n\n\nFor the examples above, we see that Shapiro-Wilk test gave a hig p-value for the data that we knew were drawn from a normal distribution, an a very low p-value for the data that we knew were not.\nThe Shapiro-Wilk test tests your data against the null hypothesis that it is drawn from a normally distributed population. It gives a p-value. If the p-value is less than 0.05 then we reject the null hypothesis and cannot suppose our data is normally distributed. In that case we would have to ditch the t-test for a difference, and choose another difference test in its place that could cope with data that was not normally distributed.\nWhy don’t we do that in the first place, I hear you ask? Why bother with this finicky t-test that requires that we go through the faff of testing the data for normality before we can use it? The answer is that it is more powerful than other, so-called non-parametric tests that can cope with non-normal data. It is more likely than they are to spot a difference if there really is a difference. So if we can use it, that is what we would rather do.\nSo, onwards, let’s do the Shapiro-Wilk test on our data\nWe want to test each garden group for normality, so we group the data by location as before and and then summarise, this time asking for the p-value returned by the Shapiro-Wilk test of normality.\n\nozone |&gt;\n  group_by(garden.location) |&gt;\n  summarise('Shapiro-Wilk p-value'=shapiro.test(ozone)$p.value)\n\n# A tibble: 2 × 2\n  garden.location `Shapiro-Wilk p-value`\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 East                            0.0953\n2 West                            0.599 \n\n\nFor both groups the p-value is more than 0.05, so at the 5% significance level we cannot reject the null hypothesis that the data are normally distributed, so we can go on and use the t-test. Yay!\n\n\n3.7.3 Graphical methods - the quantile-quantile or QQ plot.\nConfession: I don’t normally bother with numerical tests for normality such as Shapiro-Wilk. I usually use a graphical method instead.\nFor an overview of how normally distributed and non-normally distributed data looks when plotted in histograms, box plots and quantile-quntile plots, see this review\nWe have already seen two ways of plotting the data that might help suggest whether it is plausible that the data are drawn from normally distributed populations. Histograms and box plots both indicate how data is distributed, and for normally distributed data both would be symmetrical. Well, they would be, more or less, if the data set was large enough but for small data sets it can be quite hard to tell from either type of plot whether the data are drawn from a normally distributed population.\nA better type of plot for making this call is the quantile-quantile or ‘QQ’ plot which basically compares the distribution of your data to that of a normal distribution. If your data are approximately normally distributed then a qq plot will give a straight(-ish) line. Even with small data sets, this is usually easy to spot.\n\nozone |&gt;\n  ggplot(aes(sample=ozone)) +\n  stat_qq(colour=\"blue\") +\n  stat_qq_line() +\n  facet_wrap(~garden.location) +\n  theme_classic()\n\n\n\n\n\n\n\n\nNothing outrageously non-linear there, so that also suggests we can safely use the t-test.\n\n\n3.7.4 The ‘thinking about the data’ normality test\nAs you might have guessed, this isn’t a test as such, but a suggestion that you think about what kind of data you have: is it likely to be normally distributed within its subgroups or not? If the data are numerical values of some physical quantity that is the result of many independent processes, and if the data are not bounded on either side (say by 0 and 100 as for exam scores) then it is quite likely that that they are. If they are count data, or ordinal data, then it is quite likely that they are not.\nThis way of thinking may be all you can do when data sets are very small and any of the more robust tests for normality presented here leave you not much the wiser.\n\n\n3.7.5 Now for the actual two-sample t-test\nSo, it looks as though it is plausible that the data are drawn from normal distributions. That means we can go on to use a parametric test such as a t-test and have confidence in its output.\nWe can use the t.test() function for this. This needs to be given a formula and a data set as arguments. Look up t.test() in R’s help documentation, and see if you can get the t-test to tell you whether there is a significant difference between ozone levels in the east and in the west of the city.\n\nt.test(ozone~garden.location,data=ozone)\n\n\n    Welch Two Sample t-test\n\ndata:  ozone by garden.location\nt = 4.2363, df = 17.656, p-value = 0.0005159\nalternative hypothesis: true difference in means between group East and group West is not equal to 0\n95 percent confidence interval:\n  8.094171 24.065829\nsample estimates:\nmean in group East mean in group West \n             77.34              61.26 \n\n\nNote the ~ tilda symbol. This means ‘is a function of’. So this line means: do a t-test to see if there is a significant difference between the ozone levels in the two garden locations.\n\n\n3.7.6 Interpret the output of the t-test.\nStudy the output of the t-test.\n\nWhat kind of test was carried out?\nWhat data was used for the test?\nWhat is the test statistic of the data?\nHow many degrees of freedom were there? Does the number make sense? In a t-test the ’degrees of freedom is one less than the number of data points.\nWhat is the p-value?\nWhat does the p value mean?\nWhat is the confidence interval for the difference between ozone levels in east and west? Does it encompass zero?\nIs there sufficient evidence to reject the null hypothesis?\nWhat does the word ‘Welch’ tell you - look it up in the help for t.test().",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Two-sample *t*-test</span>"
    ]
  },
  {
    "objectID": "test_for_difference_non_parametric.html",
    "href": "test_for_difference_non_parametric.html",
    "title": "4  Tests for difference - non parametric",
    "section": "",
    "text": "4.1 Example\nA common scenario is that we have two sets of measurements, and we want to see if there is evidence that they are drawn from different populations. For some data types we can use a t-test to do this, but for others we cannot.\nA t-test requires in particular that the two sets of data are normally distributed around their respective means. With ordinal data this makes no sense. The mean is undefined as a concept for such data.\nTo see this , reflect that for a collection \\(X\\) of numerical data, say, 5, 3, 3, 4, and 5 we would calculate the mean as:\n\\[\n\\bar{X} = \\frac{5+3+3+4+5}{5} = \\frac{20}{5}=4\n\\]\nBut trying doing the same to five responses of a Likert scale survey. Say the responses you had to five Likert items (individual questions) were “strongly disagree”, “strongly agree”, “mildly disagree”, “strongly disagree” and “don’t care either way”. If you tried to calculate a ‘mean’ response you would be attempting to add up all these responses and to divide the ‘sum’ by five, like this:\n\\[\\text{mean response}=\\frac{\\text{stongly disagree}+\\text{strongly agree}+\\text{mildly disagree}+\\text{stongly disagree}+\\text{don't care either way}}{5} = ?\n\\] This sum makes no sense, I hope you will agree. It makes no sense, not because we are using words to describe our responses, but because, as these are ordinal data, we do not know the size of the gaps between the different points on the scale. Is the difference in agreement between the lowest two, “strongly disagree” and “midly disagree” the same as the gap between the highest two, “mildly agree” and “strongly agree”? We don’t know, mainly because ‘agreement’ is not something that can be measured easily using something like a weighing machine. And if we don’t know, then we shouldn’t really be adding these responses up or dividing them by anything.\nNevertheless, ordinal data are very common, since they are typically what is generated by survey data, where for example repondents may answer a series of questions (‘items’), each with typically five possible responses, but maybe more or fewer, these responses being ordinal in the sense that there is a definite order to them. They might encompass responses like those above, say, or something similar like “very unhappy” to “very happy”. They are also common in clinical and veterinary practice where ordinal pain scores are widely used - patients being asked (if they are human) or assessed as to their level of pain on a scale of 1-10, for example. Note that even if the pain value is recorded as a number it is actually a label, that could just as well have been recorded as one of a series of letters, A, B, C etc or emojis, or any symbol you like. You can’t take the average of a set of faces!\nThus, formally, we need another kind of test for a difference. Broadly, we need to use some form of non-parametric test where we do not assume that the data has any form of distribution, and where, often, we do not use the actual values of the measurements in our dataset but instead use only their ranks. The smallest value would be given rank 1, the next rank 2 and so on.\nThere are many non-parametric tests out there. Here we will look at only one - the Wilcoxon Rank Sum Test, often referred to as a Mann-Whitney U test for a difference. We can use this for the scenario we have painted above, where we have two sets of data and we wish to know if these provide evidence that the populations from which the samples have been drawn are in fact different.\nThis example uses actual data gathered by an Honours Project student.\nThe student wished to assess peoples’ sense of wellbeing using two different sets of questions designed to assess this. The scales chosen were the Warwick–Edinburgh Mental Well-being Scale (WEMWBS) and the New Ecological Paradigm (NEP) Scale. The student wished in particular to determine whether this sense of well-being was affected by whether a person often and actively frequented the coast and made it and the sea a substantive part of their life in one way or another. ie to find out whether there was evidence to support the notion that it could be good for your mental wellbeing to be by the sea and to make it part of your life.\nEach scale used consists of 15 questions or ‘Likert items’, each of which is answered on a 5 point ordinal scale, where a score of 1 indicates lowest wellbeing and a score of 5 indicates highest wellbeing. Thus each respondent could score anything from 15 to 75.\nThe student got responses from 374 people, 86 of whom were not “marine” users, while the other 288 say that they were marine users. The total scores from each respondent were recorded for each type of survey and stored in the file wellness.csv which you should find on the module Moodle site / Teams page. Please put this file in the data folder of your R project.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference - non parametric</span>"
    ]
  },
  {
    "objectID": "test_for_difference_non_parametric.html#script",
    "href": "test_for_difference_non_parametric.html#script",
    "title": "4  Tests for difference - non parametric",
    "section": "4.2 Script",
    "text": "4.2 Script\nThe first few chunks of this script carry out the same old-same old that we see in script after script: load packages, load data, summarise data , plot data.\nYou can run this script by running each chunk in sequence, which you do by clicking the green arrow in the top-right corner of each chunk.\nTry also to ‘Knit’ the script by clicking on the Knit button at the top of the script pane.\nIf you want to create your own script to use with your own data then you can copy and paste into it any code chunks that would be of use and adapt them as necessary.\n\n4.2.1 Load packages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/michaelhunt/git_repos/r4nqy\n\n\n\n\n4.2.2 Load data\nOur data set is in a .csv file which we have placed in the data folder within our project folder.\nNote that this data set has been stored in ‘tidy’ form: each variable appears in only column, and each observation appears in only one row.\n\nfilepath&lt;-here(\"data\",\"wellness.csv\")\nwellness&lt;-read_csv(filepath)\n\nRows: 748 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): scale, marine\ndbl (2): id, total_score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(wellness)\n\nRows: 748\nColumns: 4\n$ id          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ scale       &lt;chr&gt; \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\"…\n$ marine      &lt;chr&gt; \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\",…\n$ total_score &lt;dbl&gt; 48, 51, 37, 39, 38, 40, 54, 39, 54, 39, 51, 50, 49, 51, 54…\n\n\n\n\n4.2.3 Summarise the data\nWe’ll calculate the median score (50th percentile) and the 25th and 75th percentile scores. For ordinal data, these summary statistics are well defined, whereas means and standard deviations are not.\n\nwellness |&gt;\n  group_by(scale,marine) |&gt;\n  summarise(median.score=median(total_score),iqr_25=quantile(total_score,0.25),iqr_75=quantile(total_score,0.75))\n\n`summarise()` has grouped output by 'scale'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 5\n# Groups:   scale [2]\n  scale  marine median.score iqr_25 iqr_75\n  &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 WEMWBS No             42.5   36       49\n2 WEMWBS Yes            47     41       51\n3 nep    No             51     48.2     53\n4 nep    Yes            50     48       53\n\n\n\n\n4.2.4 Plot the data\nBox plots are particularly suitable for ordinal data since they show the 25th and 75th percentiles of the data (the bottom and top of the box) plus the 50th percentile aka the median, which is the thick line across each box. All of these percentiles are well defined quantities for ordinal data.\n\nwellness |&gt;\n  ggplot(aes(x = scale,y = total_score,fill = marine)) +\n  geom_boxplot() +\n  labs(x = \"Likert Scale\",\n       y = \"Wellbeing Score\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nLooking at the plot, what do you think each scale suggests about whether proximity to the sea makes a difference to wellbeing?\n\n\n4.2.5 Wilcoxon-Mann-Whitney U test\nFirst let’s pull out the scores as measured by the WEMWBS scale and do a test for a difference between the scores of marine users and those of non-marine users. We can use the filter() function to do this.\n\nWEMWBS&lt;-wellness |&gt; filter(scale==\"WEMWBS\")\nwilcox.test(total_score~marine,data=WEMWBS)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  total_score by marine\nW = 9077.5, p-value = 0.0001687\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe null hypothesis of this test is that there is no evidence that the data are drawn from different populations. In this case, the p-value is very small, so we can confidently reject that null hypothesis and assert that there is evidence, according to the WEMWBS scale that marine use makes a difference to peoples’ sense of wellbeing.\nDoes it make it worse or better? - we can see from the summary table and from the box plot that higher scores are associated with those people who were exposed to a marine environment.\nWe might report this results as follows, first using a plain English statement of the main finding, and then reporting the type of test use, the value of the test statistci that it calculated and the p value. In this case, because the p value is so small, we would not report its exact value, but simle give an indication of how small it is:\nWe find evidence, according to the WEMWBS scale, that the wellbeing score is 4.5 or about 10% higher for people exposed to a marine environment (Mann-Whitney U, W = 9077.5, p &lt; 0.001).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference - non parametric</span>"
    ]
  },
  {
    "objectID": "test_for_difference_non_parametric.html#exercise",
    "href": "test_for_difference_non_parametric.html#exercise",
    "title": "4  Tests for difference - non parametric",
    "section": "4.3 Exercise",
    "text": "4.3 Exercise\nAdapt the code of the last chunk so that you can do the same test but for data as recorded by the nep scale",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference - non parametric</span>"
    ]
  },
  {
    "objectID": "test_for_difference_non_parametric.html#when-should-i-use-this-wilcoxon-mann-whitney-u-test",
    "href": "test_for_difference_non_parametric.html#when-should-i-use-this-wilcoxon-mann-whitney-u-test",
    "title": "4  Tests for difference - non parametric",
    "section": "4.4 When should I use this Wilcoxon-Mann-Whitney U test?",
    "text": "4.4 When should I use this Wilcoxon-Mann-Whitney U test?\nThe test we have used here is an example of a non-parametric test. This means that it does not assume that the data follow a known mathematical distribution and, further, that it can be used with ordinal data.\nWe used the Mann-Whitney U test in particular because we were testing for a difference, and because the factor of interest - marine exposure - had just two levels - Yes or No. This test is only suitable when there are just two levels, so you can think of it as as a non-parametric alternative to a t-test.\nIn another setting where we still had just one factor (eg zone of a rocky shore) but there were more than two levels (eg low, mid and high zones of the shore) and we decided that we wanted to do a non-parametric test for a difference, then we would probably use the Kruskal-Wallis test, which you can think of as the non-parametric alternative to a one-way ANOVA.\nIn this example we used the Mann-Whitney U test because the data were ordinal and thus not suitable to use with a parametric test (but see below!). Where we can, we usually try to use a parametric test as they are more powerful than their non-parametric equivalents, meaning, if there is a trend or a difference in the data, they are better able to detect it. However those parametric tests (t-test, ANOVA, pearson correlation, PCA, GLM to name but a few) typically require not only that the data are numerical but also a host of other things, including that they follow a particular distribution, usually (but not always) the normal distribution, and this is often not the case with real biological data. Often, especially with count data, there are lots of zeros, or the data distribution is heavily skewed, usually to the right. In these cases, providing the data are independent of each other, we can usually still use a non-parametric test such as we have here. it might not be the most powerful test we can use (GLMs are typically way better if you can use them), but it will work.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference - non parametric</span>"
    ]
  },
  {
    "objectID": "test_for_difference_non_parametric.html#hang-on",
    "href": "test_for_difference_non_parametric.html#hang-on",
    "title": "4  Tests for difference - non parametric",
    "section": "4.5 Hang on!",
    "text": "4.5 Hang on!\nThe eagle eyed among you may have spotted a massive flaw in the line of argument presented above. We said that ordinal data can’t be added up, can’t be used to calculate averages and so on. Thus we can’t run parametric tests on them and have to look for alternatives, namely, non-parametric tests.\nAnd yet, these non-parametric tests are usually run on the output of Likert scales such as we have considered here, where for each person we have a number of Likert Items (ie individual questions) that together constitute the scale, that each generate a score 1-5, then we add up the scores to get a total score. But that means we are adding up ordinal data!!!\nIt turns out that you actually get much the same results with Likert scale data if you analyse them using supposedly inappropriate parametric tests such as a 2-sample t-test as you do if you use a non-parametric test such as the one we considered here, the Mann-Whitney test.\nA study by De Winter and Dodou (2010) shows this convincingly.\nde Winter, J. F. C., & Dodou, D. (2010). Five-Point Likert Items: t test versus Mann-Whitney-Wilcoxon (Addendum added October 2012). Practical Assessment, Research, and Evaluation,15, 1–16. https://doi.org/10.7275/bj1p-ts64\nFor an enlightening discussion of this paper, see this blog by Jim Frost",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference - non parametric</span>"
    ]
  }
]