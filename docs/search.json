[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "From questions to data to answers using R",
    "section": "",
    "text": "Introduction\nThis is a compilation of the methods for data analysis that you are likely to find useful in completing your studies at Newquay University Centre. It is by no means exhaustive, but should address most of your needs, most of the time.\nData analysis in the life sciences is only part of the wider process of forming and then answering as best we can well-formed questions about the way this or that aspect of the natural world works. If our question is good, and our design is good, then it is more likely than not that one of the standard analytical techniques described in this text will our needs. If not, then maybe none of them will suit and we will have to work harder to extract the answers we want from the data we have worked hard to gather.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-resource",
    "href": "index.html#how-to-use-this-resource",
    "title": "From questions to data to answers using R",
    "section": "How to use this resource",
    "text": "How to use this resource\nYou are not expected to read this text from beginning to end, although you can if you want to. Each section explains the topic at hand and also gives you a link to a skeleton R project that you can download, unzip, and use to follow along for yourself.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-to-make-a-book-like-this.",
    "href": "index.html#how-to-make-a-book-like-this.",
    "title": "From questions to data to answers using R",
    "section": "How to make a book like this.",
    "text": "How to make a book like this.\nFollow the very useful guide given by James Bartlett of Glasgow University.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "test_finder.html",
    "href": "test_finder.html",
    "title": "Test finder",
    "section": "",
    "text": "You can either use this text version of a test finder, taken from (Gilbert, McGregor, and Barnard 2017)\nor use this flowchart to see which test is appropriate for your study design and your data:\n\n\n\n\n\n%%| fig-width: 10\n%%| fig-align: \"centre\"\n%%| column: page-right\n\nflowchart TD\n  A{Difference or Trend Question?} --&gt; B[Trend]\n  B --&gt; B2{Are you testing for degree of associationor are you trying to make predictions?}\n  B2 --&gt; B33[Association:Test for correlation coefficient Pearson or  Spearman's Rank]\n  B2 --&gt; B44[Predictions: Simple linear regression]\n  A --&gt; C[Difference]\n  C --&gt; C22{Do you have replicates?}\n  C22 --&gt; C22Y[Yes]\n  C22 --&gt; C22N[No]\n  C22N --&gt; C23{Do you have count data?}\n  C23 --&gt; C23Y[Yes]\n  C23 --&gt; C23N[N0]\n  C23Y --&gt; C24(Chi square test: Goodness of fit or Test of independence)\n  C23N --&gt; C25[These data cannot be analysed]\n  C22Y --&gt; D{How many factors?}\n  D --&gt; F[Two or more]\n  F --&gt; F2{Independent samples?}\n  F2 --&gt; F2Y[Yes]\n  F2 --&gt; F2N[No]\n  F2Y --&gt; F22Y(n-way ANOVA)\n  F2N --&gt; F22N(n-way repeated measures ANOVA)\n  D --&gt; E[One]\n  E --&gt; G{How many levels?}\n  G --&gt; H[Two]\n  G --&gt; I[More than two]\n  I --&gt; J{Independent samples?}\n  J --&gt; K[No]\n  J --&gt; L[Yes]\n  K --&gt; S(Repeated measures one-way ANOVA or Friedman Test)\n  L --&gt; T(One way ANOVA or Kruskal Wallis one-way test)\n  H --&gt; M{Independent samples?}\n  M --&gt; N[No]\n  M --&gt; O[Yes]\n  N --&gt; P(paired t-test or Signed rank test)\n  O --&gt; Q(two sample t-test or Matt Whitney U test)\n  \n\n\n\n\n\n\nNow go to whichever chapter of this book covers the test you need.\n\n\n\n\nGilbert, Francis, Peter McGregor, and Chris Barnard. 2017. Asking Questions in Biology; a Guide to Hypothesis Testing, Experimental Design and Presentation in Practical Work and Research Projects. 5th ed. Benjamin Cummings.",
    "crumbs": [
      "Preliminaries",
      "Test finder"
    ]
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "1  A recommended analysis workflow",
    "section": "",
    "text": "1.1 Are you working within your Project?\nThis is intended as a rough outline of the sequence of steps one commonly goes through when working on scripts:\nDetails will differ from script to script, but this sequence of steps is very common.\nBefore we even think of the script, we need to make sure that we are working within our Project. If we are not doing this, bad things will happen. If you are, the Project name will be at the top right of the RStudio window. If you are not, save the script you are working on, and go to File/Open Project and open your Project. If you haven’t even got a ‘Project’ or don’t know what that means then just make sure that everything you need for whatever you are working on is in one folder and then turn that folder into a Project. (So a ‘Project’ is just a regular folder that has been given superpowers.) You do that by going to File/New Project/Existing Directory. Then you navigate to your folder and click on Create Project. RStudio will then restart and you will see the name of your newly anointed Project folder at the top-right of the RStudio window. You know that a folder is a ‘Project’ because it will have a .Rproj file inside it.\nIf all this sounds complicated, don’t worry. It really isn’t. Just get someone to show you how to do it and you will be fine.\nNow, to the script itself:",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#statement-of-the-questions-to-be-investigated",
    "href": "workflow.html#statement-of-the-questions-to-be-investigated",
    "title": "1  A recommended analysis workflow",
    "section": "1.2 Statement of the question(s) to be investigated",
    "text": "1.2 Statement of the question(s) to be investigated\nWithout thinking this through, you won’t know what your script is for…\nWhat is the analysis that will follow for? What question are you trying to answer? What hypotheses are you trying to test?\nSuppose we were trying to test the hypothesis that there is no difference between the petal widths of the setosa, versicolor and virginica species of iris. All we have to go on are the petal widths of the plants we happened to measure. From these measurements we want to make a statement about these three species in general.",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#open-a-notebook",
    "href": "workflow.html#open-a-notebook",
    "title": "1  A recommended analysis workflow",
    "section": "1.3 Open a notebook",
    "text": "1.3 Open a notebook\nIn RStudio, go to File/New File/ Quarto Document. Delete everything below the yaml section at the top. This strangely named section is the bit between the two lines with three dashes in. For the most part, we will not need to worry about this section. We just should not delete it entirely. What is useful to do is to amend the title to something sensible, and to add author: \"your name\" and date: \"the date in any old format\" lines. I also add a couple of final lines that suppress warnings and messages that might clutter up my printed output, so that your yaml will look something like this:\n---\ntitle: \"A typical workflow\"\nauthor: \"Who wrote this?\"\ndate: \"Today's date\"\noutput:\n  html_document:\n    df_print: paged\nexecute:\n  message: false\n  warning: false\n---\nDelete everything beneath this yaml section. The big empty space that then leaves you with is where you write your code. Remember that in quarto documents, the code goes in ‘chunks’ that are started and finished with by lines with three backticks. Any other text goes between the chunks and you can format this text using the simple rule of Markdown, available in the RStudio Help menu at Help/Markdown Quick Reference. Thus your script will end up looking something like this:\n---\ntitle: \"A typical workflow\"\nauthor: \"Who wrote this?\"\ndate: \"Today's date\"\noutput:\n  html_document:\n    df_print: paged\nexecute:\n  message: false\n  warning: false\n---\n\n## First header\n\nAny text we want to add. Note that a code chunk starts with delimiters, like this:\n\n```{r}\n\n```\n\n\nThose are back-ticks, not apostrophes!\n\n```{r}\nlibrary(tidyverse) # some actual R code\n```\n\n## Second header\n\nAny text we want to add to explain what this next chunk does\n\n```{r}\nlibrary(tidyverse) # some actual R code\n```",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#load-packages",
    "href": "workflow.html#load-packages",
    "title": "1  A recommended analysis workflow",
    "section": "1.4 Load Packages",
    "text": "1.4 Load Packages\nYou will nearly always want the first five packages, and often you will appreciate the sixth, janitor. Others, such as vegan will be useful from time to time, depending on what you are doing. If any of these lines throw an error, it is most likely because of a typo or because you have not yet installed that package. Do so in the console pane (not in this script!) using the function install.packages(\"name of package\"). Then run this whole chunk again.\n\n```{r}\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggfortify)\nlibrary(readxl)\nlibrary(cowplot)\nlibrary(janitor)\nlibrary(vegan)\n```",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#load-data",
    "href": "workflow.html#load-data",
    "title": "1  A recommended analysis workflow",
    "section": "1.5 Load data",
    "text": "1.5 Load data\nThere are several ways to do this, so details will differ depending on what file type your data is saved in and where it is stored.\nHere are some examples. In each case code here presumes that the data is stored in a subfolder called ‘data’ within the Project folder, and we use the function here() from the here package. In my experience this dramatically simplifies the business of finding your data, wherever your script is. It makes it easier for you to share your script with others and be confident that what worked for you will work for them. It does require that you are working within your project.\n\n1.5.1 If from a csv file\nIf you have your data in a data subfolder within your project, this chunk will work. Just substitute the name of your data file\n\n```{r}\nfilepath&lt;-here(\"data\",\"iris.csv\")\niris&lt;-read_csv(filepath)\nglimpse(iris)\n```\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n\n\n\n\n1.5.2 If from an Excel file\nYou will need to use read_excel() from the readxl package, and you have to specify the name of the worksheet that holds the data you want. You can, if you want, specify the exact range that is occupied by the data. However I suggest you avoid doing this unless it turns out that you need to do so. If your data is a nice, neat, rectangular block of rows and columns, you should find that you don’t need to specify the range.\n\n```{r}\nfilepath&lt;-here(\"data\",\"difference_data.xlsx\")\niris&lt;-read_excel(path = filepath,\n                 sheet = \"iris\", # delete the comma if you choose not to specify the range in the line below\n                 range= \"A1:F151\" # optional - try leaving it out first. Only include if necessary.\n                 ) |&gt;\n  clean_names()\nglimpse(iris)\n```\n\nRows: 150\nColumns: 6\n$ id           &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ sepal_length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n\n\n\n\n1.5.3 If from a URL\nYou can load data into R directly from a URL if you are given one.\nhere, we load data from a file stored in a ‘’repo’ on my github account:\n\n```{r}\niris&lt;-read_csv(\"https://raw.githubusercontent.com/mbh038/r4nqy/refs/heads/main/data/iris.csv\") |&gt;\n  clean_names()\nglimpse(iris)\n```\n\nRows: 150\nColumns: 5\n$ sepal_length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#clean-manipulate-the-data",
    "href": "workflow.html#clean-manipulate-the-data",
    "title": "1  A recommended analysis workflow",
    "section": "1.6 Clean / Manipulate the data",
    "text": "1.6 Clean / Manipulate the data\nOften we need to do some sort of data ‘wrangling’ to get the data into the form we want. For example we may wish to tidy it (this has a particular meaning when applied to data sets), to remove rows with missing values, to filter out rows from sites or time periods that we don’t want to include in our analysis, to create new columns and so on.\nFor example, let’s create a new data frame for just the setosa species of iris:\n\n```{r}\nsetosa &lt;- iris |&gt;\n  filter(species == \"setosa\") # filter picks out rows according to criteria being satisfied in some column\nglimpse(setosa)\n```\n\nRows: 50\nColumns: 5\n$ sepal_length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n\n\nor maybe we just want the columns that contain numeric data and not the one containing the species identifiers, which is text:\n\n```{r}\niris_numeric &lt;- iris |&gt;\n  select(-species) # select() retains or leaves out particular columns. Here, we leave out the species column.\nglimpse(iris_numeric)\n```\n\nRows: 150\nColumns: 4\n$ sepal_length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#summarise-the-data",
    "href": "workflow.html#summarise-the-data",
    "title": "1  A recommended analysis workflow",
    "section": "1.7 Summarise the data",
    "text": "1.7 Summarise the data\nHow big is the difference between the mean of this group over here and that group over there, and how big is that difference compared to the precision with which we know those means? We nearly always want to do this as a first way to get insight into whether we will or will not reject our hypothesis. For example, let’s find the mean petal widths of the three species, the standard errors of those means and save the results to a data frame called petal_summary\n\n```{r}\npetal_summary&lt;-iris |&gt;\n  group_by(species) |&gt;\n  summarise(mean.Pwidth = mean(petal_width),\n            se.Pwidth = sd(petal_width/sqrt(n())))\npetal_summary\n```\n\n# A tibble: 3 × 3\n  species    mean.Pwidth se.Pwidth\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 setosa           0.246    0.0149\n2 versicolor       1.33     0.0280\n3 virginica        2.03     0.0388\n\n\nWe can look at this table and already get an idea as to whether the petal widths are the same or are different for the three species.",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#plot-the-data",
    "href": "workflow.html#plot-the-data",
    "title": "1  A recommended analysis workflow",
    "section": "1.8 Plot the data",
    "text": "1.8 Plot the data\nThe next step is usually to plot the data in some way. We would typically use the ggplot2 package from tidyverse to do this.\n\n1.8.1 Bar plot with error bars\nWe could plot a bar plot with error bars, working from the summary data frame that we created:\n\n```{r}\npetal_summary |&gt;\n  ggplot(aes(x = species, y = mean.Pwidth)) +\n  geom_col(fill=\"#a6bddb\") + # this is the geom that gives us a bar plot when we have already done the calculations\n  geom_errorbar(aes(ymin = mean.Pwidth - se.Pwidth, ymax = mean.Pwidth + se.Pwidth), width = 0.15) +\n  labs( x = \"species\",\n        y = \"Mean petal width (mm)\",\n        caption = \"Error bars are ± one standard error of the mean\") + # important to say what these error bars denote\n  theme_cowplot()\n```\n\n\n\n\n\n\n\n\nNote that we have given the bars a fill colour - we got this color from this site due to the cartographer Cynthia Brewer, who is behind the various incarnations of the Brewer package in R, which is great for getting colours that work well. We have used the same colour for each species since the x-axis labels already tell us which bar relates to which species. To use a different colour for each bar would imply there is some extra information encoded by colour. Since there is not, it serves no purpose to have different colours, and potentially confuses the reader. Remember always that a plot is intended to convey a message. Anything that detracts from that message should be avoided, however pretty you think it is.\nA couple of points could be made about this type of plot:\nFirst, what about those error bars? Three types of error bar are in common usage and there are arguments in favour and against the use of each of them:\n\nthe standard deviation tells us about the spread of values in a sample, and is an estimate of the spread of values in a population;\n\nthe standard error of the mean, as used here, is an estimate of the precision with which the sample means estimates the respective population means for each of the species.\nthe confidence interval, typically a 95% confidence interval, gives us the region within which we are (say) 95% confident that the true species mean petal width might plausibly lie.\n\nWhich type of error bar is best to use depends on what story you want to tell. Here, because we are interested in whether there is evidence of a difference in the mean petal width of different species, we have gone for the standard eror of the mean.\nRegardless of which error bar you use and why, you should always tell the reader which one you have gone for, as we have in the caption to the figure.\nA second point about this bar plot is that it doesn’t tell us very much, and indeed nothing that we didn’t already know. It only conveys the mean and standard error values for each species, which is information we already have, arguably more compactly and in more easily readable form, in the table we created. Further, it potentially obscures information that might come from knowing the distribution of the data.\nHere are three other plot types that do show the distribution of petal widths for each species and thus add extra information to what we already know from the summary table\n\n\n1.8.2 Box plot\n\n```{r}\niris |&gt;\n  ggplot(aes(x=species, y=petal_width)) + # what we want to plot\n  geom_boxplot(fill=\"#a6bddb\",notch=FALSE) + # what kind of plot we want\n  geom_jitter(width=0.1, colour = \"#f03b20\",alpha=0.5) +\n  labs (x = \"species\",\n        y = \"Petal Width (mm)\") +\n  theme_cowplot() # choose a theme to give the plot a 'look' that we like\n```\n\n\n\n\n\n\n\n\nHere, we have added the points themselves on top of the box plot. When there are not too many data points, this can be useful. The ‘jitter’ adds some horizontal or vertical jitter, or both, so that the points do not lie on top of each other. In this case we see that the variability of petal widths is not the same for each species and that the data are roughly symmetrically distributed around the median values in each case. This information is useful in helping us determine which statistical test might be appropriate for these data.\n\n\n1.8.3 Violin plot\nA useful alternative to the box plot, especially when the data set is large, is the violin plot:\n\n```{r}\niris |&gt;\n  ggplot(aes(x = species, y = petal_width)) + # what we want to plot\n  geom_violin(fill=\"#a6bddb\",notch=TRUE) + # what kind of plot we want\n  #geom_jitter(width=0.1, colour = \"#f03b20\",alpha=0.5) +\n  labs (x = \"species\",\n        y = \"Petal Width (mm)\") +\n  theme_cowplot() # choose a theme to give the plot a 'look' that we like\n```\n\n\n\n\n\n\n\n\nThe widths of the blobs (I am probably supposed to call them ‘violins’!) show us the distribution of the data - where they are widest is where the data are concentrated, while the height of the blobs shows us the range of variation of the data. The positions of the blobs tells us the mean petal widths of the different species and gives us an idea of the differences between them.\n\n\n1.8.4 Ridge plot\nA bit like a violin plot. This needs the package ggridges to be installed.\n\nlibrary(ggridges)\n#| echo: fenced\n\niris |&gt;\n  ggplot(aes(x = petal_width,y = species)) + # what we want to plot\n  geom_density_ridges(fill=\"#a6bddb\") + # what kind of plot we want\n  #geom_jitter(width=0.1, colour = \"#f03b20\",alpha=0.5) +\n  labs (x = \"Petal Width (mm)\",\n        y = \"species\") +\n  theme_cowplot() # choose a theme to give the plot a 'look' that we like\n\n\n\n\n\n\n\n\nHaving seen the summary and one of these plots of the data, would you be inclined to reject, or fail to reject, a null hypothesis that said that there was no difference between the petal widths of the three species?",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#statistical-analysis",
    "href": "workflow.html#statistical-analysis",
    "title": "1  A recommended analysis workflow",
    "section": "1.9 Statistical analysis",
    "text": "1.9 Statistical analysis\nOnly now do we move on to the statistical analysis to try to answer our intial question(s). But by now, after the summary and plot(s), we may already have a pretty good idea what that answer will turn out to be.\nThe exact form of the analysis could take many forms. In a typical ecology project you might carry out several types of analysis, each one complementing the other. Here, an appropriate analysis might be to use the linear model in the form of a one-way ANOVA, since we have one factor (species) with three levels (setosa, versicolor and virginica) and an output variable that is numeric and likely to be normally distributed. We can use the lm() function for this.\n\n1.9.1 Create the model object\n\n```{r}\npw.model &lt;-lm (petal_width ~ species, data = iris)\n```\n\n\n\n1.9.2 Check the validity of the model\nWe won’t go into this here, but an important step is to check that the data satisfy the often finicky requirements of whatever statistical test we have decided to use. The autoplot() function form the ggfortify package is great for doing this graphically.\n\n```{r}\nautoplot( pw.model) + theme_cowplot()\n```\n\n\n\n\n\n\n\n\nHere we note in particular that although the spread of data within each level is not roughly the same (top left figure)), the QQ plot is pretty straight (top-right figure). This means that the data are approximately normally distributed around their respective means. Taken together, this means that these data satisfy reasonably well the requirements of a linear model, so the output of that model should be reliable.\n\n\n1.9.3 The overall picture\nTypically, statistical tests are testing the likelihood of the data being as they are, or more ‘extreme’ than they are, if the null hypothesis were true. Thus, the null hypothesis is central to statistical testing.\nThe null hypothesis is typically that the ‘nothing going on’, ‘no difference’ or ’ no association’ scenario is true. In this case, it would be that there is no difference between the petal widths of the the three species of iris being considered here.\nTypically too, a test will in the end spit out a p-value which is the probability that we would have got the data we got, or more extreme data, if the null hypothesis were true. Being a probability, it will always be a value between 0 and 1, where 0 means impossible, and 1 means certain. The closer the p-value is to zero, the less likely it is we would have got our data if the null hypothesis were true. At some point, if the p-value is small enough, we will decide that the probability of getting the data we actually got if the null hypothesis were true is so small that we reject the null hypothesis. Typically, the threshold beyond which we do this is when p = 0.05, but we could choose other thresholds. (Sounds arbitrary - yes, it is, but the choice of 0.05 is a compromise value that makes the risk of making each of two types of error - rejecting the null when we should not, and failing to reject it when we should, both acceptably small. This is a big topic which we won’t explore further here.)\nIn the end, whatever other information we get from it, the outcome of a statisical test is typically that we either reject the null hypothesis or we fail to reject it. If we reject it then we are claiming to have detected evidence for an ‘effect’ and we go on to determine how big that effect is and whether it is scientifically interesting. If we fail to reject the null, that does not necessarily mean that there is no ‘effect’ (difference, trend, association etc). That might be the case, but it might also just mean that we didn’t find evidence for one from our data.\nIt is all a bit like in a law court where the ‘null hypothesis’ is that the defendant is innocent, and at the end of the proceedings this null is either rejected (Guilty!) because the evidence is such as to make it untenable to hold onto the null hypothesis, or not rejected, because the evidence is not strong enough to convict, in which case the defendant walks free - but is not declared innocent. Formally, the court has simply found insufficient evidence to convict. In the latter case, the court would have failed to reject the null hypothesis. Crucially, it would not have declared that the defendant was innocent. In the same way, in a scientific study, we either reject or fail to reject a null hypothesis. We never ever accept the null hypothesis as true.\nActually, many researchers are unhappy wih this so-called ‘frequentist’ narrative and have sought to use an alternative ‘Bayesian’ approach to testing hypotheses. In this approach we can accept hypotheses and we can bring in prior knowledge. This is an interesting topic, but a very big one so we will not pursue it further here.\nWith all that behind us, we are in a better place to understand what the output of the test is telling us.\nFor the 1-way ANOVA, as with other examples of the linear model, this output comes in two stages:\n\n\n1.9.4 Overall picture\nIs there evidence for a difference between at least two of the mean values?\nTo see if there is evidence for this, an ANOVA test calculate the ratio between the dfference betweeN the groups compared to the differences within the groups. it calls this ratio \\(F\\). The bigger \\(F\\) is, the more likely we are to reject the null hypothesis that there is no difference between he groups.\n\n```{r}\nanova(pw.model)\n```\n\nAnalysis of Variance Table\n\nResponse: petal_width\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nspecies     2 80.413  40.207  960.01 &lt; 2.2e-16 ***\nResiduals 147  6.157   0.042                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe F value is huge. The null hypothesis of this test, as with many tests, is that there is no difference between the petal widths of the three populations from which these samples have been drawn. In that case, the F value would be one. The p-value is telling us how likely it is that we would get an F value as big or bigger than the one we got for our samples if the null hypothesis were true. Since the p-value is effectively zero here, we reject the null hypothesis: we have evidence from our data that there is a significant varation of petal width between species.\nThe degrees of freedom Df tells us the number of independent pieces of information that were used to calculate the result. Let’s not dwell on this here, but there are two that we have to report in this case: the number of levels minus one ie 3-1 = 2, and the number of individual data points in each level minus one, times the number of levels ie (50-1) x 3 = 147.\n\n\n1.9.5 Effect size\nNow that we have established that at least two species of iris have differing petal widths, we go onto investigate where the differences lie, and how big they are. This is important: effect sizes matter. It is one thing to establish that a difference is statistically significant (and typically even the tiniest difference can show up as significant in a study if the sample size is big enough), it is quite another to establish whether the difference is big enough to be scientifically interesting.\n\n```{r}\nsummary(pw.model)\n```\n\n\nCall:\nlm(formula = petal_width ~ species, data = iris)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-0.626 -0.126 -0.026  0.154  0.474 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.24600    0.02894    8.50 1.96e-14 ***\nspeciesversicolor  1.08000    0.04093   26.39  &lt; 2e-16 ***\nspeciesvirginica   1.78000    0.04093   43.49  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2047 on 147 degrees of freedom\nMultiple R-squared:  0.9289,    Adjusted R-squared:  0.9279 \nF-statistic:   960 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nThe output here is typical of that from a 1-way ANOVA analysis in R. Each line refers to one of the three levels of the factor being investigated, which is petal width in this case. By default, those levels are arranged alphabetically, so in this case the order is setosa, versicolor then virginica. The first row is always labelled (Intercept), so here that row is referring to setosa. This level is used as the ‘control’ or reference level- the one with which the others are compared. If we are happy to have setosa as that control then we can just carry on, but if we are not, then we have to tell R which level we want to play that role. We’ll go through how to do that later on.\nIn the Estimate column the value 0.246 cm in the first row refers to the actual mean petal width of the setosa plants in the sample. If we go back to the summary table we created earlier on, or look at one of the plots we created, we see that that is the case.\nFor all other rows, the value in the Estimate column is not referring to the absolute mean petal width but to the difference between the mean petal width for that species and the mean petal width of the control species. So we see that the mean petal width of the versicolor in our sample is 1.08 cm greater than that of setosa and so is equal to .246 + 1.08 = 1.326 cm, while that of the virginica is 1.78 cm greater and so is equal to 2.026 cm. Check from the table of mean values we created and the plots that this is correct.\nHere though, we are not interested in absolute values so much as we are in differences, which is why that is what the summary table here gives us. Look again at the differences between the mean petal widths for versicolor and virginica and that for setosa and compare them with the standard erros of those differences, which are given in the second column of the table. These standard errors are much smaller than the differences, meaning that we can have confidence that the differences are statistically significant.\nThis is borne out by the p-value in the right hand column of the table. The null hypothesis of this table is that there is no difference in petal width between populations of the different species from which these samples have been drawn.\nLastly, the adjusted \\(R^2\\) tells us the proportion of variation of petal width that is accounted for by taking note of the species. Here, the value is 0.93, which tells us hat little else besides species determines the relative petal widths. Ther are no other variables that we need to have taken into account.",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#report-in-plain-english",
    "href": "workflow.html#report-in-plain-english",
    "title": "1  A recommended analysis workflow",
    "section": "1.10 Report in plain English",
    "text": "1.10 Report in plain English\nYou would say something like\nWe find evidence that petal widths are not the same acros thhree species of iris, with virginica &gt; versicolor &gt; setosa. (ANOVA, df = 2, p &lt; 0.001)",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html",
    "href": "ggplot_examples.html",
    "title": "2  Building plots using the package ggplot2",
    "section": "",
    "text": "2.1 Load packages\nIn this exercise we are going to produce and improve a variety of useful and widely used plots using the package ggplot2 which is part of the larger tidyverse package.\nYou will see that the code to do each plot is very similar, whatever the type of plot, and that plots can be built up from very basic forms to become really attractive, informative versions with very little additional effort.\nYou need to read the examples in this worksheet and then fill in the missing code or alter what is provided already in the empty code chunks of the accompanying template script. Instructions for getting that are given below.\nAs you complete each code chunk, try it out by pressing the green arrow at the top right of the chunk. Sometimes you might want to try out an individual line. You can do that by placing the cursor anywhere in the line and pressing Controll-Entr (windows) or Command-Enter (Mac)\nRemember that the template is a markdown document, so you can add extra text between the code chunks to explain to yourself what is going on. You can format this test, if you wish, according to the very basic markdown rules for doing this. See Help/Markdown Quick Reference. This formatting is only useful if you ‘knit’ the script, by pressing the knit button at the top of the script pane. Try this! I suggest you knit to html. This is how the worksheet you are working from was produced.\n# install.packages(\"name of package\") # run this line once, if you need to, for any of the packages that need to be installed\nlibrary(tidyverse)\nlibrary(here)\nlibrary(palmerpenguins)\nlibrary(devtools)",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#get-the-template-script",
    "href": "ggplot_examples.html#get-the-template-script",
    "title": "2  Building plots using the package ggplot2",
    "section": "2.2 Get the template script",
    "text": "2.2 Get the template script\nThis next chunk will download the template file that you need to fill in as you work through this worksheet, and put it in the scripts subfolder within your project folder. For it to work, you need to be ‘working in your Project’ - in which case the name of the project will appear in the top right of the main RStudio window, and if you have a subfolder within the project folder called ‘scripts’. If any of that is not true, it needs to be sorted now!\n\nfile_url &lt;- \"https://raw.githubusercontent.com/mbh038/r-workshop/gh-pages/scripts/ggplot_examples_template.Rmd\"\nfile_dest &lt;- here(\"scripts\",\"my_ggplot_examples.rmd\")\ndownload.file(file_url,file_dest)",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#load-the-palmer-penguin-data",
    "href": "ggplot_examples.html#load-the-palmer-penguin-data",
    "title": "2  Building plots using the package ggplot2",
    "section": "2.3 Load the Palmer penguin data",
    "text": "2.3 Load the Palmer penguin data\nFor this exercise we use the Palmer penguins data set which comes with the package palmerpenguins\nThe palmerpenguin package contains two built-in data sets. One is called penguins:\nHere we load the data into this R session (you will now see it in the Environment pane) and we inspect it using the function glimpse().\n\ndata(penguins)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nHow many rows are there and how many columns?\nFor more detailed meta-information on the data we just type the name of the data set with a question mark before it:\n\n?penguins",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#summary-stats-on-all-the-numeric-columns",
    "href": "ggplot_examples.html#summary-stats-on-all-the-numeric-columns",
    "title": "2  Building plots using the package ggplot2",
    "section": "2.4 Summary stats on all the numeric columns",
    "text": "2.4 Summary stats on all the numeric columns\nThis is in general useful to get, at least for the columns that contain numerical data, since it shows which columns contains NAs,which is R-speak for missing data. They are how R represents what would be empty cells in an Excel spreadsheet.\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nWe see that there are some rows with NAs in for a few of the columns. We need to be aware of this when doing calculations with the data, such as taking means.\nHere, we will remove those rows:",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#remove-the-rows-with-nas",
    "href": "ggplot_examples.html#remove-the-rows-with-nas",
    "title": "2  Building plots using the package ggplot2",
    "section": "2.5 Remove the rows with NAs",
    "text": "2.5 Remove the rows with NAs\n\npenguins &lt;- penguins |&gt;\n  drop_na()",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#how-many-observations-are-there-for-each-species",
    "href": "ggplot_examples.html#how-many-observations-are-there-for-each-species",
    "title": "2  Building plots using the package ggplot2",
    "section": "2.6 How many observations are there for each species?",
    "text": "2.6 How many observations are there for each species?\nNote the use of the pipe operator |&gt;, here and throughout. Think of it as meaning and then. It feeds the output of one line into the function of the next line, where it is used as that function’s first argument.\n\npenguins |&gt;\n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      146\n2 Chinstrap    68\n3 Gentoo      119",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#mean-value-for-each-numerical-variable-for-each-species",
    "href": "ggplot_examples.html#mean-value-for-each-numerical-variable-for-each-species",
    "title": "2  Building plots using the package ggplot2",
    "section": "2.7 Mean value for each numerical variable, for each species",
    "text": "2.7 Mean value for each numerical variable, for each species\nHere is an example of the use of the group_by() then summarise() combination, whereby data is first grouped, here by species, then summary statistics (of your choice) are calculated for each group.\nIn this example the data are grouped by species, then the mean value of all the columns that contain numerical data are calculated, not just an overall value for the whole column, but for each species\n\npenguins |&gt;\n  group_by(species) |&gt;\n  summarize(across(where(is.numeric), mean, na.rm = TRUE)) |&gt;\n  ungroup() # good practice to include this at the end.\n\n# A tibble: 3 × 6\n  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n  &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie              38.8          18.3              190.       3706. 2008.\n2 Chinstrap           48.8          18.4              196.       3733. 2008.\n3 Gentoo              47.6          15.0              217.       5092. 2008.\n\n\n\n2.7.1 Scatter plots\nIs flipper length correlated with body mass?\nWe could a do correlation test to find this out, but let us first plot the data. We will show here how an elegant plot can be built up, starting from a very basic one, so that you see what each line of code for the finished version actually does. In the chunks below, run each one in turn to see the effect of each successive change that you make.\nFirst we feed the penguin data to the function ggplot(), and use its aes() argument to tell it which variables are to be ‘mapped’ to which aesthetic (which means, roughly speaking, ‘visible’) features of the plot, such as the x-axis, the y-axis, point and line colours, fill colours, symbol types and size etc:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g))\n\n\n\n\n\n\n\n\nThis produces the first layer of the eventual finished plot, an empty plot, ready to display data. Before it can do this, ggplot() needs to be told how you want to do so - what type of plot do you want? For that, we add a geom.....() line, to specify the type of plot.\nThere are lots of geom types, but for a scatter plot we use geom_point():\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis gives us a recognisable scatter plot, but it is deficient in a number of ways. For starters, we know that there are three species of penguin. It would be better if each were plotted using symbols of a different colour, shape or size. We can do this by adding in an extra argument to the aesthetic in the first line. Here we include colour = species.\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm,y = body_mass_g, colour = species)) +\n  geom_point()\n\n\n\n\n\n\n\n\nCan you guess what you should have do if you wanted not the symbol colour, but its shape or size to depend on species? Clue: change one word!\nNow we add labels, titles and so on, using the line labs(...). Note how we can actually write the arguments of this over several lines on the page, for clarity.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\", # this changes the title of the legend.\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\",\n       caption = \"Alternative place to put the information in the subtitle\")\n\n\n\n\n\n\n\n\nIt can be useful to include some combination of titles, subtitles and captions if the figure is to be used as part of a presentation or poster, but if it is to go in a report, you would normally only include a caption, and let the word-processing software do it, and if just for exploratory analysis, not even that. I normally do include axis labels, however.\nNow we use a theme to alter the overall look of the figure. There are several built-in themes you can choose from, and others from packages that you can use. I usually use theme_cowplot() from the cowplot package. Try typing ?theme at the command prompt in the console window to see what is available. Here, we use the built-in theme_bw():\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nNow we reposition the legend. We don’t have to, but we might not like the default position of the legend. If not, we can move or even remove it using another theme() line. The position argument of this can be “none” if you want to remove it, top”, “bottom”, “left”, “right” or a numerical vector in relative coordinates, where c(0,0) means bottom left within the plot, and c(1,1) means top-right. This is what we use here. Play around with different values.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\") +\n  theme_bw() +\n  theme(legend.position = c(0.2,0.8)) # try \"top\", \"left\" etc\n\n\n\n\n\n\n\n\nNicer colours. If you don’t like the default colours offered by R, there are several other palettes available, for example the Brewer palettes, borrowed from the world of maps. See https://colorbrewer2.org ,and for a list of the available palettes, type &gt;?scale_colour_brewer into the console pane then look at the help that appears in the Help pane (bottom right), and scroll down to the palettes section. Note that we dont have to alter the colours. But doing so can make your plots not only look nicer, but serve some other purpose, such as to be colour-blind friendly, or have colours that are appropriate for the variables being plotted (eg red points for red things, blue points for blue things). For an assignment or dissertation report, it is a good idea to pick a palette that you like and that works, and stick with it, so that all your plots have the same general look. Here we choose the qualitative palette \"Set2\" and use it by by adding the line scale_colour_brewer(palette=\"Set2\"). Try a few other palettes.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\") +\n  scale_colour_brewer(palette=\"Set2\") + # try other palettes eg \"Set3\"\n  theme_bw() +\n  theme(legend.position = c(0.2,0.8)) # try \"top\", \"left\" etc\n\n\n\n\n\n\n\n\nIf we like, we can add best fit lines to each subset of the data, using geom_smooth(). To produce straight line fits, geom_smooth() needs to be told to use a linear model, using the method = \"lm\" argument. By default, you will get lines with a grey 95% confidence band around them. This can be useful, but if you don’t want it, add the argument se = FALSE, as we have done below. We have also altered the linewidth.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", linewidth=0.5,se=FALSE) + # try leaving out the se argument\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\") +\n   scale_colour_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = c(0.2,0.8)) # also try legend.position = \"top\", \"left\" etc\n\n\n\n\n\n\n\n\n\n\n2.7.2 Repeat for bill length and flipper length\nModify the code of the previous plot so that you now plot bill length vs flipper length. Adjust any labels and titles as necessary. This time, put the legend in the bottom right of the plot.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=bill_length_mm,colour=species)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", linewidth=0.5,se=FALSE) + # try leaving out the se argument\n  labs(x = \"Flipper length (mm)\",\n       y = \"Bill length (mm)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and bill length for Adelie, Chinstrap, and Gentoo Penguins\") +\n   scale_colour_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = c(0.9,0.2)) # play with the values to get it where you want it\n\n\n\n\n\n\n\n\nDo you see how straightforwrd it is to adapt the code that produces one plot to get the code you need for another, similar plot?\n\n\n2.7.3 Add yet more informtion to the plot\nLet us include the information of which island the penguins come from by making the shape of the plotted points be dependent on that:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=bill_length_mm,colour=species,shape=island)) +\n  geom_point() +\n  #geom_smooth(method=\"lm\", linewidth=0.5,se=FALSE) + # try leaving out the se argument\n  labs(x = \"Flipper length (mm)\",\n       y = \"Bill length (mm)\",\n       colour= \"Species\",\n       shape=\"Island\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and bill length for Adelie, Chinstrap, and Gentoo Penguins\") +\n   scale_colour_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"right\") # play with the position to get it where you want it. Try \"top\" etc.\n\n\n\n\n\n\n\n\n\n\n2.7.4 Distribution of penguin flipper lengths\nThe distribution of a data set is often a useful thing to know. Around which value are the data grouped, how widely spread are they and are the values symmetrically or asymmetrically distributed around the central value? A number of plot types can show this for us. Here we illustrate histograms, density plots, box plots, violin plots and ridge plots.\n\n2.7.4.1 Histogram\nFirst, let’s do a basic histogram. For this we use geom_histogram(). In the ggplot line, in the aes() argument, we need only specify the variable that maps to x, since the software will count how many observations lie within specific narrow ranges of the variable, called bins. Those bin counts will be the y variable of the histogram. To find the distribution of flipper length, we use flipper_length_mmm as the x variable. So we could try\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm)) +  # why is y not specified?\n  geom_histogram()\n\n\n\n\n\n\n\n\nBut this isn’t useful. The histograms for the three species overlap each other, so we need to give each one a different colour, and we need to reduce the opacity of the bars so that the histograms behind are not obscured by the ones in front, where they overlap. Further, we need to stop ggplot from stacking the different histogram bars on top of each other where those for different species are in the same bin. Annoyingly, that is what it does by default, which makes seeing the individual distributions clearly much more difficult.\nAnother thing with histograms, something that can make them a fiddle to use, is that their usefulness in revealing a distrivution is affect by how wide the bins are. By default, ggplot chooses the bin width such that you get 30 bins altogether. This may not be optimal. Here, let’s try specifying the bin width to 4 mm (but see what happens when you try other values, especially very large and very small values).\nThis we can achieve by:\n\nincuding fill = species in the aes() argument of ggplot.\nsepcifying position = identity as an argument of geom_histogram(), to stop the stacking.\nspecifying the opacity argument alpha to be a value less than 1. Here we try alpha = 0.4` - but try other values in the range 0 (transparent) - 1 (opaque), to reduce the opacity.\nspecifying binwidth = 4 - try other values\n\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm, fill = species)) +  # why is y not specified?\n  geom_histogram(position = \"identity\", alpha = 0.4, binwidth = 4)\n\n\n\n\n\n\n\n\nSo, a lot going on, but still only three lines of code!\nNow add good axis labels, an overall theme, and choose a colour scheme you like, and the legend position, just as you have done before:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,fill=species)) +\n  geom_histogram(position=\"identity\",alpha = 0.4, binwidth = 4) +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", # specifies the legend title. See what happens if you omit this line.\n       title=\"Penguin flipper lengths\") + # we wouldn't use this for a figure going in a report.\n  scale_fill_brewer(palette=\"Set2\") + \n  theme_bw() +\n  theme(legend.position = c(0.9,0.8)) # play with the position to get it where you want it\n\n\n\n\n\n\n\n\nIn the scatter plot and the histogram, we have used colour to distinguish the different species. We can do this because our data set is tidy: there is just one column that species the species, and the same for every other variable. That same feature of the data enables to use another way to represent the different species: facet_wrap(~species). This gives us three separate plots, side by side or one above the other. See it used here:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,fill=species)) +\n  geom_histogram(position=\"identity\",alpha = 0.4, binwidth = 4) +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", \n       title=\"Penguin flipper lengths\") + \n  facet_wrap(~species) + #try adding the argument ncol = 1.\n  scale_fill_brewer(palette=\"Set2\") + # try other palettes, eg \"Set1\".\n  theme_bw() +\n  theme(legend.position=\"none\") # we don't need a legend!\n\n\n\n\n\n\n\n\nJust a thought, but do the colours here serve any useful purpose? What extra information do they convey? If you ever think that a feature of a graph conveys no additional information, consider omitting it. Here is the figue before without colours, but going for white brs with grey outlines:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm)) +\n  geom_histogram(position=\"identity\",alpha = 0.4, binwidth = 4, fill=\"white\",colour=\"grey50\") +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", # specifies the legend title. See what happens if you omit this line.\n       title=\"Penguin flipper lengths\") + # we wouldn't use this for a figure going in a report.\n  facet_wrap(~species) + #try adding the argument ncol = 1.\n  theme_bw() +\n  theme(legend.position=\"none\") # we don't need a legend!\n\n\n\n\n\n\n\n\nArguably, this is a better plot than the previous one because it excludes the potentially confusing redundancy of using different colours each species, when we already know which species is the subject of each plot.\nIf you don’t like white as the fill colour, try another one, for exampe this one that I found on Cynthia Brewer’s very useful map colour site: https://colorbrewer2.org\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm)) +\n  geom_histogram(position=\"identity\",alpha = 0.4, binwidth = 4, fill=\"#a6bddb\",colour=\"grey50\") +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", # specifies the legend title. See what happens if you omit this line.\n       title=\"Penguin flipper lengths\") + # we wouldn't use this for a figure going in a report.\n  facet_grid(island~species) + #try adding the argument ncol = 1.\n  theme_bw() +\n  theme(legend.position=\"none\") # we don't need a legend!\n\n\n\n\n\n\n\n\nDifferent fill colours would be useful if the different penguin species had distinctive dominant colours, but that isn’t the case!\n\n\n2.7.4.2 Density plot\nAn alternative to a histogram, the density plot, gives us a smoothed version of the histogram. The vertical axis on these is not a count, but a measure of the concentration of the data.\nHere is one with overlapping density plots\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,fill=species)) +\n  geom_density(alpha=0.2) +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Density\",\n       fill= \"Species\",\n       title=\"Penguin flipper lengths\") +\n  scale_fill_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"right\") # play with the position to get it where you want it\n\n\n\n\n\n\n\n\nWe can also adapt this and do what was done for the histograms and do a set of three, one for each species, using facet_wrap():\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm)) +\n  geom_density(alpha = 0.2, fill=\"#a6bddb\",colour=\"grey50\") +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", # specifies the legend title. See what happens if you omit this line.\n       title=\"Penguin flipper lengths\") + # we wouldn't use this for a figure going in a report.\n  facet_wrap(~species) + #try adding the argument ncol = 1.\n  theme_bw() +\n  theme(legend.position=\"none\") # we don't need a legend!\n\n\n\n\n\n\n\n\nWhich is more useful in this case: the overlapping plots on one chart, or the separate charts done using facet_wrap()? Whatever you think here, the answer in other cases will sometimes be one, sometimes the other. Now you have the tools to enable you to try both and make the best choice.\n\n\n2.7.4.3 Box plots\nBox plots are a really useful way to summarize the distribution of numerical response data such as flipper_length_mm across different categorical variables, such as species. We use geom_boxplot() to produce them.\nLet’s do a basic box plot of flipper lengths for each penguin species:\n\npenguins |&gt;\n  ggplot(aes(x=species,y=flipper_length_mm,fill=species)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nNow let’s use what we have done before to add code lines that\n\ninclude suitable axis labels and a title\ngive the same ‘theme’ ie overall look as the previous graphs\nfill the boxes with the same colour for each species.\nremove the legend that you now have, because you don’t need it (Why?). Use theme(legend.position=\"none\") to do this.\n\n\npenguins |&gt;\n  ggplot(aes(x=species,y=flipper_length_mm,fill=species)) +\n  geom_boxplot() +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\",\n       title=\"Penguin flipper lengths\") +\n  scale_fill_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"none\") # no legend needed\n\n\n\n\n\n\n\n\n\n\n2.7.4.4 Violin Plot using geom_violin()\nThis is a variant on the box plot. Each ‘violin’ is a sideways density plot of the distribution of the data for each species, with its own mirror image to make it look a bit like a violin. The code for these is exactly as for box plots except we use geom_violin().\n\npenguins |&gt;\n  ggplot(aes(x=species,y=flipper_length_mm,fill=species)) +\n  geom_violin() \n\n\n\n\n\n\n\n\nNow we write code to improve this, just as you did the box plot. The final code is the same as for that apart from one line!\n\npenguins |&gt;\n  ggplot(aes(x=species,y=flipper_length_mm,fill=species)) +\n  geom_violin() +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\",\n       title=\"Penguin flipper lengths\") +\n  scale_fill_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"none\") # no legend needed\n\n\n\n\n\n\n\n\n\n\n2.7.4.5 Ridge plot\nThis is a variant on the density plot, that is most useful when you have lots of categorical variables. We have only three here, the three penguin species, but let’s try it anyway.\nFor this, we need the ggridges package. This is one of many packages that extend the power of ggplot, and so work in much the same way:\n\n# library\n#install.packages(\"ggridges\") # use this once, if you have to, then comment it out.\nlibrary(ggridges) \n \n# basic example\npenguins |&gt;\nggplot(aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Species\",\n       title=\"Penguin flipper lengths\") +\n  theme_ridges() + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nNow try producing graphs like the ones above, but for body mass rather than flipper length.\n\n\n\n2.7.5 Bar chart with error bar\nThere are different ways to produce this commonly used way to summarise data. For example we might use one to compare the mean flipper lengths of the different penguin species. For a bar chart of these to be of any use at all, it needs to include error bars that show standard deviations of the samples, standard errors of the means, or confidence intervals (Why?). Which you use depends on the story you are trying to tell.\nFirst, we will add error bars that are ± one standard deviation of the samples.\nI usually first create a summary of the data, in which we calculate the means and appropriate error for each species for whichever variable I am interested in, then feed this summary table to ggplot and use geom_col() to plot the bars, with geom_errorbar() on top of that to plot the error bars.\nLet’s do that first:\n\nflipper_summary &lt;- penguins |&gt;\n  drop_na() |&gt;\n  # these two lines produce a summary table\n  group_by(species) |&gt;\n  summarise(fl.mean = mean(flipper_length_mm), fl.sd = sd(flipper_length_mm))\nflipper_summary\n\n# A tibble: 3 × 3\n  species   fl.mean fl.sd\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie       190.  6.52\n2 Chinstrap    196.  7.13\n3 Gentoo       217.  6.59\n\n\n\nflipper_summary |&gt;\n  ggplot(aes(x = species, y = fl.mean)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = fl.mean-fl.sd, ymax = fl.mean + fl.sd), width = 0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  theme_bw() \n\n\n\n\n\n\n\n\n\n2.7.5.1 Standard deviation or standard error?\nThe error bars in the plot above are ± one standard deviation. A standard deviation gives us an idea of the spread of values within a sample or population. Remember that if a population is normally distributed about its mean, there is a roughly 95% probability that a random chosen individual will lie within two standard deviations of the mean.\nSo standard deviations of samples are useful. They are a useful way to summarise the variability of the sample, they tell us about the likely variability of the next sample, and they are our best estimate of the variability of the population from which the sample was drawn.\nSometimes, though, we want to know more than that. We might want to know how closely a sample mean is likely o be to the true mean of the population from which the sample was drawn, and perhaps to get an idea as to whether two or more populations are different, given the means of samples drawn from those populations. For this, we need not the standard deviation but the standard error. These are the error bars that are most commonly displayed on bar charts when you see them in papers.\nTo calculate standard deviation error bar lengths we use a formula \\(\\text{SE} = \\frac{\\text{SD}}{\\sqrt{n}}\\) where \\(n\\) is the number of observations, SD is the standard deviation of the sample and SE is the standard error of the means of the sample. We can use the summary functions sd() to calculate the standard deviation, and n() to calculate \\(n\\).\n\nflipper_summary2 &lt;- penguins |&gt;\n  drop_na() |&gt;\n  # these two lines produce a summary table\n  group_by(species) |&gt;\n  summarise(fl.mean = mean(flipper_length_mm), fl.se = sd(flipper_length_mm)/sqrt(n()))\nflipper_summary\n\n# A tibble: 3 × 3\n  species   fl.mean fl.sd\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie       190.  6.52\n2 Chinstrap    196.  7.13\n3 Gentoo       217.  6.59\n\n\n\nflipper_summary2 |&gt;\n  ggplot(aes(x = species, y = fl.mean)) +\n  geom_col() +\n  geom_errorbar(aes(ymin=fl.mean-fl.se, ymax = fl.mean + fl.se),width=0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  theme_bw() \n\n\n\n\n\n\n\n\nThese standard error bars are always smaller than the standard deviation error bars (by a factor equal to the square root of the sample size). Here they are so small as to be barely visible. They ar useful in bar charts like this one since they give us a rough and ready way of assessing whether the differences between the samples (the heights of the bars) are likely to indicate real differences between the populations. If the bar-height differences are much greater than the size of the standard error bars, then they probably indicate significant differences between the popultions. If not, then they probably don’t.\nNow let’s alter this code so that each bar has a different fill colour, and remove the legend that then appears, since it is unnecessary?\n\nflipper_summary |&gt;\n  # we add an argument to colour each bar according to species\n  ggplot(aes(x = species, y = fl.mean, fill = species)) +\n  geom_col() +\n  geom_errorbar(aes(ymin=fl.mean-fl.sd, ymax = fl.mean + fl.sd),width=0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  theme_bw() +\n  # include an arument to remove the legend\n  theme()\n\n\n\n\n\n\n\n\nNow let us replace this colour scheme with nicer ones (not just nice, but also colour-blind friendly, perhaps) offered by the Brewer palettes.\nTo do this we can add the line scale_fill_brewer(palette = \"Set2\"). Note: we use scale_colour_brewer() to alter the colours of points, like we did above, or the outline colour of bars, and use scale_fill_brewer() to alter the fill colour of bars. This is what we want to do here.\n\nflipper_summary |&gt;\n  ggplot(aes(x = species, y = fl.mean, fill = species)) +\n  geom_col() +\n  geom_errorbar(aes(ymin=fl.mean-fl.sd, ymax = fl.mean + fl.sd), width=0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  scale_fill_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nIf you don’t like the colours of the palette “Set2” you can try another one. To find out what palettes are available, remember, you can type ?scale_fill_brewer() into the console pane then look at the help that appears in the Help pane (bottom right), and scroll down to the Palettes section.\nIf you agree that having different fill colours for the bars is actually confusing and brings no information to the plot that we do not already know, you can modify the previous plot in the manner that you did for the separate histograms:\n\nflipper_summary |&gt;\n  ggplot(aes(x = species, y = fl.mean)) + \n  # add arguments here that give fill colour \"#a6bddb\" and outline colour \"grey50\".\n  geom_col(fill = \"#a6bddb\", colour = \"grey50\") +\n  geom_errorbar(aes(ymin=fl.mean-fl.sd, ymax = fl.mean + fl.sd), width=0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "titles_labels_and_annotations.html",
    "href": "titles_labels_and_annotations.html",
    "title": "3  Titles, labels and annotations",
    "section": "",
    "text": "3.1 Use bquote()\nOften in plots one needs to use mathematical expressions, suffixes/superscripts, greeek letters or other unusual formatting.\nHere we show one way of doing this, using bquote(). Over the years I have found this to be the simplest way.\nThe four rules\nExamples of all of these are shown in the labels, title and annotations included in the plot below:\ndf &lt;- tibble(x=1:10,y=1:10)\ncor &lt;- 0.456\ndf |&gt;\n  ggplot(aes(x=x,y=y)) +\n  geom_point() +\n  labs( x = bquote(\"An axis label with suffix and superscript:\" ~ x[i]^2), \n        y = bquote(\"An axis label with greek letters:\" ~ alpha ~ Beta ~ mu ~ m),\n        title = bquote(\"Hello\"~ r[xy] == .(cor) ~ \"and\" ~ CO[2] ~ \"and\" ~ B^2 ~ \"and\" ~ m^{-1})) +\n  annotate(geom=\"text\", x = 5, y = 7, label =  deparse(bquote(\"Hello\" ~ r[xy] == 0.678 ~ \"and\" ~ B^2)), parse = TRUE) +\n  annotate(geom=\"text\", x = 7, y = 9, label =  bquote(\"A big annotation\"), size = 12) +\n  annotate(geom=\"text\", x = 7, y = 2, label =  bquote(\"A red annotation\"), colour = \"red\") +\n  theme_classic()",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Titles, labels and annotations</span>"
    ]
  },
  {
    "objectID": "titles_labels_and_annotations.html#use-bquote",
    "href": "titles_labels_and_annotations.html#use-bquote",
    "title": "3  Titles, labels and annotations",
    "section": "",
    "text": "Strings – Require quotes wrapped w/ tilde separator (e.g., “my text” ~).\nMath Expressions – Unquoted & follow ?plotmath\nNumbers – Unquoted when part of math notation\nVariables – Use .() (pass in string or numeric)",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Titles, labels and annotations</span>"
    ]
  },
  {
    "objectID": "titles_labels_and_annotations.html#plotmath",
    "href": "titles_labels_and_annotations.html#plotmath",
    "title": "3  Titles, labels and annotations",
    "section": "3.2 ?plotmath",
    "text": "3.2 ?plotmath\nplotmath expressions can be used for mathematical annotation in text within plots in R when writing titles, subtitles, axis labels, legends and annotations. It works in both base R graphics and ggplot2.\nIn the console pane, type ?plotmath in the console pane to see the full list of options.\nSome key rules are:\n\nsubscripts: O[2] gives O2\nsuperscripts: m^2 gives m2, m^{-1} gives m-1\nLower case Greek: alpha, beta etc gives \\(\\alpha, \\beta\\) etc, so mu\nUpper case Greek: Delta, Gamma etc gives \\(\\Delta, \\Gamma\\) etc",
    "crumbs": [
      "Working with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Titles, labels and annotations</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html",
    "href": "tests_for_difference_two_levels.html",
    "title": "4  Tests for difference: one factor, two levels",
    "section": "",
    "text": "4.1 Test finder flow chart\nIn this document we consider tests for difference where there are two levels being compared.\nFirst, use this flowchart to see if a two-sample t-test or its non-parametric counterpart the Mann-Whitney-U test is appropriate for your question, your study design and your data:\nflowchart TD\n  A{Difference or Trend \\nQuestion?} --&gt; B[Trend]\n  B --&gt; B2{Are you testing for \\ndegree of association\\nor are you trying \\nto make predictions?}\n  B2 --&gt; B33[Association:\\n\\nTest for\\n correlation coefficient\\nPearson or \\n Spearman's Rank]\n  B2 --&gt; B44[Predictions:\\n\\nSimple linear regression]\n  A --&gt; C[Difference]\n  C --&gt; C22{Do you have replicates?}\n  C22 --&gt; C22Y[Yes]\n  C22 --&gt; C22N[No]\n  C22N --&gt; C23{Do you have count data?}\n  C23 --&gt; C23Y[Yes]\n  C23 --&gt; C23N[N0]\n  C23Y --&gt; C24(Chi square test:\\nGoodness of fit\\nor\\nTest of independence)\n  C23N --&gt; C25[These data cannot be analysed]\n  C22Y --&gt; D{How many factors?}\n  D --&gt; F[Two or more]\n  F --&gt; F2{Independent\\nsamples?}\n  F2 --&gt; F2Y[Yes]\n  F2 --&gt; F2N[No]\n  F2Y --&gt; F22Y(n-way ANOVA)\n  F2N --&gt; F22N(n-way\\nrepeated measures\\nANOVA)\n  D --&gt; E[One]\n  E --&gt; G{How many levels?}\n  G --&gt; H[Two]\n  G --&gt; I[More than two]\n  I --&gt; J{Independent\\n samples?}\n  J --&gt; K[No]\n  J --&gt; L[Yes]\n  K --&gt; S(Repeated measures one-way ANOVA\\nor\\nFriedman Test)\n  L --&gt; T(One way ANOVA\\nor\\nKruskal Wallis one-way test)\n  H --&gt; M{Independent\\n samples?}\n  M --&gt; N[No]\n  M --&gt; O[Yes]\n  N --&gt; P(paired t-test\\nor\\nSigned rank test)\n  O --&gt; Q(two sample t-test\\nor\\nMatt Whitney U test)\nIf this chart suggests you need something other than the two sample t-test or Mann-Whitney test, you need to go to the descriptions of that test.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#two-sample-t-test-the-parametric-case",
    "href": "tests_for_difference_two_levels.html#two-sample-t-test-the-parametric-case",
    "title": "4  Tests for difference: one factor, two levels",
    "section": "4.2 Two sample t-test: the parametric case",
    "text": "4.2 Two sample t-test: the parametric case\nIn this exercise we find out how to run a two-sample t-test, to determine whether there is evidence to reject the hypothesis that two samples are drawn from the same population.\n\n4.2.1 When to use the two-sample t-test\n\nIt can be used when we have two independent samples of numerical (not ordinal) response data, and our question is whether the data provide evidence that the samples are drawn from different populations. Even when this criterion is met and the data are numerical and independent, the normality criterion described below still needs to be met. That apart, two common examples where we have two sets of data but we should not use the two sample t-test are:\n\nIf the data in your two samples are not independent because you have measured the same individual replicate before and after some event or treatment, then you should probably be using a paired t-test instead. In this you don’t have two samples, each comprising a separate and independent set of replicates. Instead, you have multiple pairs of values, one pair per replicate. The replicates are still assumed to be independent of each other\nIf your response data are the answers to a Likert scale such as might be used in a survey then they are ordinal in nature and not numerical, and you should probably be using the non-parametric equivalent of the two sample t-test, which is variously known as the Wilcoxon Rank Sum test or as the Mann Whitney U test, or its paired sample version, if appropriate.\n\nIt can be used when the data set is small. But not so small that there are no replicates. You do need replicates.\nIt can still be used when the data set is large.\nIt assumes that the data are drawn from a normally distributed population. There are various ways to test if this is plausibly the case, and you should try at least one of them, but with small samples, just where the t-test is most useful, it can be difficult to tell. In the end we can also appeal to reason: is there good reason to suppose that the data would or would not be normally distributed?\nWhen comparing the means of two samples, both samples should in principle have the same variance, which is a measure of the spread of the data, so in principle you need to check that this is at least approximately the case, or have reason to suppose that it should be. However, in an actual t-test done using R, the Welch variant of the t-test is carried out by default. This works even when the variances of the two sets are different, so in practice it is possible to ignore this equal variance requirement.\nWe only use it when we are comparing two samples, one for each of the two levels of a single factor. When we have samples for more than two levels and we use the t-test to look for a difference between any two of them, it becomes increasingly likely, the more pairs of samples we compare, that we will decide that we have found a difference because we got a p-value that was less than some pre-determined threshold (which could be anything, but is most often chosen to be 0.05) even if in reality there is none. This is the problem of high false positive rates arising from multiple pairwise testing and is where ANOVA comes in. t-tests are only used to detect evidence for a difference between two groups, not more. ANOVAs (or their non-parametric equivalent) are used when we are looking for differences between more than two groups.\n\n\n\n4.2.2 Motivation and example\nIn our example we will consider the impact of pesticide use on the masses of shells of garden snails (Cornu aspersum), as measured in gardens around a city, ten from randomly selected gardens that have used a range of pesticides for at least two years and ten that are from randomly selected gardens that have not ever used pesticides. We leave aside here the issue of how those gardens were identified and how randomisation was ensured.\n\n\n4.2.3 Questions and hypotheses\nOur question is:\nIs there evidence for a difference between snail shell masses in the gardens where pesticides were used compared to those where they were not used?\nFrom which a suitable null hypothesis is:\nThere is no difference between shell masses in the gardens, whether or not pesticides were used.\nand a suitable alternate, two-sided hypothesis is:\nThere is a difference between shell masses.\n\n\n4.2.4 The data\nSuppose we had our data arranged in a spreadsheet in three columns, one giving the garden ID, G1 to G20, one telling us whether pesticides were used in the garden, yes or no, and one telling us the masses in grams of the snail shells from each garden. Afficioados of R will see that this is ‘tidy’ data. Each variable (ID, pesticide use, shell mass) occurs in only one column, rather than being spread across several. It turns out that this way of storing your data makes it much easier to analyse.\n\n# there should be a 'garden_snails.csv' file in your data folder\n\nfilepath&lt;-here(\"data\",\"garden_snails.csv\")\nsnails&lt;-read_csv(filepath)\n\n# if not, you should be able to get it from Mike's github repo\n\n# file_url &lt;- \"https://raw.githubusercontent.com/mbh038/r-workshop/refs/heads/gh-pages/data/garden_snails.csv\"\n# snails&lt;-read_csv(file_url)\nhead(snails,20)\n\n# A tibble: 20 × 3\n   garden.ID pesticide shell_mass_g\n   &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt;\n 1 S1        Yes               1.37\n 2 S2        Yes               1.15\n 3 S3        Yes               0.73\n 4 S4        Yes               0.65\n 5 S5        Yes               1.03\n 6 S6        Yes               1.8 \n 7 S7        Yes               1.21\n 8 S8        Yes               1.41\n 9 S9        Yes               1.27\n10 S10       Yes               1.08\n11 S11       No                2   \n12 S12       No                3.99\n13 S13       No                1.99\n14 S14       No                1.75\n15 S15       No                2.81\n16 S16       No                2.15\n17 S17       No                1.87\n18 S18       No                3.46\n19 S19       No                2.8 \n20 S20       No                2.89\n\n\n\nIs this a tidy data set?\nIs the data in the pesticide column categorical?\nIf so, how many levels does it have and what are they?",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#the-process",
    "href": "tests_for_difference_two_levels.html#the-process",
    "title": "4  Tests for difference: one factor, two levels",
    "section": "4.3 The Process",
    "text": "4.3 The Process\n\n4.3.1 Step One: Summarise the data\nWith numerical data spread across more than one level of a categorical variable, we often want summary information such as mean values and standard errors of the mean for each level.\nHere we will calculate the number of replicates, the mean and the standard error of the mean for both levels of pesticide ie Yes and No:\n\nsnail.summary&lt;- snails |&gt;\ngroup_by(pesticide) |&gt;\nsummarise(n = n(),\n          mean.mass = mean(shell_mass_g),\n          se.mass = sd(shell_mass_g)/sqrt(n()))\nsnail.summary\n\n# A tibble: 2 × 4\n  pesticide     n mean.mass se.mass\n  &lt;chr&gt;     &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 No           10      2.57   0.236\n2 Yes          10      1.17   0.105\n\n\nFrom these data, does it look as though there is evidence for a difference between shell masses in the two types of garden? Clearly, the snails in the ten gardens that did not use pesticide had a higher mean shell mass than the ten from gardens that did use pesticide. But is this a fluke? How precisely do we think these sample means reflect the truth about the impact of the use of pesticides? That is what the standard error column tells us. You can think of the standard error as being an estimate of how far our sample means, drawn from just ten gardens of each type are likely to differ from the true shell masses for all gardens that did use pesticides and all gardens that did not.\nBottom line: the difference between the sample means is about ten times the size of the standard errors of each. It really does look as though snails shells in gardens where pesticides are not used are indeed heavier than in gardens where pesticides are used.\n\n\n4.3.2 Step Two: Plot the data\nRemember, before we do any statistical analysis, it is almost always a good idea to plot the data in some way. We can often get a very good idea as to the answer to our research question just from the plots we do.\nIn Figure 4.1, we will use ggplot() in R to plot a histogram of ozone levels, one for each side of the city. We will stack the histograms one above the other, all the better to help us spot any differences between east and west.\n\nsnails |&gt;\n  ggplot(aes(x=shell_mass_g)) +\n  geom_histogram(binwidth=0.2,fill=\"darkred\")+\n  facet_wrap(~pesticide,ncol=1) +\n  theme_classic()\n\n\n\n\n\n\n\nFigure 4.1: Stacked histograms\n\n\n\n\n\nInstead of histograms, we could have drawn box plots, as in:\n\nsnails |&gt;\n  ggplot(aes(x=pesticide,y=shell_mass_g))+\n  geom_boxplot()+\n  labs(x=\"Pestice use?\",\n       y=\"Shell mass (g)\") +\n  theme_classic()\n\n\n\n\n\n\n\nFigure 4.2: Side-byside box and whisker plots of the distribution of values in each snail sample. The lower and upper edges of each box show the 25th and 75th percentiles of each sample, and the thick black line between them shows the median value ie the 50th percentile\n\n\n\n\n\nor as a dot plot of the means with standard errors of the mean included as error bars, as in Figure 4.3\n\n# for this chart we will use the summary table that we created above.\n\nsnail.summary |&gt; \n  ggplot(aes(x=pesticide,y=mean.mass))+\n  geom_point(size=3) +\n  geom_errorbar(aes(ymin=mean.mass-se.mass,ymax=mean.mass+se.mass),width=0.1)+\n  ylim(0,4) + # try leaving this line out. What happens? Which is better?\n  labs(x=\"Pesticide use?\",\n       y=\"Shell mass (g)\") +\n  theme_classic()\n\n\n\n\n\n\n\nFigure 4.3: The data points show mean values, the error bars show plus or minus one standard error of the mean\n\n\n\n\n\n\nDo the data look as though they are inconsistent with the null hypothesis ?\nIn addition, do the data look as though each group is drawn from a normally distributed population? One of the types of graphs gives you no indication of that while the other two do. Which is the odd one out? Even when looking at the other two figures, when there are so few data it’s kind of hard to tell, no?\n\nLet’s now do some stats.\n\n\n4.3.3 Step Three: Check the validity of the data - are the data normally distributed?\nWe can go about establishing this in three ways: using an analytical test of normality, using a graphical method and by thinking about what kind of data we have. Let’s consider these in turn.\n\n4.3.3.1 Normality test - analytical method\nThere are several analytical tests one can run on a set of data to determine if it is plausible that it has been drawn from a normally distributed population. One is the Shapiro-Wilk test.\nFor more information on the Shapiro-Wilk test in R, type ?shapiro.test into the console window. For kicks, try it out on the examples that appear in the help window (which is the bottom right pane, Help tab). One example is testing a sample of data that explicitly is drawn from a normal distribution, the other tests a sample of data that definitely is not. What p-value do you get in each case? How closely do the histograms of each sample resemble a normal distribution?\n\n#first we create a data frame containing the two example data sets\nexample1&lt;-rnorm(500, mean = 5, sd = 3) # first example from the help pane\nexample2&lt;-runif(500, min = 2, max = 4) # second example from the help pane\n\ndf&lt;-tibble(data=c(example1,example2), distribution=c(rep(\"example 1: normal\",500),rep(\"example 2: not at all normal\",500)))\n\n# then we plot a histogram of each data set\nggplot(df,aes(x=data)) +\n  geom_histogram(bins=20,fill=\"cornflowerblue\") +\n  facet_wrap(~distribution) +\n  theme_classic()\n\n\n\n\n\n\n\n# and finally we run a Shapiro-Wilk normality test on each data set\nshapiro.test(example1) # 100 samples drawn from a normally distributed population\n\n\n    Shapiro-Wilk normality test\n\ndata:  example1\nW = 0.99803, p-value = 0.8379\n\nshapiro.test(example2) # 100 samples drawn from a uniformly (ie NOT normally) distributed population\n\n\n    Shapiro-Wilk normality test\n\ndata:  example2\nW = 0.95654, p-value = 5.666e-11\n\n\nFor the examples above, we see that Shapiro-Wilk test gave a high p-value for the data that we knew were drawn from a normal distribution, and a very low p-value for the data that we knew were not.\nThe Shapiro-Wilk test tests your data against the null hypothesis that it is drawn from a normally distributed population. It gives a p-value which, as always, is the probably of you having data as far from normality, or further, as yours are if the null hypothesis were true. If the p-value is less than 0.05 then we reject the null hypothesis and cannot suppose our data is drawn froma normally distributed population. In that case we would have to ditch the t-test for a difference, and choose another difference test in its place that could cope with data that was not normally distributed. For a two-sample t-test such as we are hoping to use here, the so-called non-parametric alternative that we could use instead is the Wilcoxon Rank Sum test, often called the Mann-Whitney U test.\nWhy don’t we do that in the first place, I hear you ask? Why bother with this finicky t-test that requires that we go through the faff of testing the data for normality before we can use it? The answer is that it is more powerful than other, so-called non-parametric tests that can cope with non-normal data. It is more likely than they are to spot a difference if there really is a difference. So if we can use it, that is what we would rather do.\nSo, onwards, let’s do the Shapiro-Wilk test on our data\nWe want to test each garden group for normality, so we group the data by location as before and and then summarise, this time asking for the p-value returned by the Shapiro-Wilk test of normality.\n\nsnails |&gt;\n  group_by(pesticide) |&gt;\n  summarise('Shapiro-Wilk p-value'=shapiro.test(shell_mass_g)$p.value)\n\n# A tibble: 2 × 2\n  pesticide `Shapiro-Wilk p-value`\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 No                         0.223\n2 Yes                        0.854\n\n\nFor both groups the p-value is more than 0.05, so at the 5% significance level we cannot reject the null hypothesis that the data are normally distributed, so we can go on and use the t-test. Yay!\n\n\n4.3.3.2 Graphical methods - the quantile-quantile or QQ plot.\nConfession: I don’t normally bother with numerical tests for normality such as Shapiro-Wilk. I usually use a graphical method instead.\nWe have already seen two ways of plotting the data that might help suggest whether it is plausible that the data are drawn from normally distributed populations. Histograms and box plots both indicate how data is distributed, and for normally distributed data both would be symmetrical. Well, they would be, more or less, if the data set was large enough but for small data sets it can be quite hard to tell from either type of plot whether the data are drawn from a normally distributed population.\nA better type of plot for making this judgement call is the quantile-quantile or ‘qq’ plot which basically compares the distribution of your data to that of a normal distribution. If your data are approximately normally distributed then a qq plot will give a straight(-ish) line. Even with small data sets, this is usually easy to spot.\n\nsnails |&gt;\n  ggplot(aes(sample=shell_mass_g)) +\n  stat_qq(colour=\"blue\") +\n  stat_qq_line() +\n  facet_wrap(~pesticide) +\n  theme_classic()\n\n\n\n\n\n\n\n\nNothing outrageously non-linear there, so that also suggests we can safely use the t-test.\nFor an overview of how normally distributed and non-normally distributed data looks when plotted in histograms, box plots and quantile-quantile plots, see this review\n\n\n4.3.3.3 The ‘thinking about the data’ normality test\nAs you might have guessed, this isn’t a test as such, but a suggestion that you think about what kind of data you have: is it likely to be normally distributed within its subgroups or not? If the data are numerical values of some physical quantity that is the result of many independent processes, and if the data are not bounded on either side (say by 0 and 100 as for exam scores) then it is quite likely that that they are. If they are count data, or ordinal data, then it is quite likely that they are not.\nThis way of thinking may be all you can do when data sets are very small and any of the more robust tests for normality presented here leave you not much the wiser.\n\n\n\n4.3.4 Do the actual two-sample t-test\nSo, it looks as though it is plausible that the data are drawn from normal distributions. That means we can go on to use a parametric test such as a two sample t-test and have confidence in its output.\nIf we were doing this in R we could use the t.test() function for this (other functions are available!). This needs to be given a formula and a data set as arguments. Look up t.test() in R’s help documentation, and see if you can get the t-test to tell you whether there is a significant difference between ozone levels in the east and in the west of the city.\n\nt.test(shell_mass_g~pesticide,data=snails)\n\n\n    Welch Two Sample t-test\n\ndata:  shell_mass_g by pesticide\nt = 5.4172, df = 12.442, p-value = 0.0001372\nalternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n95 percent confidence interval:\n 0.8397234 1.9622766\nsample estimates:\n mean in group No mean in group Yes \n            2.571             1.170 \n\n\n\n\n4.3.5 Interpret the output of the t-test.\nStudy the output of the t-test. Here are some questions to ask yourself.\n\nWhat kind of test was carried out?\n\nA Welch two sample t-test\n\nWhat data was used for the test?\n\nThe snail shell mass in g (the output variable) and pesticide use (the explanatory variable)\n\nWhat is the test statistic of the data?\n\nThis is t = 5.4172.\n\nHow many degrees of freedom were there? This number is the number of independent pices of information that were used to calculate the final result. It is usually one, two, or three or so less than the number of data points. Don’t overthink it at this stage, especially not the fact that here it is not an integer.\n\ndf = 12.442\n\nWhat is the p-value?\n\np = 0.0001372. You would most likely report this as p &lt; 0.001\n\nWhat does the p value mean?\n\nIt is the likelihood of seeing a difference between sample means as large or larger than the one we found if in fact pesticides made no difference to snail shell mass.\n\nWhat is the confidence interval for the difference between shell masses in gardens that use pesticides and in gardens that do not? Does it encompass zero? Remember that the confidence interval gives the range of values within which the true difference between mean shell masses might reasonably lie, given the data. If that range includes zero then the test is telling is that zero is a plausible value for the difference, and hence that we cannot reject the null hypothesis.\n\nThe 95% confidence interval has lower bound 0.8397 and upper bound 1.962.\n\nIs there sufficient evidence to reject the null hypothesis?\n\nYes. We see this in two ways. First the p value is much less than 0.05 and second, the 95% confidence interval does not encompass zero. In a way, the confidence interval is giving us more information than the p-value, since not only can we deduce whether there is evidence for a significant difference, we can also see how big that difference is and how precisely we know it.\n\nWhat does the word ‘Welch’ tell you - Google it or look it up in the help for t.test().\n\nIt tells us that a variant of the t-test is being used in which it does not matter if the two. samples have difference variances (spreads).",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#other-examples-where-a-two-sample-t-test-might-be-used.",
    "href": "tests_for_difference_two_levels.html#other-examples-where-a-two-sample-t-test-might-be-used.",
    "title": "4  Tests for difference: one factor, two levels",
    "section": "4.4 Other examples where a two sample t-test might be used.",
    "text": "4.4 Other examples where a two sample t-test might be used.\nRemember that t-tests in general are used when you have independent samples with multiple replicates drawn from populations corresponding to two levels of some factor (eg north coast, south coast; this beach, that beach; polluted place, clean place etc) and you have measured something numerical, like a length or a mass, temperature or concentration. You still have to do the tests for normality described above, but these are the basic criteria.\n\n4.4.1 Can you think of examples of where you might use a two-sample t-test?\nHere are a few suggestions:\n\nIs there a difference between the flight initiation distance of redstarts when confronted by dogs compared to when they are confronted by drones?\nIs the nitrate concentration of water in a river below a beaver dam different from the nitrate content above that dam?\n\nCan you think of another example?",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#what-if-i-cant-use-a-two-sample-t-test",
    "href": "tests_for_difference_two_levels.html#what-if-i-cant-use-a-two-sample-t-test",
    "title": "4  Tests for difference: one factor, two levels",
    "section": "4.5 What if I can’t use a two sample t-test?",
    "text": "4.5 What if I can’t use a two sample t-test?\n\nAssuming you have two independent samples, this might be because one or both sets failed the normality criterion, or your data are ordinal. In that case the likely alternative is the non-parametric equivalent of the t-test, variously known as the Wilcoxon Rank Sum test or the Mann Whitney U test.\nIf your data are in fact sets of paired values, for example because you measured some attribute of the same individuals before and after some treatment, or at two points in time, then you need to use the paired t-test.\nIf you only have one sample of replicates and want to compare its mean value to a threshold, then you use a one sample t-test. You might do this, for example, if you had collected sediment samples from an estuary, measured the concentration in those samples of some pollutant such as pathogens from sewage, or phosphates from farm runoff, and then wanted to see if the water was compliant with water quality thresholds as dictated by, say, the Water Framework Directive.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#the-non-parametric-case",
    "href": "tests_for_difference_two_levels.html#the-non-parametric-case",
    "title": "4  Tests for difference: one factor, two levels",
    "section": "4.6 The non-parametric case",
    "text": "4.6 The non-parametric case\nA common scenario is that we have two sets of measurements, and we want to see if there is evidence that they are drawn from different populations. For some data types we can use a t-test to do this, but for others we cannot.\nA t-test requires in particular that the two sets of data are normally distributed around their respective means. With ordinal data this makes no sense. The mean is undefined as a concept for such data.\nTo see this , reflect that for a collection \\(X\\) of numerical data, say, 5, 3, 3, 4, and 5 we would calculate the mean as:\n\\[\n\\bar{X} = \\frac{5+3+3+4+5}{5} = \\frac{20}{5}=4\n\\]\nBut trying doing the same to five responses of a Likert scale survey. Say the responses you had to five Likert items (individual questions) were “strongly disagree”, “strongly agree”, “mildly disagree”, “strongly disagree” and “don’t care either way”. If you tried to calculate a ‘mean’ response you would be attempting to add up all these responses and to divide the ‘sum’ by five, like this:\n\\[\\text{mean response}=\\frac{\\text{stongly disagree}+\\text{strongly agree}+\\text{mildly disagree}+\\text{stongly disagree}+\\text{don't care either way}}{5} = ?\n\\] This sum makes no sense, I hope you will agree. It makes no sense, not because we are using words to describe our responses, but because, as these are ordinal data, we do not know the size of the gaps between the different points on the scale. Is the difference in agreement between the lowest two, “strongly disagree” and “midly disagree” the same as the gap between the highest two, “mildly agree” and “strongly agree”? We don’t know, mainly because ‘agreement’ is not something that can be measured easily using something like a weighing machine. And if we don’t know, then we shouldn’t really be adding these responses up or dividing them by anything.\nNevertheless, ordinal data are very common, since they are typically what is generated by survey data, where for example repondents may answer a series of questions (‘items’), each with typically five possible responses, but maybe more or fewer, these responses being ordinal in the sense that there is a definite order to them. They might encompass responses like those above, say, or something similar like “very unhappy” to “very happy”. They are also common in clinical and veterinary practice where ordinal pain scores are widely used - patients being asked (if they are human) or assessed as to their level of pain on a scale of 1-10, for example. Note that even if the pain value is recorded as a number it is actually a label, that could just as well have been recorded as one of a series of letters, A, B, C etc or emojis, or any symbol you like. You can’t take the average of a set of faces!\nThus, formally, we need another kind of test for a difference. Broadly, we need to use some form of non-parametric test where we do not assume that the data has any form of distribution, and where, often, we do not use the actual values of the measurements in our dataset but instead use only their ranks. The smallest value would be given rank 1, the next rank 2 and so on.\nThere are many non-parametric tests out there. Here we will look at only one - the Wilcoxon Rank Sum Test, often referred to as a Mann-Whitney U test for a difference. We can use this for the scenario we have painted above, where we have two sets of data and we wish to know if these provide evidence that the populations from which the samples have been drawn are in fact different.\n\n4.6.1 Example\nThis example uses actual data gathered by a student at Newquay University Centre.\nThe student wished to assess peoples’ sense of wellbeing using two different sets of questions designed to assess this. The scales chosen were the Warwick–Edinburgh Mental Well-being Scale (WEMWBS) and the New Ecological Paradigm (NEP) Scale. The student wished in particular to determine whether this sense of well-being was affected by whether a person often and actively frequented the coast and made it and the sea a substantive part of their life in one way or another. ie to find out whether there was evidence to support the notion that it could be good for your mental wellbeing to be by the sea and to make it part of your life.\nEach scale used consists of 15 questions or ‘Likert items’, each of which is answered on a 5 point ordinal scale, where a score of 1 indicates lowest wellbeing and a score of 5 indicates highest wellbeing. Thus each respondent could score anything from 15 to 75.\nThe student got responses from 374 people, 86 of whom were not “marine” users, while the other 288 say that they were marine users. The total scores from each respondent were recorded for each type of survey and stored in the file wellness.csv which you should find on the module Moodle site / Teams page. Please put this file in the data folder of your R project.\n\n\n4.6.2 Script\nCode chunks for a script to carry out the analysis of this data are provided below. To use them you should create a new Quarto document using File/New File/Quarto document, from which you delete all the exemplar material below the yaml section at the top. The first few chunks of this script carry out the same old-same old that we see in script after script: load packages, load data, summarise data , plot data. Copy and paste any chunks you want to use into your own script then adapt them as necessary.\nYou can run your script by running each chunk in sequence, which you do by clicking the green arrow in the top-right corner of each chunk.\nTry also to ‘Render’ the script by clicking on the Render button at the top of the script pane.\n\n4.6.2.1 Load packages\n\nlibrary(tidyverse)\nlibrary(here)\n\n\n\n4.6.2.2 Load data\nOur data set is in a .csv file which we have placed in the data folder within our project folder.\nNote that this data set has been stored in ‘tidy’ form: each variable appears in only column, and each observation appears in only one row.\n\nfilepath&lt;-here(\"data\",\"wellness.csv\")\nwellness&lt;-read_csv(filepath)\nglimpse(wellness)\n\nRows: 748\nColumns: 4\n$ id          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ scale       &lt;chr&gt; \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\"…\n$ marine      &lt;chr&gt; \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\",…\n$ total_score &lt;dbl&gt; 48, 51, 37, 39, 38, 40, 54, 39, 54, 39, 51, 50, 49, 51, 54…\n\n\n\n\n4.6.2.3 Summarise the data\nWe’ll calculate the median score (50th percentile) and the 25th and 75th percentile scores. For ordinal data, these summary statistics are well defined, whereas means and standard deviations are not.\n\nwellness |&gt;\n  group_by(scale,marine) |&gt;\n  summarise(median.score=median(total_score),iqr_25=quantile(total_score,0.25),iqr_75=quantile(total_score,0.75))\n\n# A tibble: 4 × 5\n# Groups:   scale [2]\n  scale  marine median.score iqr_25 iqr_75\n  &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 WEMWBS No             42.5   36       49\n2 WEMWBS Yes            47     41       51\n3 nep    No             51     48.2     53\n4 nep    Yes            50     48       53\n\n\n\n\n4.6.2.4 Plot the data\nBox plots are particularly suitable for ordinal data since they show the 25th and 75th percentiles of the data (the bottom and top of the box) plus the 50th percentile aka the median, which is the thick line across each box. All of these percentiles are well defined quantities for ordinal data.\n\nwellness |&gt;\n  ggplot(aes(x = scale,y = total_score,fill = marine)) +\n  geom_boxplot() +\n  labs(x = \"Likert Scale\",\n       y = \"Wellbeing Score\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nLooking at the plot, what do you think each scale suggests about whether proximity to the sea makes a difference to wellbeing?\n\n\n4.6.2.5 Wilcoxon-Mann-Whitney U test\nFirst let’s pull out the scores as measured by the WEMWBS scale and do a test for a difference between the scores of marine users and those of non-marine users. We can use the filter() function to do this.\n\nWEMWBS&lt;-wellness |&gt; filter(scale==\"WEMWBS\") # save the WEMWBS data into a data frame called WEMWBS\nglimpse(WEMWBS)\n\nRows: 374\nColumns: 4\n$ id          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ scale       &lt;chr&gt; \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\"…\n$ marine      &lt;chr&gt; \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\",…\n$ total_score &lt;dbl&gt; 48, 51, 37, 39, 38, 40, 54, 39, 54, 39, 51, 50, 49, 51, 54…\n\n\nNow let’s do the actual Wilcoxon-Mann-Whitney U test:\n\nwilcox.test(total_score~marine,data=WEMWBS)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  total_score by marine\nW = 9077.5, p-value = 0.0001687\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe null hypothesis of this test is that there is no evidence that the data are drawn from different populations. In this case, the p-value is very small, so we can confidently reject that null hypothesis and assert that there is evidence, according to the WEMWBS scale that marine use makes a difference to peoples’ sense of wellbeing.\nDoes it make it worse or better? - we can see from the summary table and from the box plot that higher scores are associated with those people who were exposed to a marine environment.\nWe might report this results as follows, first using a plain English statement of the main finding, and then reporting the type of test use, the value of the test statistic that it calculated and the p value. In this case, because the p value is so small, we would not report its exact value, but simply give an indication of how small it is:\nWe find evidence, according to the WEMWBS scale, that the wellbeing score is 4.5 or about 10% higher for people exposed to a marine environment (Mann-Whitney U, W = 9077.5, p &lt; 0.001).\n\n\n\n4.6.3 Exercise\nAdapt the code of the last chunk so that you can do the same test but for data as recorded by the nep scale\n\n\n4.6.4 When should I use this Wilcoxon-Mann-Whitney U test?\nThe test we have used here is an example of a non-parametric test. This means that it does not assume that the data follow a known mathematical distribution and, further, that it can be used with ordinal data.\nWe used the Mann-Whitney U test in particular because we were testing for a difference, and because the factor of interest - marine exposure - had just two levels - Yes or No. This test is only suitable when there are just two levels, so you can think of it as as a non-parametric alternative to a t-test.\nIn another setting where we still had just one factor (eg zone of a rocky shore) but there were more than two levels (eg low, mid and high zones of the shore) and we decided that we wanted to do a non-parametric test for a difference, then we would probably use the Kruskal-Wallis test, which you can think of as the non-parametric alternative to a one-way ANOVA.\nIn this example we used the Mann-Whitney U test because the data were ordinal and thus not suitable to use with a parametric test (but see below!). Where we can, we usually try to use a parametric test as they are more powerful than their non-parametric equivalents, meaning, if there is a trend or a difference in the data, they are better able to detect it. However those parametric tests (t-test, ANOVA, pearson correlation, PCA, GLM to name but a few) typically require not only that the data are numerical but also a host of other things, including that they follow a particular distribution, usually (but not always) the normal distribution, and this is often not the case with real biological data. Often, especially with count data, there are lots of zeros, or the data distribution is heavily skewed, usually to the right. In these cases, providing the data are independent of each other, we can usually still use a non-parametric test such as we have here. it might not be the most powerful test we can use (GLMs are typically way better if you can use them), but it will work.\n\n\n4.6.5 Hang on!\nThe eagle eyed among you may have spotted a massive flaw in the line of argument presented above. We said that ordinal data can’t be added up, can’t be used to calculate averages and so on. Thus we can’t run parametric tests on them and have to look for alternatives, namely, non-parametric tests.\nAnd yet, these non-parametric tests are usually run on the output of Likert scales such as we have considered here, where for each person we have a number of Likert Items (ie individual questions) that together constitute the scale, that each generate a score 1-5, then we add up the scores to get a total score. But that means we are adding up ordinal data!!!\nIt turns out that you actually get much the same results with Likert scale data if you analyse them using supposedly inappropriate parametric tests such as a 2-sample t-test as you do if you use a non-parametric test such as the one we considered here, the Mann-Whitney test.\nA study by De Winter and Dodou (2010) shows this convincingly.\nde Winter, J. F. C., & Dodou, D. (2010). Five-Point Likert Items: t test versus Mann-Whitney-Wilcoxon (Addendum added October 2012). Practical Assessment, Research, and Evaluation,15, 1–16. https://doi.org/10.7275/bj1p-ts64\nFor an enlightening discussion of this paper, see this blog by Jim Frost",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#paired-data",
    "href": "tests_for_difference_two_levels.html#paired-data",
    "title": "4  Tests for difference: one factor, two levels",
    "section": "4.7 Paired data",
    "text": "4.7 Paired data\nOften one has a sample of replicated data where each element has a counterpart in another matched sample - paired data. A common scenario for this is when there are data for the same individual at two different points in time, for example before and after some event such as the application of a treatment.\nIn order to determine whether there is a difference between the two sets, one should take the paired aspect into account and not simply match the whole before-set against the whole after-set without doing this. That would be to throw away the information whereby there is likely to be a greater degree of correlation between the responses of an individual before and after the event than there is between any randomly chosen pairs of individuals before and after the event.\n\n4.7.1 Which test: paired t-test or Wilcoxon signed rank test?\nThere is a choice between at least two tests: the parametric paired t-test and the non-parametric Wilcoxon signed rank test. Ideally one would use the t-test since it is more powerful than the Wilcoxon test. This means several things, but in particular it means that, all else being equal, it can detect a small difference with higher probability than the Wilcoxon test can.\n\n\n4.7.2 The paired t-test\nWhere the data are numerical (ie not ordinal) and where the before and after data are both normally distributed around their respective mean values one would use the paired t-test in this scenario. One can test for normality using either a test such as the Shapiro-Wilk test, or graphically using either a histogram, a box plot, or (best), a quantile-quantile plot.\n\n\n4.7.3 The Wilcoxon Signed Rank test\nThe t-test, an example of a so-called parametric test, is actually pretty robust against departures from normality, but where one doubts its validity due to extreme non-normality or for other reasons such as the ordinal nature of the data, the Wilcoxon signed rank test is a useful non-parametric alternative. It is called non-parametric because it does not make any assumption about the distribution of the data values. It only uses their ranks, where the smallest value gets rank 1, the next smallest gets rank 2, and so on.\nSo, you typically use this test when you would like to use the paired t-test, but you cannot because one or both of the data sets is way off being normally distributed or is ordinal.\n\n4.7.3.1 Null Hypotheses\nIn both the t-test and the Wilcoxon signed rank tests, the null hypothesis is the usual ‘nothing going on’, ‘there is no difference’ scenario, but there is a subtle difference between them that reflects the different information that they use. In the Wilcoxon signed rank test the null is that the difference between the medians of pairs of observations is zero. This is different from the null hypothesis of the paired t–test, which is that the difference between the means of pairs is zero.\n\n\n4.7.3.2 Test output\nBoth tests will give a p value. This is the probability that the mean (t-test) or median (Wilcoxon signed rank) paired differences between the corresponding before and after sample elements would be equal to or greater than it actually is for the data if the null hypothesis were true. If the p value is less than some pre-decided ‘significance level’, usually taken to be 0.05, then we reject the null hypothesis. If it is not, then we fail to reject the null hypothesis.\n\n\n\n4.7.4 Example\nWe will use as an example a data set from Laureysens et al. (2004) that has measurements of metal content in the wood of 13 poplar clones growing in a polluted area, once from each clone in August and once again from each of them in November. The idea was to investigate the extent to which poplars could absorb metals from the soil and thus be useful in cleaning that up. Under a null hypothesis, there would be no change in the metal concentrations in the plant tissue of each clone between August and November. Under an alternate hypothesis, there would be.\nLaureysens, I. et al. (2004) ‘Clonal variation in heavy metal accumulation and biomass production in a poplar coppice culture: I. Seasonal variation in leaf, wood and bark concentrations’, Environmental Pollution, 131(3), pp. 485–494. Available at: https://doi.org/10.1016/j.envpol.2004.02.009.\nConcentrations of aluminum (in micrograms of Al per gram of wood) are shown below.\nLoad packages\n\nlibrary (tidyverse)\nlibrary(here)\nlibrary(cowplot) # to make the plots look nice\n\nLoad data\n\nfilepath &lt;- here(\"data\",\"poplars-paired_np.csv\")\npoplars &lt;- read_csv(filepath,show_col_types = FALSE)\nhead(poplars,20)\n\n# A tibble: 13 × 4\n      ID Clone          August November\n   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 Balsam_Spire      8.1     11.2\n 2     2 Beaupre          10       16.3\n 3     3 Hazendans        16.5     15.3\n 4     4 Hoogvorst        13.6     15.6\n 5     5 Raspalje          9.5     10.5\n 6     6 Unal              8.3     15.5\n 7     7 Columbia_River   18.3     12.7\n 8     8 Fritzi_Pauley    13.3     11.1\n 9     9 Trichobel         7.9     19.9\n10    10 Gaver             8.1     20.4\n11    11 Gibecq            8.9     14.2\n12    12 Primo            12.6     12.7\n13    13 Wolterson        13.4     36.8\n\n\nPlot the data\nBefore we do any test on some data to find evidence for a difference or a trend, it is a good idea to plot the data. This will reveal whatever patterns there are in the data and how likely they are to reveal a truth about the population from which they have been drawn.\nTidy the data\nIn this case there is work to do before we can plot the data. The problem is that the data is ‘untidy’. The two levels of the factor month are spread across two columns, August and November. For plotting purposes it will be useful to ‘tidy’ the data so that there is only one column containing both levels of month and another containing the aluminium concentrations. The function pivot_longer() can do this for us:\n\npoplars_tidy &lt;- poplars |&gt;\n  pivot_longer (August:November,names_to=\"month\",values_to=\"Al_conc\")\nhead(poplars_tidy,8)\n\n# A tibble: 8 × 4\n     ID Clone        month    Al_conc\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt;\n1     1 Balsam_Spire August       8.1\n2     1 Balsam_Spire November    11.2\n3     2 Beaupre      August      10  \n4     2 Beaupre      November    16.3\n5     3 Hazendans    August      16.5\n6     3 Hazendans    November    15.3\n7     4 Hoogvorst    August      13.6\n8     4 Hoogvorst    November    15.6\n\n\nNow we can plot the data as a box plot, with one box for August and one for November ie one for each level of the factor month. Had we not first tidied the data, we could not have done this.\n\npoplars_tidy |&gt;\n  ggplot(aes(x = month, y = Al_conc, fill = month, colour = month)) + \n  # alpa (= opacity) &lt; 1 in case any points are on top of each other\n  geom_boxplot(outlier.size=0,alpha=0.5) +\n  geom_point(alpha = 0.5) +\n  # group = ID makes the lines join elements of each pair\n  geom_line(aes(group=ID),colour = \"grey60\") +\n  labs(x = \"Month\",\n       y = \"Al conc.(mu g Al / g wood)\") +\n  theme_cowplot() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nDoes it look as though the difference between the medians could plausibly be zero for the population from which these samples were drawn? Or, put another way, if it was zero, how big a fluke would this sample be? That is what the p value actually tells us.\n\n\n4.7.5 Two sample paired t-test\nCheck for normality of differences\nBefore we use the t-test, we need to check that it is OK to do so. This means checking whether the paired differences are plausibly drawn from a normal distribution centred on zero.\nThe null hypothesis of the Shapiro-Wilk test is that the data set given to it is plausibly drawn from a normally distributed population. So let us give our sample of paired differences:\n\nshapiro.test(poplars$August-poplars$November)\n\n\n    Shapiro-Wilk normality test\n\ndata:  poplars$August - poplars$November\nW = 0.92667, p-value = 0.3081\n\n\nThe p value is very high. Thus we do not reject the null hypothesis and we can reasonably assume that the differences between the August and November aluminium concentrations in the sample could plausibly have been drawn from a normally distributed population, despite the outlier value in the November sample. Thus we can reasonably test for difference using a paired t-test.\nThe actual t-test\nWe can do this in R using the function t.test(), where we give to the function both the August and the November data, knowing that each August value has a counterpart November value, and we set the argument paired to TRUE.\n\nt.test(poplars$August, poplars$November, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  poplars$August and poplars$November\nt = -2.3089, df = 12, p-value = 0.03956\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -9.5239348 -0.2760652\nsample estimates:\nmean difference \n           -4.9 \n\n\nAll parts of the output have meaning and are useful, but here we will focus on just two:\n\nthe p value is equal to 0.040. Hence, if we have chosen the usual significance value of 0.05, we can take this to mean that there is evidence of a significant difference between the August and November values.\nthe lower and upper bounds of the 95% confidence interval are (-9.52, -0.28). YOu can think of this interval as the range of values within which the difference can plausibly lie, at the 95% confidence level. The key thing is that this range does not encompass zero. This means that we can be confident at the 95% level that there is a non-zero change on going from August to November, and, in particular, that the August value is lower than the November value.\n\n\n\n4.7.6 The non-parametric alternative: The Wilcoxon signed rank test\nTo be safe, because of that outlier, let us test for difference using the Wilcoxon signed rank test. In R this is done using the function wilcox.test(), with the argument paired set to TRUE.\n\nwilcox.test(poplars$August, poplars$November, paired = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  poplars$August and poplars$November\nV = 16, p-value = 0.03979\nalternative hypothesis: true location shift is not equal to 0\n\n\nWe see that the conclusion (in this case) is the same.\n\n\n4.7.7 Relation to one-sample paired test\nThe two-sample paired tests as we have done above are the same as doing a one-sample test to see if the differences between the August and November paired values is different from zero. This is true whether we do a t-test or a Wilcoxon signed rank test.\nIn either case, the first argument is the vector of differences, and the second mu is the threshold value against which we want to compare those differences, in this case zero.\n\nt.test(poplars$August - poplars$November, mu = 0, data = poplars)\n\n\n    One Sample t-test\n\ndata:  poplars$August - poplars$November\nt = -2.3089, df = 12, p-value = 0.03956\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -9.5239348 -0.2760652\nsample estimates:\nmean of x \n     -4.9 \n\n\n\nwilcox.test(poplars$August - poplars$November, mu = 0, data = poplars)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  poplars$August - poplars$November\nV = 16, p-value = 0.03979\nalternative hypothesis: true location is not equal to 0\n\n\nNote that the output from both these one-sample tests, where the one sample is the vector of differences and the threshold with which it is compared is zero, is exactly the same as the output of the two-sample tests where the two samples were the vectors between which we were interested in detecting a difference, ie the August and November values. This is not surprising since the two cases are just two ways of doing exactly the same thing, which is to ask if there is evidence from the sample for a difference in the population between the August and November concentrations of aluminium.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html",
    "href": "ANOVA_how_it_works_penguins.html",
    "title": "5  ANOVA explained",
    "section": "",
    "text": "5.1 The basic principles of ANOVA\nThere are many varieties of ANOVA but we will start with the simplest.\nWe would carry out a ‘One-way’ ANOVA (ANalysis Of VAriance) when we have independent replicates from three or more populations and we want to know if there is evidence for a difference between at least one pair of these in respect of the mean value of a numerical variable in which we ae interested. There almost certainly will be a difference between the sample means but these samples are just that - samples - randomly (we hope) drawn from their respective populations. If we drew three samples from the same population they would also almost certainly differ in their means! The differences are due to the random variation within the populations and our random method of choosing samples from them.\nWhat we want to know is whether the differences we see between our samples are big enough for us to reject the null hypothesis that there is no difference between the populations - that they are, in effect, the same population.\nHere we will go through an example in detail and work out all the mechanics, but once we have done that and seen how the output is derived from the input we will not need to do it again. We will use R to do the heavy lifting. We will just need to know when it is appropriate to use ANOVA, how to get R to do it and how to interpret the output that R produces.\nAn ANOVA analysis (bit like saying MOT test!) attempts to determine whether the differences between samples are significant by investigating the variability in the data. It investigates how the variability between samples compares to the variability within samples.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#the-basic-principles-of-anova",
    "href": "ANOVA_how_it_works_penguins.html#the-basic-principles-of-anova",
    "title": "5  ANOVA explained",
    "section": "",
    "text": "5.1.1 The Scenario\nAs our case study we consider the three species of penguin, Adelie, Chinstrap and Gentoo for which data has been gathered and made available in the palmerpenguins R package. For dozens of individuals of each species, records have been logged of the species, sex, year, bill length, bill depth, flipper length and body mass. Suppose we are interested in the shape of the bills and so create for each individual a new variable which is the ratio of bill length to bill depth. We wish to know if there is evidence from the data for whether this bill shape differs between the species. In the manner of the bills of the various species of finch spread across the islands of the Galapagos archipeligo, this might indicate a different food source for each species and more generally, indicate that each species occupies a different ecological niche from the others.\nWhen we plot the data as in Figure 5.1 we see that the species do differ in their bill shapes but that there is also a lot of variation between penguins of the same species.\n\n\n\n\n\n\n\n\nFigure 5.1: The distribution of bill shape for each of the three penguin species.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#summarise-the-data-calculate-means",
    "href": "ANOVA_how_it_works_penguins.html#summarise-the-data-calculate-means",
    "title": "5  ANOVA explained",
    "section": "5.2 Summarise the data: calculate means",
    "text": "5.2 Summarise the data: calculate means\nFirst step: calculate the grand mean\nFirst we calculate the ‘grand mean’, the mean of the bill shapes across all penguins in the data set:\n\n\n[1] 2.658853\n\n\nSecond step: calculate the group means\nNext, we calculate the mean bill shape for each species - these are what we will call the group means.\n\n\n# A tibble: 3 × 5\n  species       n pmean id_min id_max\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;int&gt;  &lt;int&gt;\n1 Adelie       10  2.06      1     10\n2 Chinstrap    10  2.66     11     20\n3 Gentoo       10  3.26     21     30",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#calculate-variabilities-around-the-means-sums-of-squared-differences",
    "href": "ANOVA_how_it_works_penguins.html#calculate-variabilities-around-the-means-sums-of-squared-differences",
    "title": "5  ANOVA explained",
    "section": "5.3 Calculate variabilities around the means: sums of squared differences",
    "text": "5.3 Calculate variabilities around the means: sums of squared differences\nNow we can calculate and plot the deviations of the data around these means. That will allow us to visualise and calculate what turn out to be the key quatities in carrying out an ANOVA analysis: the sums of squared differences. There are three of these and we will turn to them now:\n\n5.3.1 SST: the total sum of squares\nFRom the deviations of each individual arounnd the grand mean we can calculate what we will call SST: the total sum of squares. This is the sum of the squared differences between each individual penguin bill shape and the grand mean. In this and other measures of variability we square the differences so that we do not distinguish between positive and negative deviations.\nSST is a measure of the total variability of the data set. For our penguins we find that SST = 8.147\n\n\n\n\n\n\n\n\nFigure 5.2: The overall variability of the data (SST) is measured as the sum of the squared differences between each individual value and the grand mean, which is the mean of the whole data set. The individual differences are shown in the figure as vertical dotted lines. If there are n data points, then SST will have n-1 degrees of freedom.\n\n\n\n\n\nNow we determine two more measures of variability: the deviations of individual penguins from their group means, and the the deviations of those group means from the grand mean. For each of these measures, we will square them so that we do not distinguish between positive and negative deviations, and then sum these squares to get a measure of the total variabilities.\n\n\n5.3.2 SSE - Error sum of squares\nSSE is the error sum of squares. It is the sum of the squares of the deviations of the data around the three separate species means. This is a measure of the variation between individuals of the same species.\n\n\n5.3.3 SSG - group (species) sum of squares\nSSG is the group sum of squares. This is the sum of the squares of the deviations of the group (species, in this case) means from the grand mean. This is a measure of the variation between individuals of different species.\n\n\n\n\n\n\n\n\nFigure 5.3: a) Each species has a mean value for bill shape, shown by the three horizontal lines. Each individual penguin has a bill shape that differs from that by the length of the vertical dotted lines shown. When we square each of these lengths and then add up all these squares we get what we are calling here SSE: the error sum of squares. This is a measure of the variability between penguins of the same species. b) For each species, the mean bill shape differs from the grand mean by the length of the dotted lines shown. When we square and then sum these three lengths we get what we are calling here SSG: the squared sum of groups. This is a measure of the variability between penguins of different species.\n\n\n\n\n\nWhen the three species means are fitted, there is an obvious reduction in variability around the three means compared to that around the grand mean: SSE is less than SST, but it is not obvious if bill shape differs between the species.\nAt what point do we decide if the amount of variation explained by fitting the means is significant? By this, we mean, “When is the variability between the group means greater than we would expect by chance alone?\nFirst, we note that SSE and SSG partition between them the total variability SST in the data:",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#partitioning-the-sums-of-squares-sst-sse-ssg",
    "href": "ANOVA_how_it_works_penguins.html#partitioning-the-sums-of-squares-sst-sse-ssg",
    "title": "5  ANOVA explained",
    "section": "5.4 Partitioning the sums of squares: SST = SSE + SSG",
    "text": "5.4 Partitioning the sums of squares: SST = SSE + SSG\n\n\n\nSum of Squares\nValue\n\n\n\n\nSSE\n0.936\n\n\nSSG\n7.212\n\n\nSST\n8.147\n\n\nSSE + SSG\n8.148 = SST\n\n\n\nSo the total variability has been divided into two components. That due to differences between individuals of different species (SSG) and that due to differences between individuals of the same species. (SSE). Variability must be due to one or other of these components. Separating the total SS into its component SS is known as partitioning the sums of squares.\nA comparison of SSG and SSE is going to indicate whether fitting the three species means accounts for a significant amount of variability.\nHowever, to make a proper comparison, we really need to compare the variability per degree of freedom ie the variance, so first we need to discuss what we mean by degrees of freedom for each of the sums of squares calculated so far, and how we calculate them.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#partitioning-the-degrees-of-freedom",
    "href": "ANOVA_how_it_works_penguins.html#partitioning-the-degrees-of-freedom",
    "title": "5  ANOVA explained",
    "section": "5.5 Partitioning the degrees of freedom",
    "text": "5.5 Partitioning the degrees of freedom\nEvery sum of squares (SS) has been calculated using a number of independent pieces of information. In each, case, we call this number the number of degrees of freedom for the SS.\nFor SST this number is one less than the number of data points n. This is because when we calculate the deviations of each data point around a grand mean there are only n-1 of them that are independent, since by definition the sum of these deviations is zero, and so when n-1 of them have been calculated, the final one is pre-determined.\nSimilarly, when we calculate SSG, which measures the deviation of the \\(k\\) species means from the grand mean, we have \\(k\\)-1 degrees of freedom, (where in the present example \\(k\\), the number of species, is equal to three) since the deviations must sum to zero, so when \\(k\\)-1 of them have been calculated, the last one is pre-determined.\nFinally, SSE, which measures deviation around the group means will have n-k degrees of freedom, since the sum of each of the deviations around one of the group means must sum to zero, and so when all but one of them have been calculated, the final one is pre-determined. There are \\(k\\) group means, so the total degrees of freedom for SSE is n-k.\nThe degrees of freedom are additive:\n\\[\ndf(\\text{SST}) = df(\\text{SSE}) + df(\\text{SSG})\n\\]\nCheck: \\[\n\\begin{align*}\ndf(\\text{SST}) &= n-1\\\\\ndf(\\text{SSE}) &= k-1\\\\\ndf(\\text{SSG}) &= n-k\\\\\n\\therefore df(\\text{SSE}) + df(\\text{SSG}) &= k-1 + n-k\\\\\n&=n-1\\\\\n&=df(\\text{SST})\n\\end{align*}\n\\]",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#variances-per-degree-of-freedom-mean-squares",
    "href": "ANOVA_how_it_works_penguins.html#variances-per-degree-of-freedom-mean-squares",
    "title": "5  ANOVA explained",
    "section": "5.6 Variances per degree of freedom: Mean Squares",
    "text": "5.6 Variances per degree of freedom: Mean Squares\nNow we can calculate the variances which are a measure of the amount of variability per degree of freedom.\nIn this context, we call them mean squares. To find each one we divide each of the sums of squares (SS) by its corresponding degrees of freedom.\nGroup Mean Square (GMS) = SSG / k - 1. This is the variation per df between individuals of different species.\nError Mean Square (EMS) = SSE / n - k. This is the variation per df between individuals of the same species.\nTotal Mean Square (TMS) = SST / n - 1. This is the total variance per df of the dataset.\nUnlike the SS, the MS are not additive. That is, GMS + EMS \\(\\neq\\) TMS as we see in the table below where we list these values for our penguins sample:\n\n\n\nMean Square\nValue\n\n\n\n\nGMS\n2.73\n\n\nEMS\n0.0154\n\n\nTMS\n0.171",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#f-ratios",
    "href": "ANOVA_how_it_works_penguins.html#f-ratios",
    "title": "5  ANOVA explained",
    "section": "5.7 F-ratios",
    "text": "5.7 F-ratios\nIf species did not influence bill shape, we would expect as much variation between the penguins of the same species as between penguins of different species.\nWe can express this in terms of the mean squares: the mean square for groups (species) would be about the same as the mean square for error and so their ratio would be about equal to 1:\n\\[\nF=\\frac{\\text{GMS}}{\\text{EMS}}\\approx1\\quad{\\text{No difference between groups}}\n\\]\nWe call this ratio the F-ratio. This is the so-called ‘test statistic’ that ANOVA calculates from your data. F-ratios can never be negative since they are the ratio of two mean square values, both of which must be non-negative, but there is no limit to how large they can be. In fact,\n\\[\nF = \\frac{\\text{GMS}}{\\text{EMS}}\\gt 1\\quad{\\text{Possible difference between groups}}\n\\]\n\\[\nF = \\frac{\\text{GMS}}{\\text{EMS}}\\gg 1\\quad{\\text{Probable difference between groups}}\n\\]\nEven if the species were identical, the F-ratio is unlikely to be exactly 1 - it could by chance take a whole range of values. It turns out that F-ratios have a known distribution, the F-distribution, which represents the range and likelihood of all possible F ratios under the null hypothesis. ie when the species were identical.\nThe shape of the F distribution depends on the degrees of freedom of GMS and EMS, and we normally specify it by giving the values of each. In Figure 5.4 below we show F distributions for 2 and 27 degrees of freedom (ie 3 species, so k = 3, so the degrees of freedom of GMS = k-1 = 2, and 10 individuals per species, so n = 3 x 10 = 30, and hence the degrees of freedom of EMS = n-k = 30 - 3 = 27), and for 10 and 27 degrees of freedom.\n\n\n\n\n\n\n\n\nFigure 5.4: The F-distributions for (left) 2 and 27 degrees of freedom and (right) 10 and 27 degrees of freedom\n\n\n\n\n\nNote that, whatever the degrees of freedom, F-distributions are examples of so-called probability density functions. The area beneath them between any two values of F-ratio is equal to the probability of getting an F-ratio in that range. Hence the total area under the curves is equal to 1, since the F-ratio must take some value between zero and infinity, and the area under the tail to the right of any given F-ratio is the probability of getting an F-ratio bigger than that value.\nHence, the probability under the null hypothesis of getting an F-ratio as large or larger than the value we actually got is the area to the right of this F-ratio under the appropriate F distribution. We often call this probability the p-value. p for probability. p-values are the the probability of getting data as extreme (same F-ratio,) or more extreme (bigger F-ratio) as the data you got you got if the null hypothesis were true.\nIf the species were very different, then the GMS would be much greater than the EMS and the F-ratio would be greater than one. However it can be quite large even when there are no differences between the levels (here, Adelie, Chinstrap and Gentoo) of a factor (here, species). So how do we decide when the size of the F-ratio is due to a real difference between the levels rather than to chance?\nOur p-value (the probability that the F-ratio would have been as large as it is or larger under the null hypothesis) represents the strength of evidence against the null hypothesis. The smaller it is, the stronger the evidence, and, as a pragmatic choice, only when it is less than 0.05 do we regard the evidence as strong enough to reject the null. Note though that even if we had inside knowledge that the null hypothesis was in fact true, we would still get an F-ratio that large or larger and thus a p-value less than or equal to 0.05 5% of the time.\nWe find for our penguin data that the F ratio is 104\nIf we look at the F distribution in the left-hand figure in Figure 5.4 the one that corresponds to our case, with 2 and 27 degrees of freedom, we see that it has already fallen to near zero by the time F is equal to 5 or 6. There is essentially zero area beneath the curve to the right of the much larger F value we found for our data, F = 104. That means there is essentially zero chance of getting an F value this big or bigger if the null hypothesis were true (ie the p-value is esentially zero), and so we can confidently reject the null hypothesis.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works_penguins.html#what-does-the-anova-result-tell-us",
    "href": "ANOVA_how_it_works_penguins.html#what-does-the-anova-result-tell-us",
    "title": "5  ANOVA explained",
    "section": "5.8 What does the ANOVA result tell us?",
    "text": "5.8 What does the ANOVA result tell us?\nThe null hypothesis of the ANOVA is that all the samples are drawn from populations with the same mean. If the F-value is so large that we reject this null hypothesis, then we infer that at least two of the population means differ. We not learn from the ANOVA where the difference or differences lie.\nTo find that out we typically need to continue the analysis with multiple pair-wise comparisons between the samples, taking care to control for the study-wide error rate.\n\n\nAnalysis of Variance Table\n\nResponse: bill_shape\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nspecies    2 7.2117  3.6058  104.05 2.05e-13 ***\nResiduals 27 0.9357  0.0347                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ANOVA explained</span>"
    ]
  },
  {
    "objectID": "ANOVA_one_way.html",
    "href": "ANOVA_one_way.html",
    "title": "6  One-way ANOVA",
    "section": "",
    "text": "6.1 Introduction\nIn this exercise we will carry out a method of analysis known as ANOVA - this is what is commonly used when you have one or more categorical variables, such as species, sex and so on, and a numerical response variable such as body mass and you want to know if there is a difference in the response variable between when the levels of the factors take different values.\nA one-way ANOVA is used where where we have one categorical variable with three or more levels (you could also use it where there are just two levels, but we use a t-test for that). For example if you want to see if a captive bird species prefers red, green or blue food pellets, then the factor would be be food colour, and three levels of that would be the three colours.\nA two-way ANOVA is used where we have two categorical variables, each of which has at least two levels. (Even if both have two levels, we still call it a two-way ANOVA. There is no such thing as a two-way t-test!). So, sticking with the captive birds, if were interested in whether they had a preference for colour (red, blue, green) and/or shape (round, square) of food pellets, then we could use a two way ANOVA to investigae the data, where the two factors or ‘ways’ would be food colour and food shape, with food colour having three levels (red, blue and green) and food shape having two levels (round and square).\nANOVAs involving three ways or more (the horror, the horror!) are rarely used since their interpretation is in practice difficult due to the multiplicity of possible so-called ‘interaction’ effects that commonly arise, whereby the impact on the output of the levels of one factor depend on the values of the levels of one or more other factors. Best avoided!\nANOVAs, whatever their flavour (one way, two way, repeated measures, ANCOVA etc) are examples of parametric tests. That is, they can only be used if the data at least approximately meet certain requirements such as equal variance across the data sets, normality of residuals etc. At the very least, the data should be numerical and not ordinal. Hence whenever we think of using an ANOVA we need to check that these requirements are at least approximately met. If they are not, then we may choose to turn to non-parametric alternatives.\nA non-parametric alternative to a one-way ANOVAs is commonly used, especially for studies that involve ordinal data such as Likert-type outputs from surveys. It is called a Kruskal-Wallis test.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_one_way.html#example-penguin-data-from-the-palmerpenguins-package",
    "href": "ANOVA_one_way.html#example-penguin-data-from-the-palmerpenguins-package",
    "title": "6  One-way ANOVA",
    "section": "6.2 Example: penguin data from the palmerpenguins package",
    "text": "6.2 Example: penguin data from the palmerpenguins package\nIn the following, we show how a one-way ANOVA might be carried out on a set of data on penguins, where we have a numerical output such as body mass, and categorical variables such as species (three levels: Adelie, Gentoo and Chinstrap) and sex (two levels: female and male). If we were interested in whether there was a difference in the output across the levels of species, then a one-way ANOVA might well be suitable.\n\n6.2.1 Follow along yourself:\nTo follow through this exercise yourself, you should have an RStudio project folder that contains:\nIn the Project/scripts folder:\nANOVA_one_way_template.Rmd (the script where you fill in the code chunks)",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_one_way.html#a-first-look-at-the-data",
    "href": "ANOVA_one_way.html#a-first-look-at-the-data",
    "title": "6  One-way ANOVA",
    "section": "6.3 A first look at the data",
    "text": "6.3 A first look at the data\n\n6.3.1 Load packages\n\nlibrary(tidyverse) # for data manipulation and plots, and more besides\nlibrary(ggfortify) # this is useful for diagnostics\nlibrary(palmerpenguins) # for the palmer penguin data\n\nThe palmerpenguins package comes with two in-built data sets on penguins. The simplest of them is called penguins and is the one we will use in this exercise:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n6.3.2 Remove observations with missing values\nWe can see from the first few values of the glimpse table that some rows have missing values (NAs). We need to decide what to do with them. Here we will simply remove them! Here is a way to remove any row that contains missing values in one column or another:\n\npenguins_clean &lt;- penguins |&gt;\n  drop_na()\nglimpse(penguins_clean)\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nThat has removed 11 rows of data, so we haven’t lost too much information.\n\n\n6.3.3 Summary - group by species and sex\nHere we use the famliar group_by() and summarise() construction to find the mean body mass for each combination of species and sex. We also calculate the standard error of those means and the number of individuals in each group.\n\npenguins_clean |&gt;\n  group_by(species, sex) |&gt;\n  summarise(n = n(), mean_bm = mean(body_mass_g), se_bm = sd(body_mass_g)/sqrt(n()) ) |&gt;\n  ungroup()\n\n# A tibble: 6 × 5\n  species   sex        n mean_bm se_bm\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    female    73   3369.  31.5\n2 Adelie    male      73   4043.  40.6\n3 Chinstrap female    34   3527.  48.9\n4 Chinstrap male      34   3939.  62.1\n5 Gentoo    female    58   4680.  37.0\n6 Gentoo    male      61   5485.  40.1\n\n\nLooking at this table, does it look as though females and males have different weights? If so, which is heavier? Is this true for all species? Do the different species weigh the same?\n\n\n6.3.4 Plot the data\nTo get further insight into these questions, we can plot the data. Here we will do a box plot\n\npenguins_clean  |&gt;\n  ggplot(aes(x=species, y = body_mass_g, fill = sex)) +\n  geom_boxplot() +\n  labs(x = \"Species\",\n       y = \"Body mass (g)\",\n       fill = \"Sex\") +\n  scale_colour_brewer(palette = \"Set1\") +\n  theme_bw() +\n  theme(legend.position= c(0.1,0.8))\n\n\n\n\n\n\n\n\nWhat do you think now about size differences between species and the two sexes?\nThere is a lot going on here, so let’s approach this more simply to begin with and concentrate solely on the difference between the females of the species.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_one_way.html#one-way-anova",
    "href": "ANOVA_one_way.html#one-way-anova",
    "title": "6  One-way ANOVA",
    "section": "6.4 One-way ANOVA",
    "text": "6.4 One-way ANOVA\nLet’s ask the question: do the body weights differ between females of the different species?\nThere is just one factor here, species, and it has more than two levels - the three different species - and the reponse variable is numeric, so it is highly likely that the appropriate test to answer this question is a one-way ANOVA. ‘One way’ because there is one factor, and ‘ANOVA’ (instead of t-test) because there are more than two levels.\n\n6.4.1 Null hypothesis\nPretty much all of the commonly used statistics tests are asking the question: what is the probability that you would have got this data, or more extreme data, if the null hypothesis were true? Their job is to calculate that probability, which is called a p-value. There is a lot more besides, but what this means is that in carrying out any of these tests we at least need to have a hypothesis in mind and its corresponding null hypothesis. The null, remember, is typically the ‘nothing going on’, there is no effect, no difference scenario.\nSo in this case, a suitable null hypothesis would be that there is no difference in body mass between the females of the different penguin species.\nTo see if there is evidence from the data to reject this null, we will follow a sequence of steps that will be common to many analyses:\n\nget the data\nclean/prepare the data\nsummarise the data\nplot the data\nconstruct the model using whatever test is appropriate, in this case a one-way ANOVA\ncheck whether the model is valid\ninspect the model output\nreject or fail to reject the null hypothesis\nif we reject the null, carry out post-hoc tests\n(maybe) simplify the model and redo the analysis\n\nFor the penguin data, getting it was easy as it came with the palmerpenguins package.\nTo prepare the data, we start with the full data set and narrow it down to just the females, using the filter() function, and again make sure there are no lines with missing values, using drop_na(). We save this cleaned data set in an object called females.\n\n\n6.4.2 Filter and clean the data\n\nfemales &lt;- penguins |&gt;\n  filter(sex == \"female\") |&gt;\n  drop_na()\n\n\n\n6.4.3 Summarise the data\nThen let’s summarise these values to find the number of individuals, the mean body mass for each species, and the standard errors of those means:\n\nfemales |&gt;\n  group_by(species) |&gt;\n  summarise(n = n(), mean.mass_f = mean(body_mass_g), se.mass_f = sd(body_mass_g)/sqrt(n()))\n\n# A tibble: 3 × 4\n  species       n mean.mass_f se.mass_f\n  &lt;fct&gt;     &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 Adelie       73       3369.      31.5\n2 Chinstrap    34       3527.      48.9\n3 Gentoo       58       4680.      37.0\n\n\nWe should inspect this summary table and see what we already think about whether the null hypothesis is likely to be rejected, or not.\nNow let’s plot them, using a box plot (but choose your favourite plot type):\n\n\n6.4.4 Plot the data\n\nfemales  |&gt;\n  ggplot(aes(x=species, y = body_mass_g)) +\n  geom_boxplot(fill = \"#9ebcda\") +  # pick your favourite colour from https://colorbrewer2.org/\n  labs(x = \"Species\",\n       y = \"Body mass (g)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFrom the summary table and the plot, what do you think? Do the masses differ between the species?\n\n\n6.4.5 Do the actual ANOVA\nYou probably have a good idea what the answer is, as to our question, but now we will move on to the actual statistics test, in this case a one-way ANOVA.\nAn ANOVA is one variant of a range of anlysis techniques known as ‘linear models’. If you were to look under the hood, you would see that mathematics behind it is exactly the same as that behind linear regression, which we use when we have a continuous explanatory variable and where we fit straight lines onto a scatter plot. Thus it is no surprise that the ANOVA is carried out in R in exactly the same way as linear regression would be:\nFirst, we use the lm() function to construct a linear model of the data:\n\n6.4.5.1 Construct the model\n\nfemales.model &lt;- lm(body_mass_g ~ species, data = females)\n\nHere the lm() function has done all the maths of the ANOVA, and we have saved the results of that in an object called females.model. Note the use of the formula body_mass_g ~ species as the first argument of the lm() function, where this means ‘body mass as a function of species’.\n\n\n6.4.5.2 Is the model valid?\nAll linear models are only valid if the data meet a number of criteria. Chief among these for an ANOVA is that the spread of the data should be roughly the same in each subset, and that the data within each subset should be normally distributed around their respective mean values. Only if these conditions are at least approximately met can we just go on and trust the output of the model. If they are not, we need to transform the data in some way until they are, or use a different test. A commonly used non-parametric alternative to the one-way ANOVA is the Kruskal-Wallis test.\nThere are various ways we can find out whether these conditions are met. A useful one is to do it graphically, and a useful way to do that is to use the autoplot() function from the ggfortify package. Let’s do it:\n\nautoplot(females.model) + theme_bw()\n\n\n\n\n\n\n\n\nAll four graphs presented here tell us something about the validity or not of our model. Here we will just focus on the upper two:\n\ntop-left: this shows the spread of the residual masses (diference between an individual’s mass and the mean mass for its species) for each species. We see that the spread of these values is aout the same for all three species. Check!\ntop-right: this is a quantile-quantile plot, often referred to as a qq-plot. This compares the distribution of the residuals for each species with a normal distribution. If the residuals are normally distributed, we will get a straight line. If not, we won’t. To get an idea of what qq-plots, histograms and box-plots look like for data that definitely are not normally distriuted, see this useful summary. In our case, there is a hint of a curve, but this qq-plot is really a pretty good approximation to linear for a real data set. No such data is ever perfectly normally distributed, so the best we are looking for, in practice is something approximating a straight line, often with some raggedness at either end. So, check again!\n\nOn both counts, we are good to go: we can reasonably trust the output of the ANOVA.\nSo what is this output? We find this in three steps\n\n\n6.4.5.3 Inspect the model\nThe overall picture\nFirst, we use the anova() function\n\nanova(females.model)\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n           Df   Sum Sq  Mean Sq F value    Pr(&gt;F)    \nspecies     2 60350016 30175008  393.25 &lt; 2.2e-16 ***\nResiduals 162 12430757    76733                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis gives us an overview of all the data and asks the question: how likely is it that you would have got your data if species made no difference to body mass. There are three things to note:\n\nthe test statistic, here called an F-value. This is a number calculated from the data. Roughly speaking, it is the ratio of the spread of values (aka variance) between the subgroups to that within the subgroups If the validity criteria for the test have been met by the data, then this has a known distribution. The bigger the F-value, the more likely it is that the null will be rejected.\nthe degrees of freedom, here denoted as Df and listed in the first column. These are the number of independent pieces of information in the data, which here means, how many species and how many penguins.\nthe p-value, which is the probability of getting an F value as big as or bigger than the one actually found, if the null hypothesis were true. This is is the number listed at the right as Pr(&gt;F).\n\nThe F value here is huge and the p-value is tiny, so tiny that it is essentially zero. Thus we can confidently reject the null hypothesis and assert that there is evidence from the data that body mass of females differs between at least one pair of species. Which two, or between all of them, and by how much we don’t yet know. This first step just tells us whether there is some difference somewhere. If there were no evidence of any difference we would stop the analysis right here.\nBut there is a difference in this case, so we continue.\nThe detailed picture\nWe use the summary() function for this:\n\nsummary(females.model)\n\n\nCall:\nlm(formula = body_mass_g ~ species, data = females)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-827.21 -193.84   20.26  181.16  622.79 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3368.84      32.42 103.908  &lt; 2e-16 ***\nspeciesChinstrap   158.37      57.52   2.754  0.00657 ** \nspeciesGentoo     1310.91      48.72  26.904  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 277 on 162 degrees of freedom\nMultiple R-squared:  0.8292,    Adjusted R-squared:  0.8271 \nF-statistic: 393.2 on 2 and 162 DF,  p-value: &lt; 2.2e-16\n\n\nThere is a lot in ths output, so let’s just consider the coefficient table, to begin with. Focus first on the top left value, in the Estimate column. This tells us the mean body mass of the reference or ‘Intercept’ species. In this case that is ‘Adelie’, purely because ‘Adelie’ comes alphabetically before the other two species names, ‘Chinstrap’ and ‘Gentoo’. By default, R will always order levels of a factor alpabetically. This is often a nuisance, and with many data sets we have to tell R to reorder the levels the way we want them, but here the order is OK.\nSo, the mean mass of female Adelie penguins in our sample is 3368 g. Cross check that with your initial summary table and the box plot. What about the other two species? Here’s the thing: for all rows except the first in the Estimate column we are not given the absolute value but the difference between their respective mean values and the reference mean in the first, ‘Intercept’ row.\nThus, we are being told that Chinstrap females in the sample have a mean body mass that is 158.37 g heavier than that of Adelie females, so that their mean body mass is 3368.84 + 158.37 = 3527.27g. Again, cross check that with your summary table and the box plot. Is it right?\nWhat about Gentoo females? Were they heavier than Adelie penguins, and if so, by how much? What was their mean body mass.\nWhy doesn’t summary() just tell us the actual body masses instead for all three species instead of doing it in this round about way? The reason is that ANOVA is concerned with detecting evidence of difference. This is why we are being told what the differences are between each of the levels and one reference level, which here is Adelie.\nAre those differences signifcant? We use the right hand p-value column for that. Look in the rows for Chinstrap and Gentoo penguins. In both cases the p values are much less than 0.05. This is telling us that in both cases there is evidence that females of these species are significantly heavier than those of the Adelie species.\nNote that we have only been told, so far, about the magnitude and significance of differences between all the levels and the reference level. We are not told the significance of any difference between any other pair of levels. So in particular, the ANOVA does not tell us whether there is a significant difference between the masses of Chinstrap and Gentoo females (although we may have a good idea what the answer is, from our initial summary table and plot).\nTo find the answer to that, we do post-hoc tests:\n\n\n\n6.4.6 Post hoc tests.\nA final step of most ANOVA analyses is to perform so-called post-hoc (‘after the fact’) tests which make pairwise comparisons between all possible pairs of levels, tell us what the differences are between those pairs and whether these differences are significant. Whatever method is used for this, it needs to take account of the danger of making Type-one errors that arises when multiple pair-wise tests are done.\nA commonly used function for doing this is Tukey’s Honest Signficant Difference: TukeyHSD()\n\nTukeyHSD(aov(body_mass_g ~ species, data = females))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = body_mass_g ~ species, data = females)\n\n$species\n                      diff        lwr       upr     p adj\nChinstrap-Adelie  158.3703   22.32078  294.4197 0.0179471\nGentoo-Adelie    1310.9058 1195.64908 1426.1624 0.0000000\nGentoo-Chinstrap 1152.5355 1011.00620 1294.0648 0.0000000\n\n\nIn each row of the output we see the difference between the mean masses of the females of two species, where a positive value tells us that the first named species has the heavier mass. So, we see that Gentoo females in the sample were on average 1310.9 g heavier than Adelie females.\nCompare these differences with your initial summary table and your box plot. Do they agree? They should!\nThe right-hand column ‘p adj’ tells us whether these difference are significant. If the p values are less than 0.05 then they are, at the 5% significance level. In this case they all are. The p values are so tiny for the differences between Gentoo and the other two species that that they are reported as zero.\n\n\n6.4.7 Reporting the Result.\nWe try to use plain English to report our results, while still telling the reader what test was used and the key outputs of the test. Try to report the name of the test, the test statistic, the degrees of freedom, and the p-value. if. the p-value is really small then it is common to report it as p&lt;0.01, or p&lt;0.001. No one cares if it is a billionth or a squillionth. It just matters that is t is really small, if that is the case. If it is only just below 0.05, then I would report it in full, so we might write p = 0.018. If p &gt; 0.05 then conventiallly it is not reported, except to say p &gt; 0.05.\nIn this case, we might say something like:\nWe find evidence that there is a difference between the body masses of females of the penguin species Adelie, Chinstrap and Gentoo (ANOVA, df = 2, 162, F = 393, p &lt; 0.001). In particular Gentoo are more than 1 kg heavier than the other two (p&lt; 0.001) while the difference between Chinstrap and Adelie is smaller, at 158 g, but still significant (p = 0.018).",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html",
    "href": "ANOVA_two_way_with_model_simplification.html",
    "title": "7  Two-way ANOVA with model simplification",
    "section": "",
    "text": "7.1 Factorial experiments and model simplification\nThis exercise sheet is heavily indebted to Michael Crawley’s Statistics: An introduction using R, 2nd Ed, Wiley. Published in 2015 this emphasises statistics over R (in fact, much of the R he presents is written prior to the advent of the tidyverse dialect which we use here, and so may seem terse if that is what you are used to). It is very useful and is at a higher level than Beckerman, Childs and Petchey’s Getting Started in R: An introduction for biologists, 2nd Ed. OUP published in 2017. Their book also includes a simpler version of the example explored here.\nThe best model is the one that adequately explains the data with fewest parameters. This means with the smallest possible number of degrees of freedom.\nIf we have a very large number of parameters, a model can fit any data set but be of limited use in generalising beyond it (we will have overfitted the data). If we have too few we will not explain much of the variance of the data. A balance must be struck. Hence we want the minimal adequate model.\nAs Einstein almost said, a model should be as simple as possible, but no simpler. (Not to be outdone, the British statistician George Box also had a pithy saying about models: “All models are wrong, but some are useful”.)\nA factorial experiment has two or more factors, each with two or more levels, plus replication for each combination of factor levels. This means that we can investigate whether statistical interactions occur in which the effect of one factor depends on the value of another factor.\nWe take an example from a farm-trial of animal diets. There are two factors: diet and supplement. diet is a factor with three levels: barley, oats and wheat, where barley is the diet that has always been used and the other two are potential alternatives. The purpose of the trial is to see if their use makes a difference to growth outcomes. supplement is a factor with four levels: control, agrimore, supergain and supersupp, where control could mean the absence of any supplement or the supplement used up to now, whose effects we are hoping to improve upon through use of one of the others included in the trial. The response variable gain is weight gain after 6 weeks. There were 48 individual cows in total with 4 for each combination of diet and supplement. Having the same number of replicates for each combination of levels means that this is a balanced design.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#files-needed",
    "href": "ANOVA_two_way_with_model_simplification.html#files-needed",
    "title": "7  Two-way ANOVA with model simplification",
    "section": "7.2 Files needed",
    "text": "7.2 Files needed\nTo follow through this exercise, you should have an RStudio project folder that contains:\n\nIn the Project/scripts folder:\n\nANOVA_two_way_template.Rmd (the script where you fill in the code chunks)\n\nIn the Project/data folder\n\ngrowth.csv\n\n\nYou can download a complete project folder with these files in from here\nIn the following, we present the code you need to analyse this data together with explanatory text. Read the text closely so that you understand what each chunk of code is intended to do. In the accompanying template file, fill in the code as you go in the empty chunks, using this worksheet as a guide. As you complete each line of code, run it using Ctrl-Enter, or Cmd-Enter on a Mac. Alternatively, wait until you have completed the code for a chunk then run the whole chunk in one go by pressing the little green arrow at the top right of the chunk. Whichever way you choose, you are encouraged to view the code presented here as one way to do the analysis. Feel free to hack away at it and change things, to try different approaches and see what happens. That way you will learn. You may also wish to add your own text between the chunks.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#open-your-project",
    "href": "ANOVA_two_way_with_model_simplification.html#open-your-project",
    "title": "7  Two-way ANOVA with model simplification",
    "section": "7.3 Open your Project",
    "text": "7.3 Open your Project\nYou should be working within a folder that you have designated as what RStudio calls a ‘Project’. If you are, the name of your Project will appear at the top right of the RStudio window. Inside your Project folder you should have a scripts folder for scripts like the one you are working from, and a data folder for all the data files. You will also see, at the top level of the Project, the .RProj file. You can see all this in the Files pane, bottom-right.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#load-packages",
    "href": "ANOVA_two_way_with_model_simplification.html#load-packages",
    "title": "7  Two-way ANOVA with model simplification",
    "section": "7.4 Load packages",
    "text": "7.4 Load packages\nI normally load all the packages in the chunk below into every script. The most important is the tidyverse package which is a goody bag containing several other packages. Loading this saves you from having to load each of those individually. The most often used among these is readr for reading and writing data from/to files, dplyr for data manipulation, and ggplot2 for plotting. Others will be used from time to time, and we don’t really need to be aware of that when it happens or to worry about it, so long as tidyverse has been loaded.\n\nlibrary(tidyverse)  # for data manipulation and plotting, and much else besides\nlibrary(here) # for finding our data easily\nlibrary(cowplot) # gives a nice theme for plots\nlibrary(ggfortify) # for diagnostic plots",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#read-in-the-data",
    "href": "ANOVA_two_way_with_model_simplification.html#read-in-the-data",
    "title": "7  Two-way ANOVA with model simplification",
    "section": "7.5 Read in the data",
    "text": "7.5 Read in the data\nThe growth.csv data file needs to be in the data folder within the Project folder.\nIn this chunk we read the growth.csv data into an R object to which we give the name weights.\n\nfilepath &lt;- here(\"data\", \"growth.csv\")\nweights &lt;- read_csv(filepath) # this function is from the readr package, part of tidyverse\nglimpse(weights)\n\nRows: 48\nColumns: 3\n$ supplement &lt;chr&gt; \"supergain\", \"supergain\", \"supergain\", \"supergain\", \"contro…\n$ diet       &lt;chr&gt; \"wheat\", \"wheat\", \"wheat\", \"wheat\", \"wheat\", \"wheat\", \"whea…\n$ gain       &lt;dbl&gt; 17.37125, 16.81489, 18.08184, 15.78175, 17.70656, 18.22717,…\n\n\nYou see from the output of the glimpse() function that weights has three columns and 48 rows. Two columns are of data type &lt;chr&gt; which is R-speak for text, and the other is data type &lt;dbl&gt; which is R-speak for numerical data with a decimal point.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#clean-the-data",
    "href": "ANOVA_two_way_with_model_simplification.html#clean-the-data",
    "title": "7  Two-way ANOVA with model simplification",
    "section": "7.6 Clean the data",
    "text": "7.6 Clean the data\nHaving read in the data some cleaning/wrangling/tidying or processing of the data is often required before we can go further with the analysis.\nWarning! This step can be the most time-consuming of the whole analysis, particularly if you are using a large data set obtained from a third party.\nHere there is not much to do, but we would like to make R recognise the categorical variables as factors, and order the levels.\n\n7.6.1 Force R to recognise supplement and diet as factors, and to reorder their levels.\nAt the moment, the contents within the variables supplement and diet are not being recognised as levels of factors. R is just thinking of them as text (or &lt;chr&gt; in R-speak), as we can see from the output of the glimpse() function in the chunk above. Let us fix that, as it will be useful for them to be recognized for what they are so that we can order the levels in a way that makes sense for our context, our plots and our analysis.\nSometimes levels of a factor have a natural order, such as Low, Mid and High as the levels of the factor Tidal Zone and sometimes they do not, for example Apples, Oranges and Pears as levels of the factor Fruit. Here, in the case of both our factors, we only wish to impose order among the levels in so far as we would like what we regard as the control or reference level to be first. By default, R puts the levels of a factor in alphabetical order. This is the order in which the boxes of a box plot would be displayed, reading left to right. In an ANOVA setting it means that differences of outcome (in this case, weight gain of the cows) are later calculated for each combination of levels with respect to the outcome for the combination of levels that are alphabetically first, in this case barley for diet and agrimore for supplement. In both the box-plot and the ANOVA output case this default ordering is not necessarily what we want. Normally, we want what we regard as the control levels to be the reference level and in this case that means barley for diet and control for supplement.\nTo ensure that a variable is regarded as a factor, and then to get its levels in the order we would like, we use the factor() function.\nIn the following chunk, factor() is used to designate both the supplement and diet columns of the data set as factors, and the level order of each is specified, with control coming first for supplement and barley coming first for diet.\n\n# This line of code designates the supplement and diet columns of weights as factors, orders the levels of these factors as required and saves the result under the original name.\nweights &lt;- weights |&gt;\n  mutate(supplement = factor(supplement, levels = c(\"control\",\"agrimore\", \"supergain\", \"supersupp\"))) |&gt;\n  mutate(diet = factor(diet, levels = c(\"barley\", \"oats\", \"wheat\")))\n\n# check that this worked\nglimpse(weights)\n\nRows: 48\nColumns: 3\n$ supplement &lt;fct&gt; supergain, supergain, supergain, supergain, control, contro…\n$ diet       &lt;fct&gt; wheat, wheat, wheat, wheat, wheat, wheat, wheat, wheat, whe…\n$ gain       &lt;dbl&gt; 17.37125, 16.81489, 18.08184, 15.78175, 17.70656, 18.22717,…\n\n# check the level order of each factor - does the 'reference' level come first?\nlevels(weights$supplement)\n\n[1] \"control\"   \"agrimore\"  \"supergain\" \"supersupp\"\n\nlevels(weights$diet)\n\n[1] \"barley\" \"oats\"   \"wheat\" \n\n\nDo you see how the variable types of the supplement and diet columns have been changed to &lt;fct&gt;? It worked!\nTo get control to be the reference level of supplement we needed to force the issue in this way. If we hadn’t then agrimore would have been regarded as such, since it is alphabetically the first among the levels of supplement. We didn’t need to do this for diet, since the stipulated ordering of the levels is just the alphabetical order and so we would have had that by default anyway. Sometimes, though, it doesn’t hurt to throw in a little redundancy for the sake of clarity.\nSo, now we have control as the reference level for supplement and barley as the reference level for diet. Now we can see more easily in our analysis what difference is made to weight gain when we change diet or supplement or both from a ‘business as usual’ combination of a barleydiet and the control supplement.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#summarise-the-data",
    "href": "ANOVA_two_way_with_model_simplification.html#summarise-the-data",
    "title": "7  Two-way ANOVA with model simplification",
    "section": "7.7 Summarise the data",
    "text": "7.7 Summarise the data\nOur question is a difference question: is there evidence from the data that using this or that diet in combination with this or that supplement makes a difference to growth? For an answer to this we will end up doing a 2-way ANOVA including the possibility of an interaction, then, as we will see, a simpler ANOVA that ignores the possibility of the interaction. All well and good, but before we go to those lengths, we do something more basic: we calculate the mean and standard error of the mean for each of the twelve combinations of diet and supplement.\nThere isn’t a function in base R with which we can calculate standard error of the mean directly, but we can do so knowing the standard deviation of the sample \\(\\text{SD}\\) (using sd()) and the sample size \\(n\\) (using n()) using this formula:\n\\[ \\text{SE}=\\frac{SD}{\\sqrt{n}}\\]\n\n# we use the group_by() and summarise() functions from dplyr (the package within tidyverse for data manipulation)\ngrowth_summary &lt;- weights |&gt;\n  group_by(diet, supplement) |&gt;\n  summarise(mean_gain = mean(gain), se_gain = sd(gain)/sqrt(n())) |&gt;\n  ungroup()\n\n# if we type the name of an object, it gets printed out for us\ngrowth_summary |&gt; kable()\n\n\n\n\n\n\ndiet\nsupplement\nmean_gain\nse_gain\n\n\n\n\nbarley\ncontrol\n23.29665\n0.7032491\n\n\nbarley\nagrimore\n26.34848\n0.9187479\n\n\nbarley\nsupergain\n22.46612\n0.7710644\n\n\nbarley\nsupersupp\n25.57530\n1.0599015\n\n\noats\ncontrol\n20.49366\n0.5056319\n\n\noats\nagrimore\n23.29838\n0.6131592\n\n\noats\nsupergain\n19.66300\n0.3489388\n\n\noats\nsupersupp\n21.86023\n0.4132292\n\n\nwheat\ncontrol\n17.40552\n0.4604420\n\n\nwheat\nagrimore\n19.63907\n0.7099260\n\n\nwheat\nsupergain\n17.01243\n0.4852821\n\n\nwheat\nsupersupp\n19.66834\n0.4746443\n\n\n\n\n\n\n\nNote the ordering of the diet and supplement levels in their respective columns: just what we have imposed!",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#plot-the-data",
    "href": "ANOVA_two_way_with_model_simplification.html#plot-the-data",
    "title": "7  Two-way ANOVA with model simplification",
    "section": "7.8 Plot the data",
    "text": "7.8 Plot the data\nThe next step, as so often before we launch into actual statistics, is to plot the data in a way that sheds light on the question we have. Here, we can use the use the means and standard errors of the mean that we have just calculated to produce a useful kind of line plot that in this context is often referred to as an interaction plot:\n\ngrowth_summary |&gt;\n  ggplot(aes(x = supplement,y = mean_gain, colour = diet, group = diet)) +\n  geom_point(size = 2) +\n  geom_line() +\n  geom_errorbar(aes(ymin = mean_gain - se_gain, ymax = mean_gain + se_gain), width = 0.1) +\n  labs(x = \"Supplement\",\n       y = \"Mean weight gain\") +\n  scale_fill_brewer() +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nNote that on this plot the error bars are standard errors of the mean. Any caption to a figure that contains error bars should explain what those error bars mean. In particular, it should say whether they are standard deviations of the sample, standard errors of the mean or confidence intervals. These are all different from each other. A good explanation of the difference is given by (Cumming, Fidler, and Vaux 2007)\nThis interaction plot is useful in that we see that both diet and supplement have an effect on growth and that the effect of one is altered little by the value of the other, the result of which is that the lines are more or less parallel. This suggests that we have main effects of both diet and supplement, but little or no interaction between them.\n\n7.8.1 Questions\nWhat could the line plot look like if:\n\nThere were no main effect of both diet and supplement, and no interaction\nThere were a main effect of diet, no main effect of supplement and no interaction?\nThere were no main effect of diet, a main effect of supplement and no interaction?\nThere were main effects of both and an interaction between them?\n\nThe plots tell you a great deal about what main effects and/or interactions there may be.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#anova",
    "href": "ANOVA_two_way_with_model_simplification.html#anova",
    "title": "7  Two-way ANOVA with model simplification",
    "section": "7.9 ANOVA",
    "text": "7.9 ANOVA\nNow for the actual statistical test. We will conduct a two-way ANOVA, which will look to see if there is evidence that either diet or supplement or both affect growth rate (the so-called main effects), and if the effect of one depends on the nature of the other (the so-called interaction).\nThe null hypothesis is that neither has any main effect and that there is no interaction.\nNow we can use either of the functions aov() or lm() to carry out a factorial ANOVA (the choice affects only whether we get an ANOVA table or a list of parameter estimates as the default output from summary().). Here, we will use lm(), partly because we would also use it for one-way ANOVAs and linear regression, and to do so here reminds of the common mathematical machinery that underlies all these methods.\nWe estimate parameters for the main effects of each level of diet and each level of supplement, plus terms for the interaction between diet and supplement.\nThe interaction degrees of freedom are the product of those for diet and supplement ie (3-1) x (4-1) = 6.\nThe model is:\ngain ~ diet + supplement + diet:supplement\nwhich can be written more simply using the asterisk notation as:\ngain ~ diet * supplement\n\n7.9.1 Construct the model\nFirst we construct the model using lm() and store the outputs of all the maths that `lm() does in an object called model0:\n\nmodel0 &lt;- lm(gain ~ diet * supplement, data = weights)\n\n\n\n7.9.2 Do we reject the null hypothesis?\nTo get an overall picture, we first use anova() to see if there is evidence to reject the null\n\nanova(model0)\n\nAnalysis of Variance Table\n\nResponse: gain\n                Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet             2 287.171 143.586 83.5201 2.999e-14 ***\nsupplement       3  91.881  30.627 17.8150 2.952e-07 ***\ndiet:supplement  6   3.406   0.568  0.3302    0.9166    \nResiduals       36  61.890   1.719                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA table shows that there a main effect of both diet and supplement (p&lt;0.001 in both cases), but that there is no hint of an interaction between diet and supplement (p = 0.917). Does that tally with what you see in the interaction plot? Clearly therefore, the effects of diet and supplement are merely additive (ie whichever level of one you have it does not affect the impact on growth of whichever level of the other you choose).\nThe ANOVA table does not show us effect sizes or allow us to work out which if any of the levels of the two factors are significantly different. For this, summary() is more useful:\n\nsummary(model0)\n\n\nCall:\nlm(formula = gain ~ diet * supplement, data = weights)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.48756 -1.00368 -0.07452  1.03496  2.68069 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   23.2966499  0.6555863  35.536  &lt; 2e-16 ***\ndietoats                      -2.8029851  0.9271390  -3.023  0.00459 ** \ndietwheat                     -5.8911317  0.9271390  -6.354 2.34e-07 ***\nsupplementagrimore             3.0518277  0.9271390   3.292  0.00224 ** \nsupplementsupergain           -0.8305263  0.9271390  -0.896  0.37631    \nsupplementsupersupp            2.2786527  0.9271390   2.458  0.01893 *  \ndietoats:supplementagrimore   -0.2471088  1.3111726  -0.188  0.85157    \ndietwheat:supplementagrimore  -0.8182729  1.3111726  -0.624  0.53651    \ndietoats:supplementsupergain  -0.0001351  1.3111726   0.000  0.99992    \ndietwheat:supplementsupergain  0.4374395  1.3111726   0.334  0.74060    \ndietoats:supplementsupersupp  -0.9120830  1.3111726  -0.696  0.49113    \ndietwheat:supplementsupersupp -0.0158299  1.3111726  -0.012  0.99043    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.311 on 36 degrees of freedom\nMultiple R-squared:  0.8607,    Adjusted R-squared:  0.8182 \nF-statistic: 20.22 on 11 and 36 DF,  p-value: 3.295e-12\n\n\nThis is a complex model as there are 12 estimated parameters: 6 main effects and 6 interactions. Notice that although the ‘controls’ for diet and supplement (barley and control) do not appear to be in the table, they are there really, in the first row.\nThe value 23.30 kg in the first row of the Estimate column on the left, labelled ‘Intercept()’ gives us the actual weight gain outcome for the combination of the two control levels, barley as diet and control as supplement. Check that this value tallies with what is shown in summary tables above, and in the interaction plot.\nThe weight gain values for all the other combinations of the levels of each factor are given as differences from this reference level.\nSo for example in row two, where diet is changed from barley to oats but supplement is still control, the value in the table is -2.8. This means that the weight gain when the diet is changed to oats but the supplement left as the control is 2.80 kg less than the reference value, and so must be 23.30-2.80 = 20.50 kg. This agrees with the value in the summary table of mean values that was calculated above, and tallies with the interaction plot.\nIn row seven we see that the effect of the interaction between the diet oats and the supplement agrimore is - 0.247. This means that on going from the reference levels of barley and control, for which the gain is 23.30, the change in gain is not just the sum of the two main effects (-2.80 for switch of diet to oats and +3.05 for switch of supplement to agrimore, but is modified by their interaction, of size - 0.247. Hence the mean gain for a diet of oats and a supplement of agrimore is the intercept value plus the sum of the two main effects, plus the interaction term: 23.297 - 2.803 + 3.052 - 0.247 = 23.299)\nSee if you can tally the other effect values in the summary table with the mean values given in table above and in the interaction plot for other combinations of diet and supplement.\nHere is a table to help you interpret the output of the summary() function.\n\n\n\n\n\nterm\nmeaning\ntype_of_effect\nestimate\nabsolute_value\np_value\nsignificance\n\n\n\n\n(Intercept)\nbarley + control\nMain effect\n23.30\n23.30\n&lt;0.001\n***\n\n\ndietoats\noats + control\nMain effect\n-2.80\n20.49\n0.005\n**\n\n\ndietwheat\nwheat + control\nMain effect\n-5.89\n17.41\n&lt;0.001\n***\n\n\nsupplementagrimore\nbarley + agrimore\nMain effect\n3.05\n26.35\n0.002\n**\n\n\nsupplementsupergain\nbarley + supergain\nMain effect\n-0.83\n22.47\n0.376\n\n\n\nsupplementsupersupp\nbarley + supersupp\nMain effect\n2.28\n25.58\n0.019\n*.\n\n\ndietoats:supplementagrimore\noats + agrimore\nInteraction\n-0.25\n23.05\n0.852\n\n\n\ndietwheat:supplementagrimore\nwheat + agrimore\nInteraction\n-0.82\n22.48\n0.537\n\n\n\ndietoats:supplementsupergain\noats + supergain\nInteraction\n0.00\n23.30\n1.0\n\n\n\ndietwheat:supplementsupergain\nwheat + supergain\nInteraction\n0.44\n23.73\n0.741\n\n\n\ndietoats:supplementsupersupp\noats + supersupp\nInteraction\n-0.91\n22.38\n0.491\n\n\n\ndietwheat:supplementsupersupp\nwheat + supersupp\nInteraction\n-0.02\n23.28\n0.99\n\n\n\n\n\n\n\n\nThe output of the summary() function re-emphasises that none of the interaction terms are significant. It also suggests that a minimum adequate model will contain 5 parameters: an intercept, which just means that there is non-zero growth when the diet and supplement are the reference values, a difference from that growth due to changing the diet to oats, a difference due to changing it towheat, a difference due to changing the supplement to agrimore while keeping barley as the diet, and a difference due to changing the supplement instead to suppersupp..",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#model-simplification",
    "href": "ANOVA_two_way_with_model_simplification.html#model-simplification",
    "title": "7  Two-way ANOVA with model simplification",
    "section": "7.10 Model Simplification",
    "text": "7.10 Model Simplification\nGiven the results of the full interaction model, we begin model simplification by leaving out the interaction terms, to leave us with an additive model:\n\nmodel_1 &lt;- lm(gain ~ diet + supplement, data = weights)\nsummary(model_1)\n\n\nCall:\nlm(formula = gain ~ diet + supplement, data = weights)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.30792 -0.85929 -0.07713  0.92052  2.90615 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          23.4263     0.4408  53.141  &lt; 2e-16 ***\ndietoats             -3.0928     0.4408  -7.016 1.38e-08 ***\ndietwheat            -5.9903     0.4408 -13.589  &lt; 2e-16 ***\nsupplementagrimore    2.6967     0.5090   5.298 4.03e-06 ***\nsupplementsupergain  -0.6848     0.5090  -1.345 0.185772    \nsupplementsupersupp   1.9693     0.5090   3.869 0.000375 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.247 on 42 degrees of freedom\nMultiple R-squared:  0.8531,    Adjusted R-squared:  0.8356 \nF-statistic: 48.76 on 5 and 42 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#check-the-validity-of-the-additive-model",
    "href": "ANOVA_two_way_with_model_simplification.html#check-the-validity-of-the-additive-model",
    "title": "7  Two-way ANOVA with model simplification",
    "section": "7.11 Check the validity of the additive model",
    "text": "7.11 Check the validity of the additive model\nWe ought to pause here for a moment and just check that we are OK to go ahead and analyse our data using a general linear model (of which ANOVA is an example, linear regression and t-tests being others). We will use autoplot() from the ggfortify package, which gives us the standard four diagnostic plots.\n\nautoplot(model_1) + theme_cowplot() # autoplot() is from the ggfortify package.\n\n\n\n\n\n\n\n\nWell, that all looks fine. In particular, from the top-left figure we see that the variance of the residuals is more or less constant and from the top-right figure, the quantile-quantile plot, we get a pretty good approximation of a straight line which tells us that the residuals are more or less normally distributed. These are two key assumptions that must be at least approximately satisfied by data if it is going to make any sense to use a linear model to analyse it. We won’t discuss here the other two diagnostic plots, but they look fine too. So we are good to go using ANOVA with this data.\nBack to interpreting the output of the ANOVA:\nIt is clear that we need to retain all three levels of diet since the effect values of each differ from each other by an amount that is several times the standand errors, so that t &gt;&gt; 1. It is not clear that we need all the levels of supplement, however. supersupp is not obviously different from agrimore (difference = -0.727 with standard error = 0.509), yet both are clearly different from control. However supergrain is not obviously different from control (difference = -0.68, error = 0.509). Hence we are tempted to try a new model with just two levels of the factor supplement which we might sensibly call “best”, by which we mean agrimore or supersupp, and “worst” by which we mean control or supergrain. We’ll name this new factor supp2.\nThis code chunk amends the weights data frame by adding a new column to it called supp2 in which the values are either best if the supplement is agrimore or supersupp, or worst if the supplement is either of the other two\n\nweights &lt;- weights |&gt;\n  mutate(supp2 = ifelse(supplement %in% c(\"agrimore\", \"supersupp\"), \"best\", \"worst\"))\nglimpse(weights)\n\nRows: 48\nColumns: 4\n$ supplement &lt;fct&gt; supergain, supergain, supergain, supergain, control, contro…\n$ diet       &lt;fct&gt; wheat, wheat, wheat, wheat, wheat, wheat, wheat, wheat, whe…\n$ gain       &lt;dbl&gt; 17.37125, 16.81489, 18.08184, 15.78175, 17.70656, 18.22717,…\n$ supp2      &lt;chr&gt; \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"wors…\n\n\nIf we calculate the means and standard errors for weight gain under each diet for each of the two new classifications of supplement, and then plot them, we get this new interaction plot:\n\nweights |&gt;\n  group_by(diet, supp2) |&gt;\n  summarise(mean_gain = mean(gain), se_gain = sd(gain)/sqrt(n())) |&gt;\n  ungroup() |&gt;\n\n  ggplot(aes(x = supp2,y = mean_gain,colour = diet, group=diet)) +\n  geom_point(size=2) +\n  geom_line() +\n  geom_errorbar(aes(ymin = mean_gain-se_gain, ymax = mean_gain + se_gain), width=0.1) +\n  labs(x = \"Supplement\",\n       y = \"Mean weight gain\") +\n  scale_fill_brewer() +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nFrom this we can see that diet clearly makes a difference to weight gain, since the three lines are separated by a distance much larger than the standard errors, and also that the best supplement clearly makes a difference since there is a consistent drop on going from ‘best’ to ‘worst’, again by an amount that is much larger than the error bars, and there is clearly no interaction between diet and supplement, since the lines are parallel within the wiggle-room allowed by the error bars, which means that the effect of diet does not depend on supplement, and the effect of supplement does not depend on diet.\nNow we will make the simpler model, calling it model_2 (for comparison with the first additive model, model_1)\n\n# additive model whee the supplements have been condensed from four to two: best and worst\nmodel_2 &lt;- lm(gain ~ diet + supp2, data = weights)\n\nand then compare the two additive models:\n\nanova(model_1, model_2)\n\nAnalysis of Variance Table\n\nModel 1: gain ~ diet + supplement\nModel 2: gain ~ diet + supp2\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     42 65.296                           \n2     44 71.284 -2   -5.9876 1.9257 0.1584\n\n\nWhen we use anova() in this way it is testing the explanatory power of the second model against that of the first ie how much of the variance in the data does each explain. Its null hypothesis is that both models explain just as much of the variance as the other.\nThe simpler model has saved two degrees of freedom and is not significantly different in explanatory power than the more complex model (p = 0.158). Hence this is a better candidate as a minimal adequate model. All the parameters are significantly different from zero and from each other.\n\nsummary(model_2)\n\n\nCall:\nlm(formula = gain ~ diet + supp2, data = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6716 -0.9432 -0.1918  0.9293  3.2698 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  25.7593     0.3674  70.106  &lt; 2e-16 ***\ndietoats     -3.0928     0.4500  -6.873 1.76e-08 ***\ndietwheat    -5.9903     0.4500 -13.311  &lt; 2e-16 ***\nsupp2worst   -2.6754     0.3674  -7.281 4.43e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.273 on 44 degrees of freedom\nMultiple R-squared:  0.8396,    Adjusted R-squared:  0.8286 \nF-statistic: 76.76 on 3 and 44 DF,  p-value: &lt; 2.2e-16\n\n\nIn this table,\n\nline one (Intercept) tells us that the mean weight gain when on the barley diet and best supplement is 25.76 kg\nline two dietoats tells us that there is a significant drop in weight gain of 3.1 kg when diet is changed to oats.\nline three dietwheat tells us that there is a significant drop in weight gain of 5.99 kg when diet is changed to wheat.\nline four supp2worst tells us that there is a significant drop in wight gain of 2.68 kg when supplement is changed to worst.\n\nIn all cases, p&lt; 0.001, as indicated not only by the number in the Pr(&gt;|t|) column, but also by the ‘***’ in the right-most column of the table.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html#reporting-the-results",
    "href": "ANOVA_two_way_with_model_simplification.html#reporting-the-results",
    "title": "7  Two-way ANOVA with model simplification",
    "section": "7.12 Reporting the results",
    "text": "7.12 Reporting the results\nWe have now reduced our initial 12 parameter model to a four parameter model that is much more tractable and easier to communicate. Our advice would be that for maximum weight gain a diet of barley with a supplement of agrimore or supersupp would be best.\nIf we were reporting this as a statistical test, we might say something like: A diet of barley with a supplement of agrimore or supersupp was to offer significant improvements over alternatives. There was no evidence of any interaction between diet and supplement. (ANOVA 2-way, F3,44 = 76.76, p &lt; 0.001)\n\n\n\n\nCumming, Geoff, Fiona Fidler, and David L. Vaux. 2007. “Error Bars in Experimental Biology.” Journal of Cell Biology 177 (1): 7–11. https://doi.org/10.1083/jcb.200611141.",
    "crumbs": [
      "Tests for difference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "chi-square.html",
    "href": "chi-square.html",
    "title": "8  Chi-squared analysis of count data",
    "section": "",
    "text": "8.1 Chi-square goodness of fit test\nA chi-square analysis is used when our data are in the form of raw counts for two or more categorical groups eg pea plants with either yellow peas or green peas, survival rate of mice if they took drug A or took drug B, etc. Each independent observation must definitely belong to either one group or the other, and there are no replicates. That is, for each category we just have one count.\nWhat we do is compare the counts we got to some expected value according either to chance or to some prior theory.\nFor example:\nIn a chi-square ‘goodness of fit’ test, we are testing data where we have a number of counts for each of two or more possible outcomes of some procedure (heads/tails, dice scores etc). We have an idea of how these counts should be distributed under some null hypothesis (the coin is fair, the dice is fair, genetic inheritance works in this or that way etc). The chi-square goodness of fit test tests how likely it is we would have got the counts we actually got if that null hypothesis were correct. We are testing how well our actual counts ‘fit’ the expected values.\nIn a typical software implementation of the test, such as in R, we give it the counts we actually got for each possible outcome and also the expected proportion for each outcome. The test then gives us a p-value, a probability, for how likely it is that we would have got the counts we actually got, or counts even further from the null hypothesis, if that null hypothesis were correct. If this p-value is too small, and by that we usually mean less than 0.05, then we reject the null hypothesis.",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#chi-square-goodness-of-fit-test",
    "href": "chi-square.html#chi-square-goodness-of-fit-test",
    "title": "8  Chi-squared analysis of count data",
    "section": "",
    "text": "8.1.1 Example\nSuppose we have crossed pea plants that were all heterozygous for yellow/green pea colour. In the F1 generation we get 176 offspring , of which 130 were yellow and 46 were green.\nThe data here are raw counts, and an individual pea plant offspring contributes either to the yellow count or to the green count, but not to both.\nOur expected counts of yellow and green are found by simply dividing the total count of offspring, 176, in the ratio 3:1, giving us an expected 132 yellow pea plants and an expected 44 green pea plants in the offspring F1 generation.\n\n\n8.1.2 Doing the chi-square test in R\nWhat we do in R is use the chisq.test() function to see how likely it is we would have got counts of 130 and 46 if the null hypothesis, with its expected counts in the ratio 3:1, were true.\nWe do it like this:\nchisq.test(c(130,46),p=c(0.75,0.25))\nThere are two arguments. The first is the counts we got, which we enter as a ‘vector’ c(z,y,....), so we write c(130,46). The second is a vector of the proportions we expect for the two counts, where these proportions should add up to one. So for our expected 3:1 ratio we enter c(0.75,0.25).\nLet’s do it: type the above function into the console window (bottom left). You will get an output something like this:\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  c(130, 46)\nX-squared = 0.12121, df = 1, p-value = 0.7277\n\n\nThis output is typical of tests done in R. We get the ‘test statistic’ whose name varies depending on the test. Here it is called X-squared, pronounced chi-squared. This is a number that the test calculates, based on the data you have given it. For the most part, we don’t need to worry about how it does that. Then there is the p-value, which is the probability of getting this test statistic if the null hypothesis were true.\nIn this case, we see that the p-value is 0.73, which is large. We could very plausibly have got yellow:green numbers of 130 and 46 if the null hypothesis were true, so we cannot reject that null hypothesis. In other words, our data are consistent at the 5% significance level with the predictions of simple Mendelian inheritance.\n\n\n8.1.3 Reporting the result in English\nIn English, we might report this result as:\nWe found counts of 130 yellow plants and 46 green plants, which are consistent at the 5% significance level with the predictions of Mendelian inheritance (chi-squared test, X-squared = 0.12, p=0.73).\nNote that we do not say we have proved Mendelian inheritance to be correct. We haven’t. We never prove things in science. We haven’t said anything about the truth of the null hypothesis. All we can say is whether our data are or are not consistent with the null hypothesis. In this case they are. We then report the test we used and the values of the test statistic and p-value. Other tests might give you other details to report too.\n\n\n8.1.4 Exercises\nExercise 1\nSuppose you tossed a fair coin 100 times and got 45 heads and 55 tails.\n\nUnder a null hypothesis that the coin is fair, what would the expected numbers of heads and tails be?\n\nYou use R to do a chi-square test of that null hypothesis. Here is the code to do that and the output it would give:\n\nchisq.test(c(45,55),p=c(0.5,0.5)) # we could leave out the second argument here\n\n\n    Chi-squared test for given probabilities\n\ndata:  c(45, 55)\nX-squared = 1, df = 1, p-value = 0.3173\n\n\n\nWhat do you conclude?\nHow would you report the result?\n\nExercise 2\nSuppose someone told you that the competence of scientists was linked to their astrological zodiac sign. I won’t name all of these, but there are twelve of them: Pisces, Scorpio, Cancer etc. To test this hypothesis, you spend a lot of time on Primo and identify 240 scientists, currently active, that have each published at least five papers in high impact journals in the last year. All of these people, you presume, are successful scientists. You write to each of them and ask them their date of birth. Amazingly(!), all of them respond. You then assign each of them to a zodiac sign according to their birth date and get the following counts for each sign:\nIn this code chunk we have typed out the counts and collected them as a vector, using the function `c()`. we have saved this under the name stars.\n\nstars&lt;-c(22,20,17,22,20,19,18,21,19,22,23,17)\n\n\nWhat would be a suitable null hypothesis in this investigation?\nWhat proportion of the total count would we expect for each star sign if this null were true?\nThe data meet the criteria required for use of a chi-square goodness of fit test. How can we tell?\nUse the chisq.test() function to implement this test.\nOn the basis of the output of the test, do you reject the null hypothesis?\nReport the result of the test in plain English.\n\n\n\n8.1.5 Solutions\nSolution 1\nThe expectation is that half the outcomes would be heads and half would be tails.\nThe null hypothesis of this test is that heads and tails are equally likely, ie that the coin is fair. Under this null hypothesis the expected outcome is 50 heads and 50 tails. From the output of the R code we see that the p-value, the probability of getting an outcome as far or further from that, is 0.317. That is pretty high. Would you do anything if you knew that the probability of a bad (or worse) outcome was 0.317? In particular, this p-value is greater than 0.05, so we cannot reject the null hypothesis that the coin is fair. That is, even with a fair coin it is not at all unlikely that you would get head/tail numbers as different from 50/50 as 45/55 if you tossed the coin 100 times. That will happen about 1/3 of the time if you repeatedly do trials where you toss the coin 100 times.\nTo report this result, you might say something like\nFrom 100 coin tosses we got 45 heads and 55 tails. These counts are consistent at the 5% significance level with the coin being fair (chi-squared test, X-squared = 1, p=0.317).\nSolution 2\n\nH0: There is no association between the astrological star sign of a researcher and their success in science (who knew?)\nOne twelfth for each sign ie a researcher is as likely to have one star sign as any other.\nThese are count data, there are at least five counts for every sign and the counts are independent - any individual researcher only contributes to one of the twelve counts.\nNote that we do not need to include the second p=... argument in this case since the default presumption, that all proportions are equal, is true here.\n\n\nchisq.test(stars)\n\n\n    Chi-squared test for given probabilities\n\ndata:  stars\nX-squared = 2.3, df = 11, p-value = 0.9971\n\n\n\nWe see that the p-value is almost one so we emphatically do not reject the null hypothesis.\nWe find no evidence that star sign affects success in science (X-sq=2.29, df = 11, p=0.997)\n\nNote the degrees of freedom that is reported: df = 11. The degrees of freedom is the number of independent pieces of information. Here, given that we know the total number of researchers, only eleven of the individual counts are independent. Once they are known, the twelfth can be calculated.",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#two-way-chi-square-analysis-test-of-independence",
    "href": "chi-square.html#two-way-chi-square-analysis-test-of-independence",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.2 Two-way Chi square analysis: test of independence",
    "text": "8.2 Two-way Chi square analysis: test of independence\nAdapted from Chapter 5: Beckerman, Childs and Petchey: Getting Started with R\n\nFor R code to implement this section, scroll down to the next section *\n\nA common scenario where we have with count data is that there are two explanatory factors each with two or more levels that enable us to classify the data. This can happen when something is either true or not true, and a test for for this truth gives either a positive or a negative result. We might want to know if the test to determine truth was any better than flipping a coin: is there some association between what the truth is (eg I do or do not have a disease) and what the test says about that (testing positive or negative for the disease). In this example our data would be counts of people in each of four categories: have the disease / test positive, have the disease / test negative, do not have the disease / test positive and do not have the disease / test negative.\n\n8.2.1 Example - red and black ladybirds\nWe are going to analyse a scenario of this type to see if there is evidence for an association between two factors. Suppose we have some count data of ladybirds found in both an industrial and a rural location. In each location, some of the ladybirds are red and some are black. We would like to test for whether there is an association between the ladybird colour and its location. If there isn’t then we would expect the proportion of black to red ladybirds to be roughly the same in both habitats. If there is, then we would expect the proportions to be different, meaning that knowing the habitat would tell us something about the likelihood of a ladybird found there being black or red. That is way of saying that the colour of the ladybirds would not be independent of the location.\nBehind this the research purpose might be to investigate whether matching of morphological colour of the ladybirds to the prevalent colour of the environment confers an evolutionary advantage. If it does then we would expect there to be an association between morphological colour and environment so that the proportion of black to red ladybirds would be higher in a grimy industrial setting than in it would be in a rural setting.\nA Chi-Square contingency analysis can be used to investigate this. This type of analysis is used when you have\n\nCount data - for example, how many red ladybirds in a rural setting, how many in an industrial setting, how many black ladydbirds in each of the settings?\nEnough count data - typically at least 5 individuals for each combination of the levels in question, which would be rural/red, rural/black, industrial/red and industrial/black in this case.\nIndependent counts - each ladybird contributes to only one sub-total. For example, if it is red and found in a rural location, then it contributes to the count of red ladybirds found in a rural location, and not to any other sub-total, such as black ladybirds found in a rural location.\n\n\n8.2.1.1 Hypotheses\nWhat do you think a suitable hypothesis should be for this investigation, and what would the corresponding null hypothesis be?\n\nThe null hypotheses could be: H0: There is no association between habitat and ladybird colour. This means that whatever the proportion is of black to red ladybirds, it is the same in both habitats.\n\nThe alternate hypothesis could be: H1: There is an association between habitat and ladybird colour.\n\n\n# Load packages we need\nlibrary(tidyverse)\nlibrary(here)\nlibrary(cowplot) # this makes your plots look nicer\nlibrary(kableExtra)\ntheme_set(theme_cowplot()) # this sets the cowplot theme to be the default theme for any plots we make..\n\n\n# Import the data\nfilepath&lt;-here(\"data\",\"ladybirds_morph_colour.csv\")\nlady&lt;-read_csv(filepath)\n#glimpse(lady)\n\n\n\n8.2.1.2 The data\nThe data consist of counts of the number of ladybirds of each colour that were observed in 5 rural sites and 5 industrial sites. These data are in tidy form, with one variable per column. There are 20 rows, with each row containing the count of either red or black ladybirds found at a given site.\n\n\n# A tibble: 20 × 4\n   Habitat    Site  morph_colour number\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1 Rural      R1    black            10\n 2 Rural      R2    black             3\n 3 Rural      R3    black             4\n 4 Rural      R4    black             7\n 5 Rural      R5    black             6\n 6 Rural      R1    red              15\n 7 Rural      R2    red              18\n 8 Rural      R3    red               9\n 9 Rural      R4    red              12\n10 Rural      R5    red              16\n11 Industrial U1    black            32\n12 Industrial U2    black            25\n13 Industrial U3    black            25\n14 Industrial U4    black            17\n15 Industrial U5    black            16\n16 Industrial U1    red              17\n17 Industrial U2    red              23\n18 Industrial U3    red              21\n19 Industrial U4    red               9\n20 Industrial U5    red              15\n\n\nThe total counts for red and black ladybirds observed in industrial and rural settings are shown below:\n\n# Calculate the totals of each colour in each habitat.\ntotals&lt;- lady |&gt;\n  group_by(Habitat,morph_colour) |&gt;\n  summarise (total.number = sum(number))\n# totals |&gt;\n#   kbl() |&gt;\n#   kable_styling(full_width=FALSE)\ntotals\n\n# A tibble: 4 × 3\n# Groups:   Habitat [2]\n  Habitat    morph_colour total.number\n  &lt;chr&gt;      &lt;chr&gt;               &lt;dbl&gt;\n1 Industrial black                 115\n2 Industrial red                    85\n3 Rural      black                  30\n4 Rural      red                    70\n\n\n\n\n8.2.1.3 Plot the data.\nFrom these totals we can create a bar chart:\n\n# plot the data, with sensible colours\ntotals |&gt;\n  ggplot(aes(x = Habitat,y = total.number,fill=morph_colour))+\n  geom_col(position='dodge') +\n  labs(x=\"Habitat\",\n       y=\"Count\",\n       fill= \"Colour\") +\n  scale_fill_manual(values=c(black='#312626',red='#da1717')) + # this line manually sets the fill colours for us\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\n\n8.2.1.4 Interpret the graph before we do any ‘stats’\nLook at the plot - does it look as though the proportion of black to red ladybirds is the same in the two habitats? Do you expect to retain or to reject the null hypothesis, which says that there is no association between habitat and ladybird colour, and hence that the proportions are the same?\nA chi-square test of independence will enable us to determine how likely it is that we would have got proportions of black to red as different as or more different than they actually are if the null hypothesis were true.\n\n\n8.2.1.5 The Chi-square test\nTo do the chi square test, it helps to set out our count data as a 2 x 2 table of total counts:\n\nlady.mat&lt;-xtabs(number~Habitat + morph_colour, data=lady)\nlady.mat\n\n            morph_colour\nHabitat      black red\n  Industrial   115  85\n  Rural         30  70\n\n\nThis kind of table is sometimes called a contingency table.\nWhen we give these numbers to some statistical software such as R and ask it to carry out a ‘chi-square test’ it will use the data to calculate a ‘test statistic’ \\(X^2\\) by comparing the actual counts of the ladybirds in the table above with their expected counts under the null hypothesis. The further the actual counts are from their expected values, on the whole, the bigger this test statistic will be. For the gory (they are not that gory!) details on how this is done, see section 4 below but do note that, while these are interesting, if you find that kind of thing interesting, as I do, you do not need to be familiar with them to be able to apply a chi-square test. What you do need to know is when it is OK to use one, and when it is not, as is true for any statistical test.\nWe’ll turn to that issue now:\nProviding a number of conditions are met by the data (principally, that they are count data, that all the cell values are greater than or equal to about five and that they are all independent ie any ladybird contributes to the count of only one cell), this test statistic \\(X^2\\) has a so-called ‘chi-squared’ distribution. This is a known mathematical distribution, which makes it possible to calculate the probability that the statistic would be as big as it is, or bigger, if the null hypothesis were true. We call this probability the p-value.\nThis is generally how statistical tests work. They take your data and use it in a carefully chosen way to calculate some number that in general is called a test statistic but which is referred to by different names when calculated for particular tests. How it is calculated depends on the test and these days we never have to manually do the calculations ourselves. That’s taken care of by software like R. Providing the data meet certain criteria, the statistic will typically have a known probability distribution. This means that the probability that it will exceed a given value if the null hypothesis is true can be calculated. This probability is the p-value. If the p-value is very small, and by that we typically mean less than 0.05 or 0.01, then we can reject the null hypothesis.\nWhen we run a chi-square test in R on the data in the table above it gives us this as output:\n\nchisq.test(lady.mat)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  lady.mat\nX-squared = 19.103, df = 1, p-value = 1.239e-05\n\n\n\n\n8.2.1.6 Conclusion\nStudy the output of the chi-square test. Note that you are given a test-statistic (here called Chi-squared/X-squared) and a number of degrees of freedom (df) (in some tests you are given more than one of these). This is the number of independent pieces of information used to calculate the test statistic. Lastly, you are a given a p-value. This is the probability that you would have got a chi-squared value as big or bigger than the one you got if the null hypothesis were true. Here the null hypothesis is that there is no association between ladybird colour and location. Put another way, it is, roughly speaking the probability of getting the data you actually got if the null hypothesis were true.\nHere, the p-value is much less than 0.05, so we can safely reject the null hypothesis. The ladybird colour does not appear to be independent of the setting.\nAn appropriate way to report these data would be:\n‘Ladybird colour morphs are not equally common in the two habitats (Chi-sq =19.3, df = 1, p&lt;0.001)’\n\n\n8.2.1.7 Yates continuity correction\nThis is mentioned is the output of the test. What does it mean? It adjusts for the fact that our data are discrete and the chi-square distribution that we are using to calculate the p-values is continuous. That’s it.\n\n\n\n8.2.2 R script\n\nfile_url &lt;- \"https://raw.githubusercontent.com/mbh038/r4nqy/refs/heads/main/chi-square-test-of-independence-template.qmd\"\ndownload.file(file_url,\"chisq_independence_template.qmd\")\n\n\n8.2.2.1 Preliminaries\n\nCreate an R notebook called ladybirds and save it in the scripts folder within your R project folder on your machine. You will find that it is saved as ladybirds.Rmd. Leave the yaml bit at the top, between the pairs of lines with three dashes (or maybe add your name and the date), then delete everything else.\nSave the data file “ladybirds_morph_colour.csv” to the data folder in your R project folder, if is not already there.\nMake sure that your Project folder is actually what RStudio recognises as a “project”. You can navigate to and around it in the Files tab in the bottom right-hand pane of RStudio. If all is good you will see a .Rproj file at the top level of the project, and  will apear at the top right of the RStudio window. If you don’t see this, time now to turn your folder into a project!\n\nRead the rest of this section and copy the code in the chunks provided into chunks in your notebook. If you are lucky, your tutor will have provided a template notebook for you! If you are not so lucky, remember that each code chunk neeeds to be between two lines of three back ticks, like this:\n### A suitable heading\n```{r}\n# enter your code here\n\n```\nso that your notebook will end up looking something like this:\n---\ntitle: \" my R notebook\"\ndate: \"the date\"\nauthor: \" your name\"\n---\n\n### Load packages\n```{r}\nlibrary(tidyverse)\nlibrrary(here)\nlibrary(cowplot)\n```\n\n### Load data\n```{r}\n# enter your code here\n\n```\n\n### Summarise data\n```{r}\n# enter your code here\n\n```\n\nand so on...\nOn some Windows machines, a shortcut to getting the back ticks around each code chunk is to type Ctrl-Alt-I. Annoyingly, this does not seem to work on all Windows machines. If that is the case for you, you will have to type them in manually. On Macs, the shortcut is option-Cmd-I.\nNote that the code provided below is not the only way to do what we want here. You are encouraged to play with it. For example, if you want to see what a particular line does, you can ‘comment it out’ by putting a # at the begining of the line, then running that line again. What difference does it make?\n\n\n8.2.2.2 Load packages we need\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(cowplot) # this makes your plots look nicer\n\n\n\n8.2.2.3 Import the data and inspect it\n\nfilepath&lt;-here(\"data\",\"ladybirds_morph_colour.csv\") \nlady&lt;-read_csv(filepath) # read the data int an object (in this case, a data frame) called lady\nglimpse(lady) # inspect it.\n\nRows: 20\nColumns: 4\n$ Habitat      &lt;chr&gt; \"Rural\", \"Rural\", \"Rural\", \"Rural\", \"Rural\", \"Rural\", \"Ru…\n$ Site         &lt;chr&gt; \"R1\", \"R2\", \"R3\", \"R4\", \"R5\", \"R1\", \"R2\", \"R3\", \"R4\", \"R5…\n$ morph_colour &lt;chr&gt; \"black\", \"black\", \"black\", \"black\", \"black\", \"red\", \"red\"…\n$ number       &lt;dbl&gt; 10, 3, 4, 7, 6, 15, 18, 9, 12, 16, 32, 25, 25, 17, 16, 17…\n\n\nAre those sensible names for the variables? Is the data tidy?\n\n\n8.2.2.4 Calculate the totals of each colour in each habitat.\nWe will save these totals into a new data frame called totals\n\ntotals&lt;- lady |&gt;\n  group_by(Habitat,morph_colour) |&gt;\n  summarise (total.number = sum(number))\ntotals\n\n# A tibble: 4 × 3\n# Groups:   Habitat [2]\n  Habitat    morph_colour total.number\n  &lt;chr&gt;      &lt;chr&gt;               &lt;dbl&gt;\n1 Industrial black                 115\n2 Industrial red                    85\n3 Rural      black                  30\n4 Rural      red                    70\n\n\nNow that we have these totals we use them to plot a bar chart of the data, using geom_col() withinggplot:\n\n\n8.2.2.5 Plot the data\n\ntotals |&gt;\n  ggplot(aes(x = Habitat,y = total.number,fill=morph_colour))+\n  geom_col(position='dodge') +\n  labs(x=\"Habitat\",\n       y=\"Count\",\n       fill= \"Colour\") +\n  theme_cowplot() # try leaving out this line (but if you do, leave out the final '+' in the line above). What happens?\n\n\n\n\n\n\n\n\n\n\n8.2.2.6 Fix the colours\nThe fill colours we got in the figure above are defaults from R, which does not realise that a factor of interest for us is the actual colour of the ladybirds. We would like the figure to reflect that, so let us make the bars red and black for red and black ladybirds respectively.\n\n# plot the data, with sensible colours\ntotals |&gt;\n  ggplot(aes(x = Habitat,y = total.number,fill=morph_colour))+\n  geom_col(position='dodge') +\n  labs(x=\"Habitat\",\n       y=\"Count\",\n       fill= \"Colour\") +\n  scale_fill_manual(values=c(black='#312626',red='#da1717')) + # this line manually sets the fill colours for us\n  theme_cowplot() +\n  theme(legend.position=\"none\") # this line removes the legend, since we no longer need it\n\n\n\n\n\n\n\n\n\n\n8.2.2.7 Interpret the graph before we do any ‘stats’\nLook at the plot - given these sample data, does it seem plausible that the proportions among the populations of black ladybirds and red ladybirdsare the same in both industrial and rural settings, or not? Do you expect to retain or to reject the null hypothesis?\n\n\n8.2.2.8 Making the contingency table from the data\nPreparation\nWe will use the function chisq.test() to carry out the chi square test. However, this requires a matrix of the total counts and our data is in one column of a data frame, spread over 20 rows, one count per colour per site. We need to convert this data frame into a 2x2 matrix of the total counts of each colour in each setting. We can use the xtabs() function to do this.\n\nlady.mat&lt;-xtabs(number~Habitat + morph_colour, data=lady)\nlady.mat\n\n            morph_colour\nHabitat      black red\n  Industrial   115  85\n  Rural         30  70\n\n\nA matrix of this type is sometimes called a contingency table.\n\n\n8.2.2.9 The actual Chi-square test\nLet’s do it…\n\nchisq.test(lady.mat)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  lady.mat\nX-squared = 19.103, df = 1, p-value = 1.239e-05\n\n\n\n\n8.2.2.10 Conclusion\nStudy the output of the chi-square test. Note that you are given a test-statistic (here called Chi-squared/X-squared), a number of degrees of freedom (df) (in some tests you are given more than one of these). This is the number of independent pieces of information used to calculate the test statistic. Lastly, you are a given a p-value. This is the probability that you would have got a chi-squared value as big or bigger than the one you got if the null hypothesis were true, where here the null hypothesis is that there is no association between ladybird colour and location. Put another way, the p-value is, roughly speaking, the probability of getting the data you got or more extreme data if the null hypothesis were true.\nIf the p-value is small (by which we usually mean less than 0.05) then we rejectthe null hypothesis.\nYou are also told that this test was done with Yates’ continuity correction. All this means is that an adjustment has been made for the fact that our data are discrete and the chi-square distribution that we are using to calculate the p-values is continuous. That’s it. Thank you, R, for doing this, but we don’t need to worry about it.\n\n\n8.2.2.11 Reporting the result\nSelect which of the two following statements would be an appropriate way to report these data, and fill in the missing values.\n\n\n8.2.2.12 Option 1\n‘Ladybird colour morphs are not equally common in the two habitats (Chi-sq =19.3, df = 1, p&lt;0.001)’\n\n\n8.2.2.13 Option 2\n‘We find insufficient evidence to reject the null hypothesis that Ladybird colour morphs are equally distributed in the two habitats (Chi-sq =, df = , p = )’\n\n\n\n8.2.3 Exercises\n\n8.2.3.1 Exercise One\nA researcher investigates whether two species A and B are associated with one another. If one is present at a site, does the other tend to be present, and if one is absent, does the other tend to be absent?. If the species were not associated with one another, then the presence of one would say nothing about the likely presence or absence of the other. Their occurrences would be independent.\nThe researcher goes to 100 sites and finds the following:\n\n\n\nWhat was found\nNumber\n\n\n\n\nA present, B present\n33.\n\n\nA present, B absent\n28.\n\n\nB present, A absent\n12.\n\n\nA absent, B absent\n27.\n\n\n\nThey enter these into a 2 x 2 contingency table in R, as follows:\n\nAB &lt;- matrix(c(33,28,12,27),nrow = 2)\nAB # R calls this a matrix - we will refer to it as a table.\n\n     [,1] [,2]\n[1,]   33   12\n[2,]   28   27\n\n\nThink about what each row and column in this table represents: as we have constructed it, columns relate to A and rows relate to B. The left hand column gives counts of sites where A was present, the right hand column gives counts where it was absent. The top row gives gives counts of sites where B was present, the bottom row gives counts of sites where it was absent.\n\nCreate this table yourself\nIs it valid here to use a chi square test for an association between species A and species B?\nSuppose the answer to 2. is ‘Yes’, Use the table to determine whether there is evidence for an association between species A and species B.\nWhat conclusion woud you reach if all four counts in the table were the same?\nWhat conclusion would you reach if neither species was ever seen in the absence of the other - meaning that the off-diagonal elements of the table would be zero?\n\n\n\n\n8.2.4 Solutions\n\n8.2.4.1 Exercise One\n\nYes, because the data are in the form of counts, each count is independent of the others and no count is less than 5.\nWe use the matrix AB as the argument for chisq.test().\n\n\nchisq.test(AB)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  AB\nX-squared = 4.3312, df = 1, p-value = 0.03742\n\n\nWe find p &lt; 0.05, so we reject the null hypothesis of no association and can say that there is evidence, at the 5% significance level, that species A and B are associated.\n\nLet us create a new matrix, AB_uniform, with all values equal to 25:\n\n\nAB_uniform &lt;- matrix(c(25,25,25,25),nrow = 2)\nAB_uniform\n\n     [,1] [,2]\n[1,]   25   25\n[2,]   25   25\n\nchisq.test(AB_uniform)\n\n\n    Pearson's Chi-squared test\n\ndata:  AB_uniform\nX-squared = 0, df = 1, p-value = 1\n\n\nHere , p = 1: there is no evidence from these data that species A and B are associated.\n\nNow we create a matrix wih both off-diagonal elements equal to zero\n\n\nAB_diagonal &lt;- matrix(c(25,0,0,25),nrow = 2)\nAB_diagonal\n\n     [,1] [,2]\n[1,]   25    0\n[2,]    0   25\n\nchisq.test(AB_diagonal)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  AB_diagonal\nX-squared = 46.08, df = 1, p-value = 1.135e-11\n\n\nIn this case p&lt;0.001. Hence the test tells us that in this case there is evidence that species A and B are strongly associated.",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#the-chi-square-test-explained-optional",
    "href": "chi-square.html#the-chi-square-test-explained-optional",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.3 The Chi-Square Test explained (optional)",
    "text": "8.3 The Chi-Square Test explained (optional)\nYou can skip this section if you are not interested in how the chi-square test works. If you are, read on.\nLet’s recall the number of sightings of each colour of ladybird in each habitat:\n\nlady.mat\n\n            morph_colour\nHabitat      black red\n  Industrial   115  85\n  Rural         30  70\n\n\nIf there were no association between colour and habitat, then we would expect the relative proportions of colour to be independent of habitat. That is they should be the same in both rural and industrial habitats. Looking at the table above we see that two thirds (200 out of 300) of all sightings, regardless of colour, were in an industrial habitat. Hence we would expect that two thirds of all 145 black sightings would be in an industrial habitat. We thus arrive at an ‘expected’ number for sightings of black ladybirds in an industrial habitat to be \\((200/300) \\times 145 = 96.7\\). Similarly, we would expect one third (100 out of 300) of all 155 red sightings to be in a rural habitat, giving an ‘expected’ number for this combination of levels to be \\((100/300)\\times 155 = 51.7\\). More generally, the expected number in an any cell of the table, under the null hypothesis of no association between the two factors is given by\n\\[\\text{expected number}=\\frac{\\text{row total}\\times\\text{column total}}{\\text{grand total}}\\] Using this method, these are the four expected numbers for each combination of levels of the two factors we have:\n\n\n      [,1]   [,2]\n[1,] 96.67 103.33\n[2,] 48.33  51.67\n\n\nTo get a measurement of how far the actual table numbers are from their expected values we can, for each cell, square the difference between the expected and actual values, then divide the result by the expected value, and finally sum the four results that we get. This has the effect of giving equal weight to positive or negative deviations of the observed values from the expected values, and scales each squared deviation so that all four have equal weight in the final sum. The result is the chi-squared test statistic:\n\\[\n\\begin{align*}\nX^2&=\\sum_{i=1}^4\\frac{(O_i-E_i)^2}{E_i}\\\\\n&=\\frac{(115-96.67)^2}{96.67} + \\frac{(85-103.33)^2}{103.33} + \\frac{(30-48.33)^2}{48.33} + \\frac{(70-51.67)^2}{51.67}\\\\\n&=20.189\n\\end{align*}\n\\] Yates continuity correction (digression)\nYou may have noticed that the \\(X^2 = 20.189\\) value calculated above is slightly larger than the value calculated by the chisq.test() function, which found \\(X^2 = 19.096\\). This is because the function uses Yates’ continuity correction. This was suggested by Yates in 1934 to correct for the fact that the \\(X^2\\) statistic we calculate is actually discrete (because we have categorical data) whereas the chi-square distribution is continuous. This means that the value we calculate tends to be too big so that our p-values are too small. The problem is most apparent where we have small numbers and one degree of freedom, which is what you have in a \\(2 \\times 2\\) contigency table such s in the example above. Yates’ fix is quite simple: just amend the \\(X^2\\) statistic to the following:\n\\[\nX^2=\\sum_{i=1}^4\\frac{(\\lvert O_i-E_i \\rvert - 0.5)^2}{E_i}\n\\]\nwhere the vertical lines mean ‘take the absolute value of’, so that |85-103.33| = |-18.33| = 18.33\nThis gives us\n\\[\n\\begin{align*}\nX^2&=\\sum_{i=1}^4\\frac{(\\lvert O_i-E_i \\rvert - 0.5)^2}{E_i}\\\\\n&=\\frac{(|115-96.67|-0.5)^2}{96.67} + \\frac{(|85-103.33|-0.5)^2}{103.33} + \\frac{(|30-48.33|-0.5)^2}{48.33} + \\frac{(|70-51.67|-0.5)^2}{51.67}\\\\\n&=19.096\n\\end{align*}\n\\] which is exactly what the chisq.test() function gives.\nUnder a null hypothesis of no association between habitat and colour all the counts would be the ‘expected’ values, and \\(X^2\\) would be zero. The further away from these values the actual results are, the bigger \\(X^2\\) will be and the more likely it is that we can reject the null. For a sufficiently large value of \\(X^2\\) we will reject the null. To sum up, in general we will reject the null when \\(X^2\\) is large, and fail to reject it when it is small.\nBut how large is large enough to reject the null?\nTo answer this we use the fact that the sampling distribution of \\(X^2\\) is a chi-squared distribution with (in this case) \\((2-1) \\times (2-1) = 1\\) degrees of freedom. The sampling distribution is the distribution you would get if you were to repeat the study multiple times, each time calculating the \\(X^2\\) statistic from the four counts of black/red, industrial/rural. Each time you would get a slightly different value of \\(X^2\\). The spread of those hypothetical values is what we call the sampling distribution. Providing none of the cell values in the tables are too samll (see below) it turns out that this distribution has a known mathematical form, known as a chi-square distribution.\nThe p-value given in a chi-square test is the probability of getting a chi-squared statistic \\(X^2\\) as big as or bigger than the one you actually got. This is the area under the chi-squared distribution with the appropriate number of degrees of freedom to the right of the test-statistic value \\(X^2\\).\n\nxsquared&lt;-19.1\nggplot(data.frame(x = c(0,20)), aes(x = x)) + \n  stat_function(fun = dchisq, args = list(df = 1)) + \n  stat_function(fun = dchisq, args = list(df = 1), xlim = c(xsquared, 20),\n                  geom = \"area\", fill = \"#84CA72\", alpha = .2) +\n  scale_x_continuous(name = \"\", breaks = seq(0, 20,2)) +\n  geom_vline(xintercept=xsquared,linewidth=0.2,colour=\"gray80\") +\n  theme_cowplot()\n\n\n\n\n\n\n\n\n\n8.3.0.1 Why does the test statistic have this distribution?\nTo answer this, let us recognise that in our observed contingency table, where we have N observations altogether, we can think of there being a probability \\(P_i\\) that any individual observation will end up in cell i, with the actual observed frequency in that cell, \\(O_i\\) being the product of this probability and the total number of observations: \\(O_i = P_i\\times N\\). If we repeated the study again and again we would get slightly different numbers in each cell each time. The distribution of the numbers in a particular cell would follow similar rules as that of the number of heads we might get each time if we tossed a coin N times, then did the same again and again. This distribution is called a binomial distribution.\nIn other words, our observed frequencies have been obtained by sampling from a binomial distribution, where \\(O_i \\sim \\text{Binomial}(P_i,N)\\). Now, providing N is large enough and providing too that the probabilities \\(P_i\\) are not too close to 0 or 1, then the binomial distribution resembles a normal distribution. Thus, providing \\(P_i\\times N\\), that is providing the observed frequencies \\(O_i=P_i\\times N\\) are large enough, then the \\(O_i\\) will be approximately normally distributed.If this is the case, then so too is \\(\\frac{(O_i-E_i)}{\\sqrt{E_i}}\\) since the expected values \\(E_i\\) are fixed quantities and all this transformation does is turn our normal distribution into a standard normal distribution, with mean = 0 and standard deviation = 1. ie it shifts and squishes the distribution.\nHence in our expression for our test statistic \\(X^2\\) what we are doing is adding up k (= 4 in this case) squared standard normal distributions. This is the definition of a chi-squared distribution with four degrees of freedom. Thus we see that the sampling distribution of our test statistic is a chi-squared distribution.\nThe one final slightly odd detail is that when we run a chi-square test for a 2 x 2 contingency table we are told that there is one degree of freedom. In general, for \\(m \\times  n\\) table, there will be \\((m-1)\\times(n-1)\\) degrees of freedom. This is because the test presumes that the row and column total are already known. If that is the case then if one value of a 2x2 table is known, the other 3 values can be found by deduction. Hence there only one value (it doesn’t matter which one) can be chosen freely.",
    "crumbs": [
      "Count data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "9  Correlation",
    "section": "",
    "text": "9.1 The correlation coefficient\nPart of this guide to correlation draws heavily on the very helpful chapter in Statistics, by Freedman, Pisani and Purves (1991).\nFor this chapter you will find it useful to have this RStudio project folder if you wish to follow along and try out the exercises at the end.\nThe notable statistician Karl Pearson (1857 - 1936) carried out a study to investigate the resemblance between children and their parents. As part of the study, Pearson measured the heights of 1078 parents and of their children at maturity. The heights of the children are plotted against the heights of the parents in the plots below, where we distinguish between father-son and mother-daughter pairs.\nThe taller a father, the taller his sons tend to be. It is the same with mothers and daughters.\nThere is a positive association between a father’s height and the height of his sons.\nBut there is a lot variation - the association is weak.\nIf you know the height of a father, how much does that tell you about the height of his sons?\nConsider fathers who are about about 67 inches tall, and look at the wide variation in the heights of their sons - - all the points between the two vertical dotted lines. The same is true for the daughters of mother who are about 63 inches tall.\nIf there is a strong association between two variables, then knowing one helps a lot in predicting the other. But when there is a weak association, information about one variable does not help much in guessing the other. When there is no association, it does not help at all.\nSuppose we are looking at the relationship between two variables and have already plotted the scatter plot. The graph looks like a cloud of points.\nHow can we summarise it numerically?\nThe first thing we can do is to mark a point that shows the average of the x-values and the average of the y-values. This is the point of averages. It marks the centre of the cloud.\nThe next step is to measure the width of the cloud from side to side, in both the x and the y directions. This can be done using the standard deviations (SD) of the x and y values. Remember that if both x and y are normally distributed, then 95% of the data will lie within about 2 (1.96 if we want to be pernickety) standard deviations of the mean, in each direction.\nSo far, our summary statistics are:\nThese statistics tell us where the centre of the cloud is and how far spread out it is both vertically and horizontally, but they do not tell the whole story.\nConsider the following two sets of data plotted below. Both have the same centre and the same spread.\nHowever the points in the first cloud are tightly clustered around a line - there is a strong linear association between the two variables. In the second cloud, the clustering is much looser. The strength of the association is different in the two diagrams. To measure the association, one more summary statistic is needed - the correlation coefficient.\nThis coefficient is usually abbreviated as r, for no good reason.\nThe correlation coefficient is a measure of linear association or clustering around a line. The relationship between two variables can be summarized by:",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#the-correlation-coefficient",
    "href": "correlation.html#the-correlation-coefficient",
    "title": "9  Correlation",
    "section": "",
    "text": "mean of the x values, SD of the x values.\nmean of the y values, SD of the y values.\n\n\n\n\n\n\n\n\n\nthe average of the x-values, the SD of the x-values.\nthe average of the y-values, the SD of the y-values.\nthe correlation coefficient r",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#different-values-of-r.",
    "href": "correlation.html#different-values-of-r.",
    "title": "9  Correlation",
    "section": "9.2 Different values of r.",
    "text": "9.2 Different values of r.\nLet us see how this looks graphically. In the Figure below we show six scatter plots for hypothetical data. In all six pictures the average is 3 and the standard deviation is 1 for both x and y. The correlation coefficient is printed in each case.\n\n\n\n\n\n\n\n\n\nThe one top left shows a correlation of 0 and the cloud is completely formless. As x increases, y shows no tendency to increase or decrease. It just straggles around.\nThe next diagram has r = 0.4 and a linear pattern is just starting to emerge. The next has r = 0.6 with a stronger linear pattern, and so on. The closer r is to 1 the stronger is the linear association and the more tightly clustered are the points around a line.\nA correlation of 1, which does not appear in the Figure is often referred to as a perfect correlation. It means that all the points lie exactly on a line so there is a perfect linear correlation between the two variables. Correlation coefficients are always between -1 and 1.\nThe correlation between the heights of identical twins is around 0.95. A scatter diagram for the heights of twins would thus look like the bottom right diagram in the Figure. We see that even with a coefficient this big there is a still a fair degree of scatter. The heights of identical twins are far from being equal all the time.\nReal data in the life sciences never shows perfect correlation and rarely does it even show strong correlation. It is more common for it to look like Pearson’s father-son data, with weak associations and r values in the range 0.3 to 0.7. This is even more true for data from the social sciences which concern human behaviour.\nWe can also have negative associations between variables. For example women with more education tend to have fewer children. Animals with higher body weight tend to have lower metabolic rates. As one variable increases, the other decreases. When there is negative association, the correlation coefficient has a negative sign.\nBelow we show six examples of negative correlation. As in the previous figure, all the data sets have a mean of 3 and a standard deviation of 1.\n\n\n\n\n\n\n\n\n\n\nCorrelations are always between -1 and 1, but can take any value in between. A positive correlation means that the cloud slopes up: as one variable increases, so does the other. A negative correlation means that the cloud slopes down. As one variable increases, the other decreases.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#using-r-to-find-the-correlation-coefficient.",
    "href": "correlation.html#using-r-to-find-the-correlation-coefficient.",
    "title": "9  Correlation",
    "section": "9.3 Using R to find the correlation coefficient.",
    "text": "9.3 Using R to find the correlation coefficient.\nFirst, let us try to find the correlation between two sets of data where we know what the correlation coefficient is, because we created the data ourselves. Let us suppose that we have columns called x and y within a data frame called df, and that the correlation coefficient between these data is in fact 0.8.\n\ncor.test(x, y, method = \"pearson\", data = df)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = 9.1381, df = 48, p-value = 4.446e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6663100 0.8800203\nsample estimates:\n      cor \n0.7968662 \n\n\nThere are different ways to calculate the correlation coefficient. Which of them is appropriate depends mainly on the type of data, and if they are numeric, whether they are normally distributed. If they are, then we use the Pearson method. If they are not, for example because they are ordinal data, then we use the Spearman’s Rank method and write method=“spearman” instead. In this case we can relax the requirement that there is a linear association between the data sets, but there does still need to be a monotonic relationship.\nIt is important to be able to interpret and report the output.\nFirst, understand that R is carrying out a test, the null hypothesis of which is that there is no correlation between the values of x and the values of y among the populations from which the x and y data were drawn, so that the correlation coefficient between x and y within those populations is zero. It then reports a p-value: how likely it is that you would have got the data you got for this sample of data if that null hypothesis were true. As with most tests, to do this it uses the data to calculate a so-called test-statistic. How it does this need not concern us here. The details will differ from test to test, and the name given will differ. Here it is called t. It also reports the number of independent pieces of information used to calculate that statistic. This is called the degrees of freedom, here denoted df. This usually (but not necessarily) has a value that is 1,2 or 3 less than the number of data points.\nThen it reports the p-value. A tiny (close to zero) value here means that it thinks it very unlikely that the samples would be as they are if the x and y variables were not correlated in the populations from which the samples were drawn. A high (by which we usually mean greater than 0.05) value means that there is a reasonable chance that the actual non-zero correlation coefficient could have been found between x and y in the samples, even though those values were not correlated in the wider populations from which the samples were drawn. In that case we would have found no evidence that the x and y data within the population were correlated. This doesn’t mean that they aren’t, just that we have insufficient evidence to reject the null hypothesis that they are not.\nThe p-value reported here is 4.3e-12. That is R’s way of saying what in standard form would be written 4.3 x 10-12. This is a really tiny value. It is 0.0000000000043, which is a very inconvenient way to write such a small number. Hence R’s way of doing it or the standard form way of doing it. In the context of a statistical test and when p is is that small we don’t care about its exact value, we simply note that it is very, very small. We thus can confidently reject the null hypothesis and assert that the data provide evidence that x and y are correlated, in this case positively.\nFurther, it reports the actual correlation coefficient. Here it finds r = 0.797, which we happen to know to be correct because we created this data set ourselves, and a 95% confidence interval for the coefficient. The precise meaning of the confidence interval is subtle, but it is a kind of error bar for the correlation coefficient r. It means that if we drew sample after sample from the population and calculated the confidence interval for r for each sample, then 95% of the time that interval would capture the true value of r. Thus, you can reasonably think of the confidence interval as being the range of values within which the true population correlation coefficient plausibly lies, given the value that was found for the sample.\nIf the p-value is small enough that we reject the null-hypothesis, then this confidence interval should not encompass zero. Why? Because any value inside the confidence interval is a plausible value for the population correlation coefficient andif we are going to reject the null hypothesis, then zero should not be a plausible value for the population correlation coefficient, given the data.\nIf the p-value is large enough that we do not reject the null hypothesis then this confidence interval will encompass zero. Why? Becuase if the confidence interval encompasses zero, then zero is a plausible value for the correlation coefficient and so we should not reject the null.\nHere, the confidence interval is from 0.67 to 0.88. This does not encompass zero. In fact it is far from zero, so is consistent with our finding a really small p-value. On both groundss, we reject the null.\nTo report the result of this test we would say something like:\n\nWe find evidence for a strong positive correlation between x and y (Pearson r =0.80, t=9.1, df=48, p&lt;0.001)\n\nNote that when the p-value is much less than 0.05 as it is here we do not normally report its exact value, but simply write p&lt;0.01, or p&lt;0.001, and so on. The point is that these ways of reporting it tell the reader that p is way less than 0.05. This is all they need to know to see that we can confidently reject the null hypothesis.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-iris-correlation",
    "href": "correlation.html#sec-iris-correlation",
    "title": "9  Correlation",
    "section": "9.4 Correlations for real data",
    "text": "9.4 Correlations for real data\nLet us look at the Iris data set that is built into R. It contains values for the Sepal Width, Sepal Length, Petal Width and Petal Length for samples of 50 plants from each of three species of Iris, setosa, versicolor and virginica. Here are the first few rows:\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nWe will look to see if the data allow us to reject the idea that sepal width and sepal length are not correlated within the wider populations of each of these species:\nFirst, let’s plot the data\n\niris |&gt;\n  ggplot(aes(x = Sepal.Width, y = Sepal.Length, colour = Species)) +\n  geom_point() +\n  labs(x = \"Sepal Width (mm)\",\n       y = \"Sepal Length (mm)\") +\n  facet_wrap(~Species, nrow = 1, scales = \"free\") +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\n\n\n9.4.1 What we can tell from plotting the data\nHaving seen the plots, do you think it plausible that there is a linear relationship between sepal width and length? Only in this case can you use a Pearson correlation test (see below). If not linear, do you think there is at least a monotonic relationship between petal width and length (ie no peaks or troughs)? That, at least, would enable you to use a Spearman’s Rank correlation test. If neither are true, then you can’t use either test.\n\n\n9.4.2 Test for normality\nIt does look as though there is a plausibly linear relationship between sepal width and length, so we might be able to use the Pearson method for calculating correlation coefficient. This is the ‘parametric’ method that is more powerful than the ‘non-parametric’ alternative, the Spearman’s rank method.\nIn principle, however, this method requires that each group of the data be approximately normally distributed around its repective mean (that is what the word parametric is getting at), so we ought to test for this. We can do this either graphically or using a normality test such as the Shapiro wilk test. Let us do the latter here:\nFirst we do it for one species, for example the setosa. We can use the filter function from the dplyr packages within tidyverse to separate the data for this species from that for the others. We will save it as a new data frame called setosa:\n\nsetosa &lt;- iris |&gt;\n  filter(Species == \"setosa\")\n\nNow we use the shapiro.test() function to assess the normality of the Sepal.Length column data\n\n1shapiro.test(setosa$Sepal.Length)\n\n\n1\n\nNote the use of the dollar notation to pick out a particular column of a data frame. Here we pick out the Sepal.Length column of the setosa data frame.\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  setosa$Sepal.Length\nW = 0.9777, p-value = 0.4595\n\n\nand then of the Sepal.Width column of data:\n\nshapiro.test(setosa$Sepal.Width)\n\n\n    Shapiro-Wilk normality test\n\ndata:  setosa$Sepal.Width\nW = 0.97172, p-value = 0.2715\n\n\nWHen we do it for all species, for both sepal width and sepal length, we find\n\n\n# A tibble: 3 × 3\n  Species    Sepal.Length_p.val Sepal.Width_p.val\n  &lt;fct&gt;                   &lt;dbl&gt;             &lt;dbl&gt;\n1 setosa                  0.460             0.272\n2 versicolor              0.465             0.338\n3 virginica               0.258             0.181\n\n\nAll the p-values for this test are comfortably greater than 0.05 so we can reasonably presume that our data are drawn from normally distributed populations. This, plus the plausibly linear realtionships we have seen in the graphs means that can go ahead and use the Pearson method to calculate the correlation coefficient between sepal length and width for each species!\n\n\n9.4.3 Calculate the correlation coefficients\nLooking at each graph, it appears that there is a positive correlation for each species, but that this is weaker for versicolar and virginica than it is for setosa. Knowing the sepal width for that species gives you a much better idea of the sepal length, and vice-versa, than is true for the other two species.\nLet us find out:\n\niris |&gt;\n  group_by(Species) |&gt;\n  summarise(r=cor.test(Sepal.Width,Sepal.Length)$estimate,\n            lower.bound95 = cor.test(Sepal.Width,Sepal.Length)$conf.int[1],\n            upper.bound95 = cor.test(Sepal.Width,Sepal.Length)$conf.int[2],\n            \"p value\" = cor.test(Sepal.Width,Sepal.Length)$p.value)\n\n# A tibble: 3 × 5\n  Species        r lower.bound95 upper.bound95 `p value`\n  &lt;fct&gt;      &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1 setosa     0.743         0.585         0.846  6.71e-10\n2 versicolor 0.526         0.290         0.702  8.77e- 5\n3 virginica  0.457         0.205         0.653  8.43e- 4\n\n\nThe table gives the estimated value for the Pearson correlation coefficient in each case, the lower and upper bound of the 95% confidence interval for that coefficient and the p-value.\nDo these output provide evidence for a correlation between sepal length and sepal width in each case?",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#the-problem-of-missing-confounding-variables",
    "href": "correlation.html#the-problem-of-missing-confounding-variables",
    "title": "9  Correlation",
    "section": "9.5 The problem of missing confounding variables",
    "text": "9.5 The problem of missing confounding variables\nSuppose in the above analysis we had not distinguished between the three species. If we had plotted the speal length and width data we would have seen this:\n\niris |&gt;\n  ggplot(aes(x = Sepal.Width, y = Sepal.Length)) +\n  geom_point() +\n  labs(x = \"Sepal Width (mm)\",\n       y = \"Sepal Length (mm)\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThis looks like a weak negative correlation, as is confirmed by calculating the Spearman’s (in this case) rank correlation coefficient.\nIf we had in fact accounted for species and plotted the data separately for each species, or together but distinguishing each species by colour we would have seen the falsity of that conclusion:\n\niris |&gt;\n  ggplot(aes(x = Sepal.Width, y = Sepal.Length, colour = Species)) +\n  geom_point() +\n  labs(x = \"Sepal Width (mm)\",\n       y = \"Sepal Length (mm)\",\n       colour = \"Species\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nThe message here is that failure to spot important ‘missing’ variables, in the case species, can lead to grossly misleading ideas as to whether two variables are correlated, and if so, how.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#the-pearson-correlation-coefficient-measures-the-degree-of-linear-association.",
    "href": "correlation.html#the-pearson-correlation-coefficient-measures-the-degree-of-linear-association.",
    "title": "9  Correlation",
    "section": "9.6 The Pearson correlation coefficient measures the degree of linear association.",
    "text": "9.6 The Pearson correlation coefficient measures the degree of linear association.\n\n9.6.1 Pearson correlation coefficient\nSometimes the Pearson correlation coefficient r is a poor measure of the degree of association within a data set. Outliers and non-linearity are two problem cases.\nConsider first a data set where there is a very strong association between variables, but where the data set contains an outlier, and then a data set where there is a strong but non-linear association between variables. Here we mean by ‘strong’ that knowing the value of one variable gives you a very good idea of the value of the other.\n\n\n\n\n\n\n\n\n\nThe outlier in the left-hand figure above brings the correlation coefficient down to 0.08, which is close to zero. The correlation coefficient in the right-hand figure is similarly small at -0.187, despite that there is a strong association between the x and y data. The reason is that the association is non-linear.\n\n\n9.6.2 Spearman’s rank correlation coefficient rSp\nThe Spearman’s rank correlation coefficient is a valid measure of the association between two variables providing their relationship is monotonic - that is, continuously rising or flat, or continuously falling or flat. Linear relationships are monotonic, so the Spearman’s rank correlation coefficient is always a useful (if not necessarily the most powerful - Pearson would trump it if it could be used) measure of association for such cases, but it will still be valid when the relationship is monotonic but non-linear, whereas the Pearson correlation coefficient would not be.\n\nSpearman’s rank correlation coefficient rSp can also be be used for ordinal data, whereas Pearson’s r coefficient cannot be. This makes it very useful in much of ecology, animal behaviour and environmental studies where ordinal scales are commonly used.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#when-is-it-appropriate-to-calculate-a-correlation-coefficient",
    "href": "correlation.html#when-is-it-appropriate-to-calculate-a-correlation-coefficient",
    "title": "9  Correlation",
    "section": "9.7 When is it appropriate to calculate a correlation coefficient?",
    "text": "9.7 When is it appropriate to calculate a correlation coefficient?\nSo, to sum up, we note that the Pearson correlation coefficient is a measure of linear association, not of association in general. At least, this is true if you are calculating the Pearson correlation coefficient. If your data are not suitable for that and you decide to calculate the Spearman’s Rank correlation coefficient, then the condition is relaxed somewhat: there might be but there no longer needs to be a linear relationship between the two variables, but there must at least be a monotonic one. That means that, as one variable increases, the other should either increase or remain constant, or decrease or remain constant. There should be no peaks or troughs in the data.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#association-is-not-the-same-thing-as-causation",
    "href": "correlation.html#association-is-not-the-same-thing-as-causation",
    "title": "9  Correlation",
    "section": "9.8 Association is not the same thing as causation",
    "text": "9.8 Association is not the same thing as causation\nA very important and often repeated point to note is that correlation measures the degree of linear (Pearson) or monotonic (Spearman) association. But association is not the same as causation. Two entities can have a strong degree of monotonic or linear association without there being any causal relation between the two. There could be a confounding variable that underlies the association.\nFor example you would probaly find a strong positive correlation between the number of pubs in towns and the nubmer of churches. You would be mistaken, however, in thinking that this provides evidence that drink drives people to religion, or the reverse, since there is a very obvious confounding variable underlying this association, which is the population of the towns: the more people, the more pubs, the more churches, the more of pretty much anything.\nThat said, if you spot a strong association beween two variables, it may be an indicator of a causal relationship, so it is at least worth pausing for thought to decide whether this is worthy of further investigation.\nBe careful though, as spurious correlations can easily arise. See Spurious Correlations for many amusing examples of these.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#examples",
    "href": "correlation.html#examples",
    "title": "9  Correlation",
    "section": "9.9 Examples",
    "text": "9.9 Examples\n\n9.9.1 Adult human heights\nHere we use data from Our World in Data: Human Height\nThe figure below shows the mean heights of adult men vs adult women in many different countries who were born in 1996. There is a strong positive and linear correlation between these heights (rsp = 0.941, p &lt; 0.001). The Spearman’s rank correlation coefficient is calculated because the male mean heights are not normally distributed among themselves.\n\n\n\n\n\n\n\n\n\n\n\n9.9.2 Cyclones\nCyclones are areas of low atmospheric pressure around which steep gradients in air pressure can cause strong winds to develop, which in turn may create large waves if the cylone is over the oceans. Depending on where they occur, these storms are variously also known as hurricanes and typhoons. Here we consider whether the peak wind speeds are associated with the depth of the low pressure at the eye of the storm, and whether the peak wave sizes are associated with how wide the storm is. If the answer to either of these questions is yes, then an outcome of practical importance - wind speed, wave height - can be predicted at least in part by a variable that can be measured easily - pressure, distance.\n\n9.9.2.1 South West Indian Ocean intense tropical cyclones 1973 - 2024\nPressure gradients cause winds so it is reasoable to ask whether there a correlation between the peak wind speed and minimum pressure at the eye of cyclones. Here we look at data for for intense tropical cyclones in the south west Indian Ocean over the period 1973-2024. The data are taken (scraped using the R package rvest!) from: https://en.wikipedia.org/wiki/List_of_South-West_Indian_Ocean_intense_tropical_cyclones . The original data sources are available on that site.\n\n\n\n\n\n\n\n\n\nIn this figure we see that there is a weak but significant negative correlation between the peak recorded wind speed and the minimum recorded pressure at the centre of intense tropical cyclones that occurred in the south west Indian Ocean between 1973 and 2024. Neither wind speed nor pressure were normally distributed, so the Spearman’s rank correlation coefficient rSp has been calculated, and not the Pearson r coefficient.\n\n\n9.9.2.2 Significant wave height vs size of cyclone\nHere we look at results displayed in Figure 3f of a paper by Oh et al (2023). In this paper the authors seek to determine whether there is an association between the size of a cyclone, measured by the ‘R50’ distance measured outward from the storm centre to where the wind speeds have subsided to 50 kph, and the maximum ‘significant wave height’ of the swell created by the storm. Significant wave height is a widely used measure that is the average height of the heighest 1/3 of the waves, these being the ones that impact most on practical matters like the fuel consumption of ships, the erosion of shores, and so on.\n\n\n\n\n\n\n\n\n\nThe plot shows that there is a monotonically rising relationship between the R50 radius of the cyclones included in the study and the maximum significant wave height of swell created by them. The relationship is not linear however, so the only appropriate measure of correlation coefficient is the Spearman’s rank, which gives \\(r_{sp} = 0.948, p&lt;0.001\\), indicating a very strong positive association between the size of a cyclone and the height of the waves it creates.\n\nWhy do you think this relationship flattens off for larger storm sizes?\n\n\n\n\n9.9.3 Lichen abundance\nIn a (2008) paper, Jovan et al. investigated the utility of using lichen as bioindicators of air quality. Is there, they asked, an association between air quality and the abundance of this or that species of lichen?\n\n\n\n\n\n\n\n\n\n\nWe note that the relationship between air quality score and proportion of nitrophyte abundance is plausibly linear.\n\n\nThere is a strong negative correlation (rSp=-0.78, p&lt;0.001) between air quality score and proportion of nitrophyte lichen. This suggests that this proportion can be used as a bioindicator of air quality.\n\n\nA Spearman’s rank correlation coefficient was calculated in this case rather than a Pearson correlation coefficient, despite the fact that both the x and y variables are plausibly drawn from normally distributed populations of values, according to a Shapiro-Wilk test. The problem is that both the air quality score and the proportion of nitrophyte abundance are ordinal variables - something we learn from reading the paper in which this figure appears. This means that analysis using parametric tests such as the Pearson method for determining linear correlation are not appropriate. A non-parametric method such as Spearman’s rank method can be used instead.\n\n\n\n9.9.4 Soil bacteria\nHere we look at figures published in a paper by Lauber and co-workers (2009) in which they investigate the evidence for an association between soil pH and the abundance of various species of bacteria.\n\n\n\n\n\n\n\n\n\nIn the Figure above, note that the Pearson r-values for C and E are close to zero, and the p-values are greater than 0.1, meaning that at this significance level there is no evidence from these data that there is any linear association between soil pH and the relative abundances of Alphaproteobacteria or Beta/Gammaproteobacteria. From the plots, it looks in C as if there no assocation at all, whereas in E it looks as though there might be, but if so then not a linear or even monotonic association, for which the correlation coefficient (Pearson r or Spearman rSp) would be a poor measure.\n\n\n9.9.5 Birds\n\n\n\n\n\n\n\n\n\nThis figure is from a study by Pain et al (2019) on the impact on bird populations of ingestion of lead from spent lead ammunition arising from hunting using rifles and shot guns. For several species of wetland birds, the population trend (as measured by a proxy scale) is plotted against the prevalence of carcasses found to contain lead shot.\nThere is a clear negative trend here that is plausibly linear, or at least monotonic. Shapiro-Wilk tests show that neither data set is plausibly drawn from a normally distributed population, so a Spearman’s rank correlation coefficient is calculated. The result is \\(r_{\\text{Sp}} = -0.697, p &lt; 0.01\\) so we can say that there is evidence of a significant and strong negative correlation between lead shot ingestion of waterbird species and their population trends.\n\n\n9.9.6 Heart rate vs life expectancy\n\n\n\n\n\n\n\n\n\nHere we see a strong negative linear correlation between the life expectancy of different species and the log of the mean heart rate in beats per minute. On this plot, humans are almost an outlier. In this case, use of a Shapiro-Wilk test showed that both life expectancy and log of the heart rate were found to be consistent with being drawn from normally distributed populations, so the Pearson method was used to calculate the correlation coefficient.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#exercise-1",
    "href": "correlation.html#exercise-1",
    "title": "9  Correlation",
    "section": "9.10 Exercise 1",
    "text": "9.10 Exercise 1\n\n\n\n\n\n\n\n\n\nPlots A to F above show scatter plots of different data sets Y against X.\n\nWhich of them show linear behaviour?\nWhich of them show monotonic behaviour?\nFor which of them might it be appropriate to calculate the following correlation coefficients between X and Y?\n\nPearson r\nSpearman rank rsp",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#exercise-2",
    "href": "correlation.html#exercise-2",
    "title": "9  Correlation",
    "section": "9.11 Exercise 2",
    "text": "9.11 Exercise 2\nMeasurements were made on female Adelie penguins on a series of islands in the Antarctic. The bill lengths and depths of 73 individuals were recorded and are shown in the plot below.\n\n\n\n\n\n\n\n\n\nFrom this information and plot, decide\n\nWhether it is plausible that there is a linear relationship between the bill depths and lengths within the data set\nWhether the correlation coefficient within the data set is likely to be positive or negative\nWhether the correlation is ‘strong’ or ‘weak’ ie is the absolute value of the correlation coefficient likely to be close to 1 or close to zero\nWhether there might be evidence from this data that there is any correlation between bill depth and bill length in the population from which this data set was drawn.\n\n\n9.11.0.1 Tests for normality\nShapiro-Wilk tests are carried out to check for normality of the two sets of data:\n\nshapiro.test(bill_depths)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bill_depths\nW = 0.9831, p-value = 0.4364\n\n\n\nshapiro.test(bill_lengths)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bill_lengths\nW = 0.99117, p-value = 0.8952\n\n\nOn this basis, we see that we can use the Pearson method to calculate the correlation coefficient between bill depth and bill length. What is telling us this?\n\n\n9.11.0.2 Calculate correlation coefficient\nFor these data, we calculate the correlation coefficient using the Pearson method.\n\n# Pearson\ncor.test(bill_depths,bill_lengths, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  bill_depths and bill_lengths\nt = 1.3714, df = 71, p-value = 0.1746\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07209557  0.37677877\nsample estimates:\n      cor \n0.1606361 \n\n\nWhich part(s) of this output tell us that:\n\nThere is a weak positive correlation between bill length and depth within the sample?\nThere is no evidence that this correlation exists in the wider population from which this data set was drawn?\n\nHow would you report this result?",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#exercise-3",
    "href": "correlation.html#exercise-3",
    "title": "9  Correlation",
    "section": "9.12 Exercise 3",
    "text": "9.12 Exercise 3\n\nIf you haven’t done so already, click on this link to download the practice project for correlation. You may then need to unzip it.\nOpen the project in RStudio using File/Open Project then navigating to the project and clicking on the correlation.Rproj file\nOpen the notebook correlation.qmd that you will find in the notebooks subfolder.\nRun the initial code chunks to load packages and the penguin and iris data\nCheck in the Environment pane that you have created data frames called penguins and iris. Note the variable (column) names in each case.\nUsing the code in section Section 9.4 as a guide, create a faceted plot of petal length against petal width for each species. To do this you need to create a new code chunk, paste the code from Section 9.4 into it and then adapt it a necessary.\nAgain using Section 9.4 as a guide, calculate the Pearson correlation coefficient between petal length and petal width for each species, and display this, plus the lower and upper bounds of the confidence interval and the p-value for each species in a table.\n\n\nDoes it appear that the petal length and petal width are correlated for each species?\n\nIs the correlation positive or negative?\n\nFor which species is the correlation strongest?\n\nDo the correlation coefficients make sense, given the plots?\n\nMoving on to the penguin data….\n\nUse the filter() function from the dplyr package in tidyverse to filter the penguin data down to Gentoo males. Save the data in a data frame called gentoo_m. You can do this by including the following code chunk in your notebook:\n\n```{r}\ngentoo_m &lt;- penguins |&gt;\nfilter(sex == \"male\") |&gt;\nfilter(species == \"Gentoo\")\n```\n\nPlot the bill depth vs bill length using code similar to this:\n\n```{r}\ngentoo_m |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point() +\n  labs(x = \"Bill depth (mm)\",\n       y = \"Bill length (mm)\") +\n  theme_classic()\n```\n\nLook at the plot and decide if it is plausible that the association between bill depth and bill length is plausibly monotonic and if so, if it is plausibly linear. On this basis, before even considerinrg the issue of normality, decide if it is appropriate to use Pearson or Spearman’s rank measures of correlation with this data.\nIf your answer to 10. was Yes, use the shapiro.test() function to determine if it is OK to use the Pearson measure of correlation. To do this you must separately test both the bill_length_mm and bill_depth_mm columns of the gentoo_m for normality. Remember that you can pick a particular column of a data frame by using the dollar notation. For example, df$X would pick out the column X from the data frame df.\nUse the cor.test() function with the appropriate method argument (method = \"pearson\" or method = \"spearman\"), as appropriate, to determine if the bill length and bill depth variables are correlated. This means using an expression of the form cor.test(df$X, df$Y, method = \"pearson\") if you wanted to determine the Pearson correlation coefficient between the X and Y columns of the data frame df.\n\n\n\n\n\nFreedman, David, Robert Pisani, Roger Purves, and Ani Adhikari. 1991. Statistics. 2nd ed. W. W. Norton & Company.\n\n\nJovan, Sarah. 2008. “Lichen Bioindication of Biodiversity, Air Quality, and Climate: Baseline Results from Monitoring in Washington, Oregon, and California.” http://gis.nacse.org/lichenair/doc/Jovan2008.pdf.\n\n\nLauber, Christian L., Micah Hamady, Rob Knight, and Noah Fierer. 2009. “Pyrosequencing-Based Assessment of Soil pH as a Predictor of Soil Bacterial Community Structure at the Continental Scale.” Applied and Environmental Microbiology 75 (15): 5111–20. https://doi.org/10.1128/aem.00335-09.\n\n\nOh, Youjung, Sang Myeong Oh, Pil-Hun Chang, and Il-Ju Moon. 2023. “Optimal Tropical Cyclone Size Parameter for Determining Storm-Induced Maximum Significant Wave Height.” Frontiers in Marine Science 10 (February). https://doi.org/10.3389/fmars.2023.1134579.\n\n\nPain, Deborah J., Rafael Mateo, and Rhys E. Green. 2019. “Effects of Lead from Ammunition on Birds and Other Wildlife: A Review and Update.” Ambio 48 (9): 935–53. https://doi.org/10.1007/s13280-019-01159-0.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "simple-linear-regression.html",
    "href": "simple-linear-regression.html",
    "title": "10  Simple linear regression",
    "section": "",
    "text": "10.1 Introduction\nFor this chapter you will find it useful to have this RStudio project folder if you wish to follow along and try out the exercises at the end.\nA class of analytical models that you will use often go under the name General Linear Models. They include linear regression, multiple regression, ANOVA, ANCOVA, Pearson correlation and t-tests.\nDespite appearances, these models are all fundamentally linear models. They share a common framework for estimation (least squares) and a common set of criteria that the data must satisfy before they can be used. These criteria centre around the idea of normally distributed residuals. An important stage of any analysis that uses linear models is that these assumptions are checked, as part of the Plot -&gt; Model -&gt; Check Assumptions -&gt; Interpret -&gt; Plot again workflow.\nHere, we will go through an example of simple linear regression - suitable for trend data where we wish to predict a continuously varying response, given a value of a continuous explanatory variable. As we go we show code snippets from an R script that does this job, and, at the bottom, an example complete script that you could adapt to your own needs.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple-linear-regression.html#simple-linear-regression---plant-growth",
    "href": "simple-linear-regression.html#simple-linear-regression---plant-growth",
    "title": "10  Simple linear regression",
    "section": "10.2 Simple Linear Regression - plant growth",
    "text": "10.2 Simple Linear Regression - plant growth\nAs a first example, we ask the question: does plant growth rate depend on soil moisture content?\nWe predict that more moisture will probably allow higher growth rates. We note that this means there will be a clear relationship between the variables, one that should be apparent if we plot the response (dependent) variable - plant growth rate - against the explanatory (independent) variable - soil moisture content. We note that both the explanatory variable and the dependent variables are continuous - they do not have categories.\nWhat we want to do in linear regression is be able to predict the value of the dependent variable, knowing the value of the independent variable. In practice, this means drawing a ‘best fit’ straight line through the data and determining the intercept and gradient of this line.\n\n10.2.1 Load packages\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggfortify)\nlibrary(cowplot)\n\n\n\n10.2.2 Get the data\nWe have a data set to explore our question: The plant.growth.rate.csv data set is available as a csv file in the data subfolder of the skeleton project folder you have hopefullly downloaded to go along with this session.\n\nfilepath &lt;- here(\"data\",\"plant.growth.rate.csv\")\nplants &lt;- read_csv(filepath)\nglimpse(plants)\n\nRows: 50\nColumns: 2\n$ soil.moisture.content &lt;dbl&gt; 0.4696876, 0.5413106, 1.6979915, 0.8255799, 0.85…\n$ plant.growth.rate     &lt;dbl&gt; 21.31695, 27.03072, 38.98937, 30.19529, 37.06547…\n\n\nWe see that the data set contains two continuous variables, as expected.\n\n\n10.2.3 Plot the data\nWe can use the package ggplot2, which is part of tidyverse to do this:\n\nplants |&gt;\n  ggplot(aes(x=soil.moisture.content, y=plant.growth.rate)) +\n  geom_point() +\n  labs(x=\"Soil moisture content\",\n       y=\"Plant growth rate (mm/week)\") +\n  xlim(0,2) +\n  ylim(0,50) +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nFrom the plot, we note that:\n\nthere is an upward trend that is plausibly linear within this range of soil misture content. The more moisture there is in the soil, the greater the growth rate of the plants appears to be.\nthe variance of the data, that is the range of vertical spread is approximately constant for the whole range of soil moisture content. This is one of the key criteria that data must satisfy if we are to analyse them using a linear model such as simple linear regression.\nwe can estimate the intercept and gradient of a best fit line just by looking at the plot. Roughly speaking, the moisture content varies from 0 to 2, while the growth rate rises from 20 to 50, a rise of about 30. Hence the gradient is about 30/2 = 15 mm/week, while the intercept is somewhere between 15 mm and 20 mm / week.\n\nIt is always good practice to examine the data before you go on to do any statistical analysis. For all but the smallest data sets, that means plotting them.\n\n\n10.2.4 Make a simple model using linear regression\nWe use the function lm() to do this, and we save the results in an object to which we give the name model_pgr. This function needs a formula and some data as its arguments:\n\nmodel_pgr&lt;-lm(plant.growth.rate ~ soil.moisture.content, data = plants)\n\nThis reads: ‘Fit a linear model, where we hypothesize that plant growth rate is a function of soil moisture content, using the variables from the plants data frame.’\n\n\n10.2.5 Check assumptions\nBefore we rush into interpreting the output of the model, we need to check whether it was valid to use a linear model in the first place. Whatever the test within which we are using a linear model, we should do the necessary diagnostic checks at this stage.\nYou can do this using tests designed for the purpose, but I prefer to do it graphically, using a function autoplot() from the package ggfortify. You give this the model we have just created using lm() and it produces four very useful graphs. I suggest that, after once installing ggfortify you include the line library(ggfortify) at the start of every script.\nHere is how you use it:\n\nautoplot(model_pgr, smooth.colour=NA) + theme_cowplot()\n\n\n\n\n\n\n\n\nThe theme_cowplot() part is not necessary, but it gives the plots a nice look, so why not?\nThese plots are all based around the `residuals’, which is the vertical distance between observed values and fitted values ie between each point and the best fit line through the points - the line which the linear model is finding for us, by telling us its intercept and gradient.\nNote that in simple linear regression, the best fit line is the one that minimises that sum of the squared residuals.\nSo what do these plots mean?\n\nTop-left: This tells you about the structure of the model. Was it a good idea to try to fit a straight line to the data? If not, for example because the data did follow a linear trend, then there will be humps or troughs in this plot.\nTop-right: This evaluates the assumption of normality of the residuals. The dots are the residuals and the dashed line is the expectation under the normality assumption. This is a much better way to check normality than making a histogram of the residuals, especially with small samples.\nBottom-left: This examines the assumption of equal variance of the residuals. A linear model assumes that the variance is constant over all predicted values of the response variables. There should be no pattern. Often, however, there is. With count data, for example, the variance typically increases with the mean.\nBottom-right: This detects leverage - which means points that have undue influence on the gradient of the fitted line, and outliers. If you have outliers in your data, you need to decide what to do with them.\n\nIn the case of these data, we are good to go! There is no discernible pattern in either of the left-hand plots, the qq-plot is about as straight as you ever see with real data, and there are no points exerting undue high influence.\n\n\n10.2.6 Interpretation of the model\nNow that we have established that the data meet the criteria required for the model to be valid, we can go ahead and inspect its output. We will do this using two tools that we also use for every other general linear model we implement (t-test, ANOVA etc). These are anova() and summary()\nLet us first use anova():\n\nanova(model_pgr)\n\nAnalysis of Variance Table\n\nResponse: plant.growth.rate\n                      Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nsoil.moisture.content  1 2521.15 2521.15  156.08 &lt; 2.2e-16 ***\nResiduals             48  775.35   16.15                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe F value here is an example of a ‘test statistic’, a number that a test calculates from the data, from which it is possible to further calulate how likely it is that you would have got the data you got if the null hypothesis were true. This particular test statistic is the ratio of the variation in the data that is explained by the explanatory variable to the leftover variance. The bigger it is, the better the job that the explanatory variable is doing at explaining the variation in the dependent variable. The p value, which here is effectively zero, is the chance you would have got an F value this big or bigger from the data in the sample if in fact there were no relationship between plant growth rate and soil moisture content. If the p value is small (and by that we usually mean less than 0.05) then we can reject the null hypothesis that there is no relationship between plant growth rate and soil moisture content.\nHence, in this case, we emphatically reject the null: there is clear evidence that plant growth rate is at least in part explained by soil moisture content.\nNow we use the summary() function:\n\nsummary(model_pgr)\n\n\nCall:\nlm(formula = plant.growth.rate ~ soil.moisture.content, data = plants)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9089 -3.0747  0.2261  2.6567  8.9406 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             19.348      1.283   15.08   &lt;2e-16 ***\nsoil.moisture.content   12.750      1.021   12.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.019 on 48 degrees of freedom\nMultiple R-squared:  0.7648,    Adjusted R-squared:  0.7599 \nF-statistic: 156.1 on 1 and 48 DF,  p-value: &lt; 2.2e-16\n\n\nThis gives us estimates of the intercept (19.348) and gradient (12.750) of the best fit line through the data. The null hypothesis is that both these values are zero, and the p-value is our clue as to whether we can reject this null. Here, in both cases, we clearly can.\nWe also see the Adjusted R-squared value of 0.7599. This is the proportion of the variance in the dependent variable that is explained by the explanatory variable. Thus it can vary between 0 and 1. A large value like this indicates that soil moisture content is a good predictor of plant growth rate.\n\n\n10.2.7 Back to the figure\nTypically, a final step in our analysis involves including the model we have fitted into the original figure, if that is possible in a straightforward way. In the case of simple linear regression, it is. It means adding a straight line with the intercept and gradient displayed by the summary() function. We do this by adding a line geom_smooth(method = \"lm\") to our plot code:\n\nplants |&gt;\n  ggplot(aes(x=soil.moisture.content, y=plant.growth.rate)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x=\"Soil moisture content\",\n       y=\"Plant growth rate (mm/week)\") +\n  xlim(0,2) +\n  ylim(0,50) +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nThis gives both a straight line and the ‘standard error’ of that line - meaning, roughly speaking, the wiggle room within which the ‘true’ line , for the population as opposed to this sample drawn from it, probably lies.\n\n\n10.2.8 Report the result\nWe would likely want to include this plot in our report, along with a statement like:\nWe find evidence for a linear increase in plant growth rate with soil moisture content (p&lt;0.001), with an additional 12.75 mm of growth per unit increase in soil moisture content.\n\n\n10.2.9 Conclusion\nWe have carried out a simple linear regression on continuous data. This is an example of a general linear model. We first plotted the data, then we used lm() to fit the model. Next we inspected the validity of the model using autoplot. We then inspected the model itself using first anova() then summary(). Finally we included the output of the model on the plot, in this case by adding to it a straight line with the intercept and gradient determined by the regression model, and reported the result in plain English.",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple-linear-regression.html#simple-linear-regression---ocean-ph",
    "href": "simple-linear-regression.html#simple-linear-regression---ocean-ph",
    "title": "10  Simple linear regression",
    "section": "10.3 Simple linear regression - ocean pH",
    "text": "10.3 Simple linear regression - ocean pH\nIn this second example we provide the script but leave you to interpret the outcome of each step. Use the code snippets below to help you complete the notebook ocean-pH.qmd that is provided for you in the notebooks subfolder of the project lined to this chapter (if you haven’t aleady, you can download this using the link at the hed of this chapter.) If you do this then your notebook should follow exactly the same steps as for the plant growth example, but using different names.\nHere we use data from Figure 5.20 of AR6, WG1 from the IPCC. It shows ocean pH measurements from a location (137\\(^{\\circ}\\)E, 5\\(^{\\circ}\\)N) in the western Pacific between 1980 and 2020.\nYour task is to assess whether there is a significant linear trend in pH with time and if so to determiine the change in pH per year or per decade during the forty year period from 1980 to 2020.\n\n10.3.1 Load packages\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggfortify)\nlibrary(cowplot)\n\n\n\n10.3.2 Load data\n\npH_filepath &lt;- here(\"data\",\"ipcc_AR6_WGI_Figure_5_20-pH.csv\")\npH&lt;-read_csv(pH_filepath,skip=6) # we have to skip the first 6 lines because of meta-data included in the file - check it out!\nglimpse(pH)\n\nRows: 81\nColumns: 2\n$ year &lt;dbl&gt; 1984.121, 1985.171, 1986.066, 1987.060, 1987.488, 1988.058, 1989.…\n$ pH   &lt;dbl&gt; 8.100000, 8.097917, 8.086458, 8.102604, 8.071875, 8.098437, 8.095…\n\n\n\n\n10.3.3 Plot data\n\npH |&gt;\n  ggplot(aes(x = year, y = pH)) +\n  geom_point() +\n  labs(x = \"Year\",\n       y = \"Ocean pH\") +\n  scale_y_continuous(limits=c(8.0,8.12), breaks=seq(8.0,8.2,0.02)) + #&gt; 1\n  theme_cowplot()\n\n\n\n\n\n\n\n\nIs there a linear trend?\n\n\n10.3.4 Fit linear model\n\npH_model &lt;- lm(pH ~ year, data= pH)\n\n\n\n10.3.5 Check model validity\n\nautoplot(pH_model) + theme_cowplot()\n\n\n\n\n\n\n\n\nIs it reasonable to apply a linear model to these data? Remember that each of these plots tell you something about whether this is the case.\n\n\n10.3.6 Inspect model\n\n10.3.6.1 ANOVA\n\nanova(pH_model)\n\nAnalysis of Variance Table\n\nResponse: pH\n          Df    Sum Sq   Mean Sq F value    Pr(&gt;F)    \nyear       1 0.0169414 0.0169414  199.87 &lt; 2.2e-16 ***\nResiduals 79 0.0066962 0.0000848                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAre we OK to reject the null hypothesis that pH does not change over this time period?\n\n\n\n10.3.7 Summary\n\nsummary(pH_model)\n\n\nCall:\nlm(formula = pH ~ year, data = pH)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0247092 -0.0049642  0.0002185  0.0060392  0.0168809 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.234405   0.224330   50.08   &lt;2e-16 ***\nyear        -0.001583   0.000112  -14.14   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.009207 on 79 degrees of freedom\nMultiple R-squared:  0.7167,    Adjusted R-squared:  0.7131 \nF-statistic: 199.9 on 1 and 79 DF,  p-value: &lt; 2.2e-16\n\n\nWhat is the change in ocean pH per decade over the last four decades? Is this change statistically significant? Does the linear model account for much of the variance in the data?\n\n\n10.3.8 Replot the data, model included\n\npH |&gt;\n  ggplot(aes(x = year, y = pH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", colour = \"blue\") + # add the line. Method =\"lm\" for a straight line.\n  labs(x = \"Year\",\n       y = \"Ocean pH\") +\n  scale_y_continuous(limits=c(8.0,8.12), breaks=seq(8.0,8.2,0.02)) + # fix limits and break points of y-axis\n  annotate(geom=\"text\", x = 2015, y = 8.11, label = \"137W, 5N\", size = 5) + # measurement site\n  theme_cowplot()\n\n\n\n\n\n\n\n\nHow would you report this result?",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple-linear-regression.html#simple-linear-regression-body-mass-vs-metabolic-rate",
    "href": "simple-linear-regression.html#simple-linear-regression-body-mass-vs-metabolic-rate",
    "title": "10  Simple linear regression",
    "section": "10.4 Simple linear regression: Body mass vs metabolic rate",
    "text": "10.4 Simple linear regression: Body mass vs metabolic rate\nThis still needs to be filled in….\nHere we use data from",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple-linear-regression.html#sample-script",
    "href": "simple-linear-regression.html#sample-script",
    "title": "10  Simple linear regression",
    "section": "10.5 Sample script",
    "text": "10.5 Sample script\nA notebook to do linear regression might look like the following, here written as a .qmd Quarto notebook. For this to work you will need to work within a project, with the data in a sub-folder of that called “data”. Here, the data is taken to be a .csv file called mydata.csv, with two columns of data, one called x_values and the other called y_values. You need to change these to suit your own data.\nTo use this, open a new notebook of your own (File/New File/Quarto Document), delete everything below the yaml section, paste in all the code below, then adapt the code as needed. Remember to save the notebook in your project notebooks folder!\n---\ntitle: \"Sensible title\"\nauthor: \"your name\"\ndate: \"the date\"\noutput: html_notebook\n---\n\n10.5.1 load packages\n```{r}\n#| label: load packages\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggfortify)\nlibrary(cowplot)\n```\n\n\n10.5.2 load data\n```{r}\n#| label: load data\n\nfilepath &lt;- here(\"data\", \"mydata.csv\")\nmydata &lt;- read_csv(filepath)\nglimpse(mydata)\n```\n\n\n10.5.3 plot the data\n```{r}\n#| label: plot data\n\nmydata |&gt;\n  ggplot(aes(x=x_values, y=y_values)) +\n  geom_point() +\n  labs(x = \"X variable\",\n       y = \"Y variable\") +\n  theme_cowplot()\n```\n\n\n10.5.4 fit the model\n```{r}\n#| label: fit the linear model\nmydata.model &lt;- lm (y_values ~ x_values, data = mydata)\n```\n\n\n10.5.5 diagnostics\n```{r}\n#| label: diagnostics\nautoplot(mydata.model)\n```\n\n\n10.5.6 investigate the model\n```{r}\n#| label: anova\nanova(mydata.model)\n```\n```{r}\n#| label: summary\nsummary(mydata.model)\n```\n\n\n10.5.7 replot the data, now with the model included\n```{r}\n#| label: replot\nmydata |&gt;\n  ggplot(aes(x=x_values, y=y_values)) +\n  geom_point() +\n  geom_smooth(method=\"linear\") +\n  labs(x = \"X variable\",\n       y = \"Y variable\") +\n  theme_cowplot()\n```",
    "crumbs": [
      "Trend data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "qq_plots.html",
    "href": "qq_plots.html",
    "title": "11  Quantile-quantile plots",
    "section": "",
    "text": "11.0.1 Introduction\nAdapted from an exercise by Jon Yearsley (School of Biology and Environmental Science, UCD)\nQ-Q plots can play a useful role when trying to decide whether a dataset is normally distributed, and if it is not, then how it differs from normality.\nWe will investigate the types of quantile-quantile plots you get from different types of distributions.\nWe will look at data distributed according to",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#what-is-a-q-q-plot",
    "href": "qq_plots.html#what-is-a-q-q-plot",
    "title": "11  Quantile-quantile plots",
    "section": "11.1 What is a Q-Q plot?",
    "text": "11.1 What is a Q-Q plot?\nQuantiles partition a dataset into equal subsets. For example, if we wished to partition a standard normal (mean = 0, standard deviation = 1) population into 4 equal subsets, the 3 quantiles (ie the three values of x) that would do this are -0.675, 0 and 0.675. In this way, 25% of the population would have a value greater than 0.675, 25% between 0 and 0.675, 25% between -0.675 and 0 and the final 25% would have a value less than -0.675. When we draw the distribution, the areas under the curve between neighbouring quantiles will be equal:",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#normally-distributed-data.",
    "href": "qq_plots.html#normally-distributed-data.",
    "title": "11  Quantile-quantile plots",
    "section": "11.2 Normally distributed data.",
    "text": "11.2 Normally distributed data.\nBelow we show an example of 150 observations that are drawn from a normal distribution. The normal distribution is symmetric, so has no skew. Its mean is equal to its median.\nOn a Q-Q plot data that are approximtely normally distributed data lie roughly on a straight line, perhaps looking a bit ragged at each end. The box plot is symmetric with few or no outliers.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#right-skewed-data.",
    "href": "qq_plots.html#right-skewed-data.",
    "title": "11  Quantile-quantile plots",
    "section": "11.3 Right-skewed data.",
    "text": "11.3 Right-skewed data.\nRight skewed distributions are non-symmetric and have a long tail heading towards extreme values on the right-hand side of the distribution. The mean is more positive than the median.\nIn the example we show an exponential distribution.\nIn the Q-Q plot, such distributions give a distinctive convex curvature. The box-plot may show outliers out towards large values.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#left-skewed-data.",
    "href": "qq_plots.html#left-skewed-data.",
    "title": "11  Quantile-quantile plots",
    "section": "11.4 Left-skewed data.",
    "text": "11.4 Left-skewed data.\nLeft skewed distributions are non-symmetric and have a long tail heading towards extreme values on the left-hand side of the distribution. The mean is more negative than the median. The box plot may show outliers down towards small values.\nIn the example we show a negative exponential distribution.\nIn the Q-Q plot, such distributions give a distinctive concave curvature.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#under-dispersed-data",
    "href": "qq_plots.html#under-dispersed-data",
    "title": "11  Quantile-quantile plots",
    "section": "11.5 Under-dispersed data",
    "text": "11.5 Under-dispersed data\nUnder-dispersed data are data whose distribution is more concentrated around a central value than is the case for normally distributed data. There are fewer outliers and the tails of the distribution are lighter. As an example here we show 150 points drawn from a uniform distribution.\nNote the distinctive curvature of the Q-Q plot. The ‘box’ of the boxplot is bigger than for a normal distribution, since the interquartile range covers a larger range of values.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#over-dispersed-data",
    "href": "qq_plots.html#over-dispersed-data",
    "title": "11  Quantile-quantile plots",
    "section": "11.6 Over-dispersed data",
    "text": "11.6 Over-dispersed data\nOver-dispersed data are data whose distribution is more widely spread around a central value than is the case for normally distributed data. There are more outliers and the tails of the distribution are fatter. As an example here we show 150 points drawn from a laplace distribution.\nNote the distinctive curvature of the Q-Q plot - like the previous one but curving the other way. The ‘box’ of the boxplot is smaller than for a normal distribution, since the interquartile range covers a smaller range of values.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#caution",
    "href": "qq_plots.html#caution",
    "title": "11  Quantile-quantile plots",
    "section": "11.7 Caution",
    "text": "11.7 Caution\nWith small data sets, the scatter in the data can make it difficult to tell from the histogram (especially) or even the Q-Q plot whether the dataset is plausibly drawn from a normally distributed population. In that case you might choose to combine use of the plots with a normality test, such as Kolmogorov-Smirnov or Shapiro-Wilk. The null hypothesis of these is that the data ARE drawn from normally distributed populations, so the smaller the p-value when they are applied to a dataset, the less likely it is that this is true.\nWith large data sets,the Kolmogorov-Smirnov and Shapiro-Wilk tests become very sensitive to even small deviations from normality and might give a p-value that would lead you to suppose that a dataset was not drawn from a normally distributed population. Since no data set is ever truly normal, even when we consider the whole population, all we really need to know is whether the data are close enough to normal that the various tests (eg t-test, ANOVA, correlation, least square regression) that require it are going to work well enough. For these large data sets, histograms and Q-Q plots can be very useful indicators of approximate normality.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "qq_plots.html#who-cares-about-normality-anyway-the-central-limit-theorem.",
    "href": "qq_plots.html#who-cares-about-normality-anyway-the-central-limit-theorem.",
    "title": "11  Quantile-quantile plots",
    "section": "11.8 Who cares about normality anyway? The central limit theorem.",
    "text": "11.8 Who cares about normality anyway? The central limit theorem.\nLastly, for large enough data sets, we don’t actually need the data to be normally distributed for the tests that require normality to work! This is because what they require is not that the dataset itself be normal, but that the distribution of the means of many such data sets, the so-called sampling distribution, be normal. A very important mathematical result known as the Central Limit Theorem guarantees that this will be the case whatever the distribution of each of the data sets, as long as these datasets are large enough!\nHow large is large enough? There’s the rub! A common rule of thumb is that if the dataset has size N&gt;30 or so, then it is safe to use tests that require normality. Indeed, one does find that sampling distributions for data drawn from uniform or mildly skewed distributions such as the exponential distribution are roughly normal when N exceeds 30 or so, but for more skewed datasets, a larger dataset can be needed - it depends on how far from normality the distribution is. The further from normal it is the larger the dataset needs to be before the Central Limit Theorem applies to a good approximation. For a highly skewed dataset, for example one distributed according to something like a log-normal distribution, it can require N&gt;200 or so, or even more before it is OK to use t-tests and the like.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantile-quantile plots</span>"
    ]
  },
  {
    "objectID": "power-analysis.html",
    "href": "power-analysis.html",
    "title": "12  Power Analysis",
    "section": "",
    "text": "12.1 Simulation of one trial\nThis topic is all about how to design our studies such that they are likely to detect an effect, whether that is a correlation or a difference, if there really is one there to be detected. If there were, and we didn’t that would be a shame (and a waste of time and money and possibly an ethical hoohah).\nIn what follows we focus on a simple study that seeks to detect a difference between two populations, but the ideas generalise to other designs.\nSuppose we have a magic soil supplement that we hope will enhance the growth rate of tomato plants, make us rich and pay for a comfortable retirement.\nBefore riches, however, we have to be sure that it has the desired effect. For the supplement to be a money spinner let’s suppose that we need it to enhance the growth rate of plants by 10% and so we must do a study to see if this is in fact the case.\nIn a properly randomised design, we take N plants and give them ‘normal’ plant food, and another N plants and give them the normal food plus our supplement. We keep all other conditions the same for the two groups of plants.\nAfter 30 days, the plants grown with the usual food grow with a mean mass of 300g and a standard deviation of 30g. The other plants will need to have a mean mass of at least 330g, and we will suppose that they too will have a standard deviation of 30g.\nSuppose we chose N = 100. Then we can simulate the masses of the individual plants in the two samples, supposing there were a 10% effect of supplement. We do this by using the rnorm() function to draw samples of 100 replicates each from a normally distributed population, in one case with a mean of 300g and in the other with a mean of 330g, both with standard deviations of 30g\nN&lt;-100\ndelta&lt;-0.05 # ie 10% ie the mean of the treatment population is 10% greater than that of the control population\n\n# set up the sample parameters: means and standard deviations\nm1&lt;-300 \nsd1&lt;-30\nm2&lt;-(1+delta)*m1\nsd2&lt;-sd1\n\nuntreated&lt;-rnorm(N,m1,sd1) # the control sample\ntreated&lt;-rnorm(N,m2,sd2) # the treated sample\n\n# put these samples  in a tidy tibble\ntrial_data &lt;- tibble(treatment = c(rep(\"untreated\", N), rep(\"treated\", N)),\n                     mass = c(untreated, treated))\n# plot these simulated data\ntrial_data |&gt;\n  ggplot(aes(x=mass,colour = treatment, fill=treatment)) +\n  geom_density(bins=20, alpha = 0.4) +\n  # geom_jitter(colour=\"gray50\",width=0.2) +\n  labs(x=\"Plant mass (g)\",\n       y=\"Frequency\") +\n  theme_bw()\nSuppose the supplement really did work like this, and really did improve growth rates by 10%. How many plants, ie what value of N would we need in each group in order to have an 80% chance of correctly spotting this difference? This probability of detection of an effect that is there is what we call the power of a study. If the effect is smaller than 10% (or whatever boost we deemed sufficient) and we do not detect it we do not mind, since that means that our supplement had not worked as we had hoped, but it would be a waste of our time and money if there really were a sizeable effect but we did not spot it because our sample size was too small. Equally, we do not want a sample size that is far larger than is necessary to achieve sufficient power. Doing so would incur unnecessary time and money costs and possibly have ethical implications.\nIn thinking about how we might analyse our data we formulate a null hypothesis:\nThe supplement has no effect.\nIn that case we would expect the difference between the masses of the treated plants and untreated plants to be zero.\nIf in fact the supplement has an effect on growth, then the data should force us to reject this null hypothesis. In order that there be at least an 80% chance that it does this, there has to be an at least 80% chance that the mean value of our treated plants lies outside the rejection regime of the null hypothesis.\nWith one pair of samples of plants, we might or might not detect the effect, depending on which individuals ended up being included in the samples that we drew from their respective populations, since this would determine whether the mean of the treated plants was or was not in the rejection regime of the null. Thus, if we ran a t-test on our two simulated samples, we might or might not get a p-value that is less than our chosen significance level,\nt.test(mass~treatment,data=trial_data)$p.value\n\n[1] 0.02030797\nWht we want to know is: what is the chance that we would get the ‘correct’ result, which in our case is that there is a difference and which would be indicated to us by the p-value being less than our chosen significance level (for brevity’s sake, let’s just assume from now on that we chose this to be 0.05), so that we (correctly) reject the null hypothesis of there being no difference.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Power Analysis</span>"
    ]
  },
  {
    "objectID": "power-analysis.html#simulation-of-many-trials",
    "href": "power-analysis.html#simulation-of-many-trials",
    "title": "12  Power Analysis",
    "section": "12.2 Simulation of many trials",
    "text": "12.2 Simulation of many trials\nA way to find out is to carry out this simulation many times and see in what fraction of our trials we see a significant effect (that is, p &lt; 0.05), supposing that the supplement really does work. That will give us an idea of the power of our study.\nThat is what we will now do:\nWe write a function that will return TRUE or FALSE depending on the p-value of a trial in which we specifiy the mean value of the control group, the standard deviation of the control group (assumed to be the same in the treatment group), the size of the effect delta, where if delta is 0.1, say, then we mean that the effect size is a 10% increase in growth mass, N is the sample size for each group and alpha is the significance level that we choose (most likely, but not necessarily 0.05).\nIn the function we carry out a t-test for each pair of samples to determine whether we can reject the null hypothesis that the treated plants are drawn from a population with the same mass as the untreated plants.\nWe know that the null hypothesis is false because we have drawn our samples from populations that do differ in their mean values. We want to see if our test correctly rejects the null so that we detect the effect, which here is the mass difference between the means of the two groups of plants.\nIf the p-value is less than our chosen significance level then we reject the null, if not, we do not.\nRejecting the null in this case means we have a ‘True Positive’ and so we make our function return the logical value TRUE in that case. Failing to reject the null in this case is a mistake. We call this kind of mistake a ’False Negative” and when this happens me make out function return the logical value FALSE.\n\neffect_detected&lt;-function(m1,sd1,delta,N,alpha){\n  pop1&lt;-rnorm(N,m1,sd1)\n  pop2&lt;-rnorm(N,m1*(1+delta),sd1)\n  return(t.test(pop1,pop2)$p.value&lt;alpha) #return TRUE if p &lt;0.05, return FALSE if not.\n}",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Power Analysis</span>"
    ]
  },
  {
    "objectID": "power-analysis.html#calculate-the-power-of-the-study-for-a-range-of-sample-and-effect-sizes",
    "href": "power-analysis.html#calculate-the-power-of-the-study-for-a-range-of-sample-and-effect-sizes",
    "title": "12  Power Analysis",
    "section": "12.3 Calculate the power of the study for a range of sample and effect sizes",
    "text": "12.3 Calculate the power of the study for a range of sample and effect sizes\nNow we run this trial as many times as we want. Let us run it 10,000 times, for a range of sample sizes and effect sizes. In any one trial we may or may not reject the null. We want to see, for a given set of conditons, in what fraction of trials, in the long run, we do reject the null. That will be an estimate of the power of our study: the likelihood that we will detect an effect if the effect really exists, as it does here.\n\ntrials&lt;-10000\nm1&lt;-300\nsd1&lt;-30 # population standard deviation\ndeltas&lt;-c(0.01,0.02,0.03,0.04,0.05,0.1) # effect size where, for example, 0.05 means that the effect is 5% the size of the control mean\nN&lt;-100 # sample size\nalpha&lt;-0.05 # chosen significance level\n\nNs&lt;-c(seq(2,9,1),seq(10,50,10),seq(60,200,20),seq(240,480,40))\n\npower_vals_raw&lt;-matrix(rep(0,length(Ns)*length(deltas)),ncol=length(deltas))\nfor(i in 1:length(Ns)){\n  for (j in 1:length(deltas)){\n    trial_results &lt;- replicate(trials,effect_detected(m1,sd1,deltas[j],Ns[i],alpha))\n    power_vals_raw[i,j]&lt;-mean(trial_results)\n  }\n}\n\npower_vals &lt;- as.tibble(power_vals_raw)\nnames(power_vals) &lt;- deltas\npower_vals &lt;- power_vals |&gt;\n  mutate(N=Ns) |&gt;\n  pivot_longer(-N,names_to = \"effect_size\", values_to = \"power\")\n\nPlotted, this looks like:\n\npower_vals |&gt;\n  mutate(ok=power&gt;0.8) |&gt;\n  ggplot(aes(x=N,y=power,colour=effect_size)) +\n  # geom_point() +\n  geom_smooth(method = \"loess\", span=0.25, linewidth=0.6, se=FALSE) +\n  # geom_line() +\n  scale_x_continuous(breaks=seq(0,max(Ns),50)) +\n  scale_y_continuous(breaks=seq(0,1,0.2), limits=c(0,1)) +\n  geom_hline(yintercept=0.8,linetype=\"dashed\",colour=\"darkblue\", linewidth = 0.1) +\n  geom_hline(yintercept=0.9,linetype=\"dashed\",colour=\"darkblue\", linewidth = 0.1) +\n  labs(x = \"Sample size N\",\n       y = \"Power\",\n       colour =\"Effect size\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIn this plot we show how the power varies with sample size N, for different effect sizes (where 0.05, say, means a difference of 5% in populations means), with a population standard devation of 30 g on a control mean of 300g, and a significance level of 0.05.\nWhat is a suitable sample size for a power of 80%?\nWe see that we need a sample size of about 65 to get a power of 80%.\nWhat sample size would be needed for a power of 90%, or 99%?\nWe need sample sizes of 85 for a power of 90% and of 150 for a power of 99%. There are progressively diminshing returns once you try to make the power much greater than 80% or 90% or so. The sample sizes needed become very large.\nNow try varying the effect size, the population variation and the chosen significance level and see how they affect the power.\nWe should find that, all else being equal:\n\nIf the effect size is reduced, the power decreases. It is harder to tell apart two populations that do not differ by much.\nIf the population variation goes down the power increases. It is easier to tell apart two populations if there is little variation within each population.\nIf the signicance level is reduced, say to 0.01 from 0.05, the power will go down. We are reducing the type 1 error rate, but at the same increasing the type 2 error rate. It is les likely, even if there is an effect, that we will detect it.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Power Analysis</span>"
    ]
  },
  {
    "objectID": "power-analysis.html#factors-that-affect-the-power-of-a-study",
    "href": "power-analysis.html#factors-that-affect-the-power-of-a-study",
    "title": "12  Power Analysis",
    "section": "12.4 Factors that affect the power of a study",
    "text": "12.4 Factors that affect the power of a study\nWhat factors about a study force you to have larger sample sizes for a given power? \nIf the effect size is small, the population variation is large or the significance level required is small.\nHow would you know what the variation of your population is? How could you reduce this?.\nYou might look at the literature. Very likely, someone has done a study similar to yours. What variation did they see? Alternatively, you could do a pilot study and with relatively little effort get an idea yourself of the variation within your population of interest of the attrivute you want to measure.\nWhat downside might there be to reducing the population variation, supposing you could do it, in order to increase power?.\nYou can sometimes reduce the variation within the population that you are studying by restricting variation in causal factors that do not interest you for that particular study, but which might also be contributing to variation in the outcome variable that does interest you. So while your study might focus on the impact of soil treatments on growth rates of seedlings, for example, it may be that soil moisture also affects growth rate. Hence if you ensure that all seedlings are grown under the same moisture conditions, you will remove any variation in growth rates due to that, and hence reduce the total varation. This will increase the power of your study - where by that we mwan the likelihood that you will detect any difference that your soil supplement made to growth rate.\nThe downside to this approach is that you will restrict the applicability of your study. In the example above, having applied the supplement and observed some difference or not compared to a control sample, you could only make inferences to populations of seedlings that were grown under precisely the soil moisture conditions you chose for your study. You could no longer make statements about the supplement preferences of seedlings grown under any old soil moisture conditions.\nAn alternative approach, which preserves both power and range of applicability is to change the design of the study. In this case, instead of making it a one-factor study in which growth rate is measured against supplement presence or absence, we could also measure the soil moisture and include this in the analysis. If we had measured discrete levels of soil moisture, we would now have a 2-way ANOVA, and if we had measured it as a continuous variable we would have an ANCOVA.\nHow would you know what the effect size was, and could you increase it?\nThe same applies here as for the variation. You could look at the literature. Someone else has very likely done a similar study. What effect size did they observe? Or, you could do a quick and dirty pilot study. Unlike the population variation thing, however, it is easier to detect an effect if the effect size is bigger. Is there anything you can do to increase it? In an observational study in the wild, perhaps not but in a manipulative study perhaps you can. In our example we might use large doses of supplement rather than small ones, hoping to see a larger effect as a result.\nWhat assumptions have gone into this power calculation?\nWe assumed here that our samples were drawn from normally distributed populations with equal variances. In our simulations we knew that was the case because me made it so by design, but we could have given them any distribution or variance we wanted. Simulations are a very powerful tool.",
    "crumbs": [
      "Additional help",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Power Analysis</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cumming, Geoff, Fiona Fidler, and David L. Vaux. 2007. “Error Bars\nin Experimental Biology.” Journal of Cell Biology 177\n(1): 7–11. https://doi.org/10.1083/jcb.200611141.\n\n\nFreedman, David, Robert Pisani, Roger Purves, and Ani Adhikari. 1991.\nStatistics. 2nd ed. W. W. Norton & Company.\n\n\nGilbert, Francis, Peter McGregor, and Chris Barnard. 2017. Asking\nQuestions in Biology; a Guide to Hypothesis Testing, Experimental Design\nand Presentation in Practical Work and Research Projects. 5th ed.\nBenjamin Cummings.\n\n\nJovan, Sarah. 2008. “Lichen Bioindication of Biodiversity, Air\nQuality, and Climate: Baseline Results from Monitoring in Washington,\nOregon, and California.” http://gis.nacse.org/lichenair/doc/Jovan2008.pdf.\n\n\nLauber, Christian L., Micah Hamady, Rob Knight, and Noah Fierer. 2009.\n“Pyrosequencing-Based Assessment of Soil pH as a Predictor of Soil\nBacterial Community Structure at the Continental Scale.”\nApplied and Environmental Microbiology 75 (15): 5111–20. https://doi.org/10.1128/aem.00335-09.\n\n\nOh, Youjung, Sang Myeong Oh, Pil-Hun Chang, and Il-Ju Moon. 2023.\n“Optimal Tropical Cyclone Size Parameter for Determining\nStorm-Induced Maximum Significant Wave Height.” Frontiers in\nMarine Science 10 (February). https://doi.org/10.3389/fmars.2023.1134579.",
    "crumbs": [
      "References",
      "References"
    ]
  }
]