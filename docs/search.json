[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "r4nqy",
    "section": "",
    "text": "Introduction\nThis is a compilation of the methods for data analysis that you are likely to find useful in completing your studies at Newquay University Centre. It is by no means exhaustive, but should address most of your needs, most of the time.\nData analysis in the life sciences is only part of the wider process of forming and then answering as best we can well-formed questions about the way this or that aspect of the natural world works.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a compilation of the methods for data analysis that you are likely to find useful in completing your studies at Newquay University Centre. It is by no means exhaustive, but should address most of your needs, most of the time.\nData analysis in the life sciences is only part of the wider process of forming and then answering as best we can well-formed questions about the way this or that aspect of the natural world works.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "1  A recommended analysis workflow",
    "section": "",
    "text": "1.1 Are you working within your Project?\nThis is intended as a rough outline of the sequence of steps one commonly goes through when working on scripts:\nDetails will differ from script to script, but this sequence of steps is very common.\nBefore we even think of the script, we need to make sure that we are working within our Project. If we are not doing this, bad things will happen. If you are, the Project name will be at the top right of the RStudio window. If you are not, save the script you are working on, and go to File/Open Project and open your Project. If you haven’t even got a ‘Project’ or don’t know what that means then just make sure that everything you need for whatever you are working on is in one folder and then turn that folder into a Project. (So a ‘Project’ is just a regular folder that has been given superpowers.) You do that by going to File/New Project/Existing Directory. Then you navigate to your folder and click on Create Project. RStudio will then restart and you will see the name of your newly anointed Project folder at the top-right of the RStudio window. You know that a folder is a ‘Project’ because it will have a .Rproj file inside it.\nIf all this sounds complicated, don’t worry. It really isn’t. Just get someone to show you how to do it and you will be fine.\nNow, to the script itself:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#statement-of-the-questions-to-be-investigated",
    "href": "workflow.html#statement-of-the-questions-to-be-investigated",
    "title": "1  A recommended analysis workflow",
    "section": "1.2 Statement of the question(s) to be investigated",
    "text": "1.2 Statement of the question(s) to be investigated\nWithout thinking this through, you won’t know what your script is for…\nWhat is the analysis that will follow for? What question are you trying to answer? What hypotheses are you trying to test?\nSuppose we were trying to test the hypothesis that there is no difference between the petal widths of the setosa, versicolor and virginica species of iris. All we have to go on are the petal widths of the plants we happened to measure. From these measurements we want to make a statement about these three species in general.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#open-a-notebook",
    "href": "workflow.html#open-a-notebook",
    "title": "1  A recommended analysis workflow",
    "section": "1.3 Open a notebook",
    "text": "1.3 Open a notebook\nIn RStudio, go to File/New File/ Quarto Document. Delete everything below the yaml section at the top. This strangely named section is the bit between the two lines with three dashes in. For the most part, we will not need to worry about this section. We just should not delete it entirely. What is useful to do is to amend the title to something sensible, and to add author: \"your name\" and date: \"the date in any old format\" lines. I also add a couple of final lines that suppress warnings and messages that might clutter up my printed output, so that your yaml will look something like this:\n---\ntitle: \"A typical workflow\"\nauthor: \"Who wrote this?\"\ndate: \"Today's date\"\noutput:\n  html_document:\n    df_print: paged\nexecute:\n  message: false\n  warning: false\n---\nDelete everything beneath this yaml section. The big empty space that then leaves you with is where you write your code. Remember that in quarto documents, the code goes in ‘chunks’ that are started and finished with by lines with three backticks. Any other text goes between the chunks and you can format this text using the simple rule of Markdown, available in the RStudio Help menu at Help/Markdown Quick Reference. Thus your script will end up looking something like this:\n---\ntitle: \"A typical workflow\"\nauthor: \"Who wrote this?\"\ndate: \"Today's date\"\noutput:\n  html_document:\n    df_print: paged\nexecute:\n  message: false\n  warning: false\n---",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#first-header",
    "href": "workflow.html#first-header",
    "title": "1  A recommended analysis workflow",
    "section": "1.4 First header",
    "text": "1.4 First header\nAny text we want to add. Note that a code chunk starts with {r} and ends with `\n\nlibrary(tidyverse) # some actual R code",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#second-header",
    "href": "workflow.html#second-header",
    "title": "1  A recommended analysis workflow",
    "section": "1.5 Second header",
    "text": "1.5 Second header\nAny text we want to add to explain what this next chunk does\n```{r, include=FALSE}\nlibrary(tidyverse) # some actual R code\n```",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#load-packages",
    "href": "workflow.html#load-packages",
    "title": "1  A recommended analysis workflow",
    "section": "1.6 Load Packages",
    "text": "1.6 Load Packages\nYou will nearly always want the first five packages, and often you will appreciate the sixth, janitor. Others, such as vegan will be useful from time to time, depending on what you are doing. If any of these lines throw an error, it is most likely because you have not yet installed that package. Do so in the console pane (not in this script!) using the function install.packages(\"name of package\"). Then run this whole chunk again.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggfortify)\nlibrary(readxl)\nlibrary(cowplot)\nlibrary(janitor)\nlibrary(vegan)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#load-data",
    "href": "workflow.html#load-data",
    "title": "1  A recommended analysis workflow",
    "section": "1.7 Load data",
    "text": "1.7 Load data\nThere are several ways to do this, so details will differ depending on what file type your data is saved in and where it is stored.\nHere are some examples. In each case code here presumes that the data is stored in a subfolder called ‘data’ within the Project folder, and we use the function here() from the here package. In my experience this dramatically simplifies the business of finding your data, wherever your script is. It makes it easier for you to share your script with others and be confident that what worked for you will work for them. It does require that you are working within your project.\n\n1.7.1 If from a csv file\nIf you have your data in a data subfolder within your project, this chunk will work. Just substitute the name of your data file\n\nfilepath&lt;-here(\"data\",\"iris.csv\")\niris&lt;-read_csv(filepath)\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n\n\n\n\n1.7.2 If from an Excel file\nYou will need to use read_excel() from the readxl package, and you have to specify the name of the worksheet that holds the data you want. You can, if you want, specify the exact range that is occupied by the data. However I suggest you avoid doing this unless it turns out that you need to do so. If your data is a nice, neat, rectangular block of rows and columns, you should find that you don’t need to specify the range.\n\nfilepath&lt;-here(\"data\",\"difference_data.xlsx\")\niris&lt;-read_excel(path = filepath,\n                 sheet = \"iris\", # delete the comma if you choose not to specify the range in the line below\n                 range= \"A1:F151\" # optional - try leaving it out first. Only include if necessary.\n                 ) |&gt;\n  clean_names()\nglimpse(iris)\n\nRows: 150\nColumns: 6\n$ id           &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ sepal_length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n\n\n\n\n1.7.3 If from a URL\nYou can load data into R directly from a URL if you are given one, and that is in fact how you will mainly access data to be used in this statistics text.\nhere, we load data from a file stored in a ‘’repo’ on my github account:\n\niris&lt;-read_csv(\"https://raw.githubusercontent.com/mbh038/r4nqy/refs/heads/main/data/iris.csv\") |&gt;\n  clean_names()\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ sepal_length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#clean-manipulate-the-data",
    "href": "workflow.html#clean-manipulate-the-data",
    "title": "1  A recommended analysis workflow",
    "section": "1.8 Clean / Manipulate the data",
    "text": "1.8 Clean / Manipulate the data\nOften we need to do some sort of data ‘wrangling’ to get the data into the form we want. For example we may wish to tidy it (this has a particular meaning when applied to data sets), to remove rows with missing values, to filter out rows from sites or time periods that we don’t want to include in our analysis, to create new columns and so on.\nFor example, lets create a new data frame for just the setosa species of iris:\n\nsetosa &lt;- iris |&gt;\n  filter(species == \"setosa\") # filter picks out rows according to criteria being satisfied in some column\nglimpse(setosa)\n\nRows: 50\nColumns: 5\n$ sepal_length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n\n\nor maybe we just want the columns that contain numeric data and not the one containing the species identifiers, which is text:\n\niris_numeric &lt;- iris |&gt;\n  select(-species) # select() retains or leaves out particular columns. Here, we leave out the species column.\nglimpse(iris_numeric)\n\nRows: 150\nColumns: 4\n$ sepal_length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#summarise-the-data",
    "href": "workflow.html#summarise-the-data",
    "title": "1  A recommended analysis workflow",
    "section": "1.9 Summarise the data",
    "text": "1.9 Summarise the data\nHow big is the difference between the mean of this group over here and that group over there, and how big is that difference compared to the precision with which we know those means? We nearly always want to do this as a first way to get insight into whether we will or will not reject our hypothesis. For example, let’s find the mean petal widths of the three species, the standard errors of those means and save the results to a data frame called petal_summary\n\npetal_summary&lt;-iris |&gt;\n  group_by(species) |&gt;\n  summarise(mean.Pwidth = mean(petal_width),\n            se.Pwidth = sd(petal_width/sqrt(n())))\npetal_summary\n\n# A tibble: 3 × 3\n  species    mean.Pwidth se.Pwidth\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 setosa           0.246    0.0149\n2 versicolor       1.33     0.0280\n3 virginica        2.03     0.0388\n\n\nWe can look at this table and already get an idea as to whether the petal widths are the same or are different for the three species.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#plot-the-data",
    "href": "workflow.html#plot-the-data",
    "title": "1  A recommended analysis workflow",
    "section": "1.10 Plot the data",
    "text": "1.10 Plot the data\nThe next step is usually to plot the data in some way. We would typically use the ggplot2 package from tidyverse to do this.\n\n1.10.1 Bar plot with error bars\nWe could plot a bar plot with error bars, working from the summary data frame that we created:\n\npetal_summary |&gt;\n  ggplot(aes(x = species, y = mean.Pwidth)) +\n  geom_col(fill=\"#a6bddb\") + # this is the geom that gives us a bar plot when we have already done the calculations\n  geom_errorbar(aes(ymin = mean.Pwidth - se.Pwidth, ymax = mean.Pwidth + se.Pwidth), width = 0.15) +\n  labs( x = \"species\",\n        y = \"Mean petal width (mm)\",\n        caption = \"Error bars are ± one standard error of the mean\") + # important to say what these error bars denote\n  theme_cowplot()\n\n\n\n\n\n\n\n\nNote that we have given the bars a fill colour - we got this color from this site due to the cartographer Cynthia Brewer, who is behind the various incarnations of the Brewer package in R, which is great for getting colours that work well. We have used the same colour for each species since the x-axis labels already tell us which bar relates to which species. To use a different colour for each bar would imply there is some extra information encoded by colour. Since there is not, it serves no purpose to have different colours, and potentially confuses the reader. Remember always that a plot is intended to convey a message. Anything that detracts from that message should be avoided, however pretty you think it is.\nA couple of points could be made about this type of plot:\nFirst, what about those error bars? Three types of error bar are in common usage and there are arguments in favour and against the use of each of them:\n\nthe standard deviation tells us about the spread of values in a sample, and is an estimate of the spread of values in a population;\n\nthe standard error of the mean, as used here, is an estimate of the precision with which the sample means estimates the respective population means for each of the species.\nthe confidence interval, typically a 95% confidence interval, gives us the region within which we are (say) 95% confident that the true species mean petal width might plausibly lie.\n\nWhich type of error bar is best to use depends on what story you want to tell. Here, because we are interested in whether there is evidence of a difference in the mean petal width of different species, we have gone for the standard eror of the mean.\nRegardless of which error bar you use and why, you should always tell the reader which one you have gone for, as we have in the caption to the figure.\nA second point about this bar plot is that it doesn’t tell us very much, and indeed nothing that we didn’t already know. It only conveys the mean and standard error values for each species, which is information we already have, arguably more compactly and in more easily readable form, in the table we created. Further, it potentially obscures information that might come from knowing the distribution of the data.\nHere are three other plot types that do show the distribution of petal widths for each species and thus add extra information to what we already know from the summary table\n\n\n1.10.2 Box plot\n\niris |&gt;\n  ggplot(aes(x=species, y=petal_width)) + # what we want to plot\n  geom_boxplot(fill=\"#a6bddb\",notch=FALSE) + # what kind of plot we want\n  geom_jitter(width=0.1, colour = \"#f03b20\",alpha=0.5) +\n  labs (x = \"species\",\n        y = \"Petal Width (mm)\") +\n  theme_cowplot() # choose a theme to give the plot a 'look' that we like\n\n\n\n\n\n\n\n\nHere, we have added the points themselves on top of the box plot. When there are not too many data points, this can be useful. The ‘jitter’ adds some horizontal or vertical jitter, or both, so that the points do not lie on top of each other. In this case we see that the variability of petal widths is not the same for each species and that the data are roughly symmetrically distributed around the median values in each case. This information is useful in helping us determine which statistical test might be appropriate for these data.\n\n\n1.10.3 Violin plot\nA useful alternative to the box plot, especially when the data set is large, is the violin plot:\n\niris |&gt;\n  ggplot(aes(x = species, y = petal_width)) + # what we want to plot\n  geom_violin(fill=\"#a6bddb\",notch=TRUE) + # what kind of plot we want\n  #geom_jitter(width=0.1, colour = \"#f03b20\",alpha=0.5) +\n  labs (x = \"species\",\n        y = \"Petal Width (mm)\") +\n  theme_cowplot() # choose a theme to give the plot a 'look' that we like\n\n\n\n\n\n\n\n\nThe widths of the blobs (I am probably supposed to call them ‘violins’!) show us the distribution of the data - where they are widest is where the data are concentrated, while the height of the blobs shows us the range of variation of the data. The positions of the blobs tells us the mean petal widths of the different species and gives us an idea of the differences between them.\n\n\n1.10.4 Ridge plot\nA bit like a violin plot. This needs the package ggridges to be installed.\n\nlibrary(ggridges)\niris |&gt;\n  ggplot(aes(x = petal_width,y = species)) + # what we want to plot\n  geom_density_ridges(fill=\"#a6bddb\") + # what kind of plot we want\n  #geom_jitter(width=0.1, colour = \"#f03b20\",alpha=0.5) +\n  labs (x = \"Petal Width (mm)\",\n        y = \"species\") +\n  theme_cowplot() # choose a theme to give the plot a 'look' that we like\n\n\n\n\n\n\n\n\nHaving seen the summary and one of these plots of the data, would you be inclined to reject, or fail to reject, a null hypothesis that said that there was no difference between the petal widths of the three species?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#statistical-analysis",
    "href": "workflow.html#statistical-analysis",
    "title": "1  A recommended analysis workflow",
    "section": "1.11 Statistical analysis",
    "text": "1.11 Statistical analysis\nOnly now do we move on to the statistical analysis to try to answer our intial question(s). But by now, after the summary and plot(s), we may already have a pretty good idea what that answer will turn out to be.\nThe exact form of the analysis could take many forms. In a typical ecology project you might carry out several types of analysis, each one complementing the other. Here, an appropriate analysis might be to use the linear model in the form of a one-way ANOVA, since we have one factor (species) with three levels (setosa, versicolor and virginica) and an output variable that is numeric and likely to be normally distributed. We can use the lm() function for this.\n\n1.11.1 Create the model object\n\npw.model &lt;-lm (petal_width ~ species, data = iris)\n\n\n\n1.11.2 Check the validity of the model\nWe won’t go into this here, but an important step is to check that the data satisfy the often finicky requirements of whatever statistical test we have decided to use. The autoplot() function form the ggfortify package is great for doing this graphically.\n\nautoplot( pw.model) + theme_cowplot()\n\n\n\n\n\n\n\n\nHere we note in particular that although the spread of data within each level is not roughly the same (top left figure)), the QQ plot is pretty straight (top-right figure). This means that the data are approximately normally distributed around their respective means. Taken together, this means that these data satisfy reasonably well the requirements of a linear model, so the output of that model should be reliable.\n\n\n1.11.3 The overall picture\nTypically, statistical tests are testing the likelihood of the data being as they are, or more ‘extreme’ than they are, if the null hypothesis were true. Thus, the null hypothesis is central to statistical testing.\nThe null hypothesis is typically that the ‘nothing going on’, ‘no difference’ or ’ no association’ scenario is true. In this case, it would be that there is no difference between the petal widths of the the three species of iris being considered here.\nTypically too, a test will in the end spit out a p-value which is the probability that we would have got the data we got, or more extreme data, if the null hypothesis were true. Being a probability, it will always be a value between 0 and 1, where 0 means impossible, and 1 means certain. The closer the p-value is to zero, the less likely it is we would have got our data if the null hypothesis were true. At some point, if the p-value is small enough, we will decide that the probability of getting the data we actually got if the null hypothesis were true is so small that we reject the null hypothesis. Typically, the threshold beyond which we do this is when p = 0.05, but we could choose other thresholds. (Sounds arbitrary - yes, it is, but the choice of 0.05 is a compromise value that makes the risk of making each of two types of error - rejecting the null when we should not, and failing to reject it when we should, both acceptably small. This is a big topic which we won’t explore further here.)\nIn the end, whatever other information we get from it, the outcome of a statisical test is typically that we either reject the null hypothesis or we fail to reject it. If we reject it then we are claiming to have detected evidence for an ‘effect’ and we go on to determine how big that effect is and whether it is scientifically interesting. If we fail to reject the null, that does not necessarily mean that there is no ‘effect’ (difference, trend, association etc). That might be the case, but it might also just mean that we didn’t find evidence for one from our data.\nIt is all a bit like in a law court where the ‘null hypothesis’ is that the defendant is innocent, and at the end of the proceedings this null is either rejected (Guilty!) because the evidence is such as to make it untenable to hold onto the null hypothesis, or not rejected, because the evidence is not strong enough to convict, in which case the defendant walks free - but is not declared innocent. Formally, the court has simply found insufficient evidence to convict. In the latter case, the court would have failed to reject the null hypothesis. Crucially, it would not have declared that the defendant was innocent. In the same way, in a scientific study, we either reject or fail to reject a null hypothesis. We never ever accept the null hypothesis as true.\nActually, many researchers are unhappy wih this so-called ‘frequentist’ narrative and have sought to use an alternative ‘Bayesian’ approach to testing hypotheses. In this approach we can accept hypotheses and we can bring in prior knowledge. This is an interesting topic, but a very big one so we will not pursue it further here.\nWith all that behind us, we are in a better place to understand what the output of the test is telling us.\nFor the 1-way ANOVA, as with other examples of the linear model, this output comes in two stages:\n\n\n1.11.4 Overall picture\nIs there evidence for a difference between at least two of the mean values?\nTo see if there is evidence for this, an ANOVA test calculate the ratio between the dfference betweeN the groups compared to the differences within the groups. it calls this ratio \\(F\\). The bigger \\(F\\) is, the more likely we are to reject the null hypothesis that there is no difference between he groups.\n\nanova(pw.model)\n\nAnalysis of Variance Table\n\nResponse: petal_width\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nspecies     2 80.413  40.207  960.01 &lt; 2.2e-16 ***\nResiduals 147  6.157   0.042                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe F value is huge. The null hypothesis of this test, as with many tests, is that there is no difference between the petal widths of the three populations from which these samples have been drawn. In that case, the F value would be one. The p-value is telling us how likely it is that we would get an F value as big or bigger than the one we got for our samples if the null hypothesis were true. Since the p-value is effectively zero here, we reject the null hypothesis: we have evidence from our data that there is a significant varation of petal width between species.\nThe degrees of freedom Df tells us the number of independent pieces of information that were used to calculate the result. Let’s not dwell on this here, but there are two that we have to report in this case: the number of levels minus one ie 3-1 = 2, and the number of individual data points in each level minus one, times the number of levels ie (50-1) x 3 = 147.\n\n\n1.11.5 Effect size\nNow that we have established that at least two species of iris have differing petal widths, we go onto investigate where the differences lie, and how big they are. This is important: effect sizes matter. It is one thing to establish that a difference is statistically significant (and typically even the tiniest difference can show up as significant in a study if the sample size is big enough), it is quite another to establish whether the difference is big enough to be scientifically interesting.\n\nsummary(pw.model)\n\n\nCall:\nlm(formula = petal_width ~ species, data = iris)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-0.626 -0.126 -0.026  0.154  0.474 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.24600    0.02894    8.50 1.96e-14 ***\nspeciesversicolor  1.08000    0.04093   26.39  &lt; 2e-16 ***\nspeciesvirginica   1.78000    0.04093   43.49  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2047 on 147 degrees of freedom\nMultiple R-squared:  0.9289,    Adjusted R-squared:  0.9279 \nF-statistic:   960 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nThe output here is typical of that from a 1-way ANOVA analysis in R. Each line refers to one of the three levels of the factor being investigated, which is petal width in this case. By default, those levels are arranged alphabetically, so in this case the order is setosa, versicolor then virginica. The first row is always labelled (Intercept), so here that row is referring to setosa. This level is used as the ‘control’ or reference level- the one with which the others are compared. If we are happy to have setosa as that control then we can just carry on, but if we are not, then we have to tell R which level we want to play that role. We’ll go through how to do that later on.\nIn the Estimate column the value 0.246 cm in the first row refers to the actual mean petal width of the setosa plants in the sample. If we go back to the summary table we created earlier on, or look at one of the plots we created, we see that that is the case.\nFor all other rows, the value in the Estimate column is not referring to the absolute mean petal width but to the difference between the mean petal width for that species and the mean petal width of the control species. So we see that the mean petal width of the versicolor in our sample is 1.08 cm greater than that of setosa and so is equal to .246 + 1.08 = 1.326 cm, while that of the virginica is 1.78 cm greater and so is equal to 2.026 cm. Check from the table of mean values we created and the plots that this is correct.\nHere though, we are not interested in absolute values so much as we are in differences, which is why that is what the summary table here gives us. Look again at the differences between the mean petal widths for versicolor and virginica and that for setosa and compare them with the standard erros of those differences, which are given in the second column of the table. These standard errors are much smaller than the differences, meaning that we can have confidence that the differences are statistically significant.\nThis is borne out by the p-value in the right hand column of the table. The null hypothesis of this table is that there is no difference in petal width between populations of the different species from which these samples have been drawn.\nLastly, the adjusted \\(R^2\\) tells us the proportion of variation of petal width that is accounted for by taking note of the species. Here, the value is 0.93, which tells us hat little else besides species determines the relative petal widths. Ther are no other variables that we need to have taken into account.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "workflow.html#report-in-plain-english",
    "href": "workflow.html#report-in-plain-english",
    "title": "1  A recommended analysis workflow",
    "section": "1.12 Report in plain English",
    "text": "1.12 Report in plain English\nYou would say something like\nWe find evidence that petal widths are not the same acros thhree species of iris, with virginica &gt; versicolor &gt; setosa. (ANOVA, df = 2, p &lt; 0.001)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A recommended analysis workflow</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html",
    "href": "ggplot_examples.html",
    "title": "2  Building plots using the package ggplot2",
    "section": "",
    "text": "2.1 Load packages\nIn this exercise we are going to produce and improve a variety of useful and widely used plots using the package ggplot2 which is part of the larger tidyverse package.\nYou will see that the code to do each plot is very similar, whatever the type of plot, and that plots can be built up from very basic forms to become really attractive, informative versions with very little additional effort.\nYou need to read the examples in this worksheet and then fill in the missing code or alter what is provided already in the empty code chunks of the accompanying template script. Instructions for getting that are given below.\nAs you complete each code chunk, try it out by pressing the green arrow at the top right of the chunk. Sometimes you might want to try out an individual line. You can do that by placing the cursor anywhere in the line and pressing Controll-Entr (windows) or Command-Enter (Mac)\nRemember that the template is a markdown document, so you can add extra text between the code chunks to explain to yourself what is going on. You can format this test, if you wish, according to the very basic markdown rules for doing this. See Help/Markdown Quick Reference. This formatting is only useful if you ‘knit’ the script, by pressing the knit button at the top of the script pane. Try this! I suggest you knit to html. This is how the worksheet you are working from was produced.\n# install.packages(\"name of package\") # run this line once, if you need to, for any of the packages that need to be installed\nlibrary(tidyverse)\nlibrary(here)\nlibrary(palmerpenguins)\nlibrary(devtools)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#get-the-template-script",
    "href": "ggplot_examples.html#get-the-template-script",
    "title": "2  Building plots using the package ggplot2",
    "section": "2.2 Get the template script",
    "text": "2.2 Get the template script\nThis next chunk will download the template file that you need to fill in as you work through this worksheet, and put it in the scripts subfolder within your project folder. For it to work, you need to be ‘working in your Project’ - in which case the name of the project will appear in the top right of the main RStudio window, and if you have a subfolder within the project folder called ‘scripts’. If any of that is not true, it needs to be sorted now!\n\nfile_url &lt;- \"https://raw.githubusercontent.com/mbh038/r-workshop/gh-pages/scripts/ggplot_examples_template.Rmd\"\nfile_dest &lt;- here(\"scripts\",\"my_ggplot_examples.rmd\")\ndownload.file(file_url,file_dest)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#load-the-palmer-penguin-data",
    "href": "ggplot_examples.html#load-the-palmer-penguin-data",
    "title": "2  Building plots using the package ggplot2",
    "section": "2.3 Load the Palmer penguin data",
    "text": "2.3 Load the Palmer penguin data\nFor this exercise we use the Palmer penguins data set which comes with the package palmerpenguins\nThe palmerpenguin package contains two built-in data sets. One is called penguins:\nHere we load the data into this R session (you will now see it in the Environment pane) and we inspect it using the function glimpse().\n\ndata(penguins)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nHow many rows are there and how many columns?\nFor more detailed meta-information on the data we just type the name of the data set with a question mark before it:\n\n?penguins",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#summary-stats-on-all-the-numeric-columns",
    "href": "ggplot_examples.html#summary-stats-on-all-the-numeric-columns",
    "title": "2  Building plots using the package ggplot2",
    "section": "2.4 Summary stats on all the numeric columns",
    "text": "2.4 Summary stats on all the numeric columns\nThis is in general useful to get, at least for the columns that contain numerical data, since it shows which columns contains NAs,which is R-speak for missing data. They are how R represents what would be empty cells in an Excel spreadsheet.\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nWe see that there are some rows with NAs in for a few of the columns. We need to be aware of this when doing calculations with the data, such as taking means.\nHere, we will remove those rows:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#remove-the-rows-with-nas",
    "href": "ggplot_examples.html#remove-the-rows-with-nas",
    "title": "2  Building plots using the package ggplot2",
    "section": "2.5 Remove the rows with NAs",
    "text": "2.5 Remove the rows with NAs\n\npenguins &lt;- penguins |&gt;\n  drop_na()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#how-many-observations-are-there-for-each-species",
    "href": "ggplot_examples.html#how-many-observations-are-there-for-each-species",
    "title": "2  Building plots using the package ggplot2",
    "section": "2.6 How many observations are there for each species?",
    "text": "2.6 How many observations are there for each species?\nNote the use of the pipe operator |&gt;, here and throughout. Think of it as meaning and then. It feeds the output of one line into the function of the next line, where it is used as that function’s first argument.\n\npenguins |&gt;\n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      146\n2 Chinstrap    68\n3 Gentoo      119",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "ggplot_examples.html#mean-value-for-each-numerical-variable-for-each-species",
    "href": "ggplot_examples.html#mean-value-for-each-numerical-variable-for-each-species",
    "title": "2  Building plots using the package ggplot2",
    "section": "2.7 Mean value for each numerical variable, for each species",
    "text": "2.7 Mean value for each numerical variable, for each species\nHere is an example of the use of the group_by() then summarise() combination, whereby data is first grouped, here by species, then summary statistics (of your choice) are calculated for each group.\nIn this example the data are grouped by species, then the mean value of all the columns that contain numerical data are calculated, not just an overall value for the whole column, but for each species\n\npenguins |&gt;\n  group_by(species) |&gt;\n  summarize(across(where(is.numeric), mean, na.rm = TRUE)) |&gt;\n  ungroup() # good practice to include this at the end.\n\n# A tibble: 3 × 6\n  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n  &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie              38.8          18.3              190.       3706. 2008.\n2 Chinstrap           48.8          18.4              196.       3733. 2008.\n3 Gentoo              47.6          15.0              217.       5092. 2008.\n\n\n\n2.7.1 Scatter plots\nIs flipper length correlated with body mass?\nWe could a do correlation test to find this out, but let us first plot the data. We will show here how an elegant plot can be built up, starting from a very basic one, so that you see what each line of code for the finished version actually does. In the chunks below, run each one in turn to see the effect of each successive change that you make.\nFirst we feed the penguin data to the function ggplot(), and use its aes() argument to tell it which variables are to be ‘mapped’ to which aesthetic (which means, roughly speaking, ‘visible’) features of the plot, such as the x-axis, the y-axis, point and line colours, fill colours, symbol types and size etc:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g))\n\n\n\n\n\n\n\n\nThis produces the first layer of the eventual finished plot, an empty plot, ready to display data. Before it can do this, ggplot() needs to be told how you want to do so - what type of plot do you want? For that, we add a geom.....() line, to specify the type of plot.\nThere are lots of geom types, but for a scatter plot we use geom_point():\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis gives us a recognisable scatter plot, but it is deficient in a number of ways. For starters, we know that there are three species of penguin. It would be better if each were plotted using symbols of a different colour, shape or size. We can do this by adding in an extra argument to the aesthetic in the first line. Here we include colour = species.\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm,y = body_mass_g, colour = species)) +\n  geom_point()\n\n\n\n\n\n\n\n\nCan you guess what you should have do if you wanted not the symbol colour, but its shape or size to depend on species? Clue: change one word!\nNow we add labels, titles and so on, using the line labs(...). Note how we can actually write the arguments of this over several lines on the page, for clarity.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\", # this changes the title of the legend.\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\",\n       caption = \"Alternative place to put the information in the subtitle\")\n\n\n\n\n\n\n\n\nIt can be useful to include some combination of titles, subtitles and captions if the figure is to be used as part of a presentation or poster, but if it is to go in a report, you would normally only include a caption, and let the word-processing software do it, and if just for exploratory analysis, not even that. I normally do include axis labels, however.\nNow we use a theme to alter the overall look of the figure. There are several built-in themes you can choose from, and others from packages that you can use. I usually use theme_cowplot() from the cowplot package. Try typing ?theme at the command prompt in the console window to see what is available. Here, we use the built-in theme_bw():\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nNow we reposition the legend. We don’t have to, but we might not like the default position of the legend. If not, we can move or even remove it using another theme() line. The position argument of this can be “none” if you want to remove it, top”, “bottom”, “left”, “right” or a numerical vector in relative coordinates, where c(0,0) means bottom left within the plot, and c(1,1) means top-right. This is what we use here. Play around with different values.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\") +\n  theme_bw() +\n  theme(legend.position = c(0.2,0.8)) # try \"top\", \"left\" etc\n\n\n\n\n\n\n\n\nNicer colours. If you don’t like the default colours offered by R, there are several other palettes available, for example the Brewer palettes, borrowed from the world of maps. See https://colorbrewer2.org ,and for a list of the available palettes, type &gt;?scale_colour_brewer into the console pane then look at the help that appears in the Help pane (bottom right), and scroll down to the palettes section. Note that we dont have to alter the colours. But doing so can make your plots not only look nicer, but serve some other purpose, such as to be colour-blind friendly, or have colours that are appropriate for the variables being plotted (eg red points for red things, blue points for blue things). For an assignment or dissertation report, it is a good idea to pick a palette that you like and that works, and stick with it, so that all your plots have the same general look. Here we choose the qualitative palette \"Set2\" and use it by by adding the line scale_colour_brewer(palette=\"Set2\"). Try a few other palettes.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\") +\n  scale_colour_brewer(palette=\"Set2\") + # try other palettes eg \"Set3\"\n  theme_bw() +\n  theme(legend.position = c(0.2,0.8)) # try \"top\", \"left\" etc\n\n\n\n\n\n\n\n\nIf we like, we can add best fit lines to each subset of the data, using geom_smooth(). To produce straight line fits, geom_smooth() needs to be told to use a linear model, using the method = \"lm\" argument. By default, you will get lines with a grey 95% confidence band around them. This can be useful, but if you don’t want it, add the argument se = FALSE, as we have done below. We have also altered the linewidth.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=body_mass_g,colour=species)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", linewidth=0.5,se=FALSE) + # try leaving out the se argument\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and body mass for Adelie, Chinstrap, and Gentoo Penguins\") +\n   scale_colour_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = c(0.2,0.8)) # also try legend.position = \"top\", \"left\" etc\n\n\n\n\n\n\n\n\n\n\n2.7.2 Repeat for bill length and flipper length\nModify the code of the previous plot so that you now plot bill length vs flipper length. Adjust any labels and titles as necessary. This time, put the legend in the bottom right of the plot.\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=bill_length_mm,colour=species)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", linewidth=0.5,se=FALSE) + # try leaving out the se argument\n  labs(x = \"Flipper length (mm)\",\n       y = \"Bill length (mm)\",\n       colour= \"Species\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and bill length for Adelie, Chinstrap, and Gentoo Penguins\") +\n   scale_colour_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = c(0.9,0.2)) # play with the values to get it where you want it\n\n\n\n\n\n\n\n\nDo you see how straightforwrd it is to adapt the code that produces one plot to get the code you need for another, similar plot?\n\n\n2.7.3 Add yet more informtion to the plot\nLet us include the information of which island the penguins come from by making the shape of the plotted points be dependent on that:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,y=bill_length_mm,colour=species,shape=island)) +\n  geom_point() +\n  #geom_smooth(method=\"lm\", linewidth=0.5,se=FALSE) + # try leaving out the se argument\n  labs(x = \"Flipper length (mm)\",\n       y = \"Bill length (mm)\",\n       colour= \"Species\",\n       shape=\"Island\",\n       title=\"Penguin size, Palmer Station LTER\",\n       subtitle=\"Flipper length and bill length for Adelie, Chinstrap, and Gentoo Penguins\") +\n   scale_colour_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"right\") # play with the position to get it where you want it. Try \"top\" etc.\n\n\n\n\n\n\n\n\n\n\n2.7.4 Distribution of penguin flipper lengths\nThe distribution of a data set is often a useful thing to know. Around which value are the data grouped, how widely spread are they and are the values symmetrically or asymmetrically distributed around the central value? A number of plot types can show this for us. Here we illustrate histograms, density plots, box plots, violin plots and ridge plots.\n\n2.7.4.1 Histogram\nFirst, let’s do a basic histogram. For this we use geom_histogram(). In the ggplot line, in the aes() argument, we need only specify the variable that maps to x, since the software will count how many observations lie within specific narrow ranges of the variable, called bins. Those bin counts will be the y variable of the histogram. To find the distribution of flipper length, we use flipper_length_mmm as the x variable. So we could try\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm)) +  # why is y not specified?\n  geom_histogram()\n\n\n\n\n\n\n\n\nBut this isn’t useful. The histograms for the three species overlap each other, so we need to give each one a different colour, and we need to reduce the opacity of the bars so that the histograms behind are not obscured by the ones in front, where they overlap. Further, we need to stop ggplot from stacking the different histogram bars on top of each other where those for different species are in the same bin. Annoyingly, that is what it does by default, which makes seeing the individual distributions clearly much more difficult.\nAnother thing with histograms, something that can make them a fiddle to use, is that their usefulness in revealing a distrivution is affect by how wide the bins are. By default, ggplot chooses the bin width such that you get 30 bins altogether. This may not be optimal. Here, let’s try specifying the bin width to 4 mm (but see what happens when you try other values, especially very large and very small values).\nThis we can achieve by:\n\nincuding fill = species in the aes() argument of ggplot.\nsepcifying position = identity as an argument of geom_histogram(), to stop the stacking.\nspecifying the opacity argument alpha to be a value less than 1. Here we try alpha = 0.4` - but try other values in the range 0 (transparent) - 1 (opaque), to reduce the opacity.\nspecifying binwidth = 4 - try other values\n\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm, fill = species)) +  # why is y not specified?\n  geom_histogram(position = \"identity\", alpha = 0.4, binwidth = 4)\n\n\n\n\n\n\n\n\nSo, a lot going on, but still only three lines of code!\nNow add good axis labels, an overall theme, and choose a colour scheme you like, and the legend position, just as you have done before:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,fill=species)) +\n  geom_histogram(position=\"identity\",alpha = 0.4, binwidth = 4) +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", # specifies the legend title. See what happens if you omit this line.\n       title=\"Penguin flipper lengths\") + # we wouldn't use this for a figure going in a report.\n  scale_fill_brewer(palette=\"Set2\") + \n  theme_bw() +\n  theme(legend.position = c(0.9,0.8)) # play with the position to get it where you want it\n\n\n\n\n\n\n\n\nIn the scatter plot and the histogram, we have used colour to distinguish the different species. We can do this because our data set is tidy: there is just one column that species the species, and the same for every other variable. That same feature of the data enables to use another way to represent the different species: facet_wrap(~species). This gives us three separate plots, side by side or one above the other. See it used here:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,fill=species)) +\n  geom_histogram(position=\"identity\",alpha = 0.4, binwidth = 4) +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", \n       title=\"Penguin flipper lengths\") + \n  facet_wrap(~species) + #try adding the argument ncol = 1.\n  scale_fill_brewer(palette=\"Set2\") + # try other palettes, eg \"Set1\".\n  theme_bw() +\n  theme(legend.position=\"none\") # we don't need a legend!\n\n\n\n\n\n\n\n\nJust a thought, but do the colours here serve any useful purpose? What extra information do they convey? If you ever think that a feature of a graph conveys no additional information, consider omitting it. Here is the figue before without colours, but going for white brs with grey outlines:\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm)) +\n  geom_histogram(position=\"identity\",alpha = 0.4, binwidth = 4, fill=\"white\",colour=\"grey50\") +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", # specifies the legend title. See what happens if you omit this line.\n       title=\"Penguin flipper lengths\") + # we wouldn't use this for a figure going in a report.\n  facet_wrap(~species) + #try adding the argument ncol = 1.\n  theme_bw() +\n  theme(legend.position=\"none\") # we don't need a legend!\n\n\n\n\n\n\n\n\nArguably, this is a better plot than the previous one because it excludes the potentially confusing redundancy of using different colours each species, when we already know which species is the subject of each plot.\nIf you don’t like white as the fill colour, try another one, for exampe this one that I found on Cynthia Brewer’s very useful map colour site: https://colorbrewer2.org\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm)) +\n  geom_histogram(position=\"identity\",alpha = 0.4, binwidth = 4, fill=\"#a6bddb\",colour=\"grey50\") +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", # specifies the legend title. See what happens if you omit this line.\n       title=\"Penguin flipper lengths\") + # we wouldn't use this for a figure going in a report.\n  facet_grid(island~species) + #try adding the argument ncol = 1.\n  theme_bw() +\n  theme(legend.position=\"none\") # we don't need a legend!\n\n\n\n\n\n\n\n\nDifferent fill colours would be useful if the different penguin species had distinctive dominant colours, but that isn’t the case!\n\n\n2.7.4.2 Density plot\nAn alternative to a histogram, the density plot, gives us a smoothed version of the histogram. The vertical axis on these is not a count, but a measure of the concentration of the data.\nHere is one with overlapping density plots\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm,fill=species)) +\n  geom_density(alpha=0.2) +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Density\",\n       fill= \"Species\",\n       title=\"Penguin flipper lengths\") +\n  scale_fill_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"right\") # play with the position to get it where you want it\n\n\n\n\n\n\n\n\nWe can also adapt this and do what was done for the histograms and do a set of three, one for each species, using facet_wrap():\n\npenguins |&gt;\n  ggplot(aes(x=flipper_length_mm)) +\n  geom_density(alpha = 0.2, fill=\"#a6bddb\",colour=\"grey50\") +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Count\",\n       fill= \"Species\", # specifies the legend title. See what happens if you omit this line.\n       title=\"Penguin flipper lengths\") + # we wouldn't use this for a figure going in a report.\n  facet_wrap(~species) + #try adding the argument ncol = 1.\n  theme_bw() +\n  theme(legend.position=\"none\") # we don't need a legend!\n\n\n\n\n\n\n\n\nWhich is more useful in this case: the overlapping plots on one chart, or the separate charts done using facet_wrap()? Whatever you think here, the answer in other cases will sometimes be one, sometimes the other. Now you have the tools to enable you to try both and make the best choice.\n\n\n2.7.4.3 Box plots\nBox plots are a really useful way to summarize the distribution of numerical response data such as flipper_length_mm across different categorical variables, such as species. We use geom_boxplot() to produce them.\nLet’s do a basic box plot of flipper lengths for each penguin species:\n\npenguins |&gt;\n  ggplot(aes(x=species,y=flipper_length_mm,fill=species)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nNow let’s use what we have done before to add code lines that\n\ninclude suitable axis labels and a title\ngive the same ‘theme’ ie overall look as the previous graphs\nfill the boxes with the same colour for each species.\nremove the legend that you now have, because you don’t need it (Why?). Use theme(legend.position=\"none\") to do this.\n\n\npenguins |&gt;\n  ggplot(aes(x=species,y=flipper_length_mm,fill=species)) +\n  geom_boxplot() +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\",\n       title=\"Penguin flipper lengths\") +\n  scale_fill_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"none\") # no legend needed\n\n\n\n\n\n\n\n\n\n\n2.7.4.4 Violin Plot using geom_violin()\nThis is a variant on the box plot. Each ‘violin’ is a sideways density plot of the distribution of the data for each species, with its own mirror image to make it look a bit like a violin. The code for these is exactly as for box plots except we use geom_violin().\n\npenguins |&gt;\n  ggplot(aes(x=species,y=flipper_length_mm,fill=species)) +\n  geom_violin() \n\n\n\n\n\n\n\n\nNow we write code to improve this, just as you did the box plot. The final code is the same as for that apart from one line!\n\npenguins |&gt;\n  ggplot(aes(x=species,y=flipper_length_mm,fill=species)) +\n  geom_violin() +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\",\n       title=\"Penguin flipper lengths\") +\n  scale_fill_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"none\") # no legend needed\n\n\n\n\n\n\n\n\n\n\n2.7.4.5 Ridge plot\nThis is a variant on the density plot, that is most useful when you have lots of categorical variables. We have only three here, the three penguin species, but let’s try it anyway.\nFor this, we need the ggridges package. This is one of many packages that extend the power of ggplot, and so work in much the same way:\n\n# library\n#install.packages(\"ggridges\") # use this once, if you have to, then comment it out.\nlibrary(ggridges) \n \n# basic example\npenguins |&gt;\nggplot(aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges() +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Species\",\n       title=\"Penguin flipper lengths\") +\n  theme_ridges() + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nNow try producing graphs like the ones above, but for body mass rather than flipper length.\n\n\n\n2.7.5 Bar chart with error bar\nThere are different ways to produce this commonly used way to summarise data. For example we might use one to compare the mean flipper lengths of the different penguin species. For a bar chart of these to be of any use at all, it needs to include error bars that show standard deviations of the samples, standard errors of the means, or confidence intervals (Why?). Which you use depends on the story you are trying to tell.\nFirst, we will add error bars that are ± one standard deviation of the samples.\nI usually first create a summary of the data, in which we calculate the means and appropriate error for each species for whichever variable I am interested in, then feed this summary table to ggplot and use geom_col() to plot the bars, with geom_errorbar() on top of that to plot the error bars.\nLet’s do that first:\n\nflipper_summary &lt;- penguins |&gt;\n  drop_na() |&gt;\n  # these two lines produce a summary table\n  group_by(species) |&gt;\n  summarise(fl.mean = mean(flipper_length_mm), fl.sd = sd(flipper_length_mm))\nflipper_summary\n\n# A tibble: 3 × 3\n  species   fl.mean fl.sd\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie       190.  6.52\n2 Chinstrap    196.  7.13\n3 Gentoo       217.  6.59\n\n\n\nflipper_summary |&gt;\n  ggplot(aes(x = species, y = fl.mean)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = fl.mean-fl.sd, ymax = fl.mean + fl.sd), width = 0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  theme_bw() \n\n\n\n\n\n\n\n\n\n2.7.5.1 Standard deviation or standard error?\nThe error bars in the plot above are ± one standard deviation. A standard deviation gives us an idea of the spread of values within a sample or population. Remember that if a population is normally distributed about its mean, there is a roughly 95% probability that a random chosen individual will lie within two standard deviations of the mean.\nSo standard deviations of samples are useful. They are a useful way to summarise the variability of the sample, they tell us about the likely variability of the next sample, and they are our best estimate of the variability of the population from which the sample was drawn.\nSometimes, though, we want to know more than that. We might want to know how closely a sample mean is likely o be to the true mean of the population from which the sample was drawn, and perhaps to get an idea as to whether two or more populations are different, given the means of samples drawn from those populations. For this, we need not the standard deviation but the standard error. These are the error bars that are most commonly displayed on bar charts when you see them in papers.\nTo calculate standard deviation error bar lengths we use a formula \\(\\text{SE} = \\frac{\\text{SD}}{\\sqrt{n}}\\) where \\(n\\) is the number of observations, SD is the standard deviation of the sample and SE is the standard error of the means of the sample. We can use the summary functions sd() to calculate the standard deviation, and n() to calculate \\(n\\).\n\nflipper_summary2 &lt;- penguins |&gt;\n  drop_na() |&gt;\n  # these two lines produce a summary table\n  group_by(species) |&gt;\n  summarise(fl.mean = mean(flipper_length_mm), fl.se = sd(flipper_length_mm)/sqrt(n()))\nflipper_summary\n\n# A tibble: 3 × 3\n  species   fl.mean fl.sd\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie       190.  6.52\n2 Chinstrap    196.  7.13\n3 Gentoo       217.  6.59\n\n\n\nflipper_summary2 |&gt;\n  ggplot(aes(x = species, y = fl.mean)) +\n  geom_col() +\n  geom_errorbar(aes(ymin=fl.mean-fl.se, ymax = fl.mean + fl.se),width=0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  theme_bw() \n\n\n\n\n\n\n\n\nThese standard error bars are always smaller than the standard deviation error bars (by a factor equal to the square root of the sample size). Here they are so small as to be barely visible. They ar useful in bar charts like this one since they give us a rough and ready way of assessing whether the differences between the samples (the heights of the bars) are likely to indicate real differences between the populations. If the bar-height differences are much greater than the size of the standard error bars, then they probably indicate significant differences between the popultions. If not, then they probably don’t.\nNow let’s alter this code so that each bar has a different fill colour, and remove the legend that then appears, since it is unnecessary?\n\nflipper_summary |&gt;\n  # we add an argument to colour each bar according to species\n  ggplot(aes(x = species, y = fl.mean, fill = species)) +\n  geom_col() +\n  geom_errorbar(aes(ymin=fl.mean-fl.sd, ymax = fl.mean + fl.sd),width=0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  theme_bw() +\n  # include an arument to remove the legend\n  theme()\n\n\n\n\n\n\n\n\nNow let us replace this colour scheme with nicer ones (not just nice, but also colour-blind friendly, perhaps) offered by the Brewer palettes.\nTo do this we can add the line scale_fill_brewer(palette = \"Set2\"). Note: we use scale_colour_brewer() to alter the colours of points, like we did above, or the outline colour of bars, and use scale_fill_brewer() to alter the fill colour of bars. This is what we want to do here.\n\nflipper_summary |&gt;\n  ggplot(aes(x = species, y = fl.mean, fill = species)) +\n  geom_col() +\n  geom_errorbar(aes(ymin=fl.mean-fl.sd, ymax = fl.mean + fl.sd), width=0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  scale_fill_brewer(palette=\"Set2\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nIf you don’t like the colours of the palette “Set2” you can try another one. To find out what palettes are available, remember, you can type ?scale_fill_brewer() into the console pane then look at the help that appears in the Help pane (bottom right), and scroll down to the Palettes section.\nIf you agree that having different fill colours for the bars is actually confusing and brings no information to the plot that we do not already know, you can modify the previous plot in the manner that you did for the separate histograms:\n\nflipper_summary |&gt;\n  ggplot(aes(x = species, y = fl.mean)) + \n  # add arguments here that give fill colour \"#a6bddb\" and outline colour \"grey50\".\n  geom_col(fill = \"#a6bddb\", colour = \"grey50\") +\n  geom_errorbar(aes(ymin=fl.mean-fl.sd, ymax = fl.mean + fl.sd), width=0.1) +\n  labs(x = \"Species\",\n       y = \"Flipper length (mm)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Building plots using the package `ggplot2`</span>"
    ]
  },
  {
    "objectID": "titles_labels_and_annotations.html",
    "href": "titles_labels_and_annotations.html",
    "title": "3  Titles, labels and annotations",
    "section": "",
    "text": "Often in plots one needs to use mathematical expressions, suffixes/superscripts, greeek letters or other unusual formatting.\nHere we show one way of doing this, using bquote(). Over the years I have found this to be the simplest way.\n\n3.0.1 Use bquote()\nThe four rules\n\nStrings – Require quotes wrapped w/ tilde separator (e.g., “my text” ~).\nMath Expressions – Unquoted & follow ?plotmath\nNumbers – Unquoted when part of math notation\nVariables – Use .() (pass in string or numeric)\n\nExamples of all of these are shown in the labels, title and annotations included in the plot below:\n\ndf &lt;- tibble(x=1:10,y=1:10)\ncor &lt;- 0.456\ndf |&gt;\n  ggplot(aes(x=x,y=y)) +\n  geom_point() +\n  labs( x = bquote(\"An axis label with suffix and superscript:\" ~ x[i]^2), \n        y = bquote(\"An axis label with greek letters:\" ~ alpha ~ Beta ~ mu ~ m),\n        title = bquote(\"Hello\"~ r[xy] == .(cor) ~ \"and\" ~ CO[2] ~ \"and\" ~ B^2 ~ \"and\" ~ m^{-1})) +\n  annotate(geom=\"text\", x = 5, y = 7, label =  deparse(bquote(\"Hello\" ~ r[xy] == 0.678 ~ \"and\" ~ B^2)), parse = TRUE) +\n  annotate(geom=\"text\", x = 7, y = 9, label =  bquote(\"A big annotation\"), size = 12) +\n  annotate(geom=\"text\", x = 7, y = 2, label =  bquote(\"A red annotation\"), colour = \"red\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n3.0.2 ?plotmath\nplotmath expressions can be used for mathematical annotation in text within plots in R when writing titles, subtitles, axis labels, legends and annotations. It works in both base R graphics and ggplot2.\nIn the console pane, type ?plotmath in the console pane to see the full list of options.\nSome key rules are:\n\nsubscripts: O[2] gives O2\nsuperscripts: m^2 gives m2, m^{-1} gives m-1\nLower case Greek: alpha, beta etc gives \\(\\alpha, \\beta\\) etc, so mu\nUpper case Greek: Delta, Gamma etc gives \\(\\Delta, \\Gamma\\) etc",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Titles, labels and annotations</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html",
    "href": "tests_for_difference_two_levels.html",
    "title": "4  Tests for difference: one factor, two levels",
    "section": "",
    "text": "4.1 Test finder flow chart\nIn this document we consider tests for difference where there are two levels being compared.\nFirst, use this flowchart to see if a two-sample t-test or its non-parametric counterpart the Mann-Whitney-U test is appropriate for your question, your study design and your data:\nflowchart TD\n  A{Difference or Trend \\nQuestion?} --&gt; B[Trend]\n  B --&gt; B2{Are you testing for \\ndegree of association\\nor are you trying \\nto make predictions?}\n  B2 --&gt; B33[Association:\\n\\nTest for\\n correlation coefficient\\nPearson or \\n Spearman's Rank]\n  B2 --&gt; B44[Predictions:\\n\\nSimple linear regression]\n  A --&gt; C[Difference]\n  C --&gt; C22{Do you have replicates?}\n  C22 --&gt; C22Y[Yes]\n  C22 --&gt; C22N[No]\n  C22N --&gt; C23{Do you have count data?}\n  C23 --&gt; C23Y[Yes]\n  C23 --&gt; C23N[N0]\n  C23Y --&gt; C24(Chi square test:\\nGoodness of fit\\nor\\nTest of independence)\n  C23N --&gt; C25[These data cannot be analysed]\n  C22Y --&gt; D{How many factors?}\n  D --&gt; F[Two or more]\n  F --&gt; F2{Independent\\nsamples?}\n  F2 --&gt; F2Y[Yes]\n  F2 --&gt; F2N[No]\n  F2Y --&gt; F22Y(n-way ANOVA)\n  F2N --&gt; F22N(n-way\\nrepeated measures\\nANOVA)\n  D --&gt; E[One]\n  E --&gt; G{How many levels?}\n  G --&gt; H[Two]\n  G --&gt; I[More than two]\n  I --&gt; J{Independent\\n samples?}\n  J --&gt; K[No]\n  J --&gt; L[Yes]\n  K --&gt; S(Repeated measures one-way ANOVA\\nor\\nFriedman Test)\n  L --&gt; T(One way ANOVA\\nor\\nKruskal Wallis one-way test)\n  H --&gt; M{Independent\\n samples?}\n  M --&gt; N[No]\n  M --&gt; O[Yes]\n  N --&gt; P(paired t-test\\nor\\nSigned rank test)\n  O --&gt; Q(two sample t-test\\nor\\nMatt Whitney U test)\nIf this chart suggests you need something other than the two sample t-test or Mann-Whitney test, you need to go to the descriptions of that test.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#two-sample-t-test-the-parametric-case",
    "href": "tests_for_difference_two_levels.html#two-sample-t-test-the-parametric-case",
    "title": "4  Tests for difference: one factor, two levels",
    "section": "4.2 Two sample t-test: the parametric case",
    "text": "4.2 Two sample t-test: the parametric case\nIn this exercise we find out how to run a two-sample t-test, to determine whether there is evidence to reject the hypothesis that two samples are drawn from the same population.\n\n4.2.1 When to use the two-sample t-test\n\nIt can be used when we have two independent samples of numerical (not ordinal) response data, and our question is whether the data provide evidence that the samples are drawn from different populations. Even when this criterion is met and the data are numerical and independent, the normality criterion described below still needs to be met. That apart, two common examples where we have two sets of data but we should not use the two sample t-test are:\n\nIf the data in your two samples are not independent because you have measured the same individual replicate before and after some event or treatment, then you should probably be using a paired t-test instead. In this you don’t have two samples, each comprising a separate and independent set of replicates. Instead, you have multiple pairs of values, one pair per replicate. The replicates are still assumed to be independent of each other\nIf your response data are the answers to a Likert scale such as might be used in a survey then they are ordinal in nature and not numerical, and you should probably be using the non-parametric equivalent of the two sample t-test, which is variously known as the Wilcoxon Rank Sum test or as the Mann Whitney U test, or its paired sample version, if appropriate.\n\nIt can be used when the data set is small. But not so small that there are no replicates. You do need replicates.\nIt can still be used when the data set is large.\nIt assumes that the data are drawn from a normally distributed population. There are various ways to test if this is plausibly the case, and you should try at least one of them, but with small samples, just where the t-test is most useful, it can be difficult to tell. In the end we can also appeal to reason: is there good reason to suppose that the data would or would not be normally distributed?\nWhen comparing the means of two samples, both samples should in principle have the same variance, which is a measure of the spread of the data, so in principle you need to check that this is at least approximately the case, or have reason to suppose that it should be. However, in an actual t-test done using R, the Welch variant of the t-test is carried out by default. This works even when the variances of the two sets are different, so in practice it is possible to ignore this equal variance requirement.\nWe only use it when we are comparing two samples, one for each of the two levels of a single factor. When we have samples for more than two levels and we use the t-test to look for a difference between any two of them, it becomes increasingly likely, the more pairs of samples we compare, that we will decide that we have found a difference because we got a p-value that was less than some pre-determined threshold (which could be anything, but is most often chosen to be 0.05) even if in reality there is none. This is the problem of high false positive rates arising from multiple pairwise testing and is where ANOVA comes in. t-tests are only used to detect evidence for a difference between two groups, not more. ANOVAs (or their non-parametric equivalent) are used when we are looking for differences between more than two groups.\n\n\n\n4.2.2 Motivation and example\nIn our example we will consider the impact of pesticide use on the masses of shells of garden snails (Cornu aspersum), as measured in gardens around a city, ten from randomly selected gardens that have used a range of pesticides for at least two years and ten that are from randomly selected gardens that have not ever used pesticides. We leave aside here the issue of how those gardens were identified and how randomisation was ensured.\n\n\n4.2.3 Questions and hypotheses\nOur question is:\nIs there evidence for a difference between snail shell masses in the gardens where pesticides were used compared to those where they were not used?\nFrom which a suitable null hypothesis is:\nThere is no difference between shell masses in the gardens, whether or not pesticides were used.\nand a suitable alternate, two-sided hypothesis is:\nThere is a difference between shell masses.\n\n\n4.2.4 The data\nSuppose we had our data arranged in a spreadsheet in three columns, one giving the garden ID, G1 to G20, one telling us whether pesticides were used in the garden, yes or no, and one telling us the masses in grams of the snail shells from each garden. Afficioados of R will see that this is ‘tidy’ data. Each variable (ID, pesticide use, shell mass) occurs in only one column, rather than being spread across several. It turns out that this way of storing your data makes it much easier to analyse.\n\n# there should be a 'garden_snails.csv' file in your data folder\n\nfilepath&lt;-here(\"data\",\"garden_snails.csv\")\nsnails&lt;-read_csv(filepath)\n\n# if not, you should be able to get it from Mike's github repo\n\n# file_url &lt;- \"https://raw.githubusercontent.com/mbh038/r-workshop/refs/heads/gh-pages/data/garden_snails.csv\"\n# snails&lt;-read_csv(file_url)\nhead(snails,20)\n\n# A tibble: 20 × 3\n   garden.ID pesticide shell_mass_g\n   &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt;\n 1 S1        Yes               1.37\n 2 S2        Yes               1.15\n 3 S3        Yes               0.73\n 4 S4        Yes               0.65\n 5 S5        Yes               1.03\n 6 S6        Yes               1.8 \n 7 S7        Yes               1.21\n 8 S8        Yes               1.41\n 9 S9        Yes               1.27\n10 S10       Yes               1.08\n11 S11       No                2   \n12 S12       No                3.99\n13 S13       No                1.99\n14 S14       No                1.75\n15 S15       No                2.81\n16 S16       No                2.15\n17 S17       No                1.87\n18 S18       No                3.46\n19 S19       No                2.8 \n20 S20       No                2.89\n\n\n\nIs this a tidy data set?\nIs the data in the pesticide column categorical?\nIf so, how many levels does it have and what are they?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#the-process",
    "href": "tests_for_difference_two_levels.html#the-process",
    "title": "4  Tests for difference: one factor, two levels",
    "section": "4.3 The Process",
    "text": "4.3 The Process\n\n4.3.1 Step One: Summarise the data\nWith numerical data spread across more than one level of a categorical variable, we often want summary information such as mean values and standard errors of the mean for each level.\nHere we will calculate the number of replicates, the mean and the standard error of the mean for both levels of pesticide ie Yes and No:\n\nsnail.summary&lt;- snails |&gt;\ngroup_by(pesticide) |&gt;\nsummarise(n = n(),\n          mean.mass = mean(shell_mass_g),\n          se.mass = sd(shell_mass_g)/sqrt(n()))\nsnail.summary\n\n# A tibble: 2 × 4\n  pesticide     n mean.mass se.mass\n  &lt;chr&gt;     &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 No           10      2.57   0.236\n2 Yes          10      1.17   0.105\n\n\nFrom these data, does it look as though there is evidence for a difference between shell masses in the two types of garden? Clearly, the snails in the ten gardens that did not use pesticide had a higher mean shell mass than the ten from gardens that did use pesticide. But is this a fluke? How precisely do we think these sample means reflect the truth about the impact of the use of pesticides? That is what the standard error column tells us. You can think of the standard error as being an estimate of how far our sample means, drawn from just ten gardens of each type are likely to differ from the true shell masses for all gardens that did use pesticides and all gardens that did not.\nBottom line: the difference between the sample means is about ten times the size of the standard errors of each. It really does look as though snails shells in gardens where pesticides are not used are indeed heavier than in gardens where pesticides are used.\n\n\n4.3.2 Step Two: Plot the data\nRemember, before we do any statistical analysis, it is almost always a good idea to plot the data in some way. We can often get a very good idea as to the answer to our research question just from the plots we do.\nIn Figure 4.1, we will use ggplot() in R to plot a histogram of ozone levels, one for each side of the city. We will stack the histograms one above the other, all the better to help us spot any differences between east and west.\n\nsnails |&gt;\n  ggplot(aes(x=shell_mass_g)) +\n  geom_histogram(binwidth=0.2,fill=\"darkred\")+\n  facet_wrap(~pesticide,ncol=1) +\n  theme_classic()\n\n\n\n\n\n\n\nFigure 4.1: Stacked histograms\n\n\n\n\n\nInstead of histograms, we could have drawn box plots, as in:\n\nsnails |&gt;\n  ggplot(aes(x=pesticide,y=shell_mass_g))+\n  geom_boxplot()+\n  labs(x=\"Pestice use?\",\n       y=\"Shell mass (g)\") +\n  theme_classic()\n\n\n\n\n\n\n\nFigure 4.2: Side-byside box and whisker plots of the distribution of values in each snail sample. The lower and upper edges of each box show the 25th and 75th percentiles of each sample, and the thick black line between them shows the median value ie the 50th percentile\n\n\n\n\n\nor as a dot plot of the means with standard errors of the mean included as error bars, as in Figure 4.3\n\n# for this chart we will use the summary table that we created above.\n\nsnail.summary |&gt; \n  ggplot(aes(x=pesticide,y=mean.mass))+\n  geom_point(size=3) +\n  geom_errorbar(aes(ymin=mean.mass-se.mass,ymax=mean.mass+se.mass),width=0.1)+\n  ylim(0,4) + # try leaving this line out. What happens? Which is better?\n  labs(x=\"Pesticide use?\",\n       y=\"Shell mass (g)\") +\n  theme_classic()\n\n\n\n\n\n\n\nFigure 4.3: The data points show mean values, the error bars show plus or minus one standard error of the mean\n\n\n\n\n\n\nDo the data look as though they are inconsistent with the null hypothesis ?\nIn addition, do the data look as though each group is drawn from a normally distributed population? One of the types of graphs gives you no indication of that while the other two do. Which is the odd one out? Even when looking at the other two figures, when there are so few data it’s kind of hard to tell, no?\n\nLet’s now do some stats.\n\n\n4.3.3 Step Three: Check the validity of the data - are the data normally distributed?\nWe can go about establishing this in three ways: using an analytical test of normality, using a graphical method and by thinking about what kind of data we have. Let’s consider these in turn.\n\n4.3.3.1 Normality test - analytical method\nThere are several analytical tests one can run on a set of data to determine if it is plausible that it has been drawn from a normally distributed population. One is the Shapiro-Wilk test.\nFor more information on the Shapiro-Wilk test in R, type ?shapiro.test into the console window. For kicks, try it out on the examples that appear in the help window (which is the bottom right pane, Help tab). One example is testing a sample of data that explicitly is drawn from a normal distribution, the other tests a sample of data that definitely is not. What p-value do you get in each case? How closely do the histograms of each sample resemble a normal distribution?\n\n#first we create a data frame containing the two example data sets\nexample1&lt;-rnorm(500, mean = 5, sd = 3) # first example from the help pane\nexample2&lt;-runif(500, min = 2, max = 4) # second example from the help pane\n\ndf&lt;-tibble(data=c(example1,example2), distribution=c(rep(\"example 1: normal\",500),rep(\"example 2: not at all normal\",500)))\n\n# then we plot a histogram of each data set\nggplot(df,aes(x=data)) +\n  geom_histogram(bins=20,fill=\"cornflowerblue\") +\n  facet_wrap(~distribution) +\n  theme_classic()\n\n\n\n\n\n\n\n# and finally we run a Shapiro-Wilk normality test on each data set\nshapiro.test(example1) # 100 samples drawn from a normally distributed population\n\n\n    Shapiro-Wilk normality test\n\ndata:  example1\nW = 0.99783, p-value = 0.7769\n\nshapiro.test(example2) # 100 samples drawn from a uniformly (ie NOT normally) distributed population\n\n\n    Shapiro-Wilk normality test\n\ndata:  example2\nW = 0.95547, p-value = 3.847e-11\n\n\nFor the examples above, we see that Shapiro-Wilk test gave a high p-value for the data that we knew were drawn from a normal distribution, and a very low p-value for the data that we knew were not.\nThe Shapiro-Wilk test tests your data against the null hypothesis that it is drawn from a normally distributed population. It gives a p-value which, as always, is the probably of you having data as far from normality, or further, as yours are if the null hypothesis were true. If the p-value is less than 0.05 then we reject the null hypothesis and cannot suppose our data is drawn froma normally distributed population. In that case we would have to ditch the t-test for a difference, and choose another difference test in its place that could cope with data that was not normally distributed. For a two-sample t-test such as we are hoping to use here, the so-called non-parametric alternative that we could use instead is the Wilcoxon Rank Sum test, often called the Mann-Whitney U test.\nWhy don’t we do that in the first place, I hear you ask? Why bother with this finicky t-test that requires that we go through the faff of testing the data for normality before we can use it? The answer is that it is more powerful than other, so-called non-parametric tests that can cope with non-normal data. It is more likely than they are to spot a difference if there really is a difference. So if we can use it, that is what we would rather do.\nSo, onwards, let’s do the Shapiro-Wilk test on our data\nWe want to test each garden group for normality, so we group the data by location as before and and then summarise, this time asking for the p-value returned by the Shapiro-Wilk test of normality.\n\nsnails |&gt;\n  group_by(pesticide) |&gt;\n  summarise('Shapiro-Wilk p-value'=shapiro.test(shell_mass_g)$p.value)\n\n# A tibble: 2 × 2\n  pesticide `Shapiro-Wilk p-value`\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 No                         0.223\n2 Yes                        0.854\n\n\nFor both groups the p-value is more than 0.05, so at the 5% significance level we cannot reject the null hypothesis that the data are normally distributed, so we can go on and use the t-test. Yay!\n\n\n4.3.3.2 Graphical methods - the quantile-quantile or QQ plot.\nConfession: I don’t normally bother with numerical tests for normality such as Shapiro-Wilk. I usually use a graphical method instead.\nWe have already seen two ways of plotting the data that might help suggest whether it is plausible that the data are drawn from normally distributed populations. Histograms and box plots both indicate how data is distributed, and for normally distributed data both would be symmetrical. Well, they would be, more or less, if the data set was large enough but for small data sets it can be quite hard to tell from either type of plot whether the data are drawn from a normally distributed population.\nA better type of plot for making this judgement call is the quantile-quantile or ‘qq’ plot which basically compares the distribution of your data to that of a normal distribution. If your data are approximately normally distributed then a qq plot will give a straight(-ish) line. Even with small data sets, this is usually easy to spot.\n\nsnails |&gt;\n  ggplot(aes(sample=shell_mass_g)) +\n  stat_qq(colour=\"blue\") +\n  stat_qq_line() +\n  facet_wrap(~pesticide) +\n  theme_classic()\n\n\n\n\n\n\n\n\nNothing outrageously non-linear there, so that also suggests we can safely use the t-test.\nFor an overview of how normally distributed and non-normally distributed data looks when plotted in histograms, box plots and quantile-quantile plots, see this review\n\n\n4.3.3.3 The ‘thinking about the data’ normality test\nAs you might have guessed, this isn’t a test as such, but a suggestion that you think about what kind of data you have: is it likely to be normally distributed within its subgroups or not? If the data are numerical values of some physical quantity that is the result of many independent processes, and if the data are not bounded on either side (say by 0 and 100 as for exam scores) then it is quite likely that that they are. If they are count data, or ordinal data, then it is quite likely that they are not.\nThis way of thinking may be all you can do when data sets are very small and any of the more robust tests for normality presented here leave you not much the wiser.\n\n\n\n4.3.4 Do the actual two-sample t-test\nSo, it looks as though it is plausible that the data are drawn from normal distributions. That means we can go on to use a parametric test such as a two sample t-test and have confidence in its output.\nIf we were doing this in R we could use the t.test() function for this (other functions are available!). This needs to be given a formula and a data set as arguments. Look up t.test() in R’s help documentation, and see if you can get the t-test to tell you whether there is a significant difference between ozone levels in the east and in the west of the city.\n\nt.test(shell_mass_g~pesticide,data=snails)\n\n\n    Welch Two Sample t-test\n\ndata:  shell_mass_g by pesticide\nt = 5.4172, df = 12.442, p-value = 0.0001372\nalternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n95 percent confidence interval:\n 0.8397234 1.9622766\nsample estimates:\n mean in group No mean in group Yes \n            2.571             1.170 \n\n\n\n\n4.3.5 Interpret the output of the t-test.\nStudy the output of the t-test. Here are some questions to ask yourself.\n\nWhat kind of test was carried out?\n\nA Welch two sample t-test\n\nWhat data was used for the test?\n\nThe snail shell mass in g (the output variable) and pesticide use (the explanatory variable)\n\nWhat is the test statistic of the data?\n\nThis is t = 5.4172.\n\nHow many degrees of freedom were there? This number is the number of independent pices of information that were used to calculate the final result. It is usually one, two, or three or so less than the number of data points. Don’t overthink it at this stage, especially not the fact that here it is not an integer.\n\ndf = 12.442\n\nWhat is the p-value?\n\np = 0.0001372. You would most likely report this as p &lt; 0.001\n\nWhat does the p value mean?\n\nIt is the likelihood of seeing a difference between sample means as large or larger than the one we found if in fact pesticides made no difference to snail shell mass.\n\nWhat is the confidence interval for the difference between shell masses in gardens that use pesticides and in gardens that do not? Does it encompass zero? Remember that the confidence interval gives the range of values within which the true difference between mean shell masses might reasonably lie, given the data. If that range includes zero then the test is telling is that zero is a plausible value for the difference, and hence that we cannot reject the null hypothesis.\n\nThe 95% confidence interval has lower bound 0.8397 and upper bound 1.962.\n\nIs there sufficient evidence to reject the null hypothesis?\n\nYes. We see this in two ways. First the p value is much less than 0.05 and second, the 95% confidence interval does not encompass zero. In a way, the confidence interval is giving us more information than the p-value, since not only can we deduce whether there is evidence for a significant difference, we can also see how big that difference is and how precisely we know it.\n\nWhat does the word ‘Welch’ tell you - Google it or look it up in the help for t.test().\n\nIt tells us that a variant of the t-test is being used in which it does not matter if the two. samples have difference variances (spreads).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#other-examples-where-a-two-sample-t-test-might-be-used.",
    "href": "tests_for_difference_two_levels.html#other-examples-where-a-two-sample-t-test-might-be-used.",
    "title": "4  Tests for difference: one factor, two levels",
    "section": "4.4 Other examples where a two sample t-test might be used.",
    "text": "4.4 Other examples where a two sample t-test might be used.\nRemember that t-tests in general are used when you have independent samples with multiple replicates drawn from populations corresponding to two levels of some factor (eg north coast, south coast; this beach, that beach; polluted place, clean place etc) and you have measured something numerical, like a length or a mass, temperature or concentration. You still have to do the tests for normality described above, but these are the basic criteria.\n\n4.4.1 Can you think of examples of where you might use a two-sample t-test?\nHere are a few suggestions:\n\nIs there a difference between the flight initiation distance of redstarts when confronted by dogs compared to when they are confronted by drones?\nIs the nitrate concentration of water in a river below a beaver dam different from the nitrate content above that dam?\n\nCan you think of another example?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#what-if-i-cant-use-a-two-sample-t-test",
    "href": "tests_for_difference_two_levels.html#what-if-i-cant-use-a-two-sample-t-test",
    "title": "4  Tests for difference: one factor, two levels",
    "section": "4.5 What if I can’t use a two sample t-test?",
    "text": "4.5 What if I can’t use a two sample t-test?\n\nAssuming you have two independent samples, this might be because one or both sets failed the normality criterion, or your data are ordinal. In that case the likely alternative is the non-parametric equivalent of the t-test, variously known as the Wilcoxon Rank Sum test or the Mann Whitney U test.\nIf your data are in fact sets of paired values, for example because you measured some attribute of the same individuals before and after some treatment, or at two points in time, then you need to use the paired t-test.\nIf you only have one sample of replicates and want to compare its mean value to a threshold, then you use a one sample t-test. You might do this, for example, if you had collected sediment samples from an estuary, measured the concentration in those samples of some pollutant such as pathogens from sewage, or phosphates from farm runoff, and then wanted to see if the water was compliant with water quality thresholds as dictated by, say, the Water Framework Directive.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#the-non-parametric-case",
    "href": "tests_for_difference_two_levels.html#the-non-parametric-case",
    "title": "4  Tests for difference: one factor, two levels",
    "section": "4.6 The non-parametric case",
    "text": "4.6 The non-parametric case\nA common scenario is that we have two sets of measurements, and we want to see if there is evidence that they are drawn from different populations. For some data types we can use a t-test to do this, but for others we cannot.\nA t-test requires in particular that the two sets of data are normally distributed around their respective means. With ordinal data this makes no sense. The mean is undefined as a concept for such data.\nTo see this , reflect that for a collection \\(X\\) of numerical data, say, 5, 3, 3, 4, and 5 we would calculate the mean as:\n\\[\n\\bar{X} = \\frac{5+3+3+4+5}{5} = \\frac{20}{5}=4\n\\]\nBut trying doing the same to five responses of a Likert scale survey. Say the responses you had to five Likert items (individual questions) were “strongly disagree”, “strongly agree”, “mildly disagree”, “strongly disagree” and “don’t care either way”. If you tried to calculate a ‘mean’ response you would be attempting to add up all these responses and to divide the ‘sum’ by five, like this:\n\\[\\text{mean response}=\\frac{\\text{stongly disagree}+\\text{strongly agree}+\\text{mildly disagree}+\\text{stongly disagree}+\\text{don't care either way}}{5} = ?\n\\] This sum makes no sense, I hope you will agree. It makes no sense, not because we are using words to describe our responses, but because, as these are ordinal data, we do not know the size of the gaps between the different points on the scale. Is the difference in agreement between the lowest two, “strongly disagree” and “midly disagree” the same as the gap between the highest two, “mildly agree” and “strongly agree”? We don’t know, mainly because ‘agreement’ is not something that can be measured easily using something like a weighing machine. And if we don’t know, then we shouldn’t really be adding these responses up or dividing them by anything.\nNevertheless, ordinal data are very common, since they are typically what is generated by survey data, where for example repondents may answer a series of questions (‘items’), each with typically five possible responses, but maybe more or fewer, these responses being ordinal in the sense that there is a definite order to them. They might encompass responses like those above, say, or something similar like “very unhappy” to “very happy”. They are also common in clinical and veterinary practice where ordinal pain scores are widely used - patients being asked (if they are human) or assessed as to their level of pain on a scale of 1-10, for example. Note that even if the pain value is recorded as a number it is actually a label, that could just as well have been recorded as one of a series of letters, A, B, C etc or emojis, or any symbol you like. You can’t take the average of a set of faces!\nThus, formally, we need another kind of test for a difference. Broadly, we need to use some form of non-parametric test where we do not assume that the data has any form of distribution, and where, often, we do not use the actual values of the measurements in our dataset but instead use only their ranks. The smallest value would be given rank 1, the next rank 2 and so on.\nThere are many non-parametric tests out there. Here we will look at only one - the Wilcoxon Rank Sum Test, often referred to as a Mann-Whitney U test for a difference. We can use this for the scenario we have painted above, where we have two sets of data and we wish to know if these provide evidence that the populations from which the samples have been drawn are in fact different.\n\n4.6.1 Example\nThis example uses actual data gathered by a student at Newquay University Centre.\nThe student wished to assess peoples’ sense of wellbeing using two different sets of questions designed to assess this. The scales chosen were the Warwick–Edinburgh Mental Well-being Scale (WEMWBS) and the New Ecological Paradigm (NEP) Scale. The student wished in particular to determine whether this sense of well-being was affected by whether a person often and actively frequented the coast and made it and the sea a substantive part of their life in one way or another. ie to find out whether there was evidence to support the notion that it could be good for your mental wellbeing to be by the sea and to make it part of your life.\nEach scale used consists of 15 questions or ‘Likert items’, each of which is answered on a 5 point ordinal scale, where a score of 1 indicates lowest wellbeing and a score of 5 indicates highest wellbeing. Thus each respondent could score anything from 15 to 75.\nThe student got responses from 374 people, 86 of whom were not “marine” users, while the other 288 say that they were marine users. The total scores from each respondent were recorded for each type of survey and stored in the file wellness.csv which you should find on the module Moodle site / Teams page. Please put this file in the data folder of your R project.\n\n\n4.6.2 Script\nCode chunks for a script to carry out the analysis of this data are provided below. To use them you should create a new Quarto document using File/New File/Quarto document, from which you delete all the exemplar material below the yaml section at the top. The first few chunks of this script carry out the same old-same old that we see in script after script: load packages, load data, summarise data , plot data. Copy and paste any chunks you want to use into your own script then adapt them as necessary.\nYou can run your script by running each chunk in sequence, which you do by clicking the green arrow in the top-right corner of each chunk.\nTry also to ‘Render’ the script by clicking on the Render button at the top of the script pane.\n\n4.6.2.1 Load packages\n\nlibrary(tidyverse)\nlibrary(here)\n\n\n\n4.6.2.2 Load data\nOur data set is in a .csv file which we have placed in the data folder within our project folder.\nNote that this data set has been stored in ‘tidy’ form: each variable appears in only column, and each observation appears in only one row.\n\nfilepath&lt;-here(\"data\",\"wellness.csv\")\nwellness&lt;-read_csv(filepath)\nglimpse(wellness)\n\nRows: 748\nColumns: 4\n$ id          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ scale       &lt;chr&gt; \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\"…\n$ marine      &lt;chr&gt; \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\",…\n$ total_score &lt;dbl&gt; 48, 51, 37, 39, 38, 40, 54, 39, 54, 39, 51, 50, 49, 51, 54…\n\n\n\n\n4.6.2.3 Summarise the data\nWe’ll calculate the median score (50th percentile) and the 25th and 75th percentile scores. For ordinal data, these summary statistics are well defined, whereas means and standard deviations are not.\n\nwellness |&gt;\n  group_by(scale,marine) |&gt;\n  summarise(median.score=median(total_score),iqr_25=quantile(total_score,0.25),iqr_75=quantile(total_score,0.75))\n\n# A tibble: 4 × 5\n# Groups:   scale [2]\n  scale  marine median.score iqr_25 iqr_75\n  &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 WEMWBS No             42.5   36       49\n2 WEMWBS Yes            47     41       51\n3 nep    No             51     48.2     53\n4 nep    Yes            50     48       53\n\n\n\n\n4.6.2.4 Plot the data\nBox plots are particularly suitable for ordinal data since they show the 25th and 75th percentiles of the data (the bottom and top of the box) plus the 50th percentile aka the median, which is the thick line across each box. All of these percentiles are well defined quantities for ordinal data.\n\nwellness |&gt;\n  ggplot(aes(x = scale,y = total_score,fill = marine)) +\n  geom_boxplot() +\n  labs(x = \"Likert Scale\",\n       y = \"Wellbeing Score\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nLooking at the plot, what do you think each scale suggests about whether proximity to the sea makes a difference to wellbeing?\n\n\n4.6.2.5 Wilcoxon-Mann-Whitney U test\nFirst let’s pull out the scores as measured by the WEMWBS scale and do a test for a difference between the scores of marine users and those of non-marine users. We can use the filter() function to do this.\n\nWEMWBS&lt;-wellness |&gt; filter(scale==\"WEMWBS\") # save the WEMWBS data into a data frame called WEMWBS\nglimpse(WEMWBS)\n\nRows: 374\nColumns: 4\n$ id          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ scale       &lt;chr&gt; \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\", \"WEMWBS\"…\n$ marine      &lt;chr&gt; \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\",…\n$ total_score &lt;dbl&gt; 48, 51, 37, 39, 38, 40, 54, 39, 54, 39, 51, 50, 49, 51, 54…\n\n\nNow let’s do the actual Wilcoxon-Mann-Whitney U test:\n\nwilcox.test(total_score~marine,data=WEMWBS)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  total_score by marine\nW = 9077.5, p-value = 0.0001687\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe null hypothesis of this test is that there is no evidence that the data are drawn from different populations. In this case, the p-value is very small, so we can confidently reject that null hypothesis and assert that there is evidence, according to the WEMWBS scale that marine use makes a difference to peoples’ sense of wellbeing.\nDoes it make it worse or better? - we can see from the summary table and from the box plot that higher scores are associated with those people who were exposed to a marine environment.\nWe might report this results as follows, first using a plain English statement of the main finding, and then reporting the type of test use, the value of the test statistic that it calculated and the p value. In this case, because the p value is so small, we would not report its exact value, but simply give an indication of how small it is:\nWe find evidence, according to the WEMWBS scale, that the wellbeing score is 4.5 or about 10% higher for people exposed to a marine environment (Mann-Whitney U, W = 9077.5, p &lt; 0.001).\n\n\n\n4.6.3 Exercise\nAdapt the code of the last chunk so that you can do the same test but for data as recorded by the nep scale\n\n\n4.6.4 When should I use this Wilcoxon-Mann-Whitney U test?\nThe test we have used here is an example of a non-parametric test. This means that it does not assume that the data follow a known mathematical distribution and, further, that it can be used with ordinal data.\nWe used the Mann-Whitney U test in particular because we were testing for a difference, and because the factor of interest - marine exposure - had just two levels - Yes or No. This test is only suitable when there are just two levels, so you can think of it as as a non-parametric alternative to a t-test.\nIn another setting where we still had just one factor (eg zone of a rocky shore) but there were more than two levels (eg low, mid and high zones of the shore) and we decided that we wanted to do a non-parametric test for a difference, then we would probably use the Kruskal-Wallis test, which you can think of as the non-parametric alternative to a one-way ANOVA.\nIn this example we used the Mann-Whitney U test because the data were ordinal and thus not suitable to use with a parametric test (but see below!). Where we can, we usually try to use a parametric test as they are more powerful than their non-parametric equivalents, meaning, if there is a trend or a difference in the data, they are better able to detect it. However those parametric tests (t-test, ANOVA, pearson correlation, PCA, GLM to name but a few) typically require not only that the data are numerical but also a host of other things, including that they follow a particular distribution, usually (but not always) the normal distribution, and this is often not the case with real biological data. Often, especially with count data, there are lots of zeros, or the data distribution is heavily skewed, usually to the right. In these cases, providing the data are independent of each other, we can usually still use a non-parametric test such as we have here. it might not be the most powerful test we can use (GLMs are typically way better if you can use them), but it will work.\n\n\n4.6.5 Hang on!\nThe eagle eyed among you may have spotted a massive flaw in the line of argument presented above. We said that ordinal data can’t be added up, can’t be used to calculate averages and so on. Thus we can’t run parametric tests on them and have to look for alternatives, namely, non-parametric tests.\nAnd yet, these non-parametric tests are usually run on the output of Likert scales such as we have considered here, where for each person we have a number of Likert Items (ie individual questions) that together constitute the scale, that each generate a score 1-5, then we add up the scores to get a total score. But that means we are adding up ordinal data!!!\nIt turns out that you actually get much the same results with Likert scale data if you analyse them using supposedly inappropriate parametric tests such as a 2-sample t-test as you do if you use a non-parametric test such as the one we considered here, the Mann-Whitney test.\nA study by De Winter and Dodou (2010) shows this convincingly.\nde Winter, J. F. C., & Dodou, D. (2010). Five-Point Likert Items: t test versus Mann-Whitney-Wilcoxon (Addendum added October 2012). Practical Assessment, Research, and Evaluation,15, 1–16. https://doi.org/10.7275/bj1p-ts64\nFor an enlightening discussion of this paper, see this blog by Jim Frost",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "tests_for_difference_two_levels.html#paired-data",
    "href": "tests_for_difference_two_levels.html#paired-data",
    "title": "4  Tests for difference: one factor, two levels",
    "section": "4.7 Paired data",
    "text": "4.7 Paired data\nOften one has a sample of replicated data where each element has a counterpart in another matched sample - paired data. A common scenario for this is when there are data for the same individual at two different points in time, for example before and after some event such as the application of a treatment.\nIn order to determine whether there is a difference between the two sets, one should take the paired aspect into account and not simply match the whole before-set against the whole after-set without doing this. That would be to throw away the information whereby there is likely to be a greater degree of correlation between the responses of an individual before and after the event than there is between any randomly chosen pairs of individuals before and after the event.\n\n4.7.1 Which test: paired t-test or Wilcoxon signed rank test?\nThere is a choice between at least two tests: the parametric paired t-test and the non-parametric Wilcoxon signed rank test. Ideally one would use the t-test since it is more powerful than the Wilcoxon test. This means several things, but in particular it means that, all else being equal, it can detect a small difference with higher probability than the Wilcoxon test can.\n\n\n4.7.2 The paired t-test\nWhere the data are numerical (ie not ordinal) and where the before and after data are both normally distributed around their respective mean values one would use the paired t-test in this scenario. One can test for normality using either a test such as the Shapiro-Wilk test, or graphically using either a histogram, a box plot, or (best), a quantile-quantile plot.\n\n\n4.7.3 The Wilcoxon Signed Rank test\nThe t-test, an example of a so-called parametric test, is actually pretty robust against departures from normality, but where one doubts its validity due to extreme non-normality or for other reasons such as the ordinal nature of the data, the Wilcoxon signed rank test is a useful non-parametric alternative. It is called non-parametric because it does not make any assumption about the distribution of the data values. It only uses their ranks, where the smallest value gets rank 1, the next smallest gets rank 2, and so on.\nSo, you typically use this test when you would like to use the paired t-test, but you cannot because one or both of the data sets is way off being normally distributed or is ordinal.\n\n4.7.3.1 Null Hypotheses\nIn both the t-test and the Wilcoxon signed rank tests, the null hypothesis is the usual ‘nothing going on’, ‘there is no difference’ scenario, but there is a subtle difference between them that reflects the different information that they use. In the Wilcoxon signed rank test the null is that the difference between the medians of pairs of observations is zero. This is different from the null hypothesis of the paired t–test, which is that the difference between the means of pairs is zero.\n\n\n4.7.3.2 Test output\nBoth tests will give a p value. This is the probability that the mean (t-test) or median (Wilcoxon signed rank) paired differences between the corresponding before and after sample elements would be equal to or greater than it actually is for the data if the null hypothesis were true. If the p value is less than some pre-decided ‘significance level’, usually taken to be 0.05, then we reject the null hypothesis. If it is not, then we fail to reject the null hypothesis.\n\n\n\n4.7.4 Example\nWe will use as an example a data set from Laureysens et al. (2004) that has measurements of metal content in the wood of 13 poplar clones growing in a polluted area, once from each clone in August and once again from each of them in November. The idea was to investigate the extent to which poplars could absorb metals from the soil and thus be useful in cleaning that up. Under a null hypothesis, there would be no change in the metal concentrations in the plant tissue of each clone between August and November. Under an alternate hypothesis, there would be.\nLaureysens, I. et al. (2004) ‘Clonal variation in heavy metal accumulation and biomass production in a poplar coppice culture: I. Seasonal variation in leaf, wood and bark concentrations’, Environmental Pollution, 131(3), pp. 485–494. Available at: https://doi.org/10.1016/j.envpol.2004.02.009.\nConcentrations of aluminum (in micrograms of Al per gram of wood) are shown below.\nLoad packages\n\nlibrary (tidyverse)\nlibrary(here)\nlibrary(cowplot) # to make the plots look nice\n\nLoad data\n\nfilepath &lt;- here(\"data\",\"poplars-paired_np.csv\")\npoplars &lt;- read_csv(filepath,show_col_types = FALSE)\nhead(poplars,20)\n\n# A tibble: 13 × 4\n      ID Clone          August November\n   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 Balsam_Spire      8.1     11.2\n 2     2 Beaupre          10       16.3\n 3     3 Hazendans        16.5     15.3\n 4     4 Hoogvorst        13.6     15.6\n 5     5 Raspalje          9.5     10.5\n 6     6 Unal              8.3     15.5\n 7     7 Columbia_River   18.3     12.7\n 8     8 Fritzi_Pauley    13.3     11.1\n 9     9 Trichobel         7.9     19.9\n10    10 Gaver             8.1     20.4\n11    11 Gibecq            8.9     14.2\n12    12 Primo            12.6     12.7\n13    13 Wolterson        13.4     36.8\n\n\nPlot the data\nBefore we do any test on some data to find evidence for a difference or a trend, it is a good idea to plot the data. This will reveal whatever patterns there are in the data and how likely they are to reveal a truth about the population from which they have been drawn.\nTidy the data\nIn this case there is work to do before we can plot the data. The problem is that the data is ‘untidy’. The two levels of the factor month are spread across two columns, August and November. For plotting purposes it will be useful to ‘tidy’ the data so that there is only one column containing both levels of month and another containing the aluminium concentrations. The function pivot_longer() can do this for us:\n\npoplars_tidy &lt;- poplars |&gt;\n  pivot_longer (August:November,names_to=\"month\",values_to=\"Al_conc\")\nhead(poplars_tidy,8)\n\n# A tibble: 8 × 4\n     ID Clone        month    Al_conc\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt;\n1     1 Balsam_Spire August       8.1\n2     1 Balsam_Spire November    11.2\n3     2 Beaupre      August      10  \n4     2 Beaupre      November    16.3\n5     3 Hazendans    August      16.5\n6     3 Hazendans    November    15.3\n7     4 Hoogvorst    August      13.6\n8     4 Hoogvorst    November    15.6\n\n\nNow we can plot the data as a box plot, with one box for August and one for November ie one for each level of the factor month. Had we not first tidied the data, we could not have done this.\n\npoplars_tidy |&gt;\n  ggplot(aes(x = month, y = Al_conc, fill = month, colour = month)) + \n  # alpa (= opacity) &lt; 1 in case any points are on top of each other\n  geom_boxplot(outlier.size=0,alpha=0.5) +\n  geom_point(alpha = 0.5) +\n  # group = ID makes the lines join elements of each pair\n  geom_line(aes(group=ID),colour = \"grey60\") +\n  labs(x = \"Month\",\n       y = \"Al conc.(mu g Al / g wood)\") +\n  theme_cowplot() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nDoes it look as though the difference between the medians could plausibly be zero for the population from which these samples were drawn? Or, put another way, if it was zero, how big a fluke would this sample be? That is what the p value actually tells us.\n\n\n4.7.5 Two sample paired t-test\nCheck for normality of differences\nBefore we use the t-test, we need to check that it is OK to do so. This means checking whether the paired differences are plausibly drawn from a normal distribution centred on zero.\nThe null hypothesis of the Shapiro-Wilk test is that the data set given to it is plausibly drawn from a normally distributed population. So let us give our sample of paired differences:\n\nshapiro.test(poplars$August-poplars$November)\n\n\n    Shapiro-Wilk normality test\n\ndata:  poplars$August - poplars$November\nW = 0.92667, p-value = 0.3081\n\n\nThe p value is very high. Thus we do not reject the null hypothesis and we can reasonably assume that the differences between the August and November aluminium concentrations in the sample could plausibly have been drawn from a normally distributed population, despite the outlier value in the November sample. Thus we can reasonably test for difference using a paired t-test.\nThe actual t-test\nWe can do this in R using the function t.test(), where we give to the function both the August and the November data, knowing that each August value has a counterpart November value, and we set the argument paired to TRUE.\n\nt.test(poplars$August, poplars$November, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  poplars$August and poplars$November\nt = -2.3089, df = 12, p-value = 0.03956\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -9.5239348 -0.2760652\nsample estimates:\nmean difference \n           -4.9 \n\n\nAll parts of the output have meaning and are useful, but here we will focus on just two:\n\nthe p value is equal to 0.040. Hence, if we have chosen the usual significance value of 0.05, we can take this to mean that there is evidence of a significant difference between the August and November values.\nthe lower and upper bounds of the 95% confidence interval are (-9.52, -0.28). YOu can think of this interval as the range of values within which the difference can plausibly lie, at the 95% confidence level. The key thing is that this range does not encompass zero. This means that we can be confident at the 95% level that there is a non-zero change on going from August to November, and, in particular, that the August value is lower than the November value.\n\n\n\n4.7.6 The non-parametric alternative: The Wilcoxon signed rank test\nTo be safe, because of that outlier, let us test for difference using the Wilcoxon signed rank test. In R this is done using the function wilcox.test(), with the argument paired set to TRUE.\n\nwilcox.test(poplars$August, poplars$November, paired = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  poplars$August and poplars$November\nV = 16, p-value = 0.03979\nalternative hypothesis: true location shift is not equal to 0\n\n\nWe see that the conclusion (in this case) is the same.\n\n\n4.7.7 Relation to one-sample paired test\nThe two-sample paired tests as we have done above are the same as doing a one-sample test to see if the differences between the August and November paired values is different from zero. This is true whether we do a t-test or a Wilcoxon signed rank test.\nIn either case, the first argument is the vector of differences, and the second mu is the threshold value against which we want to compare those differences, in this case zero.\n\nt.test(poplars$August - poplars$November, mu = 0, data = poplars)\n\n\n    One Sample t-test\n\ndata:  poplars$August - poplars$November\nt = -2.3089, df = 12, p-value = 0.03956\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -9.5239348 -0.2760652\nsample estimates:\nmean of x \n     -4.9 \n\n\n\nwilcox.test(poplars$August - poplars$November, mu = 0, data = poplars)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  poplars$August - poplars$November\nV = 16, p-value = 0.03979\nalternative hypothesis: true location is not equal to 0\n\n\nNote that the output from both these one-sample tests, where the one sample is the vector of differences and the threshold with which it is compared is zero, is exactly the same as the output of the two-sample tests where the two samples were the vectors between which we were interested in detecting a difference, ie the August and November values. This is not surprising since the two cases are just two ways of doing exactly the same thing, which is to ask if there is evidence from the sample for a difference in the population between the August and November concentrations of aluminium.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tests for difference: one factor, two levels</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works.html",
    "href": "ANOVA_how_it_works.html",
    "title": "5  Analysis of Variance aka ANOVA",
    "section": "",
    "text": "5.1 What is ANOVA?\nMaterial used from Chapter One of Grafen and Hails: Modern Statistics for the Life Sciences\nfertilizer &lt;- fertilizer |&gt;\n  mutate(FERTIL=as.factor(FERTIL))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysis of Variance aka ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works.html#what-is-anova",
    "href": "ANOVA_how_it_works.html#what-is-anova",
    "title": "5  Analysis of Variance aka ANOVA",
    "section": "",
    "text": "5.1.1 The basic principles of ANOVA\nIn a simple case we consider the comparison of three means. This is done by the analysis of variance (ANOVA). In this case we will go through an example in detail and work out all the mechanics, but once we have done that and seen how the output is derived from the input we will not need to do it again. We will use R to do the heavy lifting. We will just need to know when it is appropriate to use ANOVA, how to get R to do it and how to interpret the output that R produces.\n\n\n5.1.2 The Scenario\nSuppose we have three fertilizers and wish to compare their efficacy. This has been done in a field experiment where each fertilizer is applied to 10 plots and the 30 plots are later harvested, with the crop yields being calculated. We end up with three groups of 10 figures and we wish to know if there are any differences between these groups.\nWhen we plot the data we see that the fertilizers do differ in the amount of yield produced but that there is also a lot of variation between the plots that were given the same fertilizer.\n\ng1&lt;-ggplot(fertilizer,aes(x=FERTIL,y=YIELD, fill= FERTIL,alpha=0.1))+\n  geom_boxplot()+\n  geom_point(aes(colour=FERTIL))+\n  scale_y_continuous(limits=c(0,10))+\n  labs(x='Fertilizer', y='Yield')+\n  theme_cowplot()+\n  theme(legend.position = \"none\")\n\n\ng2&lt;-ggplot(fertilizer,aes(x=plot,y=YIELD,colour=FERTIL))+\n  geom_point()+\n  scale_y_continuous(limits=c(0,10))+\n  labs(x='Plot number',y='Yield per plot (tonnes)')+\n  theme_cowplot()\n\n\ngrid.arrange(g2,g1,nrow=1)\n\n\n\n\n\n\n\n\n\n\n5.1.3 What does an ANOVA do?\nAn ANOVA (ANalysis Of VAriance) analysis attempts to determine whether the differences between the effect of the fertilizers is significant by investigating the variability in the data. We investigate how the variability between groups compares to the variability within groups.\n\n\n5.1.4 Grand Mean\nFirst we calculate the ‘grand mean’, the mean of the yields across all 30 plots:\n\ngrand_mean=mean(fertilizer$YIELD)\ngrand_mean\n\n[1] 4.643667\n\n\n\n5.1.4.1 Deviations from the grand mean\n\nSST.plot&lt;-g2+geom_hline(yintercept=grand_mean,linetype='dashed')+\n  geom_segment(aes(x = plot, y = YIELD, xend = plot, yend = grand_mean),linetype='dotted')\nSST.plot\n\n\n\n\n\n\n\n\n\n\n5.1.4.2 Mean value of yield for each fertilizer\n\nf_means&lt;-fertilizer |&gt;\n  group_by(FERTIL) |&gt;\n  summarise(fmean=mean(YIELD))\nf_means\n\n# A tibble: 3 × 2\n  FERTIL fmean\n  &lt;fct&gt;  &lt;dbl&gt;\n1 1       5.44\n2 2       4.00\n3 3       4.49\n\nfertilizer&lt;-mutate(fertilizer,fmean=c(rep(f_means$fmean[1],10),rep(f_means$fmean[2],10),rep(f_means$fmean[3],10)))\n\nf1&lt;-filter(fertilizer,FERTIL==1)\nf2&lt;-filter(fertilizer,FERTIL==2)\nf3&lt;-filter(fertilizer,FERTIL==3)\n\n\ng3&lt;-ggplot()+\n  \n  geom_point(data=f1,aes(x=plot,y=YIELD))+\n  geom_segment(aes(x=min(f1$plot),y=f1$fmean[1],xend=max(f1$plot),yend=f1$fmean[1]))+\n  geom_segment(aes(x = f1$plot, y = f1$YIELD, xend = f1$plot, yend = f1$fmean[1]),linetype='dotted')+\n  \n  geom_point(data=f2,aes(x=plot,y=YIELD))+\n  geom_segment(aes(x=min(f2$plot),y=f2$fmean[1],xend=max(f2$plot),yend=f2$fmean[1]))+\n  geom_segment(aes(x = f2$plot, y = f2$YIELD, xend = f2$plot, yend = f2$fmean[1]),linetype='dotted')+\n  \n  geom_point(data=f3,aes(x=plot,y=YIELD))+\n  geom_segment(aes(x=min(f3$plot),y=f3$fmean[1],xend=max(f3$plot),yend=f3$fmean[1]))+\n  geom_segment(aes(x = f3$plot, y = f3$YIELD, xend = f3$plot, yend = f3$fmean[1]),linetype='dotted')+\n  \n  scale_y_continuous(limits=c(0,10))+\n  labs(x='Plot number',y='Yield per plot (tonnes)',title=\"SSE: Error sum of squares\")+\n  \n  theme_cowplot()\n\n\n\n\n5.1.5 Measures of variability\n\n5.1.5.1 SST - Total sum of squares\n\nSST=sum((fertilizer$YIELD-grand_mean)^2)\nSST\n\n[1] 36.4449\n\n\nSST is the total sum of squares. It is the sum of squares of the deviations of the data around the grand mean. This is a measure of the total variability of the data set.\n\n\n5.1.5.2 SSE - Error sum of squares\n\nSSE&lt;-fertilizer |&gt;\n  group_by(FERTIL) |&gt;\n  mutate(fmean=mean(YIELD)) |&gt;\n  mutate(se=(YIELD-fmean)^2) |&gt;\n  summarise(sse=sum(se),.groups = 'drop') |&gt;\n  summarise(SSE=sum(sse),.groups = 'drop') |&gt;\n  pull(SSE)\n\nSSE is the error sum of squares. It is the sum of the squares of the deviations of the data around the three separate group means. This is a measure of the variation between plots that have been given the same fertilizer.\n\n\n5.1.5.3 SSF - Fertilizer sum of squares\n\nSSF&lt;-fertilizer |&gt;\n  group_by(FERTIL) |&gt;\n  mutate(fmean=mean(YIELD)) |&gt;\n  mutate(se=(fmean-grand_mean)^2) |&gt;\n  summarise(sse=sum(se),.groups = 'drop') |&gt;\n  summarise(SSF=sum(sse),.groups = 'drop') |&gt;\n  pull(SSF)\n\nSSF is the fertilizer sum of squares. This is the sum of the squares of the deviations of the group means from the grand mean. This is a measure of the variation between plots given different fertilizers.\n\ng4&lt;-ggplot()+\n  \n  geom_hline(yintercept=grand_mean,linetype='dashed')+\n  \n  geom_segment(aes(x=min(f1$plot),y=f1$fmean[1],xend=max(f1$plot),yend=f1$fmean[1]))+\n  geom_segment(aes(x = mean(f1$plot), y = grand_mean, xend = mean(f1$plot), yend = f1$fmean[1]),linetype='dotted')+\n  \n  geom_segment(aes(x=min(f2$plot),y=f2$fmean[1],xend=max(f2$plot),yend=f2$fmean[1]))+\n  geom_segment(aes(x = mean(f2$plot), y = grand_mean, xend = mean(f2$plot), yend = f2$fmean[1]),linetype='dotted')+\n  \n  geom_segment(aes(x=min(f3$plot),y=f3$fmean[1],xend=max(f3$plot),yend=f3$fmean[1]))+\n  geom_segment(aes(x = mean(f3$plot), y = grand_mean, xend = mean(f3$plot), yend = f3$fmean[1]),linetype='dotted')+\n  \n  scale_y_continuous(limits=c(2.5,7.5))+\n  labs(x='Plot number',y='Yield per plot (tonnes)',title=\"SSF:Fertilizer sum of squares\")+\n  theme_cowplot()\n\n\ngrid.arrange(g3,g4,nrow=1)\n\n\n\n\n\n\n\n\nWhen the three group means are fitted, there is an obvious reduction in variability around the three means compared to that around the grand mean, but it is not obvious if the fertilizers have had an effect on yield.\nAt what point do we decide if the amount of variation explained by fitting the means is significant? By this, we mean, “When is the variability between the group means greater than we would expect by chance alone?\nFirst, we note that SSF and SSE partition between them the total variability in the data:\n\n\n\n5.1.6 SST = SSF + SSE\n\nSST\n\n[1] 36.4449\n\nSSF\n\n[1] 10.82275\n\nSSE\n\n[1] 25.62215\n\nSSF+SSE\n\n[1] 36.4449\n\n\nSo the total variability has been divided into two components. That due to differences between plots given different treatments and that due to differences between plots given the same treatment. Variability must be due to one or other of these components. Separating the total SS into its component SS is known as partitioning the sums of squares.\nA comparison of SSF and SSE is going to indicate whether fitting the three fertilizer means accounts for a significant amount of variability.\nHowever, to make a proper comparison, we really need to compare the variability per degree of freedom ie the variance.\n\n\n5.1.7 Partitioning the degrees of freedom\nEvery sum of squares (SS) has been calculated using a number of independent pieces of information. In each, case, we call this number the number of degrees of freedom for the SS.\nFor SST this number is one less than the number of data points n. This is because when we calculate the deviations of each data point around a grand mean there are only n-1 of them that are independent, since by definition the sum of these deviations is zero, and so when n-1 of them have been calculated, the final one is pre-determined.\nSimilarly, when we calculate SSF, which measures the deviation of the group means from the grand mean, we have \\(k\\)-1 degrees of freedom, (where in the present example \\(k\\), the number of treatments, is equal to three) since the deviations must sum to zero, so when \\(k\\)-1 of them have been calculated, the last one is pre-determined.\nFinally, SSE, which measure deviation around the group means will have n-k degrees of freedom, since the sum of each of the deviations around one of the group means must sum to zero, and so when all but one of them have been calculated, the final one is pre-determined. There are \\(k\\) group means, so the total degrees of freedom for SSE is n-k.\nThe degrees of freedom are additive: \\[\ndf(\\text{SST}) = df(\\text{SSE}) + df(\\text{SSF})\n\\] Check:\n\\[\\begin{align*}\ndf(\\text{SST}) &= n-1\\\\\ndf(\\text{SSE}) &= k-1\\\\\ndf(\\text{SSF}) &= n-k\\\\\n\\therefore df(\\text{SSE}) + df(\\text{SSF}) &= k-1 + n-k\\\\\n&=n-1\\\\\n&=df(\\text{SST})\n\n\\end{align*}\\]\n\n\n5.1.8 Mean Squares\nNow we can calculate the variances which are a measure of the amount of variability per degree of freedom.\nIn this context, we call them mean squares. To find each one we divided each of the sums of squares (SS) by their corresponding degrees of freedom.\nFertiliser Mean Square (FMS) = SSF / k - 1. This is the variation per df between plots given different fertilisers.\nError Mean Square (EMS) = SSE / n - k. This is the variation per df between plots given the same fertiliser.\nTotal Mean Square (TMS) = SST / n - 1. This is the total variance per df of the dataset.\nUnlike the SS, the MS are not additive. That is, FMS + EMS \\(\\neq\\) TMS.\n\n\n5.1.9 F-ratios\nIf none of the fertilizers influenced yield, we would expect as much variation between the plots treated with the same fertilizer as between the plots treated with different fertilizers.\nWe can express this in terms of the mean squares: the mean square for fertilizer would be the same as the mean square for error:\n\\[\n\\frac{\\text{FMS}}{\\text{EMS}}=1\n\\] We call this ratio the F-ratio. It is the end result of ANOVA. F-ratios can never be negative since they are the ratio of two mean square values, both of which must be non-negative, but there is no limit to how large they can be.\nEven if the fertilizers were identical, the F-ratio is unlikely to be exactly 1 - it could by chance take a whole range of values. The F-distribution represents the range and likelihood of all possible F ratios under the null hypothesis. ie when the fertilizers were identical.\nThe shape of the F distribution depends on the degrees of freedom of FMS and EMS, and we normally specify it by giving the values of each. Below we show F distributions for 2 and 27 degrees of freedom (ie 3 plots, so k = 3, so the degrees of freedom of FMS = k-1 =2, and 10 plants per plot, so n = 3 x 10 =30, and hence the degrees of freedom of EMS = n-k = 30 - 3 = 27), and for 10 and 27 degrees of freedom.\n\ndfs&lt;-c(rep(\"df = 2, 27\",601),rep(\"df = 10, 27\",601))\nxs&lt;-rep(seq(0,6,0.01),2)\nys&lt;-c(df(xs[1:601],2,27),df(xs[602:1202],10,27))\nfdata&lt;-tibble(x=xs,y=ys,dfs=dfs)\n\nfdata |&gt;\n  mutate(dfs = fct_relevel(dfs,\"df = 2, 27\",\"df = 10, 27\")) |&gt;\n  ggplot(aes(x=xs,y=ys)) +\n  xlim(0,6) +\n  ylim(0,1) +\n  labs(x=\"F-ratio\",\n       y=\"Probability density\",\n       caption=\"The F-distributions for (left) 2 and 27 degrees of freedom and (right) 10 and 27 degrees of freedom\") +\n  geom_line(colour=\"darkblue\") +\n  facet_wrap(~dfs) +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nNote that, whatever the degrees of freedom, F-distributions are examples of so-called probability density functions. The area beneath them between any two values of F-ratio is equal to the probability of getting an F-ratio in that range. Hence the total area under the curves is equal to 1, since the F-ratio must take some value between zero and infinity, and the area under the tail to the right of any given F-ratio is the probability of getting an F-ratio bigger than that value.\nHence, the probability under the null hypothesis of getting an F-ratio as large or larger than the value we actually got is the area to the right of this F-ratio under the appropriate F distribution. We often call this probability the p-value. p for probability. p-values are the the probability of getting data as extreme (same F-ratio,) or more extreme (bigger F-ratio) as the data you got you got if the null hypothesis were true.\nIf the fertilizers were very different, then the FMS would be much greater than the EMS and the F-ratio would be greater than one. However it can be quite large even when there are no treatment differences. So how do we decide when the size of the F-ratio is due to treatment rather than to chance?\nTraditionally, we decide that it sufficiently larger than one to be due to treatment differences if it would be this large or larger under the null hypothesis only 5% or less of the time. If we had inside knowledge that the null hypothesis was in fact true then we would still get an F-ratio that large or larger 5% of the time.\nOur p-value ie the probability that the F-ratio would have been as large as it is or larger, under the null hypothesis, represents the strength of evidence against the null hypothesis. The smaller it is, the stronger the evidence, and only when it is less than 0.05 do we regard the evidence as strong enough to reject the null.\n\nFratio&lt;-function(p,k,st_dev){\n  n&lt;-p*k # k plots, p replicates per plot\n  plots&lt;-tibble(plot=c(rep(\"a\",p),rep(\"b\",p),rep(\"c\",p)),response=rnorm(n,mean=0,sd=st_dev))\n  lm.mod&lt;-lm(response~plot,data=plots)\n  tidy(anova(lm.mod))$statistic[1]\n}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysis of Variance aka ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works.html#anova-example-1",
    "href": "ANOVA_how_it_works.html#anova-example-1",
    "title": "5  Analysis of Variance aka ANOVA",
    "section": "5.2 ANOVA example 1",
    "text": "5.2 ANOVA example 1\nWe will carry out the ANOVA analysis of the fertilizer data discussed on the previous tab.\nOur question is whether yield depends on fertilizer.\nWhat is our null hypothesis?\nStart a new notebook with these two code chunks to begin with:\n```{r global-options, include=FALSE}\nknitr::opts_chunk$set(fig.width=12, fig.height=8, warning=FALSE, message=FALSE,echo=FALSE)\n```\n\n```{r load packages, message=FALSE,warning=FALSE,echo=FALSE}\nlibrary(tidyverse)\nlibrary(here)\nlibrary(cowplot)\nlibrary(gridExtra)\nlibrary(ggfortify)\n```\nLoad the fertilizer.csv data into an object call `fertilizer\nIs it tidy data? If not, tidy it.\nConvert the FERTIL column to a factor, using this code:\n```{r make_factor}\nfertilizer &lt;- fertilizer |&gt;\n  mutate(FERTIL=as.factor(FERTIL))\n```\nMake a box plot of yield vs fertilizer, like the one on the previous tab.\nNow use the lm() function to create the anova model (There are several ways to do this in R - this is just one)\n\nfertil.model&lt;-lm(YIELD~FERTIL,data=fertilizer)\n\nNow inspect the model:\n\nanova(fertil.model)\n\nAnalysis of Variance Table\n\nResponse: YIELD\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nFERTIL     2 10.823  5.4114  5.7024 0.008594 **\nResiduals 27 25.622  0.9490                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSo we find that we can reject the null hypothesis. There is thus evidence that fertilizer does affect yield (ANOVA, df = 2,27, F=5.7, p&lt; 0.01)\nWhat this test has not done so far is show us where the differences lie. An ANOVA is a holistic test that tells you whether or not there is evidence for a difference between at least one pair of groups being compared. To identify which gruopd, if any, are differeny, we need to do so-called post-hoc tests.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysis of Variance aka ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works.html#anova-example-2",
    "href": "ANOVA_how_it_works.html#anova-example-2",
    "title": "5  Analysis of Variance aka ANOVA",
    "section": "5.3 ANOVA example 2",
    "text": "5.3 ANOVA example 2\nAn experiment was performed to compare four melon varieties. It was designed so that each variety was grown in six plots, but two plots growing variety 3 were accidentally destroyed.\nWe wish to find out if there is evidence for a difference in yield between the varieties.\n\n5.3.1 Null hypothesis\nWhat is the null hypothesis of this study?\nThe data are in the melons.csv dataset.\nWrite code chunks to\n\n\n5.3.2 Load data and inspect the data\n\nfilepath&lt;-here(\"data\",\"melons.csv\")\nmelons&lt;-read_csv(filepath)\nglimpse(melons)\n\nRows: 22\nColumns: 2\n$ YIELDM  &lt;dbl&gt; 25.12, 17.25, 26.42, 16.08, 22.15, 15.92, 40.25, 35.25, 31.98,…\n$ VARIETY &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4,…\n\n\n\n\n5.3.3 Prepare the data\nEnsure that the VARIETY column is a factor\n\nmelons &lt;- melons |&gt;\n  mutate(VARIETY=as.factor(VARIETY))\n\n\n\n5.3.4 Plot the data\nCreate a scatter plot of the yield.\n\nmelons |&gt;\n  ggplot(aes(x=VARIETY,y=YIELDM)) +\n  geom_point() +\n  scale_y_continuous(limits=c(0,45),breaks=seq(0,45,5))+\n  labs(x = \"Melon variety\", y=\"Yield\") +\n  theme_cowplot()\n\n\n\n\n\n\n\n\n\n\n5.3.5 Summarise the data\nCreate a summary table that shows the mean yield for each variety.\n\nmelons |&gt;\n  group_by(VARIETY) |&gt;\n  summarise(\n    N=n(),\n    Mean=round(mean(YIELDM),2)\n  )\n\n# A tibble: 4 × 3\n  VARIETY     N  Mean\n  &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt;\n1 1           6  20.5\n2 2           6  37.4\n3 3           4  20.5\n4 4           6  29.9\n\n\n\n\n5.3.6 1-way ANOVA\n\n5.3.6.1 Create the model\n\nmelons.model&lt;-lm(YIELDM~VARIETY,data=melons)\n\n\n\n5.3.6.2 Check the validity of the model\n\nautoplot(melons.model,smooth.colour=NA) + theme_cowplot()\n\n\n\n\n\n\n\n\nDO the data look as though they meet the criteria for an ANOVA? - the variance of the residuals is roughly constant across all groups and the qq-plot is fairly straight. We could confirm with a normality test if we like. For example, we could use a Shapiro-Wilk test.\nWhat is the null hypothesis of this test?\n\nshapiro.test(melons.model$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  melons.model$residuals\nW = 0.94567, p-value = 0.2586\n\n\nWhat do we conclude from this test?\n\n\n5.3.6.3 Inspect the model\n\nanova(melons.model)\n\nAnalysis of Variance Table\n\nResponse: YIELDM\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nVARIETY    3 1115.28  371.76  23.798 1.735e-06 ***\nResiduals 18  281.19   15.62                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhat conclusions would you draw from the output of this model and the table of mean yields for each variety?\n\n\n\n5.3.7 Confidence intervals of the means\nLet us find the 95% confidence intervals for each mean\nThese we calculate as\n\\[\n\\text{Mean}\\pm t_{\\text{crit}}\\text{SE}_{\\text{mean}}\n\\] For a 95% confidence interval and 18 degrees of freedom, \\(t_{\\text{crit}}\\) is 2.1, so we find that the intervals are:\n\nt_crit&lt;-qt(0.975,18)\nmelons |&gt;\n  group_by(VARIETY) |&gt;\n  summarise(\n    Mean=round(mean(YIELDM),2),\n    LB=round(Mean-t_crit*sqrt(15.6)/sqrt(n()),2),\n    UB=round(Mean+t_crit*sqrt(15.6)/sqrt(n()),2)\n    )\n\n# A tibble: 4 × 4\n  VARIETY  Mean    LB    UB\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1        20.5  17.1  23.9\n2 2        37.4  34.0  40.8\n3 3        20.5  16.3  24.6\n4 4        29.9  26.5  33.3",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysis of Variance aka ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_how_it_works.html#anova-example-2-solution",
    "href": "ANOVA_how_it_works.html#anova-example-2-solution",
    "title": "5  Analysis of Variance aka ANOVA",
    "section": "5.4 ANOVA example 2 solution",
    "text": "5.4 ANOVA example 2 solution\nAn experiment was performed to compare four melon varieties. It was designed so that each variety was grown in six plots, but two plots growing variety 3 were accidentally destroyed.\nWe wish to find out if there is evidence for a difference in yield between the varieties.\n\n5.4.1 Null hypothesis\nWhat is the null hypothesis of this study?\nThe data are in the melons.csv dataset.\nWrite code chunks to\n\n\n5.4.2 Load data and inspect the data\n\nfilepath&lt;-here(\"data\",\"melons.csv\")\nmelons&lt;-read_csv(filepath)\nglimpse(melons)\n\nRows: 22\nColumns: 2\n$ YIELDM  &lt;dbl&gt; 25.12, 17.25, 26.42, 16.08, 22.15, 15.92, 40.25, 35.25, 31.98,…\n$ VARIETY &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4,…\n\n\n\n\n5.4.3 Prepare the data\nEnsure that the VARIETY column is a factor\n\nmelons &lt;- melons |&gt;\n  mutate(VARIETY=as.factor(VARIETY))\n\n\n\n5.4.4 Plot the data\nCreate a scatter plot of the yield.\n\nmelons |&gt;\n  ggplot(aes(x=VARIETY,y=YIELDM)) +\n  geom_point() +\n  scale_y_continuous(limits=c(0,45),breaks=seq(0,45,5))+\n  labs(x = \"Melon variety\", y=\"Yield\") +\n  theme_cowplot()\n\n\n\n\n\n\n\n\n\n\n5.4.5 Summarise the data\nCreate a summary table that shows the mean yield for each variety.\n\nmelons |&gt;\n  group_by(VARIETY) |&gt;\n  summarise(\n    N=n(),\n    Mean=round(mean(YIELDM),2)\n  )\n\n# A tibble: 4 × 3\n  VARIETY     N  Mean\n  &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt;\n1 1           6  20.5\n2 2           6  37.4\n3 3           4  20.5\n4 4           6  29.9\n\n\n\n\n5.4.6 1-way ANOVA\n\n5.4.6.1 Create the model\n\nmelons.model&lt;-lm(YIELDM~VARIETY,data=melons)\n\n\n\n5.4.6.2 Check the validity of the model\n\nautoplot(melons.model,smooth.colour=NA) + theme_cowplot()\n\n\n\n\n\n\n\n\nThe data look as though they meet the criteria for an ANOVA - the variance of the residuals is roughly constant across all groups and the qq-plot is fairly straight. We could confirm with a normality test if we like. For example, we could use a Shapiro-Wilk test.\nThe null hypothesis of this test is that the residuals are drawn from a population that is normally distributed\n\nshapiro.test(melons.model$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  melons.model$residuals\nW = 0.94567, p-value = 0.2586\n\n\nSince p&gt;0.05 we conclude that there is no reason to reject the null hypothesis that the residuals are normally disributed.\n\n\n5.4.6.3 Inspect the model\n\nanova(melons.model)\n\nAnalysis of Variance Table\n\nResponse: YIELDM\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nVARIETY    3 1115.28  371.76  23.798 1.735e-06 ***\nResiduals 18  281.19   15.62                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(melons.model)\n\n\nCall:\nlm(formula = YIELDM ~ VARIETY, data = melons)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4233 -2.2781 -0.5933  2.6694  5.9300 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.4900     1.6136  12.699 2.02e-10 ***\nVARIETY2     16.9133     2.2819   7.412 7.14e-07 ***\nVARIETY3     -0.0275     2.5513  -0.011  0.99152    \nVARIETY4      9.4067     2.2819   4.122  0.00064 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.952 on 18 degrees of freedom\nMultiple R-squared:  0.7986,    Adjusted R-squared:  0.7651 \nF-statistic:  23.8 on 3 and 18 DF,  p-value: 1.735e-06\n\n\nWhat conclusions would you draw from the output of this model and the table of mean yields for each variety?\nWe see that the null hypothesis is rejected with a p-value of less than 0.001. We conclude that there are significant differences in the mean yield of melons across the varieties. We estimate that variety 2 has the highest mean yield and varieties 1 and 3 have the lowest mean yields.\nThe unexplained variance ie the error s for each group is 15.6 with 18 degrees of freedom. So the standard error for each group is \\(\\frac{s}{\\sqrt{n}}\\) where s=\\(\\sqrt{15.6}\\) = 3.95 divided by the number of elements in each group, giving us standard errors of 1.97 for variety 3, and 1.61 for the other varieties.\n\n\n\n5.4.7 Confidence intervals of the means\nLet us find the 95% confidence intervals for each mean\nThese we calculate as\n\\[\n\\text{Mean}\\pm t_{\\text{crit}}\\text{SE}_{\\text{mean}}\n\\] For a 95% confidence interval and 18 degrees of freedom, \\(t_{\\text{crit}}\\) is 2.1, so we find that the intervals are:\n\nt_crit&lt;-qt(0.975,18)\nmelons |&gt;\n  group_by(VARIETY) |&gt;\n  summarise(\n    Mean=round(mean(YIELDM),2),\n    LB=round(Mean-t_crit*sqrt(15.6)/sqrt(n()),2),\n    UB=round(Mean+t_crit*sqrt(15.6)/sqrt(n()),2)\n    )\n\n# A tibble: 4 × 4\n  VARIETY  Mean    LB    UB\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1        20.5  17.1  23.9\n2 2        37.4  34.0  40.8\n3 3        20.5  16.3  24.6\n4 4        29.9  26.5  33.3",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysis of Variance aka ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_one_way.html",
    "href": "ANOVA_one_way.html",
    "title": "6  One-way ANOVA",
    "section": "",
    "text": "6.0.1 Introduction\nIn this exercise we will carry out a method of analysis known as ANOVA - this is what is commonly used when you have one or more categorical variables, such as species, sex and so on, and a numerical response variable such as body mass and you want to know if there is a difference in the response variable between when the levels of the factors take different values.\nA one-way ANOVA is used where where we have one categorical variable with three or more level (you could also use it where there are just two levels, but we use a t-test for that). For example if you want to see if a captive bird species prefers red, green or blue food pellets, then the factor would be be food colour, and three levels of that would be the three colours.\nA two-way ANOVA is used where we have two categorical variables, each of which has at least two levels. (Even if both have two levels, we still call it a two-way ANOVA. There is no such thing as a two-way t-test!). So, sticking with the captive birds, if were interested in whether they had a preference for colour (red, blue, green) and/or shape (round, square) of food pellets, then we could use a two way ANOVA to investigae the data, where the two factors or ‘ways’ would be food colour and food shape, with food colour having three levels (red, blue and green) and food shape having two levels (round and square).\nANOVAs involving three ways or more (the horror, the horror!) are rarely used since their interpretation is in practice difficult due to the multiplicity of possible so-called ‘interaction’ effects that commonly arise, whereby the impact on the output of the levels of one factor depend on the values of the levels of one or more other factors. Best avoided!\nANOVAs, whatever their flavour (one way, two way, repeated measures, ANCOVA etc) are examples of parametric tests. That is, they can only be used if the data at least approximately meet certain requirements such as equal variance across the data sets, normality of residuals etc. At the very least, the data should be numerical and not ordinal. Hence whenever we think of using an ANOVA we need to check that these requirements are at least approximately met. If they are not, then we may choose to turn to non-parametric alternatives.\nA non-parametric alternative to a one-way ANOVAs is commonly used, especially for studies that involve ordinal data such as Likert-type outputs from surveys. It is called a Kruskal-Wallis test.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_one_way.html#example-penguin-data-from-the-palmerpenguins-package",
    "href": "ANOVA_one_way.html#example-penguin-data-from-the-palmerpenguins-package",
    "title": "6  One-way ANOVA",
    "section": "6.1 Example: penguin data from the palmerpenguins package",
    "text": "6.1 Example: penguin data from the palmerpenguins package\nIn the following, we show how a one-way ANOVA might be carried out on a set of data on penguins, where we have a numerical output such as body mass, and categorical variables such as species (three levels: Adelie, Gentoo and Chinstrap) and sex (two levels: female and male). If we were interested in whether there was a difference in the output across the levels of species, then a one-way ANOVA might well be suitable.\n\n6.1.1 Load packages\n\nlibrary(tidyverse) # for data manipulation and plots, and more besides\nlibrary(ggfortify) # this is useful for diagnostics\nlibrary(palmerpenguins) # for the palmer penguin data\n\nThe palmerpenguins package comes with two in-built data sets on penguins. The simplest of them is called penguins and is the one we will use in this exercise:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n6.1.2 Remove observations with missing values\nWe can see from the first few values of the glimpse table that some rows have missing values (NAs). We need to decide what to do with them. Here we will simply remove them! Here is a way to remove any row that contains missing values in one column or another:\n\npenguins_clean &lt;- penguins |&gt;\n  drop_na()\nglimpse(penguins_clean)\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nThat has removed 11 rows of data, so we haven’t lost too much information.\n\n\n6.1.3 Summary - group by species and sex\nHere we use the famliar group_by() and summarise() construction to find the mean body mass for each combination of species and sex. We also calculate the standard error of those means and the number of individuals in each group.\n\npenguins_clean |&gt;\n  group_by(species, sex) |&gt;\n  summarise(n = n(), mean_bm = mean(body_mass_g), se_bm = sd(body_mass_g)/sqrt(n()) ) |&gt;\n  ungroup()\n\n# A tibble: 6 × 5\n  species   sex        n mean_bm se_bm\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    female    73   3369.  31.5\n2 Adelie    male      73   4043.  40.6\n3 Chinstrap female    34   3527.  48.9\n4 Chinstrap male      34   3939.  62.1\n5 Gentoo    female    58   4680.  37.0\n6 Gentoo    male      61   5485.  40.1\n\n\nLooking at this table, does it look as though females and males have different weights? If so, which is heavier? Is this true for all species? Do the different species weigh the same?\n\n\n6.1.4 Plot the data\nTo get further insight into these questions, we can plot the data. Here we will do a box plot\n\npenguins_clean  |&gt;\n  ggplot(aes(x=species, y = body_mass_g, fill = sex)) +\n  geom_boxplot() +\n  labs(x = \"Species\",\n       y = \"Body mass (g)\",\n       fill = \"Sex\") +\n  scale_colour_brewer(palette = \"Set1\") +\n  theme_bw() +\n  theme(legend.position= c(0.1,0.8))\n\n\n\n\n\n\n\n\nWhat do you think now about size differences between species and the two sexes?\nThere is a lot going on here, so let’s approach this more simply to begin with and concentrate solely on the difference between the females of the species.\n\n\n6.1.5 One-way ANOVA\nLet’s ask the question: do the body weights differ between females of the different species?\nThere is just one factor here, species, and it has more than two levels - the three different species - and the reponse variable is numeric, so it is highly likely that the appropriate test to answer this question is a one-way ANOVA. ‘One way’ because there is one factor, and ‘ANOVA’ (instead of t-test) because there are more than two levels.\n\n6.1.5.1 Null hypothesis\nPretty much all of the commonly used statistics tests are asking the question: what is the probability that you would have got this data, or more extreme data, if the null hypothesis were true? Their job is to calculate that probability, which is called a p-value. There is a lot more besides, but what this means is that in carrying out any of these tests we at least need to have a hypothesis in mind and its corresponding null hypothesis. The null, remember, is typically the ‘nothing going on’, there is no effect, no difference scenario.\nSo in this case, a suitable null hypothesis would be that there is no difference in body mass between the females of the different penguin species.\nTo see if there is evidence from the data to reject this null, we will follow a sequence of steps that will be common to many analyses:\n\nget the data\nclean/prepare the data\nsummarise the data\nplot the data\nconstruct the model using whatever test is appropriate, in this case a one-way ANOVA\ncheck whether the model is valid\ninspect the model output\nreject or fail to reject the null hypothesis\nif we reject the null, carry out post-hoc tests\n(maybe) simplify the model and redo the analysis\n\nFor the penguin data, getting it was easy as it came with the palmerpenguins package.\nTo prepare the data, we start with the full data set and narrow it down to just the females, using the filter() function, and again make sure there are no lines with missing values, using drop_na(). We save this cleaned data set in an object called females.\n\nfemales &lt;- penguins |&gt;\n  filter(sex == \"female\") |&gt;\n  drop_na()\n\nThen let’s summarise these values to find the number of individuals, the mean body mass for each species, and the standard errors of those means:\n\nfemales |&gt;\n  group_by(species) |&gt;\n  summarise(n = n(), mean.mass_f = mean(body_mass_g), se.mass_f = sd(body_mass_g)/sqrt(n()))\n\n# A tibble: 3 × 4\n  species       n mean.mass_f se.mass_f\n  &lt;fct&gt;     &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 Adelie       73       3369.      31.5\n2 Chinstrap    34       3527.      48.9\n3 Gentoo       58       4680.      37.0\n\n\nWe should inspect this summary table and see what we already think about whether the null hypothesis is likely to be rejected, or not.\nNow let’s plot them, using a box plot (but choose your favourite plot type):\n\nfemales  |&gt;\n  ggplot(aes(x=species, y = body_mass_g)) +\n  geom_boxplot(fill = \"#9ebcda\") +  # pick your favourite colour from https://colorbrewer2.org/\n  labs(x = \"Species\",\n       y = \"Body mass (g)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFrom the summary table and the plot, what do you think? Do the masses differ between the species?\n\n\n6.1.5.2 The actual ANOVA\nYou probably have a good idea what the answer is, as to our question, but now we will move on to the actual statistics test, in this case a one-way ANOVA.\nAn ANOVA is one variant of a range of anlysis techniques known as ‘linear models’. If you were to look under the hood, you would see that mathematics behind it is exactly the same as that behind linear regression, which we use when we have a continuous explanatory variable and where we fit straight lines onto a scatter plot. Thus it is no surprise that the ANOVA is carried out in R in exactly the same way as linear regression would be:\nFirst, we use the lm() function to construct a linear model of the data:\n\n\n6.1.5.3 Construct the model\n\nfemales.model &lt;- lm(body_mass_g ~ species, data = females)\n\nHere the lm() function has done all the maths of the ANOVA, and we have saved the results of that in an object called females.model. Note the use of the formula body_mass_g ~ species as the first argument of the lm() function, where this means ‘body mass as a function of species’.\n\n\n6.1.5.4 Is the model valid?\nAll linear models are only valid if the data meet a number of criteria. Chief among these for an ANOVA is that the spread of the data should be roughly the same in each subset, and that the data within each subset should be normally distributed around their respective mean values. Only if these conditions are at least approximately met can we just go on and trust the output of the model. If they are not, we need to transform the data in some way until they are, or use a different test. A commonly used non-parametric alternative to the one-way ANOVA is the Kruskal-Wallis test.\nThere are various ways we can find out whether these conditions are met. A useful one is to do it graphically, and a useful way to do that is to use the autoplot() function from the ggfortify package. Let’s do it:\n\nautoplot(females.model) + theme_bw()\n\n\n\n\n\n\n\n\nAll four graphs presented here tell us something about the validity or not of our model. Here we will just focus on the upper two:\n\ntop-left: this shows the spread of the residual masses (diference between an individual’s mass and the mean mass for its species) for each species. We see that the spread of these values is aout the same for all three species. Check!\ntop-right: this is a quantile-quantile plot, often referred to as a qq-plot. This compares the distribution of the residuals for each species with a normal distribution. If the residuals are normally distributed, we will get a straight line. If not, we won’t. To get an idea of what qq-plots, histograms and box-plots look like for data that definitely are not normally distriuted, see this useful summary. In our case, there is a hint of a curve, but this qq-plot is really a pretty good approximation to linear for a real data set. No such data is ever perfectly normally distributed, so the best we are looking for, in practice is something approximating a straight line, often with some raggedness at either end. So, check again!\n\nOn both counts, we are good to go: we can reasonably trust the output of the ANOVA.\nSo what is this output? We find this in three steps\n\n\n6.1.5.5 The overall picture\nFirst, we use the anova() function\n\nanova(females.model)\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n           Df   Sum Sq  Mean Sq F value    Pr(&gt;F)    \nspecies     2 60350016 30175008  393.25 &lt; 2.2e-16 ***\nResiduals 162 12430757    76733                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis gives us an overview of all the data and asks the question: how likely is it that you would have got your data if species made no difference to body mass. There are three things to note:\n\nthe test statistic, here called an F-value. This is a number calculated from the data. Roughly speaking, it is the ratio of the spread of values (aka variance) between the subgroups to that within the subgroups If the validity criteria for the test have been met by the data, then this has a known distribution. The bigger the F-value, the more likely it is that the null will be rejected.\nthe degrees of freedom, here denoted as Df and listed in the first column. These are the number of independent pieces of information in the data, which here means, how many species and how many penguins.\nthe p-value, which is the probability of getting an F value as big as or bigger than the one actually found, if the null hypothesis were true. This is is the number listed at the right as Pr(&gt;F).\n\nThe F value here is huge and the p-value is tiny, so tiny that it is essentially zero. Thus we can confidently reject the null hypothesis and assert that there is evidence from the data that body mass of females differs between at least one pair of species. Which two, or between all of them, and by how much we don’t yet know. This first step just tells us whether there is some difference somewhere. If there were no evidence of any difference we would stop the analysis right here.\nBut there is a difference in this case, so we continue.\n\n\n6.1.5.6 The detailed picture\nWe use the summary() function for this:\n\nsummary(females.model)\n\n\nCall:\nlm(formula = body_mass_g ~ species, data = females)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-827.21 -193.84   20.26  181.16  622.79 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3368.84      32.42 103.908  &lt; 2e-16 ***\nspeciesChinstrap   158.37      57.52   2.754  0.00657 ** \nspeciesGentoo     1310.91      48.72  26.904  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 277 on 162 degrees of freedom\nMultiple R-squared:  0.8292,    Adjusted R-squared:  0.8271 \nF-statistic: 393.2 on 2 and 162 DF,  p-value: &lt; 2.2e-16\n\n\nThere is a lot in ths output, so let’s just consider the coefficient table, to begin with. Focus first on the top left value, in the Estimate column. This tells us the mean body mass of the reference or ‘Intercept’ species. In this case that is ‘Adelie’, purely because ‘Adelie’ comes alphabetically before the other two species names, ‘Chinstrap’ and ‘Gentoo’. By default, R will always order levels of a factor alpabetically. This is often a nuisance, and with many data sets we have to tell R to reorder the levels the way we want them, but here the order is OK.\nSo, the mean mass of female Adelie penguins in our sample is 3368 g. Cross check that with your initial summary table and the box plot. What about the other two species? Here’s the thing: for all rows except the first in the Estimate column we are not given the absolute value but the difference between their respective mean values and the reference mean in the first, ‘Intercept’ row.\nThus, we are being told that Chinstrap females in the sample have a mean body mass that is 158.37 g heavier than that of Adelie females, so that their mean body mass is 3368.84 + 158.37 = 3527.27g. Again, cross check that with your summary table and the box plot. Is it right?\nWhat about Gentoo females? Were they heavier than Adelie penguins, and if so, by how much? What was their mean body mass.\nWhy doesn’t summary() just tell us the actual body masses instead for all three species instead of doing it in this round about way? The reason is that ANOVA is concerned with detecting evidence of difference. This is why we are being told what the differences are between each of the levels and one reference level, which here is Adelie.\nAre those differences signifcant? We use the right hand p-value column for that. Look in the rows for Chinstrap and Gentoo penguins. In both cases the p values are much less than 0.05. This is telling us that in both cases there is evidence that females of these species are significantly heavier than those of the Adelie species.\nNote that we have only been told, so far, about the magnitude and significance of differences between all the levels and the reference level. We are not told the significance of any difference between any other pair of levels. So in particular, the ANOVA does not tell us whether there is a significant difference between the masses of Chinstrap and Gentoo females (although we may have a good idea what the answer is, from our initial summary table and plot).\nTo find the answer to that, we do post-hoc tests:\n\n\n6.1.5.7 Post hoc tsts.\nA final step of most ANOVA analyses is to perform so-called post-hoc (‘after the fact’) tests which make pairwise comparisons between all possible pairs of levels, tell us what the differences are between those pairs and whether the differences are significant. Whatever method is used for this, it needs to take account of the danger of making Type-one errors that arises when multiple pair-wise tests are done.\nA commonly used function for doing this is Tukey’s Honest Signficant Difference: TukeyHSD()\n\nTukeyHSD(aov(body_mass_g ~ species, data = females))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = body_mass_g ~ species, data = females)\n\n$species\n                      diff        lwr       upr     p adj\nChinstrap-Adelie  158.3703   22.32078  294.4197 0.0179471\nGentoo-Adelie    1310.9058 1195.64908 1426.1624 0.0000000\nGentoo-Chinstrap 1152.5355 1011.00620 1294.0648 0.0000000\n\n\nIn each row of the output we see the difference between the mean masses of the females of two species, where a positive value tells us that the first named species has the heavier mass. So, we see that Gentoo females in the sample were on average 1310.9 g heavier than Adelie females.\nCompare these differences with your initial summary table and your box plot. Do they agree? They should!\nThe right-hand column ‘p adj’ tells us whether these difference are significant. If the p values are less than 0.05 then they are, at the 5% significance level. In this case they all are. The p values are so tiny for the differences between Gentoo and the other two species that that they are reported as zero.\n\n\n\n6.1.6 Reporting the Result.\nWe try to use plain English to report our results, while still telling the reader what test was used and the key outputs of the test. Try to report the name of the test, the test statistic, the degrees of freedom, and the p-value. if. the p-value is really small then it is common to report it as p&lt;0.01, or p&lt;0.001. No one cares if it is a billionth or a squillionth. It just matters that is t is really small, if that is the case. If it is only just below 0.05, then I would report it in full, so we might write p = 0.018. If p &gt; 0.05 then conventiallly it is not reported, except to say p &gt; 0.05.\nIn this case, we might say something like:\nWe find evidence that there is a difference between the body masses of females of the penguin species Adelie, Chinstrap and Gentoo (ANOVA, df = 2, 162, F = 393, p &lt; 0.001). In particular Gentoo are more than 1 kg heavier than the other two (p&lt; 0.001) while the difference between Chinstrap and Adelie is smaller, at 158 g, but still significant (p = 0.018).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA_two_way_with_model_simplification.html",
    "href": "ANOVA_two_way_with_model_simplification.html",
    "title": "7  Two-way ANOVA with model simplification",
    "section": "",
    "text": "This exercise sheet is heavily indebted to Michael Crawley’s Statistics: An introduction using R, 2nd Ed, Wiley. Published in 2015 this emphasises statistics over R (in fact, much of the R he presents is written prior to the advent of the tidyverse dialect which we use here, and so may seem terse if that is what you are used to). It is very useful and is at a higher level than Beckerman, Childs and Petchey’s Getting Started in R: An introduction for biologists, 2nd Ed. OUP published in 2017. Their book also includes a simpler version of the example explored here.\n\n7.0.1 Factorial experiments and model simplification\nThe best model is the one that adequately explains the data with fewest parameters. This means with the smallest possible number of degrees of freedom.\nIf we have a very large number of parameters, a model can fit any data set but be of limited use in generalising beyond it (we will have overfitted the data). If we have too few we will not explain much of the variance of the data. A balance must be struck. Hence we want the minimal adequate model.\nAs Einstein almost said, a model should be as simple as possible, but no simpler. (Not to be outdone, the British statistician George Box also had a pithy saying about models: “All models are wrong, but some are useful”.)\nA factorial experiment has two or more factors, each with two or more levels, plus replication for each combination of factor levels. This means that we can investigate whether statistical interactions occur in which the effect of one factor depends on the value of another factor.\nWe take an example from a farm-trial of animal diets. There are two factors: diet and supplement. diet is a factor with three levels: barley, oats and wheat, where barley is the diet that has always been used and the other two are potential alternatives. The purpose of the trial is to see if their use makes a difference to growth outcomes. supplement is a factor with four levels: control, agrimore, supergain and supersupp, where control could mean the absence of any supplement or the supplement used up to now, whose effects we are hoping to improve upon through use of one of the others included in the trial. The response variable gain is weight gain after 6 weeks. There were 48 individual cows in total with 4 for each combination of diet and supplement. Having the same number of replicates for each combination of levels means that this is a balanced design.\n\n\n\n\n\n\n\n\n\n\n\n7.0.2 Files needed\n\nTo be put in the Project/scripts folder:\n\nANOVA_two_way_with_model_simplification.html (this worksheet)\nANOVA_two_way_template.Rmd (the script where you fill in the code chunks)\n\nTo be put in Project/data folder\n\ngrowth.csv\n\n\nIn the following, we present the code you need to analyse this data together with explanatory text. Read the text closely so that you understand what each chunk of code is intended to do. In the accompanying template file, fill in the code as you go in the empty chunks, using this worksheet as a guide. As you complete each line of code, run it using Ctrl-Enter or Cmd-Enter on a Mac. Alternatively, wait until you have completed the code for a chunk then run the whole chunk in one go by pressing the little green arrow at the top right of the chunk. Whichever way you choose, you are encouraged to view the code presented here as one way to do the analysis. Feel free to hack away at it and change things, to try different approaches and see what happens. That way you will learn. You may also wish to add your own text between the chunks.\n\n\n7.0.3 Open your Project\nYou should be working within a folder that you have designated as what RStudio calls a ‘Project’. If you are, the name of your Project will appear at the top right of the RStudio window. Inside your Project folder you should have a scripts folder for scripts like the one you are working from, and a data folder for all the data files. You will also see, at the top level of the Project, the .RProj file. You can see all this in the Files pane, bottom-right.\n\n\n7.0.4 Load packages\nI normally load all the packages in the chunk below into every script. The most important is the tidyverse package which is a goody bag containing several other packages. Loading this saves you from having to load each of those individually. The most often used among these is readr for reading and writing data from/to files, dplyr for data manipulation, and ggplot2 for plotting. Others will be used from time to time, and we don’t really need to be aware of that when it happens or to worry about it, so long as tidyverse has been loaded.\n\nlibrary(tidyverse)  # for data manipulation and plotting, and much else besides\nlibrary(here) # for finding our data easily\nlibrary(cowplot) # gives a nice theme for plots\nlibrary(ggfortify) # for diagnostic plots\n\n\n\n7.0.5 Read in the data\nThe growth.csv data file needs to be in the data folder within the Project folder.\nIn this chunk we read the growth.csv data into an R object to which we give the name weights.\n\nfilepath &lt;- here(\"data\", \"growth.csv\")\nweights &lt;- read_csv(filepath) # this function is from the readr package, part of tidyverse\nglimpse(weights)\n\nRows: 48\nColumns: 3\n$ supplement &lt;chr&gt; \"supergain\", \"supergain\", \"supergain\", \"supergain\", \"contro…\n$ diet       &lt;chr&gt; \"wheat\", \"wheat\", \"wheat\", \"wheat\", \"wheat\", \"wheat\", \"whea…\n$ gain       &lt;dbl&gt; 17.37125, 16.81489, 18.08184, 15.78175, 17.70656, 18.22717,…\n\n\nYou see from the output of the glimpse() function that weights has three columns and 48 rows. Two columns are of data type &lt;chr&gt; which is R-speak for text, and the other is data type &lt;dbl&gt; which is R-speak for numerical data with a decimal point.\n\n\n7.0.6 Make R recognise the categorical variables as factors, and order the levels.\nAt the moment, the contents within the variables supplement and diet are not being recognised as levels of factors. R is just thinking of them as text (or &lt;chr&gt; in R-speak), as we can see from the output of the glimpse() function in the chunk above. Let us fix that, as it will be useful for them to be recognized for what they are so that we can order the levels in a way that makes sense for our context, our plots and our analysis.\nSometimes levels of a factor have a natural order, such as Low, Mid and High as the levels of the factor Tidal Zone and sometimes they do not, for example Apples, Oranges and Pears as levels of the factor Fruit. Here, in the case of both our factors, we only wish to impose order among the levels in so far as we would like what we regard as the control or reference level to be first. By default, R puts the levels of a factor in alphabetical order. This is the order in which the boxes of a box plot would be displayed, reading left to right. In an ANOVA setting it means that differences of outcome (in this case, weight gain of the cows) are later calculated for each combination of levels with respect to the outcome for the combination of levels that are alphabetically first, in this case barley for diet and agrimore for supplement. In both the box-plot and the ANOVA output case this default ordering is not necessarily what we want. Normally, we want what we regard as the control levels to be the reference level and in this case that means barley for diet and control for supplement.\nTo ensure that a variable is regarded as a factor, and then to get its levels in the order we would like, we use the factor() function.\nIn the following chunk, factor() is used to designate both the supplement and diet columns of the data set as factors, and the level order of each is specified, with control coming first for supplement and barley coming first for diet.\n\n# This line of code designates the supplement and diet columns of weights as factors, orders the levels of these factors as required and saves the result under the original name.\nweights &lt;- weights |&gt;\n  mutate(supplement = factor(supplement, levels = c(\"control\",\"agrimore\", \"supergain\", \"supersupp\"))) |&gt;\n  mutate(diet = factor(diet, levels = c(\"barley\", \"oats\", \"wheat\")))\n\n# check that this worked\nglimpse(weights)\n\nRows: 48\nColumns: 3\n$ supplement &lt;fct&gt; supergain, supergain, supergain, supergain, control, contro…\n$ diet       &lt;fct&gt; wheat, wheat, wheat, wheat, wheat, wheat, wheat, wheat, whe…\n$ gain       &lt;dbl&gt; 17.37125, 16.81489, 18.08184, 15.78175, 17.70656, 18.22717,…\n\n# check the level order of each factor - does the 'reference' level come first?\nlevels(weights$supplement)\n\n[1] \"control\"   \"agrimore\"  \"supergain\" \"supersupp\"\n\nlevels(weights$diet)\n\n[1] \"barley\" \"oats\"   \"wheat\" \n\n\nDo you see how the variable types of the supplement and diet columns have been changed to &lt;fct&gt;? It worked!\nTo get control to be the reference level of supplement we needed to force the issue in this way. If we hadn’t then agrimore would have been regarded as such, since it is alphabetically the first among the levels of supplement. We didn’t need to do this for diet, since the stipulated ordering of the levels is just the alphabetical order and so we would have had that by default anyway. Sometimes, though, it doesn’t hurt to throw in a little redundancy for the sake of clarity.\nSo, now we have control as the reference level for supplement and barley as the reference level for diet. Now we can see more easily in our analysis what difference is made to weight gain when we change diet or supplement or both from a ‘business as usual’ combination of a barleydiet and the control supplement.\n\n\n7.0.7 Summarise the data\nOur question is a difference question: is there evidence from the data that using this or that diet in combination with this or that supplement makes a difference to growth? For an answer to this we will end up doing a 2-way ANOVA including the possibility of an interaction, then, as we will see, a simpler ANOVA that ignores the possibility of the interaction. All well and good, but before we go to those lengths, we do something more basic: we calculate the mean and standard error of the mean for each of the twelve combinations of diet and supplement.\nThere isn’t a function in base R with which we can calculate standard error of the mean directly, but we can do so knowing the standard deviation of the sample \\(\\text{SD}\\) (using sd()) and the sample size \\(n\\) (using n()) using this formula:\n\\[ \\text{SE}=\\frac{SD}{\\sqrt{n}}\\]\n\n# we use the group_by() and summarise() functions from dplyr (the package within tidyverse for data manipulation)\ngrowth_summary &lt;- weights |&gt;\n  group_by(diet, supplement) |&gt;\n  summarise(mean_gain = mean(gain), se_gain = sd(gain)/sqrt(n())) |&gt;\n  ungroup()\n\n# if we type the name of an object, it gets printed out for us\ngrowth_summary |&gt; kable()\n\n\n\n\n\n\ndiet\nsupplement\nmean_gain\nse_gain\n\n\n\n\nbarley\ncontrol\n23.29665\n0.7032491\n\n\nbarley\nagrimore\n26.34848\n0.9187479\n\n\nbarley\nsupergain\n22.46612\n0.7710644\n\n\nbarley\nsupersupp\n25.57530\n1.0599015\n\n\noats\ncontrol\n20.49366\n0.5056319\n\n\noats\nagrimore\n23.29838\n0.6131592\n\n\noats\nsupergain\n19.66300\n0.3489388\n\n\noats\nsupersupp\n21.86023\n0.4132292\n\n\nwheat\ncontrol\n17.40552\n0.4604420\n\n\nwheat\nagrimore\n19.63907\n0.7099260\n\n\nwheat\nsupergain\n17.01243\n0.4852821\n\n\nwheat\nsupersupp\n19.66834\n0.4746443\n\n\n\n\n\n\n\nNote the ordering of the diet and supplement levels in their respective columns: just what we have imposed!\n\n\n7.0.8 Plot the data\nThe next step, as so often before we launch into actual statistics, is to plot the data in a way that sheds light on the question we have. Here, we can use the use the means and standard errors of the mean that we have just calculated to produce a useful kind of line plot that in this context is often referred to as an interaction plot:\n\ngrowth_summary |&gt;\n  ggplot(aes(x = supplement,y = mean_gain, colour = diet, group = diet)) +\n  geom_point(size = 2) +\n  geom_line() +\n  geom_errorbar(aes(ymin = mean_gain - se_gain, ymax = mean_gain + se_gain), width = 0.1) +\n  labs(x = \"Supplement\",\n       y = \"Mean weight gain\") +\n  scale_fill_brewer() +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nNote that on this plot the error bars are standard errors of the mean. Any caption to a figure that contains error bars should explain what those error bars mean. In particular, it should say whether they are standard deviations of the sample, standard errors of the mean or confidence intervals. These are all different from each other. A good explanation of the difference is given by [Cummings et al][1]. (2007).\nThis interaction plot is useful in that we see that both diet and supplement have an effect on growth and that the effect of one is altered little by the value of the other, the result of which is that the lines are more or less parallel. This suggests that we have main effects of both diet and supplement, but little or no interaction between them.\n\n7.0.8.1 Questions\nWhat could the line plot look like if:\n\nThere were no main effect of both diet and supplement, and no interaction\nThere were a main effect of diet, no main effect of supplement and no interaction?\nThere were no main effect of diet, a main effect of supplement and no interaction?\nThere were main effects of both and an interaction between them?\n\nThe plots tell you a great deal about what main effects and/or interactions there may be.\n\n\n\n7.0.9 ANOVA\nNow for the actual statistical test. We will conduct a two-way ANOVA, which will look to see if there is evidence that either diet or supplement or both affect growth rate (the so-called main effects), and if the effect of one depends on the nature of the other (the so-called interaction).\nThe null hypothesis is that neither has any main effect and that there is no interaction.\nNow we can use either of the functions aov() or lm() to carry out a factorial ANOVA (the choice affects only whether we get an ANOVA table or a list of parameter estimates as the default output from summary().). Here, we will use lm(), partly because we would also use it for one-way ANOVAs and linear regression, and to do so here reminds of the common mathematical machinery that underlies all these methods.\nWe estimate parameters for the main effects of each level of diet and each level of supplement, plus terms for the interaction between diet and supplement.\nThe interaction degrees of freedom are the product of those for diet and supplement ie (3-1) x (4-1) = 6.\nThe model is:\ngain ~ diet + supplement + diet:supplement\nwhich can be written more simply using the asterisk notation as:\ngain ~ diet * supplement\n\n7.0.9.1 Construct the model\nFirst we construct the model using lm() and store the outputs of all the maths that `lm() does in an object called model0:\n\nmodel0 &lt;- lm(gain ~ diet * supplement, data = weights)\n\n\n\n7.0.9.2 Do we reject the null hypothesis?\nTo get an overall picture, we first use anova() to see if there is evidence to reject the null\n\nanova(model0)\n\nAnalysis of Variance Table\n\nResponse: gain\n                Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet             2 287.171 143.586 83.5201 2.999e-14 ***\nsupplement       3  91.881  30.627 17.8150 2.952e-07 ***\ndiet:supplement  6   3.406   0.568  0.3302    0.9166    \nResiduals       36  61.890   1.719                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA table shows that there a main effect of both diet and supplement (p&lt;0.001 in both cases), but that there is no hint of an interaction between diet and supplement (p = 0.917). Does that tally with what you see in the interaction plot? Clearly therefore, the effects of diet and supplement are merely additive (ie whichever level of one you have it does not affect the impact on growth of whichever level of the other you choose).\nThe ANOVA table does not show us effect sizes or allow us to work out which if any of the levels of the two factors are significantly different. For this, summary() is more useful:\n\nsummary(model0)\n\n\nCall:\nlm(formula = gain ~ diet * supplement, data = weights)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.48756 -1.00368 -0.07452  1.03496  2.68069 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   23.2966499  0.6555863  35.536  &lt; 2e-16 ***\ndietoats                      -2.8029851  0.9271390  -3.023  0.00459 ** \ndietwheat                     -5.8911317  0.9271390  -6.354 2.34e-07 ***\nsupplementagrimore             3.0518277  0.9271390   3.292  0.00224 ** \nsupplementsupergain           -0.8305263  0.9271390  -0.896  0.37631    \nsupplementsupersupp            2.2786527  0.9271390   2.458  0.01893 *  \ndietoats:supplementagrimore   -0.2471088  1.3111726  -0.188  0.85157    \ndietwheat:supplementagrimore  -0.8182729  1.3111726  -0.624  0.53651    \ndietoats:supplementsupergain  -0.0001351  1.3111726   0.000  0.99992    \ndietwheat:supplementsupergain  0.4374395  1.3111726   0.334  0.74060    \ndietoats:supplementsupersupp  -0.9120830  1.3111726  -0.696  0.49113    \ndietwheat:supplementsupersupp -0.0158299  1.3111726  -0.012  0.99043    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.311 on 36 degrees of freedom\nMultiple R-squared:  0.8607,    Adjusted R-squared:  0.8182 \nF-statistic: 20.22 on 11 and 36 DF,  p-value: 3.295e-12\n\n\nThis is a complex model as there are 12 estimated parameters: 6 main effects and 6 interactions. Notice that although the ‘controls’ for diet and supplement (barley and control) do not appear to be in the table, they are there really, in the first row.\nThe value 23.30 kg in the first row of the Estimate column on the left, labelled ‘Intercept()’ gives us the actual weight gain outcome for the combination of the two control levels, barley as diet and control as supplement. Check that this value tallies with what is shown in summary tables above, and in the interaction plot.\nThe weight gain values for all the other combinations of the levels of each factor are given as differences from this reference level.\nSo for example in row two, where diet is changed from barley to oats but supplement is still control, the value in the table is -2.8. This means that the weight gain when the diet is changed to oats but the supplement left as the control is 2.80 kg less than the reference value, and so must be 23.30-2.80 = 20.50 kg. This agrees with the value in the summary table of mean values that was calculated above, and tallies with the interaction plot.\nIn row seven we see that the effect of the interaction between the diet oats and the supplement agrimore is - 0.247. This means that on going from the reference levels of barley and control, for which the gain is 23.30, the change in gain is not just the sum of the two main effects (-2.80 for switch of diet to oats and +3.05 for switch of supplement to agrimore, but is modified by their interaction, of size - 0.247. Hence the mean gain for a diet of oats and a supplement of agrimore is the intercept value plus the sum of the two main effects, plus the interaction term: 23.297 - 2.803 + 3.052 - 0.247 = 23.299)\nSee if you can tally the other effect values in the summary table with the mean values given in table above and in the interaction plot for other combinations of diet and supplement.\nHere is a table to help you interpret the output of the summary() function.\n\n\n\n\n\nterm\nmeaning\ntype_of_effect\nestimate\nabsolute_value\np_value\nsignificance\n\n\n\n\n(Intercept)\nbarley + control\nMain effect\n23.30\n23.30\n&lt;0.001\n***\n\n\ndietoats\noats + control\nMain effect\n-2.80\n20.49\n0.005\n**\n\n\ndietwheat\nwheat + control\nMain effect\n-5.89\n17.41\n&lt;0.001\n***\n\n\nsupplementagrimore\nbarley + agrimore\nMain effect\n3.05\n26.35\n0.002\n**\n\n\nsupplementsupergain\nbarley + supergain\nMain effect\n-0.83\n22.47\n0.376\n\n\n\nsupplementsupersupp\nbarley + supersupp\nMain effect\n2.28\n25.58\n0.019\n*.\n\n\ndietoats:supplementagrimore\noats + agrimore\nInteraction\n-0.25\n23.05\n0.852\n\n\n\ndietwheat:supplementagrimore\nwheat + agrimore\nInteraction\n-0.82\n22.48\n0.537\n\n\n\ndietoats:supplementsupergain\noats + supergain\nInteraction\n0.00\n23.30\n1.0\n\n\n\ndietwheat:supplementsupergain\nwheat + supergain\nInteraction\n0.44\n23.73\n0.741\n\n\n\ndietoats:supplementsupersupp\noats + supersupp\nInteraction\n-0.91\n22.38\n0.491\n\n\n\ndietwheat:supplementsupersupp\nwheat + supersupp\nInteraction\n-0.02\n23.28\n0.99\n\n\n\n\n\n\n\n\nThe output of the summary() function re-emphasises that none of the interaction terms are significant. It also suggests that a minimum adequate model will contain 5 parameters: an intercept, which just means that there is non-zero growth when the diet and supplement are the reference values, a difference from that growth due to changing the diet to oats, a difference due to changing it towheat, a difference due to changing the supplement to agrimore while keeping barley as the diet, and a difference due to changing the supplement instead to suppersupp..\n\n\n\n7.0.10 Model Simplification\nGiven the results of the full interaction model, we begin model simplification by leaving out the interaction terms, to leave us with an additive model:\n\nmodel_1 &lt;- lm(gain ~ diet + supplement, data = weights)\nsummary(model_1)\n\n\nCall:\nlm(formula = gain ~ diet + supplement, data = weights)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.30792 -0.85929 -0.07713  0.92052  2.90615 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          23.4263     0.4408  53.141  &lt; 2e-16 ***\ndietoats             -3.0928     0.4408  -7.016 1.38e-08 ***\ndietwheat            -5.9903     0.4408 -13.589  &lt; 2e-16 ***\nsupplementagrimore    2.6967     0.5090   5.298 4.03e-06 ***\nsupplementsupergain  -0.6848     0.5090  -1.345 0.185772    \nsupplementsupersupp   1.9693     0.5090   3.869 0.000375 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.247 on 42 degrees of freedom\nMultiple R-squared:  0.8531,    Adjusted R-squared:  0.8356 \nF-statistic: 48.76 on 5 and 42 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n7.0.11 Check the validity of the linear model\nWe ought to pause here for a moment and just check that we are OK to go ahead and analyse our data using a general linear model (of which ANOVA is an example, linear regression and t-tests being others). We will use autoplot() from the ggfortify package, which gives us the standard four diagnostic plots.\n\nautoplot(model_1) + theme_cowplot() # autoplot() is from the ggfortify package.\n\n\n\n\n\n\n\n\nWell, that all looks fine. In particular, from the top-left figure we see that the variance of the residuals is more or less constant and from the top-right figure, the quantile-quantile plot, we get a pretty good approximation of a straight line which tells us that the residuals are more or less normally distributed. These are two key assumptions that must be at least approximately satisfied by data if it is going to make any sense to use a linear model to analyse it. We won’t discuss here the other two diagnostic plots, but they look fine too. So we are good to go using ANOVA with this data.\nBack to interpreting the output of the ANOVA:\nIt is clear that we need to retain all three levels of diet since the effect values of each differ from each other by an amount that is several times the standand errors, so that t &gt;&gt; 1. It is not clear that we need all the levels of supplement, however. supersupp is not obviously different from agrimore (difference = -0.727 with standard error = 0.509), yet both are clearly different from control. However supergrain is not obviously different from control (difference = -0.68, error = 0.509). Hence we are tempted to try a new model with just two levels of the factor supplement which we might sensibly call “best”, by which we mean agrimore or supersupp, and “worst” by which we mean control or supergrain. We’ll name this new factor supp2.\nThis code chunk amends the weights data frame by adding a new column to it called supp2 in which the values are either best if the supplement is agrimore or supersupp, or worst if the supplement is either of the other two\n\nweights &lt;- weights |&gt;\n  mutate(supp2 = ifelse(supplement %in% c(\"agrimore\", \"supersupp\"), \"best\", \"worst\"))\nglimpse(weights)\n\nRows: 48\nColumns: 4\n$ supplement &lt;fct&gt; supergain, supergain, supergain, supergain, control, contro…\n$ diet       &lt;fct&gt; wheat, wheat, wheat, wheat, wheat, wheat, wheat, wheat, whe…\n$ gain       &lt;dbl&gt; 17.37125, 16.81489, 18.08184, 15.78175, 17.70656, 18.22717,…\n$ supp2      &lt;chr&gt; \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"wors…\n\n\nIf we calculate the means and standard errors for weight gain under each diet for each of the two new classifications of supplement, and then plot them, we get this new interaction plot:\n\nweights |&gt;\n  group_by(diet, supp2) |&gt;\n  summarise(mean_gain = mean(gain), se_gain = sd(gain)/sqrt(n())) |&gt;\n  ungroup() |&gt;\n\n  ggplot(aes(x = supp2,y = mean_gain,colour = diet, group=diet)) +\n  geom_point(size=2) +\n  geom_line() +\n  geom_errorbar(aes(ymin = mean_gain-se_gain, ymax = mean_gain + se_gain), width=0.1) +\n  labs(x = \"Supplement\",\n       y = \"Mean weight gain\") +\n  scale_fill_brewer() +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nFrom this we can see that diet clearly makes a difference to weight gain, since the three lines are separated by a distance much larger than the standard errors, and also that the best supplement clearly makes a difference since there is a consistent drop on going from ‘best’ to ‘worst’, again by an amount that is much larger than the error bars, and there is clearly no interaction between diet and supplement, since the lines are parallel within the wiggle-room allowed by the error bars, which means that the effect of diet does not depend on supplement, and the effect of supplement does not depend on diet.\nNow we will make the simpler model, calling it model_2 (for comparison with the first additive model, model_1)\n\n# additive model whee the supplements have been condensed from four to two: best and worst\nmodel_2 &lt;- lm(gain ~ diet + supp2, data = weights)\n\nand then compare the two additive models:\n\nanova(model_1, model_2)\n\nAnalysis of Variance Table\n\nModel 1: gain ~ diet + supplement\nModel 2: gain ~ diet + supp2\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     42 65.296                           \n2     44 71.284 -2   -5.9876 1.9257 0.1584\n\n\nWhen we use anova() in this way it is testing the explanatory power of the second model against that of the first ie how much of the variance in the data does each explain. Its null hypothesis is that both models explain just as much of the variance as the other.\nThe simpler model has saved two degrees of freedom and is not significantly different in explanatory power than the more complex model (p = 0.158). Hence this is a better candidate as a minimal adequate model. All the parameters are significantly different from zero and from each other.\n\nsummary(model_2)\n\n\nCall:\nlm(formula = gain ~ diet + supp2, data = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6716 -0.9432 -0.1918  0.9293  3.2698 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  25.7593     0.3674  70.106  &lt; 2e-16 ***\ndietoats     -3.0928     0.4500  -6.873 1.76e-08 ***\ndietwheat    -5.9903     0.4500 -13.311  &lt; 2e-16 ***\nsupp2worst   -2.6754     0.3674  -7.281 4.43e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.273 on 44 degrees of freedom\nMultiple R-squared:  0.8396,    Adjusted R-squared:  0.8286 \nF-statistic: 76.76 on 3 and 44 DF,  p-value: &lt; 2.2e-16\n\n\nIn this table,\n\nline one (Intercept) tells us that the mean weight gain when on the barley diet and best supplement is 25.76 kg\nline two dietoats tells us that there is a significant drop in weight gain of 3.1 kg when diet is changed to oats.\nline three dietwheat tells us that there is a significant drop in weight gain of 5.99 kg when diet is changed to wheat.\nline four supp2worst tells us that there is a significant drop in wight gain of 2.68 kg when supplement is changed to worst.\n\nIn all cases, p&lt; 0.001, as indicated not only by the number in the Pr(&gt;|t|) column, but also by the ‘***’ in the right-most column of the table.\n\n\n7.0.12 Reporting the results\nWe have now reduced our initial 12 parameter model to a four parameter model that is much more tractable and easier to communicate. Our advice would be that for maximum weight gain a diet of barley with a supplement of agrimore or supersupp would be best.\nIf we were reporting this as a statistical test, we might say something like: A diet of barley with a supplement of agrimore or supersupp was to offer significant improvements over alternatives. There was no evidence of any interaction between diet and supplement. (ANOVA 2-way, F3,44 = 76.76, p &lt; 0.001)\n[1]: Cumming, G., Fidler, F., & Vaux, D. L. (2007). Error bars in experimental biology. Journal of Cell Biology, 177(1), 7–11. https://doi.org/10.1083/jcb.200611141",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-way ANOVA with model simplification</span>"
    ]
  },
  {
    "objectID": "chi-square.html",
    "href": "chi-square.html",
    "title": "8  Chi-squared analysis of count data",
    "section": "",
    "text": "8.1 When to use chi-square analysis\nA chi-square analysis is used when our data are in the form of\nWhat we do is compare the counts we got to some expected value according either to the long term results of chance or to some prior theory.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#when-to-use-chi-square-analysis",
    "href": "chi-square.html#when-to-use-chi-square-analysis",
    "title": "8  Chi-squared analysis of count data",
    "section": "",
    "text": "raw counts for two or more categorical groups eg pea plants with either yellow peas or green peas, survival rate of mice if they took drug A or took drug B, etc.\nEach independent observation must definitely belong to either one group or the other\nthere are no replicates. That is, for each category we just have one count.\nEach count is five or more",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#example-tossing-a-coin",
    "href": "chi-square.html#example-tossing-a-coin",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.2 Example: tossing a coin",
    "text": "8.2 Example: tossing a coin\n\nIf we were tossing a fair coin 1000 times we would expect 500 heads and 500 tails, ie heads and tails in the proportion 1:1.\nIn reality, if the coin were fair, we would probably get roughly the same number of heads and tails, but probably not exactly 500 of each.\nHow far from 50:50 would the proportion of heads and tails have to be before we would be justified in rejecting the idea that the coin is fair?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#example-throwing-a-dice",
    "href": "chi-square.html#example-throwing-a-dice",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.3 Example: throwing a dice",
    "text": "8.3 Example: throwing a dice\n\nIf we threw a fair dice a large number of times we would expect each possible score, from 1 to 6, to occur the same number of times.\nie each score would occur 1/6th of the time.\nIn reality we would probably get each score roughly 1/6 of the time, but not exactly 1/6.\nHow far from the expected proportions could the numbers of each score get before we would be justified in thinking that the dice was not fair?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#example-mendelian-inheritance",
    "href": "chi-square.html#example-mendelian-inheritance",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.4 Example: Mendelian inheritance",
    "text": "8.4 Example: Mendelian inheritance\n\n\n\n\n\n\n\n\n\nHow far from 3:1 would the yellow:green ratio need to be before we would justified in claiming that the outcome was inconsistent with the Mendelian prediction?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#chi-square-goodness-of-fit-test",
    "href": "chi-square.html#chi-square-goodness-of-fit-test",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.5 Chi-square Goodness of Fit Test",
    "text": "8.5 Chi-square Goodness of Fit Test\n\nIn a chi-square ‘goodness of fit’ test, we are testing data where we have a number of counts for each of two or more possible outcomes of some procedure (heads/tails, dice scores, pea colour etc).\nWe have an idea of how these counts should be distributed under some null hypothesis (the coin is fair, the dice is fair, genetic inheritance works in this or that way etc).\nThe chi-square goodness of fit test tests how likely it is we would have got the counts we actually got or more extreme counts if that null hypothesis were correct.\nWe are testing how well our actual counts ‘fit’ the expected values.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#chi-square-goodness-of-fit-test-in-r",
    "href": "chi-square.html#chi-square-goodness-of-fit-test-in-r",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.6 Chi-square Goodness of Fit Test in R",
    "text": "8.6 Chi-square Goodness of Fit Test in R\n\nIn a typical software implementation of the test, such as in R, we give it the counts we actually got for each possible outcome and also the expected proportion for each outcome.\nThe test then gives us a p-value, a probability, for how likely it is that we would have got the counts we actually got, or counts even further from the null hypothesis, if that null hypothesis were correct.\nIf this p-value is too small, and by that we usually mean less than 0.05, then we reject the null hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#example-mendelian-pea-plants",
    "href": "chi-square.html#example-mendelian-pea-plants",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.7 Example: Mendelian pea plants",
    "text": "8.7 Example: Mendelian pea plants\n\nSuppose we have self-fertilized an F1 generation of pea plants that were all heterozygous for yellow/green pea colour. In the F2 generation we get 176 offspring, of which 130 are yellow and 46 are green.\n\nWe ask, are these numbers consistent with a null hypothesis that Mendelian genetics is the underlying mechanism of inheriance here, or, if that were true, are they so unlikely that we sould reject that null?\n\nThe data here are raw counts, an individual pea plant offspring contributes either to the yellow count or to the green count, but not to both, and both counts are much larger than 5.\nHence, a Chi-square goodness of fit test can be used to test whether our data are plausibly consistent with the null hypothesis.\nOur expected counts of yellow and green are found by simply dividing the total count of offspring, 176, in the ratio 3:1, giving us an expected 132 yellow pea plants and an expected 44 green pea plants in the offspring F1 generation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#doing-the-chi-square-test-in-r",
    "href": "chi-square.html#doing-the-chi-square-test-in-r",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.8 Doing the chi-square test in R",
    "text": "8.8 Doing the chi-square test in R\nWe type this function into the console pane of RStudio:\nchisq.test(c(130,46),p=c(0.75,0.25))\nWe have given the function chisq.test() two arguments, separated by a comma:\n\nThe first argument c(130,46) is a vector (denoted by c()) of the two observed counts for yellow and green.\nthe second argument p = c(0.75,0.25) is another vector, this time of the proportions in which we expect the two colours to appear. If these proportions were all equal, as is common but not the case here, then we could leave out this argument, since that is the default presumption.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#output-of-chi-square-goodness-of-fit-test",
    "href": "chi-square.html#output-of-chi-square-goodness-of-fit-test",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.9 Output of chi-square goodness of fit test:",
    "text": "8.9 Output of chi-square goodness of fit test:\n\nchisq.test(c(130, 46), p = c(0.75, 0.25))\n\n\n    Chi-squared test for given probabilities\n\ndata:  c(130, 46)\nX-squared = 0.12121, df = 1, p-value = 0.7277\n\n\nThis output is typical of tests done in R.\n\nWe get the ‘test statistic’ whose name varies depending on the test. Here it is called X-squared, pronounced kai-squared. This is a number that the test calculates, based on the data you have given it. For the most part, we don’t need to worry about how it does that. Then there is the p-value, which is the probability of getting this test statistic if the null hypothesis were true.\nIn this case, we see that the p-value is 0.73, which is large.\nWe could very plausibly have got yellow:green numbers of 130 and 46 if the null hypothesis were true, so we cannot reject that null hypothesis.\nIn other words, our data are consistent at the 5% significance level with the predictions of simple Mendelian inheritance.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#reporting-the-result-in-english",
    "href": "chi-square.html#reporting-the-result-in-english",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.10 Reporting the result in English",
    "text": "8.10 Reporting the result in English\nIn English, we might report this result as:\nWe found counts of 130 yellow plants and 46 green plants, which are consistent at the 5% significance level with the predictions of Mendelian inheritance (chi-squared test, X-squared = 0.12, p=0.73).\n\n8.10.1 Caution\nNote that we do not say we have proved Mendelian inheritance to be correct. We haven’t. We never prove things in science. We haven’t said anything about the truth of the null hypothesis. All we can say is whether our data are or are not consistent with the null hypothesis. In this case they are. We then report the test we used and the values of the test statistic and p-value. Other tests might give you other details to report too.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#exercise-1-is-this-coin-a-fair-coin",
    "href": "chi-square.html#exercise-1-is-this-coin-a-fair-coin",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.11 Exercise 1: Is this coin a fair coin?",
    "text": "8.11 Exercise 1: Is this coin a fair coin?\nSuppose you tossed a fair coin 100 times and got 43 heads and 57 tails.\nUnder a null hypothesis that the coin is fair, what would the expected numbers of heads and tails be?\nNow, you use R to do a chi-square test of that null hypothesis. Here is the code to do that and the output it would give:\n\nchisq.test(c(43,57),p=c(0.5,0.5)) # we could leave out the second argument here\n\n\n    Chi-squared test for given probabilities\n\ndata:  c(43, 57)\nX-squared = 1.96, df = 1, p-value = 0.1615\n\n\n\nWhat do you conclude?\nHow would you report the result?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#solution-1",
    "href": "chi-square.html#solution-1",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.12 Solution 1",
    "text": "8.12 Solution 1\n\nThe expectation is that half the outcomes would be heads and half would be tails.\nThe null hypothesis of this test is that heads and tails are equally likely, ie that the coin is fair.\nUnder this null hypothesis the expected outcome is 50 heads and 50 tails.\nFrom the output of the R code we see that the p-value, the probability of getting an outcome as far or further from that, is 0.161. That is pretty high. Would you do anything if you knew that the probability of a bad (or worse) outcome was 0.161 (ie about 1 in 6)?\nIn particular, this p-value is greater than 0.05, so we cannot reject the null hypothesis that the coin is fair. That is, even with a fair coin it is not unlikely that you would get head/tail numbers as different or more different from 50/50 as 43/57 if you tossed the coin 100 times. That will happen about 1/6 of the time if you repeatedly do trials where you toss the coin 100 times.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#solution-1-continued",
    "href": "chi-square.html#solution-1-continued",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.13 Solution 1 (continued)",
    "text": "8.13 Solution 1 (continued)\n\nTo report this result, you might say something like\n\nFrom 100 coin tosses we got 43 heads and 57 tails. These counts are consistent at the 5% significance level with the coin being fair (chi-squared test, X2 = 1.96, df = 1, p &gt; 0.05).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#exercise-2-is-a-scientists-competence-associated-with-their-astrological-star-sign",
    "href": "chi-square.html#exercise-2-is-a-scientists-competence-associated-with-their-astrological-star-sign",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.14 Exercise 2: Is a scientist’s competence associated with their astrological star sign?",
    "text": "8.14 Exercise 2: Is a scientist’s competence associated with their astrological star sign?\nSuppose someone told you that the competence of scientists was linked to their astrological zodiac sign. I won’t name all of these, but there are twelve of them: Pisces, Scorpio, Cancer etc.\nTo test this hypothesis, you spend a lot of time on Primo and identify 240 scientists, currently active, that have each published at least five papers in high impact journals in the last year. All of these people, you presume, are successful scientists. You write to each of them and ask them their date of birth.\nAmazingly(!) all of them respond. You then assign each of them to a zodiac sign according to their birth date and get the following counts for each sign:\nIn this code chunk we have typed out the counts and collected them as a vector, using the function c(), we have saved this under the name stars.\n\n1stars&lt;-c(22,20,17,22,20,19,18,21,19,22,23,17)\n\n\n1\n\nCreate a stars vector",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#exercise-2-continued",
    "href": "chi-square.html#exercise-2-continued",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.15 Exercise 2 (continued)",
    "text": "8.15 Exercise 2 (continued)\n\nWhat would be a suitable null hypothesis in this investigation?\nWhat proportion of the total count would we expect for each star sign if this null were true?\nDo the data meet the criteria required for use of a chi-square goodness of fit test.\nIf you have access to R, use the chisq.test() function to implement this test.\nOn the basis of the output of the test, do you reject the null hypothesis?\nReport the result of the test in plain English.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#solution-2",
    "href": "chi-square.html#solution-2",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.16 Solution 2",
    "text": "8.16 Solution 2\n\nH0: There is no association between the astrological star sign of a researcher and their success in science (who knew?)\nOne twelfth for each sign. ie a researcher is as likely to have one star sign as any other.\nThese are count data, there are at least five counts for every sign and the counts are independent - any individual researcher only contributes to one of the twelve counts.\nNote that we do not need to include the second p=... argument in this case since the default presumption, that all proportions are equal, is true here.\n\n\nchisq.test(stars)\n\n\n    Chi-squared test for given probabilities\n\ndata:  stars\nX-squared = 2.3, df = 11, p-value = 0.9971",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "chi-square.html#solution-2-continued",
    "href": "chi-square.html#solution-2-continued",
    "title": "8  Chi-squared analysis of count data",
    "section": "8.17 Solution 2 (continued)",
    "text": "8.17 Solution 2 (continued)\n\nchisq.test(stars)\n\n\n    Chi-squared test for given probabilities\n\ndata:  stars\nX-squared = 2.3, df = 11, p-value = 0.9971\n\n\n\nWe see that the p-value is almost one so we emphatically do not reject the null hypothesis.\nWe find no evidence that star sign affects success in science (X-sq=2.29, df = 11, p=0.997)\n\nNote the degrees of freedom that is reported: df = 11. The degrees of freedom is the number of independent pieces of information. Here, given that we know the total number of researchers, only eleven of the individual counts are independent. Once they are known, the twelfth can be calculated.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-squared analysis of count data</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "9  Correlation",
    "section": "",
    "text": "9.1 Correlation\nThis guide to correlation draws heavily on the very helpful chapter in Statistics, by David Freedman, Robert Pisani, Roger Purves and Ani Adhikari, 2nd ed., Norton.\nThe notable statistician Karl Pearson (1857 - 1936) carried out a study to investigate the resemblance between children and their parents. As part of the study, Pearson measured the heights of 1078 parents and of their children at maturity. The heights of the children are plotted against the heights of the parents in the plots below, where we distinguish between father-son and mother-daughter pairs.\nThe taller a father, the taller his sons tend to be. It is the same with mothers and daughters.\nThere is a positive association between a father’s height and the height of his sons.\nBut there is a lot variation - the association is weak.\nIf you know the height of a father, how much does that tell you about the height of his sons?\nConsider fathers who are about about 67 inches tall, and look at the wide variation in the heights of their sons - - all the points between the two vertical dotted lines. The same is true for the daughters of mother who are about 63 inches tall.\nIf there is a strong association between two variables, then knowing one helps a lot in predicting the other. But when there is a weak association, information about one variable does not help much in guessing the other. When there is no association, it does not help at all.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#correlation",
    "href": "correlation.html#correlation",
    "title": "9  Correlation",
    "section": "",
    "text": "9.1.1 The correlation coefficient\nSuppose we are looking at the relationship between two variables and have already plotted the scatter plot. The graph looks like a cloud of points.\nHow can we summarise it numerically?\nThe first thing we can do is to mark a point that shows the average of the x-values and the average of the y-values. This is the point of averages. It marks the centre of the cloud.\nThe next step is to measure the width of the cloud from side to side, in both the x and the y directions. This can be done using the standard deviations (SD) of the x and y values. Remember that if both x and y are normally distributed, then 95% of the data will lie within about 2 (1.96 if we want to be pernickety) standard deviations of the mean, in each direction.\n\n\n\n\n\n\n\n\n\nSo far, our summary statistics are:\n\nmean of the x values, SD of the x values.\nmean of the y values, SD of the y values.\n\nThese statistics tell us where the centre of the cloud is and how far spread out it is both vertically and horizontally, but they do not tell the whole story.\nConsider the following two sets of data plotted below. Both have the same centre and the same spread.\n\n\n\n\n\n\n\n\n\nHowever the points in the first cloud are tightly clustered around a line - there is a strong linear association between the two variables. In the second cloud, the clustering is much looser. The strength of the association is different in the two diagrams. To measure the association, one more summary statistic is needed - the correlation coefficient.\nThis coefficient is usually abbreviated as r, for no good reason.\n\nThe correlation coefficient is a measure of linear association or clustering around a line. The relationship between two variables can be summarized by:\n\nthe average of the x-values, the SD of the x-values.\nthe average of the y-values, the SD of the y-values.\nthe correlation coefficient r\n\n\n\n\n9.1.2 Different values of r.\nLet us see how this looks graphically. In the Figure below we show six scatter plots for hypothetical data. In all six pictures the average is 3 and the standard deviation is 1 for both x and y. The correlation coefficient is printed in each case.\n\n\n\n\n\n\n\n\n\nThe one top left shows a correlation of 0 and the cloud is completely formless. As x increases, y shows no tendency to increase or decrease. It just straggles around.\nThe next diagram has r = 0.4 and a linear pattern is just starting to emerge. The next has r = 0.6 with a stronger linear pattern, and so on. The closer r is to 1 the stronger is the linear association and the more tightly clustered are the points around a line.\nA correlation of 1, which does not appear in the Figure is often referred to as a perfect correlation. It means that all the points lie exactly on a line so there is a perfect linear correlation between the two variables. Correlation coefficients are always between -1 and 1.\nThe correlation between the heights of identical twins is around 0.95. A scatter diagram for the heights of twins would thus look like the bottom right diagram in the Figure. We see that even with a coefficient this big there is a still a fair degree of scatter. The heights of identical twins are far from being equal all the time.\nReal data in the life sciences never shows perfect correlation and rarely does it even show strong correlation. It is more common for it to look like Pearson’s father-son data, with weak associations and r values in the range 0.3 to 0.7. This is even more true for data from the social sciences which concern human behaviour.\nWe can also have negative associations between variables. For example women with more education tend to have fewer children. Animals with higher body weight tend to have lower metabolic rates. As one variable increases, the other decreases. When there is negative association, the correlation coefficient has a negative sign.\nBelow we show six examples of negative correlation. As in the previous figure, all the data sets have a mean of 3 and a standard deviation of 1.\n\n\n\n\n\n\n\n\n\n\nCorrelations are always between -1 and 1, but can take any value in between. A positive correlation means that the cloud slopes up: as one variable increases, so does the other. A negative correlation means that the cloud slopes down. As one variable increases, the other decreases.\n\n\n\n9.1.3 Using R to find the correlation coefficient.\nFirst, let us try to find the correlation between two sets of data where we know what the correlation coefficient is, because we created the data ourselves. We will take the x and y data used above for which the correlation coefficient was fixed to be 0.8\n\ncor.test(x,y,method=\"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = 9.2585, df = 48, p-value = 2.965e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6720816 0.8823564\nsample estimates:\n      cor \n0.8006498 \n\n\nThere are different ways to calculate the correlation coefficient. Which of them is appropriate depends mainly on the type of data, and if they are numeric, whether they are normally distributed. If they are, then we use the Pearson method. If they are not, for example because they are ordinal data, then we use the Spearman’s Rank method and write method=“spearman” instead. In this case we can relax the requirement that there is a linear association between the data sets, but there does still need to be a monotonic relationship.\nIt is important to be able to interpret and report the output.\nFirst, understand that R is carrying out a test, the null hypothesis of which is that there is no correlation between the values of x and the values of y among the populations from which the x and y data were drawn, so that the correlation coefficient between x and y within those populations is zero. It then reports a p-value: how likely it is that you would have got the data you got for this sample of data if that null hypothesis were true. As with most tests, to do this it uses the data to calculate a so-called test-statistic. How it does this need not concern us here. The details will differ from test to test, and the name given will differ. Here it is called t. It also reports the number of independent pieces of information used to calculate that statistic. This is called the degrees of freedom, here denoted df. This usually (but not necessarily) has a value that is 1,2 or 3 less than the number of data points.\nThen it reports the p-value. A tiny (close to zero) value here means that it thinks it very unlikely that the samples would be as they are if the x and y variables were not correlated in the populations from which the samples were drawn. A high (by which we usually mean greater than 0.05) value means that there is a reasonable chance that the actual non-zero correlation coefficient could have been found between x and y in the samples, even though those values were not correlated in the wider populations from which the samples were drawn. In that case we would have found no evidence that the x and y data within the population were correlated. This doesn’t mean that they aren’t, just that we have insufficient evidence to reject the null hypothesis that they are not.\nThe p-value reported here is 4.3e-12. That is R’s way of saying what in standard form would be written 4.3 x 10-12. This is a really tiny value. It is 0.0000000000043, which is a very inconvenient way to write such a small number. Hence R’s way of doing it or the standard form way of doing it. In the context of a statistical test and when p is is that small we don’t care about its exact value, we simply note that it is very, very small. We thus can confidently reject the null hypothesis and assert that the data provide evidence that x and y are correlated, in this case positively.\nFurther, it reports the actual correlation coefficient. Here it finds r = 0.797, which we happen to know to be correct because we created this data set ourselves, and a 95% confidence interval for the coefficient. The precise meaning of the confidence interval is subtle, but it is a kind of error bar for the correlation coefficient r. It means that if we drew sample after sample from the population and calculated the confidence interval for r for each sample, then 95% of the time that interval would capture the true value of r. Thus, you can reasonably think of the confidence interval as being the range of values within which the true population correlation coefficient plausibly lies, given the value that was found for the sample.\nIf the p-value is small enough that we reject the null-hypothesis, then this confidence interval should not encompass zero. Why? Because any value inside the confidence interval is a plausible value for the population correlation coefficient andif we are going to reject the null hypothesis, then zero should not be a plausible value for the population correlation coefficient, given the data.\nIf the p-value is large enough that we do not reject the null hypothesis then this confidence interval will encompass zero. Why? Becuase if the confidence interval encompasses zero, then zero is a plausible value for the correlation coefficient and so we should not reject the null.\nHere, the confidence interval is from 0.67 to 0.88. This does not encompass zero. In fact it is far from zero, so is consistent with our finding a really small p-value. On both groundss, we reject the null.\nTo report the result of this test we would say something like:\n\nWe find evidence for a strong positive correlation between x and y (Pearson r =0.80, t=9.1, df=48, p&lt;0.001)\n\nNote that when the p-value is much less than 0.05 as it is here we do not normally report its exact value, but simply write p&lt;0.01, or p&lt;0.001, and so on. The point is that these ways of reporting it tell the reader that p is way less than 0.05. This is all they need to know to see that we can confidently reject the null hypothesis.\n\n\n9.1.4 Correlations for real data\nLet us look at the Iris data set that is built into R. It contains values for the Sepal Width, Sepal Length, Petal Width and Petal Length for samples of 50 plants from each of three species of Iris, setosa, versicolor and virginica. Here are the first few rows:\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nWe will look to see if the data allow us to reject the idea that sepal width and sepal length are not correlated within the wider populations of each of these species:\nFirst, let’s plot the data\n\niris |&gt;\n  ggplot(aes(x=Sepal.Width,y=Sepal.Length,colour=Species)) +\n  geom_point() +\n  labs(x=\"Sepal Width (mm)\",\n       y=\"Sepal Length (mm)\") +\n  facet_wrap(~Species,nrow=1,scales=\"free\") +\n  theme_classic() +\n  theme(legend.position=\"none\") +\n  theme(strip.background=element_blank())\n\n\n\n\n\n\n\n\n\n9.1.4.1 What we can tell from plotting the data\nHaving seen the plots, do you think it plausible that there is a linear relationship between sepal width and length? Only in this case can you use a Pearson correlation test (see below). If not linear, do you think there is at least a monotonic relationship between petal width and length (ie no peaks or troughs)? That, at least, would enable you to use a Spearman’s Rank correlation test. If neither are true, then you can’t use either test.\n\n\n9.1.4.2 Test for normality\nIt does look as though there is a plausibly linear relationship between sepal width and length, so we might be able to use the Pearson method for calculating correlation coefficient. This is the ‘parametric’ method that is more powerful than the ‘non-parametric’ alternative, the Spearman’s rank method.\nIn principle, however, this method requires that each group of the data be approximately normally distributed around its repective mean (that is what the word parametric is getting at), so we ought to test for this. We can do this either graphically or using a normality test such as the Shapiro wilk test. Let us do the latter here:\n\n\n# A tibble: 3 × 3\n  Species    Sepal.Length_p.val Sepal.Width_p.val\n  &lt;fct&gt;                   &lt;dbl&gt;             &lt;dbl&gt;\n1 setosa                  0.460             0.272\n2 versicolor              0.465             0.338\n3 virginica               0.258             0.181\n\n\nAll the p-values for this test are comfortably greater than 0.05 so we can reasonably presume that our data are drawn from normally distributed populations. This, plus the plausibly linear realtionships we have seen in the graphs means that can go ahead and use the Pearson method to calculate the correlation coefficient between sepal length and width for each species!\n\n\n9.1.4.3 Calculate the correlation coefficients\nLooking at each graph, it appears that there is a positive correlation for each species, but that this is weaker for versicolar and virginica than it is for setosa. Knowing the sepal width for that species gives you a much better idea of the sepal length, and vice-versa, than is true for the other two species.\nLet us find out:\n\niris |&gt;\n  group_by(Species) |&gt;\n  summarise(r=cor.test(Sepal.Width,Sepal.Length)$estimate,\n            lower.bound95=cor.test(Sepal.Width,Sepal.Length)$conf.int[1],\n            upper.bound95=cor.test(Sepal.Width,Sepal.Length)$conf.int[2],\n            \"p value\"=cor.test(Sepal.Width,Sepal.Length)$p.value) |&gt;\n  kbl(digits=3) |&gt;\n  kable_styling(full_width=0.7)\n\n\n\n\nSpecies\nr\nlower.bound95\nupper.bound95\np value\n\n\n\n\nsetosa\n0.743\n0.585\n0.846\n0.000\n\n\nversicolor\n0.526\n0.290\n0.702\n0.000\n\n\nvirginica\n0.457\n0.205\n0.653\n0.001\n\n\n\n\n\n\n\nThe table gives the estimated value for the Pearson correlation coefficient in each case, the lower and upper bound of the 95% confidence interval for that coefficient and the p-value.\nDo these output provide evidence for a correlation between sepal length and sepal width in each case?\n\n\n\n9.1.5 The problem of missing variables\nSuppose in the above analysis we had not distinguished between the three species. If we had plotted the speal length and width data we would have seen this:\n\niris |&gt;\n  ggplot(aes(x=Sepal.Width,y=Sepal.Length)) +\n  geom_point() +\n  labs(x=\"Sepal Width (mm)\",\n       y=\"Sepal Length (mm)\") +\n  theme_classic() +\n  theme(legend.position=\"none\") +\n  theme(strip.background=element_blank())\n\n\n\n\n\n\n\n\nThis looks like a weak negative correlation, as is confirmed by calculating the Spearman’s (in this case) rank correlation coefficient.\nThe message here is that failure to spot importnat ‘missing’ variables, in the case species, can lead to grossly misleading ideas as to whether two variables are correlated, and if so, how.\n\n\n9.1.6 The correlation coefficient measures the degree of linear association.\n\n9.1.6.1 Pearson correlation coefficient\nSometimes the Pearson correlation coefficient r is a poor measure of the degree of association within a data set. Outliers and non-linearity are two problem cases.\nConsider first a data set where there is a very strong association between variables, but where the data sset contains an outlier, and then a data set where there is a strong but non-linear association between variables. Here we mean by ‘strong’ that knowing the value of one variable gives you a very good idea of the value of the other.\n\n\n\n\n\n\n\n\n\nThe outlier in the left-hand figure above brings the correlation coefficient down to 0.08, which is close to zero. The correlation coefficient in the right-hand figure is similarly small at 0.019, despite that there is a strong association between the x and y data. The reason is that the association is non-linear.\n\n\n9.1.6.2 Spearman’s rank correlation coefficient rSp\nThe Spearman’s rank correlation coefficient is a valid measure of the association between two variables providing their relationship is monotonic - that is, continuously rising or flat, or continuously falling or flat. Linear relationships are monotonic, so the Spearman’s rank correlation coefficient is always a useful (if not necessarily the most powerful - Pearson would trump it if it could be used) measure of association for such cases, but it will still be valid when the relationship is monotonic but non-linear, whereas the Pearson correlation coefficient would not be.\n\nSpearman’s rank correlation coefficient rSp can also be be used for ordinal data, whereas Pearson’s r coefficient cannot be. This makes it very useful in much of ecology, animal behaviour and environmental studies where ordinal scales are commonly used.\n\n\n\n9.1.7 When is it appropriate to calculate a correlation coefficient?\nSo, to sum up, we note that the Pearson correlation coefficient is a measure of linear association, not of association in general. At least, this is true if you are calculating the Pearson correlation coefficient. If your data are not suitable for that and you decide to calculate the Spearman’s Rank correlation coefficient, then the condition is relaxed somewhat: there might be but there no longer needs to be a linear relationship between the two variables, but there must be a monotonic one. That means that, as one variable increases, the other should either increase or remain constant, or decrease or remain constant - that is, there should be no peaks or troughs in the data.\n\n\n9.1.8 Association is not causation\nA very important and often-repeated point to note is that correlation measures association. But association is not the same as causation.\nSee Spurious Correlations for some amusing examples.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#examples",
    "href": "correlation.html#examples",
    "title": "9  Correlation",
    "section": "9.2 Examples",
    "text": "9.2 Examples\n\n9.2.1 Cyclones\nCyclones are areas of low atmospheric pressure around which steep gradients in air pressure can cause strong winds to develop, which in turn may create large waves if the cylone is over the oceans. Depending on where they occur, these storms are variously also known as hurricanes and typhoons. Here we consider whether the peak wind speeds are associated with the depth of the low pressure at the eye of the storm, and whether the peak wave sizes are associated with how wide the storm is. If the answer to either of these questions is yes, then an outcome of practical importance - wind speed, wave height - can be predicted at least in part by a variable that can be measured easily - pressure, distance.\n\n9.2.1.1 South West Indian Ocean intense tropical cyclones 1973 - 2024\nPressure gradients cause winds so it is reasoable to ask whether there a correlation between the peak wind speed and minimum pressure at the eye of cyclones. Here we look at data for for intense tropical cyclones in the south west Indian Ocean over the period 1973-2024. The data are taken (scraped using the R package rvest!) from: https://en.wikipedia.org/wiki/List_of_South-West_Indian_Ocean_intense_tropical_cyclones . The original data sources are available on that site.\n\n\n\n\n\n\n\n\n\nIn this figure we see that there is a weak but significant negative correlation between the peak recorded wind speed and the minimum recorded pressure at the centre of intense tropical cyclones that occurred in the south west Indian Ocean between 1973 and 2024. Neither wind speed nor pressure were normally distributed, so the Spearman’s rank correlation coefficient rSp has been calculated, and not the Pearson r coefficient.\n\n\n9.2.1.2 Significant wave height vs size of cyclone\nHere we look at results displayed in Figure 3f of\n\nOh, Y. et al. (2023) ‘Optimal tropical cyclone size parameter for determining storm-induced maximum significant wave height’, Frontiers in Marine Science, 10, p. 1134579. Available at: https://doi.org/10.3389/fmars.2023.1134579.\nThe authors seek to determine whether there is an association between the size of a cylone, measured by the ‘R50’ distance measured outward from the storm centre to where the wind speeds have subsided to 50 kph, and the maximum ‘significant wave height’ of the swell created by the storm. Significant wave height is a widely used measure that is the average height of the heighest 1/3 of the waves, these being the ones that impact most on practical matters like the fuel consumption of ships, the erosion of shores, and so on.\n\n\n\n\n\n\n\n\n\nThe plot shows that there is a monotonically rising relationship between the R50 radius of the cyclones included in the study and the maximum significant wave height of swell created by them. The relationship is not linear however, so the only appropriate measure of correlation coefficient is the Spearman’s rank, which gives \\(r_{sp} = 0.948, p&lt;0.001\\), indicating a very strong positive association between the size of a cyclone and the height of the waves it creates.\n\nWhy do you think this relationship flattens off for larger storm sizes?\n\n\n\n\n9.2.2 Lichen abundance\n\nJovan, S. (2008). Lichen Bioindication of Biodiversity, Air Quality, and Climate: Baseline Results From Monitoring in Washington, Oregon, and California. http://gis.nacse.org/lichenair/doc/Jovan2008.pdf\n\nIn this paper the authors investigate the utility of using lichen as bioindicators of air quality. is there an association between air quality and the abundance of this or that species of lichen?\n\n\n\n\n\n\n\n\n\n\nWe note that the relationship between air quality score and proportion of nitrophyte abundance is plausibly linear.\n\n\nThere is a strong negative correlation (rSp=-0.78, p&lt;0.001) between air quality score and proportion of nitrophyte lichen. This suggests that this proportion can be used as a bioindicator of air quality.\n\n\nA Spearman’s rank correlation coefficient was calculated in this case rather than a Pearson correlation coefficient, despite the fact that both the x and y variables are plausibly drawn from normally distributed populations of values, according to a Shapiro-Wilk test. The problem is that both the air quality score and the proportion of nitrophyte abundance are ordinal variables - something we learn from reading the paper in which this figure appears. This means that analysis using parametric tests such as the Pearson method for determining linear correlation are not appropriate. A non-parametric method such as Spearman’s rank method can be used instead.\n\n\n\n9.2.3 Soil bacteria\n\nLauber, C. L., Hamady, M., Knight, R., & Fierer, N. (2009). Pyrosequencing-based assessment of soil pH as a predictor of soil bacterial community structure at the continental scale. Applied and Environmental Microbiology, 75(15), 5111–5120. https://doi.org/10.1128/AEM.00335-09\n\n\n\n\n\n\n\n\n\n\nIn the Figure above, note that the Pearson r-values for C and E are close to zero, and the p-values are greater than 0.1, meaning that at this significance level there is no evidence from these data that there is any linear association between soil pH and the relative abundances of Alphaproteobacteria or Beta/Gammaproteobacteria. From the plots, it looks in C as if there no assocation at all, whereas in E it looks as though there might be, but if so then not a linear or even monotonic association, for which the correlation coefficient (Pearson r or Spearman rSp) would be a poor measure.\n\n\n9.2.4 Birds\n\nPain, D.J., Mateo, R. and Green, R.E. (2019) ‘Effects of lead from ammunition on birds and other wildlife: A review and update’, Ambio, 48(9), pp. 935–953. https://doi.org/10.1007/s13280-019-01159-0\n\n\n\n\n\n\n\n\n\n\nThis figure is from a study on the impact on bird populations of ingestion of lead from spent lead ammunition arising from hunting using rifles and shot guns. For several species of wetland birds, the population trend (as measured by a proxy scale) is plotted against the prevalence of carcasses found to contain lead shot.\nThere is a clear negative trend here that is plausibly linear, or at least monotonic. Shapiro-Wilk tests show that neither data set is plausibly drawn from a normally distributed population, so a Spearman’s rank correlation coefficient is calculated. The result is \\(r_{\\text{Sp}} = -0.697, p &lt; 0.01\\) so we can say that there is a evidence of a significant and strong negative correlation between lead shot ingestion of waterbird species and their population trends.\n\n\n9.2.5 Heart rate vs life expectancy\n\n\n\n\n\n\n\n\n\nHere we see a strong negative linear correlation between the life expectancy of different species and the log of the mean heart rate in beats per minute. On this plot, humans are almost an outlier. In this case, use of a Shapirro Wilk test showed that both life expectancy and log of the heart rate were found to be consistent with being drawn from normally distributed populations, so the Pearson method was used to calculate the correlation coefficient.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#exercise-1",
    "href": "correlation.html#exercise-1",
    "title": "9  Correlation",
    "section": "9.3 Exercise 1",
    "text": "9.3 Exercise 1\n\n\n\n\n\n\n\n\n\nPlots A to F above show scatter plots of different data sets Y against X.\n\nWhich of them show linear behaviour?\nWhich of them show monotonic behaviour?\nFor which of them might it be appropriate to calculate the following correlation coefficients between X and Y?\n\nPearson r\nSpearman rank rsp",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#exercise-2",
    "href": "correlation.html#exercise-2",
    "title": "9  Correlation",
    "section": "9.4 Exercise 2",
    "text": "9.4 Exercise 2\nMeasurements were made on female Adelie penguins on a series of islands in the Antarctic. The bill lengths and depths of 73 individuals were recorded and are shown in the plot below.\n\n\n\n\n\n\n\n\n\nFrom this information and plot, decide\n\nWhether it is plausible that there is a linear relationship between the bill depths and lengths within the data set\nWhether the correlation coefficient within the data set is likely to be positive or negative\nWhether the correlation is ‘strong’ or ‘weak’ ie is the absolute value of the correlation coefficient likely to be close to 1 or close to zero\nWhether there might be evidence from this data that there is any correlation between bill depth and bill length in the population from which this data set was drawn.\n\n\n9.4.0.1 Tests for normality\nShapiro-Wilk tests are carried out to check for normality of the two sets of data:\n\nshapiro.test(bill_depths)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bill_depths\nW = 0.9831, p-value = 0.4364\n\n\n\nshapiro.test(bill_lengths)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bill_lengths\nW = 0.99117, p-value = 0.8952\n\n\nOn this basis, we see that we can use the Pearson method to calculate the correlation coefficient between bill depth and bill length. What is telling us this?\n\n\n9.4.0.2 Calculate correlation coefficient\nFor these data, we calculate the correlation coefficient using the Pearson method.\n\n# Pearson\ncor.test(bill_depths,bill_lengths, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  bill_depths and bill_lengths\nt = 1.3714, df = 71, p-value = 0.1746\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07209557  0.37677877\nsample estimates:\n      cor \n0.1606361 \n\n\nWhich part(s) of this output tell us that:\n\nThere is a weak positive correlation between bill length and depth within the sample?\nThere is no evidence that this correlation exists in the wider population from which this data set was drawn?\n\nHow would you report this result?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#exercise-3",
    "href": "correlation.html#exercise-3",
    "title": "9  Correlation",
    "section": "9.5 Exercise 3",
    "text": "9.5 Exercise 3\nOpen a new R notebook\nIn the usual way, include to start with code chunks to\n\nLoad the packages needed tidyverse and here.\nRead the data set iris.csv (which should be in your data folder already) into an object called iris\n\nYou can do this with this code chunk:\n```{r}     \nfilepath&lt;-here(\"data\",\"iris.csv\")\niris&lt;-read_csv(filepath)\nglimpse(iris)\n```\n\nCreate a faceted plot of sepal length against sepal width for each species.\nCalculate the Pearson correlation coefficient between sepal length and sepal width for each species, and display this, plus the lower and upper bounds of the confidence interval and the p-value for each species in a table.\n\nFor (4) and (5) you can adapt code used on the main Correlation tab.\nNow:\n\nDoes it appear that the sepal length and sepal width are correlated for each species?\n\nIs the correlation positive or negative?\n\nFor which species is the correlation strongest?\n\nDo the correlation coefficients make sense, given the plots?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "simple linear regression.html",
    "href": "simple linear regression.html",
    "title": "10  Simple linear regression",
    "section": "",
    "text": "10.1 Introduction\nA class of analytical models that you will use often go under the name General Linear Models. They include linear regression, multiple regression, ANOVA, ANCOVA, Pearson correlation and t-tests.\nDespite appearances, these models are all fundamentally linear models. They share a common framework for estimation (least squares) and a common set of criteria that the data must satisfy before they can be used. These criteria centre around the idea of normally distributed residuals. An important stage of any analysis that uses linear models is that these assumptions are checked, as part of the Plot -&gt; Model -&gt; Check Assumptions -&gt; Interpret -&gt; Plot again workflow.\nHere, we will go through an example of simple linear regression - suitable for trend data where we wish to predict a continuously varying response, given a value of a continuous explanatory variable. As we go we show code snippets from an R script that does this job, and, at the bottom, an example complete script that you could adapt to your own needs.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple linear regression.html#simple-linear-regression---plant-growth",
    "href": "simple linear regression.html#simple-linear-regression---plant-growth",
    "title": "10  Simple linear regression",
    "section": "10.2 Simple Linear Regression - plant growth",
    "text": "10.2 Simple Linear Regression - plant growth\nAs a first example, we ask the question: does plant growth rate depend on soil moisture content?\nWe predict that more moisture will probably allow higher growth rates. We note that this means there will be a clear relationship between the variables, one that should be apparent if we plot the response (dependent) variable - plant growth rate - against the explanatory (independent) variable - soil moisture content. We note that both the explanatory variable and the dependent variables are continuous - they do not have categories.\nWhat we want to do in linear regression is be able to predict the value of the dependent variable, knowing the value of the independent variable. In practice, this means drawing a ‘best fit’ straight line through the data and determining the intercept and gradient of this line.\n\n10.2.1 Load packages\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggfortify)\nlibrary(cowplot)\n# un-comment and run the next line if you have not yet installed mbhR.\n# remotes::install_github(\"mbh038/mbhR\")\nlibrary(mbhR)\n\n\n\n10.2.2 Get the data\nWe have a data set to explore our question: The plants data set is available through the mbhR package which you have already installed and loaded\n\ndata(plants)\nglimpse(plants)\n\nRows: 50\nColumns: 2\n$ soil.moisture.content &lt;dbl&gt; 0.4696876, 0.5413106, 1.6979915, 0.8255799, 0.85…\n$ plant.growth.rate     &lt;dbl&gt; 21.31695, 27.03072, 38.98937, 30.19529, 37.06547…\n\n\nWe see that the data set contains two continuous variables, as expected.\n\n\n10.2.3 Plot the data\nWe can use the package ggplot2, which is part of tidyverse to do this:\n\nplants |&gt;\n  ggplot(aes(x=soil.moisture.content, y=plant.growth.rate)) +\n  geom_point() +\n  labs(x=\"Soil moisture content\",\n       y=\"Plant growth rate (mm/week)\") +\n  xlim(0,2) +\n  ylim(0,50) +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nFrom the plot, we note that:\n\nthere is an upward trend that is plausibly linear within this range of soil misture content. The more moisture there is in the soil, the greater the growth rate of the plants appears to be.\nthe variance of the data, that is the range of vertical spread is approximately constant for the whole range of soil moisture content. This is one of the key criteria that data must satisfy if we are to analyse them using a linear model such as simple linear regression.\nwe can estimate the intercept and gradient of a best fit line just by looking at the plot. Roughly speaking, the moisture content varies from 0 to 2, while the growth rate rises from 20 to 50, a rise of about 30. Hence the gradient is about 30/2 = 15 mm/week, while the intercept is somewhere between 15 mm and 20 mm / week.\n\nIt is always good practice to examine the data before you go on to do any statistical analysis. For all but the smallest data sets, that means plotting them.\n\n\n10.2.4 Make a simple model using linear regression\nWe use the function lm() to do this, and we save the results in an object to which we give the name model_pgr. This function needs a formula and some data as its arguments:\n\nmodel_pgr&lt;-lm(plant.growth.rate ~ soil.moisture.content, data = plants)\n\nThis reads: ‘Fit a linear model, where we hypothesize that plant growth rate is a function of soil moisture content, using the variables from the plants data frame.’\n\n\n10.2.5 Check assumptions\nBefore we rush into interpreting the output of the model, we need to check whether it was valid to use a linear model in the first place. Whatever the test within which we are using a linear model, we should do the necessary diagnostic checks at this stage.\nYou can do this using tests designed for the purpose, but I prefer to do it graphically, using a function autoplot() from the package ggfortify. You give this the model we have just created using lm() and it produces four very useful graphs. I suggest that, after once installing ggfortify you include the line library(ggfortify) at the start of every script.\nHere is how you use it:\n\nautoplot(model_pgr, smooth.colour=NA) + theme_cowplot()\n\n\n\n\n\n\n\n\nThe theme_cowplot() part is not necessary, but it gives the plots a nice look, so why not?\nThese plots are all based around the `residuals’, which is the vertical distance between observed values and fitted values ie between each point and the best fit line through the points - the line which the linear model is finding for us, by telling us its intercept and gradient.\nNote that in simple linear regression, the best fit line is the one that minimises that sum of the squared residuals.\nSo what do these plots mean?\n\nTop-left: This tells you about the structure of the model. Was it a good idea to try to fit a straight line to the data? If not, for example because the data did follow a linear trend, then there will be humps or troughs in this plot.\nTop-right: This evaluates the assumption of normality of the residuals. The dots are the residuals and the dashed line is the expectation under the normality assumption. This is a much better way to check normality than making a histogram of the residuals, especially with small samples.\nBottom-left: This examines the assumption of equal variance of the residuals. A linear model assumes that the variance is constant over all predicted values of the response variables. There should be no pattern. Often, however, there is. With count data, for example, the variance typically increases with the mean.\nBottom-right: This detects leverage - which means points that have undue influence on the gradient of the fitted line, and outliers. If you have outliers in your data, you need to decide what to do with them.\n\nIn the case of these data, we are good to go! There is no discernible pattern in either of the left-hand plots, the qq-plot is about as straight as you ever see with real data, and there are no points exerting undue high influence.\n\n\n10.2.6 Interpretation of the model\nNow that we have established that the data meet the criteria required for the model to be valid, we can go ahead and inspect its output. We will do this using two tools that we also use for every other general linear model we implement (t-test, ANOVA etc). These are anova() and summary()\nLet us first use anova():\n\nanova(model_pgr)\n\nAnalysis of Variance Table\n\nResponse: plant.growth.rate\n                      Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nsoil.moisture.content  1 2521.15 2521.15  156.08 &lt; 2.2e-16 ***\nResiduals             48  775.35   16.15                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe F value here is an example of a ‘test statistic’, a number that a test calculates from the data, from which it is possible to further calulate how likely it is that you would have got the data you got if the null hypothesis were true. This particular test statistic is the ratio of the variation in the data that is explained by the explanatory variable to the leftover variance. The bigger it is, the better the job that the explanatory variable is doing at explaining the variation in the dependent variable. The p value, which here is effectively zero, is the chance you would have got an F value this big or bigger from the data in the sample if in fact there were no relationship between plant growth rate and soil moisture content. If the p value is small (and by that we usually mean less than 0.05) then we can reject the null hypothesis that there is no relationship between plant growth rate and soil moisture content.\nHence, in this case, we emphatically reject the null: there is clear evidence that plant growth rate is at least in part explained by soil moisture content.\nNow we use the summary() function:\n\nsummary(model_pgr)\n\n\nCall:\nlm(formula = plant.growth.rate ~ soil.moisture.content, data = plants)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9089 -3.0747  0.2261  2.6567  8.9406 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             19.348      1.283   15.08   &lt;2e-16 ***\nsoil.moisture.content   12.750      1.021   12.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.019 on 48 degrees of freedom\nMultiple R-squared:  0.7648,    Adjusted R-squared:  0.7599 \nF-statistic: 156.1 on 1 and 48 DF,  p-value: &lt; 2.2e-16\n\n\nThis gives us estimates of the intercept (19.348) and gradient (12.750) of the best fit line through the data. The null hypothesis is that both these values are zero, and the p-value is our clue as to whether we can reject this null. Here, in both cases, we clearly can.\nWe also see the Adjusted R-squared value of 0.7599. This is the proportion of the variance in the dependent variable that is explained by the explanatory variable. Thus it can vary between 0 and 1. A large value like this indicates that soil moisture content is a good predictor of plant growth rate.\n\n\n10.2.7 Back to the figure\nTypically, a final step in our analysis involves including the model we have fitted into the original figure, if that is possible in a straightforward way. In the case of simple linear regression, it is. It means adding a straight line with the intercept and gradient displayed by the summary() function. We do this by adding a line geom_smooth(method = \"lm\") to our plot code:\n\nplants |&gt;\n  ggplot(aes(x=soil.moisture.content, y=plant.growth.rate)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x=\"Soil moisture content\",\n       y=\"Plant growth rate (mm/week)\") +\n  xlim(0,2) +\n  ylim(0,50) +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nThis gives both a straight line and the ‘standard error’ of that line - meaning, roughly speaking, the wiggle room within which the ‘true’ line , for the population as opposed to this sample drawn from it, probably lies.\n\n\n10.2.8 Report the result\nWe would likely want to include this plot in our report, along with a statement like:\nWe find evidence for a linear increase in plant growth rate with soil moisture content (p&lt;0.001), with an additional 12.75 mm of growth per unit increase in soil moisture content.\n\n\n10.2.9 Conclusion\nWe have carried out a simple linear regression on continuous data. This is an example of a general linear model. We first plotted the data, then we used lm() to fit the model. Next we inspected the validity of the model using autoplot. We then inspected the model itself using first anova() then summary(). Finally we included the output of the model on the plot, in this case by adding to it a straight line with the intercept and gradient determined by the regression model, and reported the result in plain English.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple linear regression.html#simple-linear-regression---ocean-ph",
    "href": "simple linear regression.html#simple-linear-regression---ocean-ph",
    "title": "10  Simple linear regression",
    "section": "10.3 Simple linear regression - ocean pH",
    "text": "10.3 Simple linear regression - ocean pH\nIn this second example we provide. the script but leave you to interpret the outcome of each step. You can use the first example to help you do this.\nHere we use data from Figure 5.20 of AR6, WG1 from the IPCC. It shows ocean pH measurements from a location (137\\(^{\\circ}\\)E, 5\\(^{\\circ}\\)N) in the western Pacific between 1980 and 2020.\nYour task is to assess whether there is a significant linear trend in pH with time and if so to determiine the change in pH per year or per decade during the forty year period from 1980 to 2020.\n\n10.3.1 Load packages\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggfortify)\nlibrary(cowplot)\n\n\n\n10.3.2 Load data\n\npH_filepath &lt;- here(\"data\",\"ipcc_AR6_WGI_Figure_5_20-pH.csv\")\npH&lt;-read_csv(pH_filepath,skip=6) # we have to skip the first 6 lines becuase of meta-data - check it out!\nglimpse(pH)\n\nRows: 81\nColumns: 2\n$ year &lt;dbl&gt; 1984.121, 1985.171, 1986.066, 1987.060, 1987.488, 1988.058, 1989.…\n$ pH   &lt;dbl&gt; 8.100000, 8.097917, 8.086458, 8.102604, 8.071875, 8.098437, 8.095…\n\n\n\n\n10.3.3 Plot data\n\npH |&gt;\n  ggplot(aes(x = year, y = pH)) +\n  geom_point() +\n  labs(x = \"Year\",\n       y = \"Ocean pH\") +\n  scale_y_continuous(limits=c(8.0,8.12), breaks=seq(8.0,8.2,0.02)) +\n  theme_cowplot()\n\n\n\n\n\n\n\n\nIs there a linear trend?\n\n\n10.3.4 Fit linear model\n\npH_model &lt;- lm(pH ~ year, data= pH)\n\n\n\n10.3.5 Check model validity\n\nautoplot(pH_model) + theme_cowplot()\n\n\n\n\n\n\n\n\nIs it reasonable to apply a linear model to these data? Remember that each of these plots tell you something about whether this is the case.\n\n\n10.3.6 Inspect model\n\n10.3.6.1 ANOVA\n\nanova(pH_model)\n\nAnalysis of Variance Table\n\nResponse: pH\n          Df    Sum Sq   Mean Sq F value    Pr(&gt;F)    \nyear       1 0.0169414 0.0169414  199.87 &lt; 2.2e-16 ***\nResiduals 79 0.0066962 0.0000848                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAre we OK to reject the null hypothesis that pH does not change over this time period?\n\n\n\n10.3.7 Summary\n\nsummary(pH_model)\n\n\nCall:\nlm(formula = pH ~ year, data = pH)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0247092 -0.0049642  0.0002185  0.0060392  0.0168809 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.234405   0.224330   50.08   &lt;2e-16 ***\nyear        -0.001583   0.000112  -14.14   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.009207 on 79 degrees of freedom\nMultiple R-squared:  0.7167,    Adjusted R-squared:  0.7131 \nF-statistic: 199.9 on 1 and 79 DF,  p-value: &lt; 2.2e-16\n\n\nWhat is the change in ocean pH per decade over the last four decades? Is this change statistically significant? Does the linear model account for much of the variance in the data?\n\n\n10.3.8 Replot the data, model included\n\npH |&gt;\n  ggplot(aes(x = year, y = pH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", colour = \"blue\") + # add the line. Method =\"lm\" for a straight line.\n  labs(x = \"Year\",\n       y = \"Ocean pH\") +\n  scale_y_continuous(limits=c(8.0,8.12), breaks=seq(8.0,8.2,0.02)) + # fix limits and break points of y-axis\n  annotate(geom=\"text\", x = 2015, y = 8.11, label = \"137W, 5N\", size = 5) + # measurement site\n  theme_cowplot()\n\n\n\n\n\n\n\n\nHow would you report this result?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple linear regression.html#simple-linear-regression-body-mass-vs-metabolic-rate",
    "href": "simple linear regression.html#simple-linear-regression-body-mass-vs-metabolic-rate",
    "title": "10  Simple linear regression",
    "section": "10.4 Simple linear regression: Body mass vs metabolic rate",
    "text": "10.4 Simple linear regression: Body mass vs metabolic rate\nHere we use data from",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple linear regression.html#sample-script",
    "href": "simple linear regression.html#sample-script",
    "title": "10  Simple linear regression",
    "section": "10.5 Sample script",
    "text": "10.5 Sample script\nA notebook to do linear regression might look like the following, here written as a .Rmd notebook. For this to work you will need to work within a project, with the data in a sub-folder of that called “data”. Here, the data is taken to be a .csv file called mydata.csv, with two columns of data, one called x_values and the other called y_values. You need to change these to suit your own data.\nTo use this, open a new notebook of your own (File/New File/R Notebook), delete everything, paste in all the code below, then adapt the code as needed. Remember to save the notebook in your project scripts folder!\n---\ntitle: \"Sensible title\"\nauthor: \"your name\"\ndate: \"the date\"\noutput: html_notebook\n---\n\n### load packages\n```{r}\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggfortify)\nlibrary(cowplot)\n```\n\n### load data\n```{r load_data}\nfilepath &lt;- here(\"data\", \"mydata.csv\")\nmydata &lt;- read_csv(filepath)\nglimpse(mydata)\n```\n\n### plot the data\n```{r}\nmydata |&gt;\n  ggplot(aes(x=x_values, y=y_values)) +\n  geom_point() +\n  labs(x = \"X variable\",\n       y = \"Y variable\") +\n  theme_cowplot()\n```\n\n### fit the model\n```{r}\nmydata.model &lt;- lm (y_values ~ x_values, data = mydata)\n```\n\n### diagnostics\n```{r}\nautoplot(mydata.model)\n```\n\n### investigate the model\n```{r}\nanova(mydata.model)\n```\n\n```{r}\nsummary(mydata.model)\n```\n\n### replot the data, now with the model included\n```{r}\nmydata |&gt;\n  ggplot(aes(x=x_values, y=y_values)) +\n  geom_point() +\n  geom_smooth(method=\"linear\") +\n  labs(x = \"X variable\",\n       y = \"Y variable\") +\n  theme_cowplot()\n```",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  }
]